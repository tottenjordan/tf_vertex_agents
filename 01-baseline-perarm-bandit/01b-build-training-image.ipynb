{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64583cdb-2502-4f40-a254-8fa68abe7a32",
   "metadata": {},
   "source": [
    "# Build custom container for Vertex training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e1beb37-0c48-4110-8ca0-24ab78c72b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/tf_vertex_agents/01-baseline-perarm-bandit\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66298299-d4ad-460e-8ab3-ea9fbc012086",
   "metadata": {},
   "source": [
    "## Load env config\n",
    "\n",
    "* use the prefix from `00-env-setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d2542f-d87b-436d-b4bb-08edcbb532ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX: rec-bandits-v2\n"
     ]
    }
   ],
   "source": [
    "# PREFIX = 'mabv1'\n",
    "VERSION        = \"v2\"                       # TODO\n",
    "PREFIX         = f'rec-bandits-{VERSION}'   # TODO\n",
    "\n",
    "print(f\"PREFIX: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35449112-3c26-4ea9-8c3a-21c283951350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"rec-bandits-v2\"\n",
      "VERSION                  = \"v2\"\n",
      "\n",
      "BUCKET_NAME              = \"rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "BUCKET_URI               = \"gs://rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_ID      = \"hybrid_vertex.movielens_ds_rec_bandits_v2\"\n",
      "BIGQUERY_TABLE_ID        = \"hybrid_vertex.movielens_ds_rec_bandits_v2.training_dataset\"\n",
      "\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "RL_SUB_DIR               = \"per_arm_rl\"\n",
      "\n",
      "REPOSITORY               = \"rl-movielens-rec-bandits-v2\"\n",
      "IMAGE_NAME               = \"train-perarm-feats-v2\"\n",
      "DOCKERNAME               = \"Dockerfile_perarm_feats\"\n",
      "\n",
      "IMAGE_URI                = \"gcr.io/hybrid-vertex/train-perarm-feats-v2\"\n",
      "\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-rec-bandits-v2/train-perarm-feats-v2\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b16af06-e188-4f4e-a04b-4e0eb8873ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil ls $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be79c652-45c8-4b5f-b410-4908829375a3",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92272ed8-bfb5-4b40-b7f4-cdbe029f3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261a452d-8ce7-427b-8091-d7d41c57e556",
   "metadata": {},
   "source": [
    "# Build Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d192a4dd-b3e1-4c48-b69f-fd022078e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tree src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9bdef1-ac23-49be-96fd-25795f483643",
   "metadata": {},
   "source": [
    "## Container Image Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4076530-a340-4c57-91af-e2decd035b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCKERNAME = \"Dockerfile_train_my_perarm_env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eedbe408-e2fe-4944-912f-f2b1f6587440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCKERNAME        = Dockerfile_train_my_perarm_env\n",
      "REPOSITORY        = rl-movielens-rec-bandits-v2\n",
      "IMAGE_NAME        = train-perarm-feats-v2\n",
      "REMOTE_IMAGE_NAME = us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-rec-bandits-v2/train-perarm-feats-v2\n",
      "IMAGE_URI         = gcr.io/hybrid-vertex/train-perarm-feats-v2\n"
     ]
    }
   ],
   "source": [
    "print(f\"DOCKERNAME        = {DOCKERNAME}\")\n",
    "print(f\"REPOSITORY        = {REPOSITORY}\")\n",
    "print(f\"IMAGE_NAME        = {IMAGE_NAME}\")\n",
    "print(f\"REMOTE_IMAGE_NAME = {REMOTE_IMAGE_NAME}\")\n",
    "print(f\"IMAGE_URI         = {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf953a8e-d282-4d7b-9a7d-fd0bee28c70a",
   "metadata": {},
   "source": [
    "## Create Artifact Repository\n",
    "\n",
    "If you don't have an existing artifact repository, create one using the gcloud command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8288932-054a-46e6-90e8-326e015f2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gcloud artifacts repositories create $REPOSITORY --repository-format=docker --location=$LOCATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1374bc-7b85-4e65-86ab-2721ce4908b3",
   "metadata": {},
   "source": [
    "## Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a8b25d-e55c-4a00-ab0e-1151eae8eaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/tf_vertex_agents'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_path = '..'\n",
    "os.chdir(root_path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bffd9bb-6d22-4d4e-9eb8-893472702837",
   "metadata": {},
   "source": [
    "### Create train image\n",
    "\n",
    "* see [example Dockerfile for GPU](https://github.com/GoogleCloudPlatform/cloudml-samples/blob/main/pytorch/containers/quickstart/mnist/Dockerfile-gpu) jobs in Vertex AI\n",
    "* see deep learning container [example here](https://cloud.google.com/deep-learning-containers/docs/derivative-container), and here for [available DL containers](https://cloud.google.com/deep-learning-containers/docs/choosing-container#versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7022f8d1-43d0-4291-91b8-4cb27491c2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_profiling : False\n"
     ]
    }
   ],
   "source": [
    "gpu_profiling = False # True | False\n",
    "\n",
    "print(f\"gpu_profiling : {gpu_profiling}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4596864b-f059-438d-a04f-7416c436b644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_BASE_IMAGE : python:3.10\n",
      "NVTOP_RUN        : None\n",
      "RUN_EXPORT       : RUN export PYTHONPATH=${PYTHONPATH}:${APP_HOME}/\n"
     ]
    }
   ],
   "source": [
    "if gpu_profiling:\n",
    "    # TRAIN_BASE_IMAGE = 'tensorflow/tensorflow:2.13.0-gpu'\n",
    "    TRAIN_BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-gpu.2-13.py310'\n",
    "    NVTOP_RUN = 'RUN apt update && apt -y install nvtop'\n",
    "    # NVTOP_RUN = 'RUN apt-get update && apt-get -y install nvtop'\n",
    "else:\n",
    "    TRAIN_BASE_IMAGE = 'python:3.10'\n",
    "    NVTOP_RUN = None\n",
    "    \n",
    "RUN_EXPORT = \"RUN export PYTHONPATH=${PYTHONPATH}:${APP_HOME}/\"\n",
    "    \n",
    "print(f\"TRAIN_BASE_IMAGE : {TRAIN_BASE_IMAGE}\")\n",
    "print(f\"NVTOP_RUN        : {NVTOP_RUN}\")\n",
    "print(f\"RUN_EXPORT       : {RUN_EXPORT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b1b1fc6-022b-4e56-b62f-673179bab7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FROM python:3.10\n",
      "\n",
      "ENV PYTHONUNBUFFERED True\n",
      "\n",
      "ENV APP_HOME /workspace\n",
      "\n",
      "WORKDIR $APP_HOME\n",
      "\n",
      "COPY /requirements.txt $APP_HOME/requirements.txt\n",
      "RUN pip install --upgrade pip\n",
      "RUN pip install --no-cache-dir -r $APP_HOME/requirements.txt\n",
      "RUN pip install cloudml-hypertune\n",
      "\n",
      "COPY src/per_arm_rl $APP_HOME/src/per_arm_rl\n",
      "\n",
      "RUN export PYTHONPATH=${PYTHONPATH}:${APP_HOME}/\n",
      "\n",
      "# Sets up the entry point to invoke the task.\n",
      "ENTRYPOINT [\"python3\", \"-m\", \"src.per_arm_rl.task\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dockerfile = f'''\n",
    "FROM {TRAIN_BASE_IMAGE}\n",
    "\n",
    "ENV PYTHONUNBUFFERED True\n",
    "\n",
    "ENV APP_HOME /workspace\n",
    "\n",
    "WORKDIR $APP_HOME\n",
    "\n",
    "COPY /requirements.txt $APP_HOME/requirements.txt\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r $APP_HOME/requirements.txt\n",
    "RUN pip install cloudml-hypertune\n",
    "\n",
    "COPY src/per_arm_rl $APP_HOME/src/per_arm_rl\n",
    "\n",
    "{RUN_EXPORT}\n",
    "\n",
    "# Sets up the entry point to invoke the task.\n",
    "ENTRYPOINT [\"python3\", \"-m\", \"src.per_arm_rl.task\"]\n",
    "'''\n",
    "print(dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cc6cb35-fcb6-4b4d-a665-b6c038ca6f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DOCKERNAME}', 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04bd12e-c61b-4a5c-b2cb-b7843642ce0e",
   "metadata": {},
   "source": [
    "## Build image with Cloud Build\n",
    "\n",
    "Building images with Cloud Build is best practices\n",
    "* images are centrally stored and better managed for robust CI/CD\n",
    "* building images on local workbench instance can alter notebook image config (base image for notebooks vs train images are different)\n",
    "* if building locally, consider using virutal environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb3ef36-2cfe-4199-963b-6c65bd0ec621",
   "metadata": {},
   "source": [
    "### Files that will be included in Cloud Build image\n",
    "* to adjust this see the `gcloudignore` section at the end of `00-env-setup.ipynb` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f9a8891-faa3-4659-a36e-521bf9f186e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile_perarm_feats\n",
      "requirements.txt\n",
      "Dockerfile_train_my_perarm_env\n",
      "Dockerfile_perarm_feats_tpu\n",
      "cloudbuild.yaml\n",
      "src/ranking/ranking_trainer.py\n",
      "src/ranking/stationary_stochastic_repeated_feature_py_environment.py\n",
      "src/ranking/repeated_feature_perarm_network.py\n",
      "src/ranking/jw_perarm_mv_env.py\n",
      "src/ranking/jw_perarm_mv_env_tf.py\n",
      "src/ranking/tmp_cascading_bandit_perarm_env.py\n",
      "src/per_arm_rl/utils_config.py\n",
      "src/per_arm_rl/perarm_task.py\n",
      "src/per_arm_rl/__init__.py\n",
      "src/per_arm_rl/my_per_arm_py_env.py\n",
      "src/per_arm_rl/policy_util.py\n",
      "src/per_arm_rl/train_utils.py\n",
      "src/per_arm_rl/trainer_baseline.py\n",
      "src/per_arm_rl/data_utils.py\n",
      "src/per_arm_rl/data_config.py\n",
      "src/perarm_features/train_perarm.py\n",
      "src/perarm_features/reward_factory.py\n",
      "src/perarm_features/emb_features.py\n",
      "src/perarm_features/agent_factory.py\n",
      "src/perarm_features/__init__.py\n",
      "src/perarm_features/trainer_common.py\n",
      "src/perarm_features/task.py\n",
      "src/perarm_features/eval_perarm.py\n",
      "src/perarm_features/ranking_bandit_policy.py\n"
     ]
    }
   ],
   "source": [
    "# check eligble files\n",
    "!gcloud meta list-files-for-upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d830847-8c15-4d85-b344-f79c877aaee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/$_DOCKERNAME']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e3b8e87-3e4c-410c-a554-3be3b8ed3e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCKERNAME        : Dockerfile_train_my_perarm_env\n",
      "IMAGE_URI         : gcr.io/hybrid-vertex/train-perarm-feats-v2\n",
      "FILE_LOCATION     : .\n",
      "MACHINE_TYPE      : e2-highcpu-32\n"
     ]
    }
   ],
   "source": [
    "# image definitions for training\n",
    "# IMAGE_URI               = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "MACHINE_TYPE            ='e2-highcpu-32'\n",
    "FILE_LOCATION           = \".\" # './src'\n",
    "\n",
    "print(f\"DOCKERNAME        : {DOCKERNAME}\")\n",
    "print(f\"IMAGE_URI         : {IMAGE_URI}\")\n",
    "print(f\"FILE_LOCATION     : {FILE_LOCATION}\")\n",
    "print(f\"MACHINE_TYPE      : {MACHINE_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d11ac85-16f7-4050-8632-cb6e4337a65a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 29 file(s) totalling 269.0 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://hybrid-vertex_cloudbuild/source/1697639014.913847-2ee8d30d87614b799f61a94a79ed5577.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/global/builds/247122fb-d163-4f35-b7da-544c86a10ef4].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/247122fb-d163-4f35-b7da-544c86a10ef4?project=934903580331 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"247122fb-d163-4f35-b7da-544c86a10ef4\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1697639014.913847-2ee8d30d87614b799f61a94a79ed5577.tgz#1697639015293470\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1697639014.913847-2ee8d30d87614b799f61a94a79ed5577.tgz#1697639015293470...\n",
      "/ [1 files][ 60.1 KiB/ 60.1 KiB]                                                \n",
      "Operation completed over 1 objects/60.1 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  309.8kB\n",
      "Step 1/11 : FROM python:3.10\n",
      "3.10: Pulling from library/python\n",
      "0a9573503463: Already exists\n",
      "1ccc26d841b4: Already exists\n",
      "800d84653581: Already exists\n",
      "7c632e57ea62: Already exists\n",
      "f9a1922eee8a: Already exists\n",
      "b0fef9d6962c: Pulling fs layer\n",
      "a8d4dcc5b913: Pulling fs layer\n",
      "ffdc35a04c11: Pulling fs layer\n",
      "ffdc35a04c11: Verifying Checksum\n",
      "ffdc35a04c11: Download complete\n",
      "a8d4dcc5b913: Verifying Checksum\n",
      "a8d4dcc5b913: Download complete\n",
      "b0fef9d6962c: Verifying Checksum\n",
      "b0fef9d6962c: Download complete\n",
      "b0fef9d6962c: Pull complete\n",
      "a8d4dcc5b913: Pull complete\n",
      "ffdc35a04c11: Pull complete\n",
      "Digest: sha256:a20ae344b668753817b8bffb8628e068278d8ddbde1657f7b836fd74559beacd\n",
      "Status: Downloaded newer image for python:3.10\n",
      " ---> be529ebc3048\n",
      "Step 2/11 : ENV PYTHONUNBUFFERED True\n",
      " ---> Running in f6a254db3c8d\n",
      "Removing intermediate container f6a254db3c8d\n",
      " ---> 57578c31916b\n",
      "Step 3/11 : ENV APP_HOME /workspace\n",
      " ---> Running in 4b0a2a8fce9c\n",
      "Removing intermediate container 4b0a2a8fce9c\n",
      " ---> c4dcfa222222\n",
      "Step 4/11 : WORKDIR $APP_HOME\n",
      " ---> Running in de213da5c5df\n",
      "Removing intermediate container de213da5c5df\n",
      " ---> d0f33c8f92fa\n",
      "Step 5/11 : COPY /requirements.txt $APP_HOME/requirements.txt\n",
      " ---> ff8d4e05c0e7\n",
      "Step 6/11 : RUN pip install --upgrade pip\n",
      " ---> Running in 6ad543d0854a\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (23.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-23.3-py3-none-any.whl (2.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 27.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0.1\n",
      "    Uninstalling pip-23.0.1:\n",
      "      Successfully uninstalled pip-23.0.1\n",
      "Successfully installed pip-23.3\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 6ad543d0854a\n",
      " ---> daa04f759a93\n",
      "Step 7/11 : RUN pip install --no-cache-dir -r $APP_HOME/requirements.txt\n",
      " ---> Running in f98e7c378b1a\n",
      "Collecting google-cloud-aiplatform==1.33.1 (from google-cloud-aiplatform[cloud_profiler]==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading google_cloud_aiplatform-1.33.1-py2.py3-none-any.whl.metadata (27 kB)\n",
      "Collecting google-cloud-storage (from -r /workspace/requirements.txt (line 2))\n",
      "  Downloading google_cloud_storage-2.12.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting numpy (from -r /workspace/requirements.txt (line 3))\n",
      "  Downloading numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 6.2 MB/s eta 0:00:00\n",
      "Collecting tensorflow==2.13.0 (from -r /workspace/requirements.txt (line 4))\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting tf-agents==0.17.0 (from -r /workspace/requirements.txt (line 5))\n",
      "  Downloading tf_agents-0.17.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tensorflow-datasets==4.9.0 (from -r /workspace/requirements.txt (line 6))\n",
      "  Downloading tensorflow_datasets-4.9.0-py3-none-any.whl (5.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 92.8 MB/s eta 0:00:00\n",
      "Collecting tensorboard (from -r /workspace/requirements.txt (line 7))\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorboard-plugin-profile (from -r /workspace/requirements.txt (line 8))\n",
      "  Downloading tensorboard_plugin_profile-2.14.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting tensorboard-plugin-wit (from -r /workspace/requirements.txt (line 9))\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 309.8 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server (from -r /workspace/requirements.txt (line 10))\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting tensorflow-io (from -r /workspace/requirements.txt (line 11))\n",
      "  Downloading tensorflow_io-0.34.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
      "Collecting urllib3 (from -r /workspace/requirements.txt (line 12))\n",
      "  Downloading urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting pillow (from -r /workspace/requirements.txt (line 13))\n",
      "  Downloading Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.33.1->google-cloud-aiplatform[cloud_profiler]==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading google_api_core-2.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0 (from google-cloud-aiplatform==1.33.1->google-cloud-aiplatform[cloud_profiler]==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading proto_plus-1.22.3-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 (from google-cloud-aiplatform==1.33.1->google-cloud-aiplatform[cloud_profiler]==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Collecting packaging>=14.3 (from google-cloud-aiplatform==1.33.1->google-cloud-aiplatform[cloud_profiler]==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting google-cloud-bigquery<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform==1.33.1->google-cloud-aiplatform[cloud_profiler]==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading google_cloud_bigquery-3.12.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform==1.33.1->google-cloud-aiplatform[cloud_profiler]==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading google_cloud_resource_manager-1.10.4-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting shapely<2.0.0 (from google-cloud-aiplatform==1.33.1->google-cloud-aiplatform[cloud_profiler]==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 291.1 MB/s eta 0:00:00\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 226.5 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting numpy (from -r /workspace/requirements.txt (line 3))\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 234.3 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 251.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4)) (65.5.1)\n",
      "Collecting six>=1.12.0 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting tensorboard (from -r /workspace/requirements.txt (line 7))\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 279.7 MB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.4/78.4 kB 259.1 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
      "Collecting cloudpickle>=1.3 (from tf-agents==0.17.0->-r /workspace/requirements.txt (line 5))\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gin-config>=0.4.0 (from tf-agents==0.17.0->-r /workspace/requirements.txt (line 5))\n",
      "  Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.3/61.3 kB 250.7 MB/s eta 0:00:00\n",
      "Collecting gym<=0.23.0,>=0.17.0 (from tf-agents==0.17.0->-r /workspace/requirements.txt (line 5))\n",
      "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 624.4/624.4 kB 322.4 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pygame==2.1.3 (from tf-agents==0.17.0->-r /workspace/requirements.txt (line 5))\n",
      "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.7/13.7 MB 121.7 MB/s eta 0:00:00\n",
      "Collecting tensorflow-probability~=0.20.1 (from tf-agents==0.17.0->-r /workspace/requirements.txt (line 5))\n",
      "  Downloading tensorflow_probability-0.20.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting array-record (from tensorflow-datasets==4.9.0->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading array_record-0.4.1-py310-none-any.whl.metadata (503 bytes)\n",
      "Collecting click (from tensorflow-datasets==4.9.0->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting dm-tree (from tensorflow-datasets==4.9.0->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.8/152.8 kB 266.9 MB/s eta 0:00:00\n",
      "Collecting etils>=0.9.0 (from etils[enp,epath]>=0.9.0->tensorflow-datasets==4.9.0->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading etils-1.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting promise (from tensorflow-datasets==4.9.0->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting psutil (from tensorflow-datasets==4.9.0->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading psutil-5.9.6-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting requests>=2.19.0 (from tensorflow-datasets==4.9.0->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets==4.9.0->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting toml (from tensorflow-datasets==4.9.0->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tqdm (from tensorflow-datasets==4.9.0->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 221.7 MB/s eta 0:00:00\n",
      "\u001b[91mWARNING: google-cloud-aiplatform 1.33.1 does not provide the extra 'cloud-profiler'\n",
      "\u001b[0mCollecting werkzeug<2.1.0dev,>=2.0.0 (from google-cloud-aiplatform[cloud_profiler]==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 289.2/289.2 kB 288.0 MB/s eta 0:00:00\n",
      "Collecting google-auth<3.0dev,>=2.23.3 (from google-cloud-storage->-r /workspace/requirements.txt (line 2))\n",
      "  Downloading google_auth-2.23.3-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0 (from google-cloud-storage->-r /workspace/requirements.txt (line 2))\n",
      "  Downloading google_cloud_core-2.3.3-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting google-resumable-media>=2.6.0 (from google-cloud-storage->-r /workspace/requirements.txt (line 2))\n",
      "  Downloading google_resumable_media-2.6.0-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage->-r /workspace/requirements.txt (line 2))\n",
      "  Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard->-r /workspace/requirements.txt (line 7))\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->-r /workspace/requirements.txt (line 7))\n",
      "  Downloading Markdown-3.5-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/site-packages (from tensorboard->-r /workspace/requirements.txt (line 7)) (0.41.2)\n",
      "Collecting gviz-api>=1.9.0 (from tensorboard-plugin-profile->-r /workspace/requirements.txt (line 8))\n",
      "  Downloading gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting fsspec (from etils[enp,epath]>=0.9.0->tensorflow-datasets==4.9.0->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading fsspec-2023.9.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting importlib_resources (from etils[enp,epath]>=0.9.0->tensorflow-datasets==4.9.0->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading importlib_resources-6.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting zipp (from etils[enp,epath]>=0.9.0->tensorflow-datasets==4.9.0->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.33.1->google-cloud-aiplatform[cloud_profiler]==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.33.1->google-cloud-aiplatform[cloud_profiler]==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading grpcio_status-1.59.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0dev,>=2.23.3->google-cloud-storage->-r /workspace/requirements.txt (line 2))\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0dev,>=2.23.3->google-cloud-storage->-r /workspace/requirements.txt (line 2))\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 15.3 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0dev,>=2.23.3->google-cloud-storage->-r /workspace/requirements.txt (line 2))\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r /workspace/requirements.txt (line 7))\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting python-dateutil<3.0dev,>=2.7.2 (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.33.1->google-cloud-aiplatform[cloud_profiler]==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 292.6 MB/s eta 0:00:00\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.33.1->google-cloud-aiplatform[cloud_profiler]==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gym-notices>=0.0.4 (from gym<=0.23.0,>=0.17.0->tf-agents==0.17.0->-r /workspace/requirements.txt (line 5))\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->tensorflow-datasets==4.9.0->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading charset_normalizer-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->tensorflow-datasets==4.9.0->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.5/61.5 kB 224.0 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->tensorflow-datasets==4.9.0->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting decorator (from tensorflow-probability~=0.20.1->tf-agents==0.17.0->-r /workspace/requirements.txt (line 5))\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.13.0->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 283.6 MB/s eta 0:00:00\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 (from google-cloud-aiplatform==1.33.1->google-cloud-aiplatform[cloud_profiler]==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 325.7 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.33.1->google-cloud-aiplatform[cloud_profiler]==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading grpcio_status-1.58.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.57.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Downloading grpcio_status-1.56.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.56.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.55.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.54.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.54.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.54.0-py3-none-any.whl (5.1 kB)\n",
      "  Downloading grpcio_status-1.53.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.53.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.53.0-py3-none-any.whl (5.1 kB)\n",
      "  Downloading grpcio_status-1.51.3-py3-none-any.whl (5.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
      "  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage->-r /workspace/requirements.txt (line 2))\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.9/83.9 kB 225.8 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r /workspace/requirements.txt (line 7))\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 278.5 MB/s eta 0:00:00\n",
      "Downloading google_cloud_aiplatform-1.33.1-py2.py3-none-any.whl (2.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 292.1 MB/s eta 0:00:00\n",
      "Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 524.1/524.1 MB 213.7 MB/s eta 0:00:00\n",
      "Downloading tf_agents-0.17.0-py3-none-any.whl (1.4 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 284.7 MB/s eta 0:00:00\n",
      "Downloading google_cloud_storage-2.12.0-py2.py3-none-any.whl (120 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.5/120.5 kB 265.0 MB/s eta 0:00:00\n",
      "Downloading tensorboard_plugin_profile-2.14.0-py3-none-any.whl (5.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 222.7 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 250.3 MB/s eta 0:00:00\n",
      "Downloading tensorflow_io-0.34.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (28.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 28.8/28.8 MB 192.8 MB/s eta 0:00:00\n",
      "Downloading tensorflow_io_gcs_filesystem-0.34.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 275.8 MB/s eta 0:00:00\n",
      "Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 kB 265.4 MB/s eta 0:00:00\n",
      "Downloading Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 281.7 MB/s eta 0:00:00\n",
      "Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading etils-1.5.1-py3-none-any.whl (140 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.5/140.5 kB 273.1 MB/s eta 0:00:00\n",
      "Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading google_api_core-2.12.0-py3-none-any.whl (121 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.4/121.4 kB 267.4 MB/s eta 0:00:00\n",
      "Downloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182.3/182.3 kB 177.6 MB/s eta 0:00:00\n",
      "Downloading google_cloud_bigquery-3.12.0-py2.py3-none-any.whl (220 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 220.4/220.4 kB 281.7 MB/s eta 0:00:00\n",
      "Downloading google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_cloud_resource_manager-1.10.4-py2.py3-none-any.whl (320 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 321.0/321.0 kB 300.1 MB/s eta 0:00:00\n",
      "Downloading google_resumable_media-2.6.0-py2.py3-none-any.whl (80 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.3/80.3 kB 264.2 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 271.6 MB/s eta 0:00:00\n",
      "Downloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 262.1 MB/s eta 0:00:00\n",
      "Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 302.0 MB/s eta 0:00:00\n",
      "Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.9/22.9 MB 196.3 MB/s eta 0:00:00\n",
      "Downloading Markdown-3.5-py3-none-any.whl (101 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.7/101.7 kB 118.1 MB/s eta 0:00:00\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 216.1 MB/s eta 0:00:00\n",
      "Downloading proto_plus-1.22.3-py3-none-any.whl (48 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.1/48.1 kB 196.3 MB/s eta 0:00:00\n",
      "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.6/62.6 kB 204.7 MB/s eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 440.8/440.8 kB 306.5 MB/s eta 0:00:00\n",
      "Downloading tensorflow_probability-0.20.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.9/6.9 MB 252.3 MB/s eta 0:00:00\n",
      "Downloading array_record-0.4.1-py310-none-any.whl (3.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 265.6 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 244.4 MB/s eta 0:00:00\n",
      "Downloading psutil-5.9.6-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 283.6/283.6 kB 300.4 MB/s eta 0:00:00\n",
      "Downloading tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 252.3 MB/s eta 0:00:00\n",
      "Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.3/158.3 kB 291.2 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.7/138.7 kB 284.4 MB/s eta 0:00:00\n",
      "Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 230.9/230.9 kB 290.6 MB/s eta 0:00:00\n",
      "Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.4/173.4 kB 279.5 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.1.0-py3-none-any.whl (33 kB)\n",
      "Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Building wheels for collected packages: gym, promise\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697630 sha256=1dbe7e7756419dfcca659ee116a611d88fff780b201ce2e3594de661611c1965\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fzisasrn/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21484 sha256=62df90408533841c841e2842680bf512abef08250324c1451415d6b30c298094\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fzisasrn/wheels/54/4e/28/3ed0e1c8a752867445bab994d2340724928aa3ab059c57c8db\n",
      "Successfully built gym promise\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, gym-notices, gin-config, flatbuffers, dm-tree, zipp, wrapt, werkzeug, urllib3, typing-extensions, tqdm, toml, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, shapely, pygame, pyasn1, psutil, protobuf, pillow, packaging, oauthlib, numpy, markdown, keras, importlib_resources, idna, grpcio, google-crc32c, gast, fsspec, etils, decorator, cloudpickle, click, charset-normalizer, certifi, cachetools, absl-py, tensorflow-probability, tensorflow-io, rsa, requests, python-dateutil, pyasn1-modules, proto-plus, promise, opt-einsum, h5py, gym, gviz-api, googleapis-common-protos, google-resumable-media, google-pasta, astunparse, tf-agents, tensorflow-metadata, tensorboard-plugin-profile, requests-oauthlib, grpcio-status, google-auth, grpc-google-iam-v1, google-auth-oauthlib, google-api-core, array-record, tensorflow-datasets, tensorboard, google-cloud-core, tensorflow, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "Successfully installed absl-py-1.4.0 array-record-0.4.1 astunparse-1.6.3 cachetools-5.3.1 certifi-2023.7.22 charset-normalizer-3.3.0 click-8.1.7 cloudpickle-3.0.0 decorator-5.1.1 dm-tree-0.1.8 etils-1.5.1 flatbuffers-23.5.26 fsspec-2023.9.2 gast-0.4.0 gin-config-0.5.0 google-api-core-2.12.0 google-auth-2.23.3 google-auth-oauthlib-1.0.0 google-cloud-aiplatform-1.33.1 google-cloud-bigquery-3.12.0 google-cloud-core-2.3.3 google-cloud-resource-manager-1.10.4 google-cloud-storage-2.12.0 google-crc32c-1.5.0 google-pasta-0.2.0 google-resumable-media-2.6.0 googleapis-common-protos-1.61.0 grpc-google-iam-v1-0.12.6 grpcio-1.59.0 grpcio-status-1.48.2 gviz-api-1.10.0 gym-0.23.0 gym-notices-0.0.8 h5py-3.10.0 idna-3.4 importlib_resources-6.1.0 keras-2.13.1 libclang-16.0.6 markdown-3.5 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.2 pillow-10.1.0 promise-2.3 proto-plus-1.22.3 protobuf-3.20.3 psutil-5.9.6 pyasn1-0.5.0 pyasn1-modules-0.3.0 pygame-2.1.3 python-dateutil-2.8.2 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 shapely-1.8.5.post1 six-1.16.0 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorboard-plugin-profile-2.14.0 tensorboard-plugin-wit-1.8.1 tensorflow-2.13.0 tensorflow-datasets-4.9.0 tensorflow-estimator-2.13.0 tensorflow-io-0.34.0 tensorflow-io-gcs-filesystem-0.34.0 tensorflow-metadata-1.14.0 tensorflow-probability-0.20.1 termcolor-2.3.0 tf-agents-0.17.0 toml-0.10.2 tqdm-4.66.1 typing-extensions-4.5.0 urllib3-2.0.7 werkzeug-2.0.3 wrapt-1.15.0 zipp-3.17.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container f98e7c378b1a\n",
      " ---> 796576c4c77e\n",
      "Step 8/11 : RUN pip install cloudml-hypertune\n",
      " ---> Running in 7c77078cc99f\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: cloudml-hypertune\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3973 sha256=88f1dd9a98c6a2607c7111009474e4cb1a327715d16d44b07d597501268414a1\n",
      "  Stored in directory: /root/.cache/pip/wheels/c6/2d/bb/9c72de7c488cd8e60172c4920c09e404c490020162205b64ba\n",
      "Successfully built cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: There was an error checking the latest version of pip.\n",
      "\u001b[0mRemoving intermediate container 7c77078cc99f\n",
      " ---> b64edfa92d01\n",
      "Step 9/11 : COPY src/per_arm_rl $APP_HOME/src/per_arm_rl\n",
      " ---> c25cf0abad15\n",
      "Step 10/11 : RUN export PYTHONPATH=${PYTHONPATH}:${APP_HOME}/\n",
      " ---> Running in 7c1a6bc5ed13\n",
      "Removing intermediate container 7c1a6bc5ed13\n",
      " ---> 2ae9122fd876\n",
      "Step 11/11 : ENTRYPOINT [\"python3\", \"-m\", \"src.per_arm_rl.task\"]\n",
      " ---> Running in 096643af12fc\n",
      "Removing intermediate container 096643af12fc\n",
      " ---> 7acc20e86f3e\n",
      "Successfully built 7acc20e86f3e\n",
      "Successfully tagged gcr.io/hybrid-vertex/train-perarm-feats-v2:latest\n",
      "PUSH\n",
      "Pushing gcr.io/hybrid-vertex/train-perarm-feats-v2\n",
      "The push refers to repository [gcr.io/hybrid-vertex/train-perarm-feats-v2]\n",
      "382fe9b4a2d7: Preparing\n",
      "9f218f02a46b: Preparing\n",
      "0acec0193eb7: Preparing\n",
      "4038a59dc37d: Preparing\n",
      "ba197aea15f2: Preparing\n",
      "febd55c90937: Preparing\n",
      "cc310f7f9db4: Preparing\n",
      "610ed9401654: Preparing\n",
      "9241e0c9bb41: Preparing\n",
      "d3de4ba9f72c: Preparing\n",
      "0c2d6fc19d6a: Preparing\n",
      "2ef3351afa6d: Preparing\n",
      "5cc3a4df1251: Preparing\n",
      "2fa37f2ee66e: Preparing\n",
      "febd55c90937: Waiting\n",
      "cc310f7f9db4: Waiting\n",
      "610ed9401654: Waiting\n",
      "9241e0c9bb41: Waiting\n",
      "d3de4ba9f72c: Waiting\n",
      "0c2d6fc19d6a: Waiting\n",
      "2ef3351afa6d: Waiting\n",
      "5cc3a4df1251: Waiting\n",
      "2fa37f2ee66e: Waiting\n",
      "ba197aea15f2: Pushed\n",
      "9f218f02a46b: Pushed\n",
      "382fe9b4a2d7: Pushed\n",
      "610ed9401654: Layer already exists\n",
      "cc310f7f9db4: Layer already exists\n",
      "febd55c90937: Pushed\n",
      "d3de4ba9f72c: Layer already exists\n",
      "9241e0c9bb41: Layer already exists\n",
      "0c2d6fc19d6a: Layer already exists\n",
      "2ef3351afa6d: Layer already exists\n",
      "4038a59dc37d: Pushed\n",
      "5cc3a4df1251: Layer already exists\n",
      "2fa37f2ee66e: Layer already exists\n",
      "0acec0193eb7: Pushed\n",
      "latest: digest: sha256:c3a27e1f1b74f8442b696291d3ed2776506977a9e11f617fcb7472b6d7cdf46b size: 3264\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                                STATUS\n",
      "247122fb-d163-4f35-b7da-544c86a10ef4  2023-10-18T14:23:35+00:00  3M10S     gs://hybrid-vertex_cloudbuild/source/1697639014.913847-2ee8d30d87614b799f61a94a79ed5577.tgz  gcr.io/hybrid-vertex/train-perarm-feats-v2 (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --config ./cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE \\\n",
    "    --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629eb246-f2fc-48a0-8478-4f9c99dddd0b",
   "metadata": {},
   "source": [
    "## (Optional) Build Image Locally\n",
    "\n",
    "Building images with Cloud Build is best practices\n",
    "* images are centrally stored and better managed for robust CI/CD\n",
    "* building images on local workbench instance can alter notebook image config (base image for notebooks vs train images are different)\n",
    "* if building locally, consider using virutal environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5278eb39-1dbb-48fd-b872-9cd55643a1ed",
   "metadata": {},
   "source": [
    "Provide a name for your dockerfile and make sure you are authenticated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53874306-777f-459f-9250-4a59dbc283a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gcloud auth configure-docker $REGION-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25faef9c-cb5e-425d-9ee6-7ae8003a95af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy these commands into terminal:\n",
      "\n",
      "virtualenv vertex_env\n",
      "source vertex_env/bin/activate\n"
     ]
    }
   ],
   "source": [
    "print(\"copy these commands into terminal:\\n\")\n",
    "print(f\"virtualenv vertex_env\")\n",
    "print(f\"source vertex_env/bin/activate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "027de7fe-dd2a-4be5-80c4-877f7abe4753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy these commands into terminal:\n",
      "\n",
      "export REMOTE_IMAGE_NAME=us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-rec-bandits-v2/train-perarm-feats-v2\n",
      "export DOCKERNAME=Dockerfile_perarm_feats\n",
      "docker build -t $REMOTE_IMAGE_NAME -f ./$DOCKERNAME .\n"
     ]
    }
   ],
   "source": [
    "# # set variables if running in terminal\n",
    "print(\"copy these commands into terminal:\\n\")\n",
    "print(f\"export REMOTE_IMAGE_NAME={REMOTE_IMAGE_NAME}\")\n",
    "print(f\"export DOCKERNAME={DOCKERNAME}\")\n",
    "print(f\"docker build -t $REMOTE_IMAGE_NAME -f ./$DOCKERNAME .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a82cbe6-0ed5-478f-a38f-e6dedc114e2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !docker build -t $REMOTE_IMAGE_NAME -f $DOCKERNAME ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6110439-f66a-4d6b-8105-17a08ce9b8da",
   "metadata": {},
   "source": [
    "### Push container to Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54a714cf-053c-4f14-b375-2a42ae2092f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy this command into terminal:\n",
      "\n",
      "docker push $REMOTE_IMAGE_NAME\n"
     ]
    }
   ],
   "source": [
    "# ### push the container to registry\n",
    "\n",
    "print(\"copy this command into terminal:\\n\")\n",
    "print(f\"docker push $REMOTE_IMAGE_NAME\")\n",
    "\n",
    "# !docker push $REMOTE_IMAGE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bc85f6-a86e-48f2-a040-89f35376d57c",
   "metadata": {},
   "source": [
    "### GPU profiling\n",
    "\n",
    "> enter these commands in the Vertex interactive terminal:\n",
    "\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt -y install nvtop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18df8169-e4ed-45fb-9279-85e966bc6f3e",
   "metadata": {},
   "source": [
    "**Finished**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
