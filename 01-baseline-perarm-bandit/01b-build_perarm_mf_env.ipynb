{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e888c2fa-8bba-4d0c-8ae5-5a83224215f3",
   "metadata": {},
   "source": [
    "# Build per-arm Bandit model with TF-Agents and execute locally with Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf77d870-0f7d-4736-a5ed-6b14ad825e52",
   "metadata": {},
   "source": [
    "## background\n",
    "\n",
    "**Arm features**\n",
    "* In some bandits use cases, each arm has its own features. For example, in movie recommendation problems, the user features play the role of the context and the movies play the role of the arms (aka actions) \n",
    "* Each movie has its own features, such as `text description`, `metadata`, `trailer content` features and so on\n",
    "\n",
    "These problems are often referred to as `arm features problems`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a02fe987-d962-44b5-ae30-8669d80f3464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiplatform SDK version: 1.33.1\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa85e9e-47cb-41ec-b3d0-e4cc9e494613",
   "metadata": {},
   "source": [
    "## Load env config\n",
    "\n",
    "* use the prefix from `00-env-setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5c2458-ac47-4542-a005-222e2529fb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX: rec-bandits-v2\n"
     ]
    }
   ],
   "source": [
    "# PREFIX = 'mabv1'\n",
    "VERSION        = \"v2\"                       # TODO\n",
    "PREFIX         = f'rec-bandits-{VERSION}'   # TODO\n",
    "\n",
    "print(f\"PREFIX: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a605f33-a1f3-4e98-be57-a5cdae4bc8f8",
   "metadata": {},
   "source": [
    "**run the next cell to populate env vars**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26c0d6d1-8264-46d2-a1a9-98ef8539e33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"rec-bandits-v2\"\n",
      "VERSION                  = \"v2\"\n",
      "\n",
      "BUCKET_NAME              = \"rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "BUCKET_URI               = \"gs://rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "DATA_PATH_KFP_DEMO       = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data/kfp_demo_data/u.data\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_NAME    = \"mvlens_rec_bandits_v2\"\n",
      "BIGQUERY_TABLE_NAME      = \"training_dataset\"\n",
      "\n",
      "REPOSITORY               = \"rl-movielens-rec-bandits-v2\"\n",
      "\n",
      "DOCKERNAME_01            = \"Dockerfile_train_my_perarm_env\"\n",
      "IMAGE_NAME_01            = \"train-my-perarm-env-v2\"\n",
      "IMAGE_URI_01             = \"gcr.io/hybrid-vertex/train-my-perarm-env-v2\"\n",
      "\n",
      "DOCKERNAME_02            = \"Dockerfile_perarm_feats\"\n",
      "IMAGE_NAME_02            = \"train-perarm-feats-v2\"\n",
      "IMAGE_URI_02             = \"gcr.io/hybrid-vertex/train-perarm-feats-v2\"\n",
      "\n",
      "DOCKERNAME_03            = \"Dockerfile_ranking_bandit\"\n",
      "IMAGE_NAME_03            = \"train-rank-bandit-v2\"\n",
      "IMAGE_URI_03             = \"gcr.io/hybrid-vertex/train-rank-bandit-v2\"\n",
      "\n",
      "DOCKERNAME_04            = \"Dockerfile_train_mab_e2e\"\n",
      "IMAGE_NAME_04            = \"train-mab-e2e-v2\"\n",
      "IMAGE_URI_04             = \"gcr.io/hybrid-vertex/train-mab-e2e-v2\"\n",
      "\n",
      "DOCKERNAME_04_pred       = \"Dockerfile_pred_mab_e2e\"\n",
      "IMAGE_NAME_04_pred       = \"pred-mab-e2e-v2\"\n",
      "IMAGE_URI_04_pred        = \"gcr.io/hybrid-vertex/pred-mab-e2e-v2\"\n",
      "\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-rec-bandits-v2/local_docker_tfa\"\n",
      "\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "RL_SUB_DIR               = \"per_arm_rl\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf267dfa-3e59-4ea1-a0b6-e1c7f1ce8675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil ls $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e03648-23ef-4ebb-b4f4-c28dd2a3c7b0",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edf25e4c-afd4-4be2-9784-ee43124f9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1000b866-e997-43bf-9891-6ce80aa69019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents import TFAgent\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "from tf_agents.bandits.environments import (environment_utilities,\n",
    "                                            movielens_py_environment,\n",
    "                                            movielens_per_arm_py_environment)\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import TFEnvironment, tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.metrics.tf_metric import TFStepMetric\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "# GPU\n",
    "from numba import cuda \n",
    "import gc\n",
    "\n",
    "# tf exceptions and vars\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7abfc0-1196-4d7a-9cc9-5ff50c5d927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# my project\n",
    "from src.per_arm_rl import train_utils\n",
    "from src.per_arm_rl import data_utils\n",
    "from src.per_arm_rl import data_config\n",
    "from src.per_arm_rl import trainer_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10411736-7b03-418a-883e-0157cc183d04",
   "metadata": {},
   "source": [
    "### detect GPUs & reset devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3cc5e8e-4b02-4df4-8eea-7c40466cd241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a75044a2-d511-4727-84d2-780648fc191a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e69b4-7fe1-40fd-b42c-e3a24f8f4598",
   "metadata": {},
   "source": [
    "### Initialize Google Cloud SDK Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef21e778-2043-450e-a78b-4c75416bf7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99900b37-e159-43dd-b63b-e7c0e20a4ab1",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29776aca-6af4-4002-934d-fef353b60b33",
   "metadata": {},
   "source": [
    "### copy sample data to $DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041283b5-de0f-4fbc-a40f-81f28e38ed0c",
   "metadata": {},
   "source": [
    "### Load data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bad14ac8-0789-4ad5-bb52-e22e23db3860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER_AGE_LOOKUP: {1.0: 0, 35.0: 1, 45.0: 2, 18.0: 3, 50.0: 4, 56.0: 5, 25.0: 6}\n",
      "USER_AGE_DIM: 7\n",
      "USER_OCC_LOOKUP: {b'salesman': 0, b'programmer': 1, b'writer': 2, b'librarian': 3, b'marketing': 4, b'homemaker': 5, b'scientist': 6, b'entertainment': 7, b'engineer': 8, b'executive': 9, b'student': 10, b'technician': 11, b'none': 12, b'artist': 13, b'doctor': 14, b'lawyer': 15, b'retired': 16, b'administrator': 17, b'other': 18, b'educator': 19, b'healthcare': 20}\n",
      "USER_OCC_DIM: 21\n",
      "MOVIE_GEN_LOOKUP: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18}\n",
      "MOVIE_GEN_DIM: 19\n",
      "MOVIELENS_NUM_MOVIES: 1682\n",
      "MOVIELENS_NUM_USERS: 943\n"
     ]
    }
   ],
   "source": [
    "# test variables are set\n",
    "print(f\"USER_AGE_LOOKUP: {data_config.USER_AGE_LOOKUP}\")\n",
    "print(f\"USER_AGE_DIM: {data_config.USER_AGE_DIM}\")\n",
    "\n",
    "print(f\"USER_OCC_LOOKUP: {data_config.USER_OCC_LOOKUP}\")\n",
    "print(f\"USER_OCC_DIM: {data_config.USER_OCC_DIM}\")\n",
    "\n",
    "print(f\"MOVIE_GEN_LOOKUP: {data_config.MOVIE_GEN_LOOKUP}\")\n",
    "print(f\"MOVIE_GEN_DIM: {data_config.MOVIE_GEN_DIM}\")\n",
    "\n",
    "print(f\"MOVIELENS_NUM_MOVIES: {data_config.MOVIELENS_NUM_MOVIES}\")\n",
    "print(f\"MOVIELENS_NUM_USERS: {data_config.MOVIELENS_NUM_USERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "048786b6-e466-4cb6-84e2-50c6184405e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(data_config.USER_OCC_LOOKUP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201610e6-b3be-4e09-a290-ea3c24230c70",
   "metadata": {},
   "source": [
    "### Read TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1265b4ca-e231-475e-a63e-515b673cf39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40cb3c57-c48b-499d-8a2e-fc1006818b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://rec-bandits-v2-hybrid-vertex-bucket/data/ml-ratings-100k-full.tfrecord']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT = \"train\" # \"train\" | \"val\"\n",
    "\n",
    "train_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{DATA_GCS_PREFIX}/', delimiter=\"/\"): # {SPLIT}\n",
    "    if '.tfrecord' in blob.name:\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8580ef60-9448-44be-8b81-19e7ebae7a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MapDataset element_spec={'bucketized_user_age': TensorSpec(shape=(), dtype=tf.float32, name=None), 'movie_genres': TensorSpec(shape=(1,), dtype=tf.int64, name=None), 'movie_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'timestamp': TensorSpec(shape=(), dtype=tf.int64, name=None), 'user_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'user_occupation_text': TensorSpec(shape=(), dtype=tf.string, name=None), 'user_rating': TensorSpec(shape=(), dtype=tf.float32, name=None)}>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "\n",
    "train_dataset = train_dataset.map(data_utils.parse_tfrecord)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15e942d0-e779-4de9-9e32-6dc65fd27fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([45.], dtype=float32)>,\n",
      " 'movie_genres': <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[7]])>,\n",
      " 'movie_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'357'], dtype=object)>,\n",
      " 'timestamp': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([879024327])>,\n",
      " 'user_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'138'], dtype=object)>,\n",
      " 'user_occupation_text': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'doctor'], dtype=object)>,\n",
      " 'user_rating': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([4.], dtype=float32)>}\n",
      "{'bucketized_user_age': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([25.], dtype=float32)>,\n",
      " 'movie_genres': <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[4]])>,\n",
      " 'movie_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'709'], dtype=object)>,\n",
      " 'timestamp': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([875654590])>,\n",
      " 'user_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'92'], dtype=object)>,\n",
      " 'user_occupation_text': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'entertainment'], dtype=object)>,\n",
      " 'user_rating': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "for x in train_dataset.batch(1).take(2):\n",
    "    pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695f980f-8ae9-415c-8978-08e0e9112c1e",
   "metadata": {},
   "source": [
    "# Define RL modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e084e425-594d-40d6-ac04-e63c68e6dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bcaf16-ef88-4dce-a8e2-72f20d244dff",
   "metadata": {},
   "source": [
    "# Train RL modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd649a-1055-4376-8c1c-1543bfd849f1",
   "metadata": {},
   "source": [
    "## set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30fd48c5-b137-4823-80d0-7ac462684fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE       : 128\n",
      "TRAINING_LOOPS   : 110\n",
      "STEPS_PER_LOOP   : 2\n",
      "RANK_K           : 20\n",
      "NUM_ACTIONS      : 10\n",
      "PER_ARM          : True\n",
      "TIKHONOV_WEIGHT  : 0.001\n",
      "AGENT_ALPHA      : 10.0\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters.\n",
    "BATCH_SIZE       = 128      # Training and prediction batch size.\n",
    "TRAINING_LOOPS   = 110     # Number of training iterations.\n",
    "STEPS_PER_LOOP   = 2      # Number of driver steps per training iteration.\n",
    "\n",
    "# Set MovieLens simulation environment parameters.\n",
    "RANK_K           = 20     # Rank for matrix factorization in the MovieLens environment; also the observation dimension.\n",
    "NUM_ACTIONS      = 10     # Number of actions (movie items) to choose from.\n",
    "PER_ARM          = True   # Use the non-per-arm version of the MovieLens environment.\n",
    "\n",
    "# Set agent parameters.\n",
    "TIKHONOV_WEIGHT  = 0.001   # LinUCB Tikhonov regularization weight.\n",
    "AGENT_ALPHA      = 10.0    # LinUCB exploration parameter that multiplies the confidence intervals.\n",
    "\n",
    "print(f\"BATCH_SIZE       : {BATCH_SIZE}\")\n",
    "print(f\"TRAINING_LOOPS   : {TRAINING_LOOPS}\")\n",
    "print(f\"STEPS_PER_LOOP   : {STEPS_PER_LOOP}\")\n",
    "print(f\"RANK_K           : {RANK_K}\")\n",
    "print(f\"NUM_ACTIONS      : {NUM_ACTIONS}\")\n",
    "print(f\"PER_ARM          : {PER_ARM}\")\n",
    "print(f\"TIKHONOV_WEIGHT  : {TIKHONOV_WEIGHT}\")\n",
    "print(f\"AGENT_ALPHA      : {AGENT_ALPHA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c024c122-7701-4391-97c1-48660589b098",
   "metadata": {},
   "source": [
    "## Define RL environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fed298e-0361-4b28-9ee9-e82af4c6d4ea",
   "metadata": {},
   "source": [
    "One can define a bandit environment by subclassing `BanditTFEnvironment`, or, similarly to RL environments, one can define a `BanditPyEnvironment` and wrap it with `TFPyEnvironment`\n",
    "\n",
    "> See `src.per_arm_rl.my_per_arm_py_env.py` for an example custom environment which implements a per-arm version of the MovieLens environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e648e97f-a9ff-46d3-a7f3-09b04723b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.per_arm_rl import my_per_arm_py_env as my_per_arm_py_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6374c0c6-44d3-4972-90ca-577626e8f13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "943"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_config.MOVIELENS_NUM_USERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00eddb55-19f3-4bd8-89cf-3a1a75bcba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = my_per_arm_py_env.MyMovieLensPerArmPyEnvironment(\n",
    "    project_number = PROJECT_NUM\n",
    "    , data_path = DATA_PATH\n",
    "    , bucket_name = BUCKET_NAME\n",
    "    , data_gcs_prefix = f\"{DATA_GCS_PREFIX}\"\n",
    "    , user_age_lookup_dict = data_config.USER_AGE_LOOKUP\n",
    "    , user_occ_lookup_dict = data_config.USER_OCC_LOOKUP\n",
    "    , movie_gen_lookup_dict = data_config.MOVIE_GEN_LOOKUP\n",
    "    , num_users = data_config.MOVIELENS_NUM_USERS\n",
    "    , num_movies = data_config.MOVIELENS_NUM_MOVIES\n",
    "    , rank_k = RANK_K\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , num_actions = NUM_ACTIONS\n",
    ")\n",
    "\n",
    "environment = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b3af019-dea7-43b9-bcf1-ade3c723fb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 2,\n",
       " 3: 3,\n",
       " 4: 4,\n",
       " 5: 5,\n",
       " 6: 6,\n",
       " 7: 7,\n",
       " 8: 8,\n",
       " 9: 9,\n",
       " 10: 10,\n",
       " 12: 11,\n",
       " 13: 12,\n",
       " 14: 13,\n",
       " 15: 14,\n",
       " 16: 15,\n",
       " 17: 16,\n",
       " 18: 17,\n",
       " 19: 18}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.movie_gen_lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34665952-999c-48bf-8f04-a863d0a3e1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MapDataset element_spec={'bucketized_user_age': TensorSpec(shape=(), dtype=tf.float32, name=None), 'movie_genres': TensorSpec(shape=(1,), dtype=tf.int64, name=None), 'movie_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'timestamp': TensorSpec(shape=(), dtype=tf.int64, name=None), 'user_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'user_occupation_text': TensorSpec(shape=(), dtype=tf.string, name=None), 'user_rating': TensorSpec(shape=(), dtype=tf.float32, name=None)}>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2af0b8a6-a490-4701-a4fa-1549e0ff6151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.0001,  7.0001, 10.0001, ..., 10.0001,  2.0001, 10.0001])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._user_occ_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b78a117-6c3a-4fd6-90c5-3ba60c684268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(22,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(10, 21), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f290dca3-8db5-4d17-a289-ef7599b0774e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': {'global': TensorSpec(shape=(22,), dtype=tf.float32, name=None),\n",
       "                 'per_arm': TensorSpec(shape=(10, 21), dtype=tf.float32, name=None)},\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.time_step_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d31c452-5e93-40aa-8a63-16950bbc9071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(9, dtype=int32))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.action_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2105b5f-1d7d-448e-a222-b117288444e5",
   "metadata": {},
   "source": [
    "## Define RL agent/algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ddb82b68-49fe-48d5-a3a2-27a2fc9c88bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep Spec (for each batch):\n",
      " TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': DictWrapper({'global': TensorSpec(shape=(22,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(10, 21), dtype=tf.float32, name=None)}),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}) \n",
      "\n",
      "Action Spec (for each batch)  :\n",
      " BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(9, dtype=int32)) \n",
      "\n",
      "Reward Spec (for each batch)  :\n",
      " TensorSpec(shape=(), dtype=tf.float32, name='reward') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = lin_ucb_agent.LinearUCBAgent(\n",
    "    time_step_spec = environment.time_step_spec()\n",
    "    , action_spec = environment.action_spec()\n",
    "    , tikhonov_weight = TIKHONOV_WEIGHT\n",
    "    , alpha = AGENT_ALPHA\n",
    "    , dtype = tf.float32\n",
    "    , accepts_per_arm_features = PER_ARM\n",
    "    , summarize_grads_and_vars = True\n",
    "    , enable_summaries = True\n",
    ")\n",
    "\n",
    "print(\"TimeStep Spec (for each batch):\\n\", agent.time_step_spec, \"\\n\")\n",
    "print(\"Action Spec (for each batch)  :\\n\", agent.action_spec, \"\\n\")\n",
    "print(\"Reward Spec (for each batch)  :\\n\", environment.reward_spec(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0312eafa-bff0-443f-bf73-2e814b542767",
   "metadata": {},
   "source": [
    "#### The flow of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c04b975-bcd1-47fb-b14d-66b2360b74df",
   "metadata": {},
   "source": [
    "First, let us have a look at the data specification in the agent. The `training_data_spec` attribute of the agent specifies what elements and structure the training data should have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f3eb808-2cd8-4617-8592-1aaaf88d3ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data spec:  Trajectory(\n",
      "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(9, dtype=int32)),\n",
      " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
      " 'observation': DictWrapper({'global': TensorSpec(shape=(22,), dtype=tf.float32, name=None)}),\n",
      " 'policy_info': PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=(), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=(), chosen_arm_features=TensorSpec(shape=(21,), dtype=tf.float32, name=None)),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "print('training data spec: ', agent.training_data_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760d851e-6b53-4542-abf6-153132da27b6",
   "metadata": {},
   "source": [
    "If we have a closer look to the `observation` part of the spec, we see that it does not contain per-arm features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3dd0294b-df84-4a28-90be-6fc319e1a77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation spec in training:  {'global': TensorSpec(shape=(22,), dtype=tf.float32, name=None)}\n"
     ]
    }
   ],
   "source": [
    "print('observation spec in training: ', agent.training_data_spec.observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab4548-4618-4e15-953c-dbb16faab6a7",
   "metadata": {},
   "source": [
    "What happened to the per-arm features? To answer this question, first we note that when the LinUCB agent trains, it does not need the per-arm features of all arms, it only needs those of the **chosen arm**. Hence, it makes sense to drop the tensor of shape `[BATCH_SIZE, NUM_ACTIONS, PER_ARM_DIM]`, as it is very wasteful, especially if the number of actions is large.\n",
    "\n",
    "But still, the per-arm features of the chosen arm must be somewhere! To this end, we make sure that the LinUCB policy stores the features of the chosen arm within the `policy_info` field of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55976caa-2105-4cd9-b872-e82cb3c51a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen arm features:  TensorSpec(shape=(21,), dtype=tf.float32, name=None)\n"
     ]
    }
   ],
   "source": [
    "print('chosen arm features: ', agent.training_data_spec.policy_info.chosen_arm_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df5785f-66f8-45f8-aa03-58cf3a117d64",
   "metadata": {},
   "source": [
    "## Define RL metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc7f42-13cd-4e7a-9220-934166fcd9ef",
   "metadata": {},
   "source": [
    "Bandits' most important metric is **regret**, calculated as the difference between the reward collected by the agent and the expected reward of an oracle policy that has access to the reward functions of the environment. The [RegretMetric](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/metrics/tf_metrics.py) thus needs a `baseline_reward_fn` function that calculates the best achievable expected reward given an observation. For our example, we need to take the maximum of the no-noise equivalents of the reward functions that we already defined for the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8680c971-1b81-439f-877d-1893f4a8a158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.bandits.metrics.tf_metrics.RegretMetric at 0x7f3bdf5feaa0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_reward_fn = functools.partial(\n",
    "    train_utils.compute_optimal_reward_with_my_environment\n",
    "    , environment=environment\n",
    ")\n",
    "\n",
    "regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n",
    "regret_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03fe5b14-d1af-4816-9506-4f3994d0c8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.bandits.metrics.tf_metrics.SuboptimalArmsMetric at 0x7f3a4482f4f0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_action_fn = functools.partial(\n",
    "    train_utils.compute_optimal_action_with_my_environment,\n",
    "    environment=environment,\n",
    ")\n",
    "    \n",
    "suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n",
    "  optimal_action_fn\n",
    ")\n",
    "suboptimal_arms_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b60cec8c-6dd5-4816-a3b1-ec61d0b53187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf_agents.bandits.metrics.tf_metrics.RegretMetric at 0x7f3bdf5feaa0>,\n",
       " <tf_agents.bandits.metrics.tf_metrics.SuboptimalArmsMetric at 0x7f3a4482f4f0>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [regret_metric, suboptimal_arms_metric]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5195e0ff-6a36-4cb8-b170-fb0c8910660a",
   "metadata": {},
   "source": [
    "## Train (locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765c72aa-73fc-4900-9a5b-c9e831cfd922",
   "metadata": {},
   "source": [
    "A **policy** in a bandit problem works the same way as in an RL problem: it provides an action (or a distribution of actions), given an observation as input.\n",
    "* For more details, see the [TF-Agents Policy tutorial](https://github.com/tensorflow/agents/blob/master/docs/tutorials/3_policies_tutorial.ipynb).\n",
    "* As with environments, there are two ways to construct a policy: One can create a `PyPolicy` and wrap it with `TFPyPolicy`, or directly create a `TFPolicy`\n",
    "\n",
    "**Replay buffers**\n",
    "* Reinforcement learning algorithms use `replay buffers` to store trajectories of experience when executing a policy in an environment. * During training, replay buffers are queried for a subset of the trajectories (either a sequential subset or a sample) to \"replay\" the agent's experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db72a780-63b6-4eb7-a274-3d9e357fdf85",
   "metadata": {},
   "source": [
    "### Define the training logic (on-policy training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9523045-4f43-4a67-83a6-275aa3118e31",
   "metadata": {},
   "source": [
    "> The following function is the same as [trainer.train](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/agents/examples/v2/trainer.py#L130), but it keeps track of intermediate metric values and saves different artifacts to different locations. You can also directly invoke [trainer.train](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/agents/examples/v2/trainer.py#L130) which also trains the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e887e408-45db-4c95-aa84-268b0f573252",
   "metadata": {},
   "source": [
    "### set Vertex Exeperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bea2c0a5-96f2-44fb-8e28-3f5b68698d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : baseline-perarm-local-v1\n",
      "RUN_NAME          : run-20231107-150316\n",
      "LOG_DIR           : gs://rec-bandits-v2-hybrid-vertex-bucket/baseline-perarm-local-v1/run-20231107-150316/tb-logs\n",
      "ROOT_DIR          : gs://rec-bandits-v2-hybrid-vertex-bucket/baseline-perarm-local-v1/run-20231107-150316/root\n",
      "ARTIFACTS_DIR     : gs://rec-bandits-v2-hybrid-vertex-bucket/baseline-perarm-local-v1/run-20231107-150316/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'baseline-perarm-local-v1'\n",
    "\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "LOG_DIR           = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/tb-logs\"\n",
    "ROOT_DIR          = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    experiment=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bbf357-011e-48fe-a714-1d9f8012329d",
   "metadata": {},
   "source": [
    "### train RL agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf7498c-4257-48ee-ab96-8903d13bcd95",
   "metadata": {},
   "source": [
    "> TODO: out-of-bounds index error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bd567c85-810d-4bcf-b920-29fad009349f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0: train loss = 0.5099999904632568\n",
      "step = 10: train loss = 1.0199999809265137\n",
      "step = 20: train loss = 0.9800000190734863\n",
      "step = 30: train loss = 1.0099999904632568\n",
      "step = 40: train loss = 0.949999988079071\n",
      "step = 50: train loss = 0.8399999737739563\n",
      "step = 60: train loss = 0.7699999809265137\n",
      "step = 70: train loss = 0.8100000023841858\n",
      "step = 80: train loss = 0.8799999952316284\n",
      "step = 90: train loss = 1.1399999856948853\n",
      "step = 100: train loss = 1.0199999809265137\n",
      "train runtime_mins: 6\n"
     ]
    }
   ],
   "source": [
    "#start the timer and training\n",
    "start_time = time.time()\n",
    "\n",
    "metric_results = trainer_baseline.train(\n",
    "    # root_dir=LOG_DIR,\n",
    "    artifact_dir=ARTIFACTS_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    "    agent=agent,\n",
    "    environment=environment,\n",
    "    training_loops=TRAINING_LOOPS,\n",
    "    steps_per_loop=STEPS_PER_LOOP,\n",
    "    additional_metrics=metrics,\n",
    "    save_policy=True\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "runtime_mins = int((end_time - start_time) / 60)\n",
    "print(f\"train runtime_mins: {runtime_mins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ceaca0-c58d-4c37-90ea-f4a00e034aee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3401ad6-ac68-49a6-8d16-3b3edf667eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "22644f6b-1996-4c16-a81c-431b5cd68977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ccc5df8f18c0acbe\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ccc5df8f18c0acbe\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=$LOG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac6fd2-87c4-4c35-9ad5-96ff18235297",
   "metadata": {},
   "source": [
    "### load trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "588302a5-f514-44db-807a-af741e78b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil ls $ARTIFACTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6e580390-784b-4d3c-8a19-35fa3c4a1a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_policy = tf.saved_model.load(ARTIFACTS_DIR)\n",
    "# trained_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58e1c3-2005-41df-a1ad-a4fc285a4bda",
   "metadata": {},
   "source": [
    "# Debugging notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c9ab8477-c608-49f1-bcd5-cdc17b927d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment._u_hat\n",
    "\n",
    "sampled_user_ages = np.ones(BATCH_SIZE)\n",
    "sampled_user_occ = np.ones(BATCH_SIZE)\n",
    "\n",
    "SAMPLED_USER_INDICES = np.random.randint(data_config.MOVIELENS_NUM_USERS, size=BATCH_SIZE)\n",
    "SAMPLED_USER_INDICES\n",
    "\n",
    "combined_user_features = np.concatenate(\n",
    "    (\n",
    "        environment._u_hat[SAMPLED_USER_INDICES]\n",
    "        , sampled_user_ages.reshape(-1,1)\n",
    "        , sampled_user_occ.reshape(-1,1)\n",
    "    )\n",
    "    , axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb70266c-a9e5-46af-b131-0c47569ade49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([161, 765, 664, 855, 878,  14,  56, 321, 887, 754, 884,  69, 473,\n",
       "        56, 136, 241, 758, 513, 319,  12, 132, 382, 684, 857, 675, 527,\n",
       "       569,  46, 129, 424, 179, 392, 210, 313, 759, 244, 458, 467, 820,\n",
       "       185, 169, 287, 718, 889, 426, 241, 549,  99, 704, 110, 287, 438,\n",
       "       837, 336, 586, 290,  84, 475, 887,  21, 586,  97, 258,  22, 343,\n",
       "       916, 562,  34, 741, 266, 557, 514, 362,  53, 730, 532,  99, 852,\n",
       "       571, 940, 392, 762, 443, 278, 666, 202,  59, 481, 198, 262, 892,\n",
       "       690, 790, 746, 761, 749, 308,  37,  80, 370, 781, 136, 217, 646,\n",
       "       137, 827, 183, 366, 433, 874, 560, 839, 293, 689, 461, 113, 297,\n",
       "       476, 660, 438, 562, 344, 126, 760, 481, 857, 414, 728])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLED_USER_INDICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "00ca5868-d8e8-473e-9645-b43f377213f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01291269, -0.02463202, -0.00912144, ..., -0.02481789,\n",
       "         0.00289364,  0.01819377],\n",
       "       [-0.04131135,  0.06962406, -0.00452757, ..., -0.02675546,\n",
       "        -0.02063719, -0.02562911],\n",
       "       [-0.03796224, -0.03537715, -0.00614115, ...,  0.0305048 ,\n",
       "        -0.03006611, -0.01280255],\n",
       "       ...,\n",
       "       [-0.00444794, -0.01411317,  0.01979582, ...,  0.02230501,\n",
       "         0.01115231, -0.00133995],\n",
       "       [-0.00854008,  0.00115751,  0.00909269, ...,  0.02653135,\n",
       "         0.00677221, -0.00530296],\n",
       "       [-0.00273581, -0.0172236 ,  0.01085442, ...,  0.00161616,\n",
       "         0.01074529,  0.0073932 ]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_u_hats_pred = environment._u_hat[SAMPLED_USER_INDICES]\n",
    "sampled_u_hats_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "79805063-2ecf-4176-a183-27438b1847ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0658043 ,  0.00597506, -0.00613256, ...,  0.01743993,\n",
       "        -0.03573092,  0.03894605],\n",
       "       [-0.01402104, -0.04662602,  0.05257856, ...,  0.01171281,\n",
       "         0.01670101, -0.01658576],\n",
       "       [-0.00565798, -0.02561845,  0.02336183, ..., -0.01843041,\n",
       "        -0.02539826, -0.0020144 ],\n",
       "       ...,\n",
       "       [-0.00744452, -0.02502129,  0.00616532, ..., -0.03812664,\n",
       "         0.03884653,  0.01103627],\n",
       "       [-0.02403119,  0.00809611,  0.02288736, ..., -0.0200555 ,\n",
       "         0.00571077, -0.00570774],\n",
       "       [-0.04224209, -0.01092715, -0.05854604, ...,  0.02310407,\n",
       "        -0.03948996, -0.0340745 ]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment._u_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "23b9dffd-bd52-45ad-bc73-1acb0ce11ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # user_ids_pred = np.array([1,2,3,4,5,6,7])\n",
    "# user_age_pred = np.array([1,1,1,1,1,1,1,1])\n",
    "# user_occ_pred = np.array([1,1,1,1,1,1,1,1])\n",
    "\n",
    "# combined_user_features = np.concatenate(\n",
    "#     (\n",
    "#         # user_ids_pred\n",
    "#         sampled_u_hats_pred\n",
    "#         , user_age_pred.reshape(-1,1)\n",
    "#         , user_occ_pred.reshape(-1,1)\n",
    "#     )\n",
    "#     , axis=1\n",
    "# )\n",
    "\n",
    "# combined_user_features.shape"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
