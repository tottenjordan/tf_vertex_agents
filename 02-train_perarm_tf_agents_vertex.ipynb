{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01cffb8-8045-4314-b56a-8ac9154c6066",
   "metadata": {},
   "source": [
    "# Scale per-arm Banidt training with Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c6cb3-7970-4fab-a102-8c8749ecd3fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29580d03-390d-4d90-a5c0-14c16338ddbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Notebook Objectives:\n",
    "* Create hyperparameter tuning and training custom container\n",
    "* Submit hyperparameter tuning job (optional)\n",
    "* Create custom prediction container\n",
    "* Submit custom container training job\n",
    "* Deploy trained model to Endpoint\n",
    "* Predict on the Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64261791-880e-41c7-b6e9-a39698c66d51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### TODO - fix vars -Create hyperparameter tuning and training custom container\n",
    "\n",
    "Create a custom container that can be used for both hyperparameter tuning and training. The associated source code is in `src/training/`. This serves as the inner script of the custom container.\n",
    "As before, the training function is the same as [trainer.train](https://github.com/tensorflow/agents/blob/r0.8.0/tf_agents/bandits/agents/examples/v2/trainer.py#L104), but it keeps track of intermediate metric values, supports hyperparameter tuning, and (for training) saves artifacts to different locations. The training logic for hyperparameter tuning and training is the same.\n",
    "\n",
    "#### Execute hyperparameter tuning:\n",
    "- The code does not save model artifacts. It takes in command-line arguments as hyperparameter values from the Vertex AI Hyperparameter Tuning service, and reports training result metric to Vertex AI at each trial using cloudml-hypertune.\n",
    "- Note that if you decide to save model artifacts, saving them to the same directory may cause overwriting errors if you use parallel trials in the hyperparameter tuning job. The recommended approach is to save each trial's artifacts to a different sub-directory. This would also allow you to recover all the artifacts from different trials and can potentially save you from re-training.\n",
    "- Read more about hyperparameter tuning for custom containers [here](https://cloud.google.com/vertex-ai/docs/training/containers-overview#hyperparameter_tuning_with_custom_containers); read about hyperparameter tuning support [here](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview).\n",
    "\n",
    "#### Execute training:\n",
    "- The code saves model artifacts to `os.environ[\"AIP_MODEL_DIR\"]` in addition to `ARTIFACTS_DIR`, as required [here](https://github.com/googleapis/python-aiplatform/blob/v0.8.0/google/cloud/aiplatform/training_jobs.py#L2202).\n",
    "- If you want to make changes to the function, make sure to still save the trained policy as a SavedModel to clean directories, and avoid saving checkpoints and other artifacts, so that deploying the model to endpoints works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ea275-15ba-40b6-acb1-94d5d18996b1",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f515b2b9-a185-4336-8dd4-2e23e0076fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/jt-github/tf_vertex_agents\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2ae96-b5b5-4b10-9d9c-0b00d533ef46",
   "metadata": {},
   "source": [
    "### set vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd715ec5-f0fb-432b-bef4-a05a57586c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'mabv1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "463543b0-77b0-4d77-ba64-a6c84b70851e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID       = hybrid-vertex\n",
      "PROJECT_NUM      = 934903580331\n",
      "VPC_NETWORK_NAME = ucaip-haystack-vpc-network\n",
      "LOCATION         = us-central1\n",
      "REGION           = us-central1\n",
      "BQ_LOCATION      = US\n"
     ]
    }
   ],
   "source": [
    "# creds, PROJECT_ID = google.auth.default()\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "PROJECT_NUM              = !gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\"\n",
    "PROJECT_NUM              = PROJECT_NUM[0]\n",
    "\n",
    "VERTEX_SA                = f'{PROJECT_NUM}-compute@developer.gserviceaccount.com'\n",
    "\n",
    "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
    "\n",
    "# locations / regions for cloud resources\n",
    "LOCATION                 = 'us-central1'        \n",
    "REGION                   = LOCATION\n",
    "BQ_LOCATION              = 'US'\n",
    "\n",
    "print(f\"PROJECT_ID       = {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM      = {PROJECT_NUM}\")\n",
    "print(f\"VPC_NETWORK_NAME = {VPC_NETWORK_NAME}\")\n",
    "print(f\"LOCATION         = {LOCATION}\")\n",
    "print(f\"REGION           = {REGION}\")\n",
    "print(f\"BQ_LOCATION      = {BQ_LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac94b90-68ae-4729-b126-af84c3c56a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET_NAME         : mabv1-hybrid-vertex-bucket\n",
      "BUCKET_URI          : gs://mabv1-hybrid-vertex-bucket\n",
      "DATA_PATH           : gs://mabv1-hybrid-vertex-bucket/data\n",
      "VPC_NETWORK_FULL    : projects/934903580331/global/networks/ucaip-haystack-vpc-network\n",
      "BIGQUERY_DATASET_ID : hybrid-vertex.movielens_dataset_mabv1\n",
      "BIGQUERY_TABLE_ID   : hybrid-vertex.movielens_dataset_mabv1.training_dataset\n"
     ]
    }
   ],
   "source": [
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "# Location of the MovieLens 100K dataset's \"u.data\" file.\n",
    "DATA_GCS_PREFIX          = \"data\"\n",
    "DATA_PATH                = f\"{BUCKET_URI}/{DATA_GCS_PREFIX}\"\n",
    "ARTIFACTS_DIR            = f\"{BUCKET_URI}/artifacts\"\n",
    "\n",
    "VPC_NETWORK_FULL         = f\"projects/{PROJECT_NUM}/global/networks/{VPC_NETWORK_NAME}\"\n",
    "\n",
    "# BigQuery parameters (used for the Generator, Ingester, Logger)\n",
    "BIGQUERY_DATASET_ID      = f\"{PROJECT_ID}.movielens_dataset_{PREFIX}\"\n",
    "BIGQUERY_TABLE_ID        = f\"{BIGQUERY_DATASET_ID}.training_dataset\"\n",
    "\n",
    "print(f\"BUCKET_NAME         : {BUCKET_NAME}\")\n",
    "print(f\"BUCKET_URI          : {BUCKET_URI}\")\n",
    "print(f\"DATA_PATH           : {DATA_PATH}\")\n",
    "print(f\"VPC_NETWORK_FULL    : {VPC_NETWORK_FULL}\")\n",
    "print(f\"BIGQUERY_DATASET_ID : {BIGQUERY_DATASET_ID}\")\n",
    "print(f\"BIGQUERY_TABLE_ID   : {BIGQUERY_TABLE_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f5799-966e-4995-aae9-449768c1af48",
   "metadata": {},
   "source": [
    "### create GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "396332ce-7720-4a75-9920-e9079c39a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bucket\n",
    "# ! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0efc43f0-6e84-4679-9c56-7d0c34a240fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://mabv1-hybrid-vertex-bucket/archived/\n",
      "                                 gs://mabv1-hybrid-vertex-bucket/data/\n",
      "                                 gs://mabv1-hybrid-vertex-bucket/data_stats/\n",
      "                                 gs://mabv1-hybrid-vertex-bucket/perarm-local-test/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ab55c-3ca8-4348-b902-749b19a6c57f",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e52ce64-2afe-4d2f-a43d-68775ee7cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce8b63e1-84a3-4417-adf4-ff3f0dfe9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents import TFAgent\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "from tf_agents.bandits.environments import (environment_utilities,\n",
    "                                            movielens_py_environment)\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import TFEnvironment, tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.metrics.tf_metric import TFStepMetric\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "# my project\n",
    "from src.per_arm_rl import data_utils\n",
    "from src.per_arm_rl import data_config\n",
    "\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63cd20c0-7446-49df-9a1e-41a89065e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# # bigquery client\n",
    "# bqclient = bigquery.Client(\n",
    "#     project=PROJECT_ID,\n",
    "#     # location=LOCATION\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df188f63-506d-43dd-90ab-08246de887fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE_DATA_URI = \"gs://cloud-samples-data/vertex-ai/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/u.data\"\n",
    "\n",
    "# ! gsutil cp $SAMPLE_DATA_URI $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dda7603-ff83-4878-af18-66da744b9648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20289631  2023-07-13T14:31:03Z  gs://mabv1-hybrid-vertex-bucket/data/ml-ratings-100k-train.tfrecord#1689258663238090  metageneration=1\n",
      "TOTAL: 1 objects, 20289631 bytes (19.35 MiB)\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17376b28-6faf-4e03-baf3-73126a743e52",
   "metadata": {},
   "source": [
    "## Create training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "f7d34603-7d82-4207-bd6a-7713a8db3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "RL_SUB_DIR = 'per_arm_rl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e6fc1f84-11d0-4798-90ba-69f90f6be96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training subfolder\n",
    "# ! rm -rf {REPO_DOCKER_PATH_PREFIX}/{RL_SUB_DIR}\n",
    "# ! mkdir {REPO_DOCKER_PATH_PREFIX}/{RL_SUB_DIR}\n",
    "# ! touch {REPO_DOCKER_PATH_PREFIX}/{RL_SUB_DIR}/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "fc82a317-4b4e-4eb9-84ca-6735fda799b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/per_arm_rl/policy_util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{RL_SUB_DIR}/policy_util.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"The utility module for reinforcement learning policy.\"\"\"\n",
    "import collections\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "\n",
    "from tf_agents.agents import TFAgent\n",
    "from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import TFEnvironment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.metrics.tf_metric import TFStepMetric\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "def train(\n",
    "    agent: TFAgent\n",
    "    , environment: TFEnvironment\n",
    "    , training_loops: int\n",
    "    , steps_per_loop: int\n",
    "    , additional_metrics: Optional[List[TFStepMetric]] = None\n",
    "    , training_data_spec_transformation_fn: Optional[Callable[[T],T]] = None\n",
    "    , run_hyperparameter_tuning: bool = False\n",
    "    , root_dir: Optional[str] = None\n",
    "    , artifacts_dir: Optional[str] = None\n",
    "    , model_dir: Optional[str] = None\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Performs `training_loops` iterations of training on the agent's policy.\n",
    "\n",
    "    Uses the `environment` as the problem formulation and source of immediate\n",
    "    feedback and the agent's algorithm, to perform `training-loops` iterations\n",
    "    of on-policy training on the policy. Has hyperparameter mode and regular\n",
    "    training mode.\n",
    "    If one or more baseline_reward_fns are provided, the regret is computed\n",
    "    against each one of them. Here is example baseline_reward_fn:\n",
    "    def baseline_reward_fn(observation, per_action_reward_fns):\n",
    "     rewards = ... # compute reward for each arm\n",
    "     optimal_action_reward = ... # take the maximum reward\n",
    "     return optimal_action_reward\n",
    "\n",
    "    Args:\n",
    "      agent: An instance of `TFAgent`.\n",
    "      environment: An instance of `TFEnvironment`.\n",
    "      training_loops: An integer indicating how many training loops should be run.\n",
    "      steps_per_loop: An integer indicating how many driver steps should be\n",
    "        executed and presented to the trainer during each training loop.\n",
    "      additional_metrics: Optional; list of metric objects to log, in addition to\n",
    "        default metrics `NumberOfEpisodes`, `AverageReturnMetric`, and\n",
    "        `AverageEpisodeLengthMetric`.\n",
    "      training_data_spec_transformation_fn: Optional; function that transforms\n",
    "        the data items before they get to the replay buffer.\n",
    "      run_hyperparameter_tuning: Optional; whether this training logic is\n",
    "        executed for the purpose of hyperparameter tuning. If so, then it does\n",
    "        not save model artifacts.\n",
    "      root_dir: Optional; path to the directory where training artifacts are\n",
    "        written; usually used for a default or auto-generated location. Do not\n",
    "        specify this argument if using hyperparameter tuning instead of training.\n",
    "      artifacts_dir: Optional; path to an extra directory where training\n",
    "        artifacts are written; usually used for a mutually agreed location from\n",
    "        which artifacts will be loaded. Do not specify this argument if using\n",
    "        hyperparameter tuning instead of training.\n",
    "\n",
    "    Returns:\n",
    "      A dict mapping metric names (eg. \"AverageReturnMetric\") to a list of\n",
    "      intermediate metric values over `training_loops` iterations of training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ====================================================\n",
    "    # get data spec\n",
    "    # ====================================================\n",
    "    if run_hyperparameter_tuning and not (root_dir is None and artifacts_dir is None):\n",
    "        raise ValueError(\n",
    "            \"Do not specify `root_dir` or `artifacts_dir` when\" +\n",
    "            \" running hyperparameter tuning.\"\n",
    "        )\n",
    "\n",
    "    if training_data_spec_transformation_fn is None:\n",
    "        data_spec = agent.policy.trajectory_spec\n",
    "    else:\n",
    "        data_spec = training_data_spec_transformation_fn(\n",
    "            agent.policy.trajectory_spec\n",
    "        )\n",
    "        \n",
    "    # ====================================================\n",
    "    # define replay buffer\n",
    "    # ====================================================\n",
    "    replay_buffer = trainer._get_replay_buffer(\n",
    "        data_spec = data_spec\n",
    "        , batch_size = environment.batch_size\n",
    "        , steps_per_loop = steps_per_loop\n",
    "        , async_steps_per_loop = 1\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # metrics\n",
    "    # ====================================================\n",
    "    # `step_metric` records the number of individual rounds of bandit interaction;\n",
    "    # that is, (number of trajectories) * batch_size.\n",
    "    \n",
    "    step_metric = tf_metrics.EnvironmentSteps()\n",
    "    \n",
    "    metrics = [\n",
    "        tf_metrics.NumberOfEpisodes()\n",
    "        , tf_metrics.AverageEpisodeLengthMetric(batch_size=environment.batch_size)\n",
    "    ]\n",
    "    if additional_metrics:\n",
    "        metrics += additional_metrics\n",
    "\n",
    "    if isinstance(environment.reward_spec(), dict):\n",
    "        metrics += [\n",
    "            tf_metrics.AverageReturnMultiMetric(\n",
    "                reward_spec=environment.reward_spec()\n",
    "                , batch_size=environment.batch_size\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        metrics += [\n",
    "            tf_metrics.AverageReturnMetric(batch_size=environment.batch_size)\n",
    "        ]\n",
    "\n",
    "    # Store intermediate metric results, indexed by metric names.\n",
    "    metric_results = collections.defaultdict(list)\n",
    "\n",
    "    # ====================================================\n",
    "    # Driver\n",
    "    # ====================================================\n",
    "    \n",
    "    if training_data_spec_transformation_fn is not None:\n",
    "        add_batch_fn = lambda data: replay_buffer.add_batch(\n",
    "            training_data_spec_transformation_fn(data)\n",
    "        )\n",
    "    else:\n",
    "        add_batch_fn = replay_buffer.add_batch\n",
    "\n",
    "    observers = [add_batch_fn, step_metric] + metrics\n",
    "\n",
    "    driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        env=environment\n",
    "        , policy=agent.collect_policy\n",
    "        , num_steps=steps_per_loop * environment.batch_size\n",
    "        , observers=observers\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # training_loop\n",
    "    # ====================================================\n",
    "    training_loop = trainer._get_training_loop(\n",
    "        driver = driver\n",
    "        , replay_buffer = replay_buffer\n",
    "        , agent = agent\n",
    "        , steps = steps_per_loop\n",
    "        , async_steps_per_loop = 1\n",
    "    )\n",
    "    if not run_hyperparameter_tuning:\n",
    "        saver = policy_saver.PolicySaver(agent.policy)\n",
    "\n",
    "    for train_step in range(training_loops):\n",
    "        training_loop(\n",
    "            train_step = train_step\n",
    "            , metrics = metrics\n",
    "        )\n",
    "        metric_utils.log_metrics(metrics)\n",
    "    \n",
    "        for metric in metrics:\n",
    "            metric.tf_summaries(train_step = step_metric.result())\n",
    "            metric_results[type(metric).__name__].append(metric.result().numpy())\n",
    "    \n",
    "    if not run_hyperparameter_tuning:\n",
    "        saver.save(model_dir)\n",
    "        saver.save(artifacts_dir)\n",
    "    \n",
    "    return metric_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9436ba2-20b1-4c83-9daa-5fc11c39c384",
   "metadata": {},
   "source": [
    "### train task\n",
    "\n",
    "**TODO:**\n",
    "* add vertex experiments to train task - following logic to not log experiment if HPT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "8cab82ce-0948-40f9-920a-2711ae774b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/per_arm_rl/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{RL_SUB_DIR}/task.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"The entrypoint for training a policy.\"\"\"\n",
    "import argparse\n",
    "import functools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Union\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "import hypertune\n",
    "\n",
    "from . import policy_util\n",
    "from . import data_utils\n",
    "from . import train_utils\n",
    "from . import data_config\n",
    "from . import my_per_arm_py_env\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.environments import environment_utilities\n",
    "from tf_agents.bandits.environments import movielens_py_environment\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "PER_ARM = True  # Use the non-per-arm version of the MovieLens environment.\n",
    "\n",
    "def get_args(\n",
    "    raw_args: List[str]\n",
    ") -> argparse.Namespace:\n",
    "    \"\"\"Parses parameters and hyperparameters for training a policy.\n",
    "\n",
    "    Args:\n",
    "      raw_args: A list of command line arguments.\n",
    "\n",
    "    Returns:\n",
    "      An argpase.Namespace object mapping (hyper)parameter names to the parsed\n",
    "      values.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--project_id\"\n",
    "        , type=str\n",
    "        , default='hybrid-vertex'\n",
    "    )\n",
    "    # Whether to execute hyperparameter tuning or training\n",
    "    parser.add_argument(\n",
    "        \"--run-hyperparameter-tuning\"\n",
    "        , action=\"store_true\"\n",
    "        , help=\"Whether to perform hyperparameter tuning instead of regular training.\"\n",
    "    )\n",
    "    # Whether to train using the best hyperparameters learned from a previous\n",
    "    # hyperparameter tuning job.\n",
    "    parser.add_argument(\n",
    "        \"--train-with-best-hyperparameters\"\n",
    "        , action=\"store_true\"\n",
    "        , help=\"Whether to train using the best hyperparameters learned from a previous hyperparameter tuning job.\"\n",
    "    )\n",
    "    # Path parameters\n",
    "    parser.add_argument(\n",
    "        \"--artifacts-dir\"\n",
    "        , type=str\n",
    "        , help=\"Extra directory where model artifacts are saved.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--profiler-dir\"\n",
    "        , default=None\n",
    "        , type=str\n",
    "        , help=\"Directory for TensorBoard Profiler artifacts.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data-path\", type=str, help=\"Path to MovieLens 100K's 'u.data' file.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--best-hyperparameters-bucket\"\n",
    "        , type=str\n",
    "        , help=\"Path to MovieLens 100K's 'u.data' file.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--best-hyperparameters-path\"\n",
    "        , type=str\n",
    "        , help=\"Path to JSON file containing the best hyperparameters.\"\n",
    "    )\n",
    "    # Hyperparameters\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\"\n",
    "        , default=8\n",
    "        , type=int\n",
    "        , help=\"Training and prediction batch size.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--training-loops\"\n",
    "        , default=4\n",
    "        , type=int\n",
    "        , help=\"Number of training iterations.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--steps-per-loop\"\n",
    "        , default=2\n",
    "        , type=int\n",
    "        , help=\"Number of driver steps per training iteration.\"\n",
    "    )\n",
    "    # MovieLens simulation environment parameters\n",
    "    parser.add_argument(\n",
    "        \"--rank-k\"\n",
    "        , default=20\n",
    "        , type=int\n",
    "        , help=\"Rank for matrix factorization in the MovieLens environment; also the observation dimension.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-actions\"\n",
    "        , default=20\n",
    "        , type=int\n",
    "        , help=\"Number of actions (movie items) to choose from.\"\n",
    "    )\n",
    "    # LinUCB agent parameters\n",
    "    parser.add_argument(\n",
    "        \"--tikhonov-weight\"\n",
    "        , default=0.001\n",
    "        , type=float\n",
    "        , help=\"LinUCB Tikhonov regularization weight.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--agent-alpha\"\n",
    "        , default=10.0\n",
    "        , type=float\n",
    "        , help=\"LinUCB exploration parameter that multiplies the confidence intervals.\"\n",
    "    )\n",
    "\n",
    "    ### new\n",
    "    parser.add_argument(\n",
    "        \"--bucket_name\"\n",
    "        , default=\"tmp\"\n",
    "        , type=str\n",
    "        , help=\" \"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--data_gcs_prefix\"\n",
    "        , default=\"data\"\n",
    "        , type=str\n",
    "        , help=\"\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--data_path\"\n",
    "        , default=\"gs://tmp/tmp\"\n",
    "        , type=str\n",
    "        , help=\"\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--project_number\"\n",
    "        , default=\"934903580331\"\n",
    "        , type=str\n",
    "        , help=\"\"\n",
    "    )\n",
    "    # distribute\n",
    "    parser.add_argument(\n",
    "        \"--distribute\"\n",
    "        , default=\"single\"\n",
    "        , type=str\n",
    "        , help=\"\"\n",
    "    )\n",
    "    # artifacts_dir\n",
    "    parser.add_argument(\n",
    "        \"--artifacts_dir\"\n",
    "        , default=\"gs://BUCKET/EXPERIMENT/RUN_NAME/artifacts\"\n",
    "        , type=str\n",
    "        , help=\"\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--root_dir\"\n",
    "        , default=\"gs://BUCKET/EXPERIMENT/RUN_NAME/root\"\n",
    "        , type=str\n",
    "        , help=\"\"\n",
    "    )\n",
    "    \n",
    "    return parser.parse_args(raw_args)\n",
    "\n",
    "def execute_task(\n",
    "    args: argparse.Namespace\n",
    "    , best_hyperparameters_blob: Union[storage.Blob, None]\n",
    "    , hypertune_client: Union[hypertune.HyperTune, None]\n",
    ") -> None:\n",
    "    \"\"\"Executes training, or hyperparameter tuning, for the policy.\n",
    "\n",
    "    Parses parameters and hyperparameters from the command line, reads best\n",
    "    hyperparameters if applicable, constructs the logical modules for RL, and\n",
    "    executes training or hyperparameter tuning. Tracks the training process\n",
    "    and resources using TensorBoard Profiler if applicable.\n",
    "\n",
    "    Args:\n",
    "      args: An argpase.Namespace object of (hyper)parameter values.\n",
    "      best_hyperparameters_blob: An object containing best hyperparameters in\n",
    "        Google Cloud Storage.\n",
    "      hypertune_client: Client for submitting hyperparameter tuning metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # [Do Not Change] Set the root directory for training artifacts.\n",
    "    # TODO - JT\n",
    "    MODEL_DIR = os.environ[\"AIP_MODEL_DIR\"] if not args.run_hyperparameter_tuning else \"\"\n",
    "    root_dir = args.root_dir if not args.run_hyperparameter_tuning else \"\"\n",
    "    logging.info(f'root_dir: {root_dir}')\n",
    "\n",
    "    # Use best hyperparameters learned from a previous hyperparameter tuning job.\n",
    "    logging.info(args.train_with_best_hyperparameters)\n",
    "    if args.train_with_best_hyperparameters:\n",
    "        logging.info(f'train_with_best_hyperparameters engaged...')\n",
    "        best_hyperparameters = json.loads(\n",
    "            best_hyperparameters_blob.download_as_string()\n",
    "        )\n",
    "        \n",
    "        if \"batch-size\" in best_hyperparameters:\n",
    "            args.batch_size = int(best_hyperparameters[\"batch-size\"])\n",
    "        if \"training-loops\" in best_hyperparameters:\n",
    "            args.training_loops = int(best_hyperparameters[\"training-loops\"])\n",
    "        if \"steps-per-loop\" in best_hyperparameters:\n",
    "            args.step_per_loop = int(best_hyperparameters[\"steps-per-loop\"])\n",
    "\n",
    "    # Define RL environment.\n",
    "    env = my_per_arm_py_env.MyMovieLensPerArmPyEnvironment(\n",
    "        project_number = args.project_number\n",
    "        , data_path = args.data_path\n",
    "        , bucket_name = args.bucket_name\n",
    "        , data_gcs_prefix = args.data_gcs_prefix\n",
    "        , user_age_lookup_dict = data_config.USER_AGE_LOOKUP\n",
    "        , user_occ_lookup_dict = data_config.USER_OCC_LOOKUP\n",
    "        , movie_gen_lookup_dict = data_config.MOVIE_GEN_LOOKUP\n",
    "        , num_users = data_config.MOVIELENS_NUM_USERS\n",
    "        , num_movies = data_config.MOVIELENS_NUM_MOVIES\n",
    "        , rank_k = args.rank_k\n",
    "        , batch_size = args.batch_size\n",
    "        , num_actions = args.num_actions\n",
    "    )\n",
    "    environment = tf_py_environment.TFPyEnvironment(env)\n",
    "    \n",
    "    strategy = train_utils.get_train_strategy(distribute_arg=args.distribute)\n",
    "    logging.info(f'TF training strategy (execute task) = {strategy}')\n",
    "    \n",
    "    with strategy.scope():\n",
    "        # Define RL agent/algorithm.\n",
    "        agent = lin_ucb_agent.LinearUCBAgent(\n",
    "            time_step_spec=environment.time_step_spec()\n",
    "            , action_spec=environment.action_spec()\n",
    "            , tikhonov_weight=args.tikhonov_weight\n",
    "            , alpha=args.agent_alpha\n",
    "            , dtype=tf.float32\n",
    "            , accepts_per_arm_features=PER_ARM # TODO - streamline\n",
    "        )\n",
    "    logging.info(\"TimeStep Spec (for each batch):\\n%s\\n\", agent.time_step_spec)\n",
    "    logging.info(\"Action Spec (for each batch):\\n%s\\n\", agent.action_spec)\n",
    "    logging.info(\"Reward Spec (for each batch):\\n%s\\n\", environment.reward_spec())\n",
    "\n",
    "    # Define RL metric.\n",
    "    optimal_reward_fn = functools.partial(\n",
    "        environment_utilities.compute_optimal_reward_with_movielens_environment\n",
    "        , environment=environment\n",
    "    )\n",
    "    \n",
    "    regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n",
    "    metrics = [regret_metric]\n",
    "\n",
    "    # Perform on-policy training with the simulation MovieLens environment.\n",
    "    if args.profiler_dir is not None:\n",
    "        tf.profiler.experimental.start(args.profiler_dir)\n",
    "  \n",
    "    metric_results = policy_util.train(\n",
    "        agent=agent\n",
    "        , environment=environment\n",
    "        , training_loops=args.training_loops\n",
    "        , steps_per_loop=args.steps_per_loop\n",
    "        , additional_metrics=metrics\n",
    "        , run_hyperparameter_tuning=args.run_hyperparameter_tuning\n",
    "        , root_dir=root_dir if not args.run_hyperparameter_tuning else None\n",
    "        , artifacts_dir=args.artifacts_dir\n",
    "        if not args.run_hyperparameter_tuning else None\n",
    "        , model_dir = MODEL_DIR\n",
    "    )\n",
    "    \n",
    "    if args.profiler_dir is not None:\n",
    "        tf.profiler.experimental.stop()\n",
    "\n",
    "    # Report training metrics to Vertex AI for hyperparameter tuning\n",
    "    if args.run_hyperparameter_tuning:\n",
    "        hypertune_client.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag=\"final_average_return\"\n",
    "            , metric_value=metric_results[\"AverageReturnMetric\"][-1]\n",
    "            # , global_step=args.training_loops\n",
    "        )\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Entry point for training or hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    args = get_args(sys.argv[1:])\n",
    "    # =============================================\n",
    "    # set GCP clients\n",
    "    # =============================================\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from google.cloud import storage\n",
    "\n",
    "    project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    storage_client = storage.Client(project=project_number)\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=project_number,\n",
    "        location='us-central1',\n",
    "        # experiment=args.experiment_name\n",
    "    )\n",
    "    \n",
    "    # =============================================\n",
    "    # GPUs\n",
    "    # =============================================\n",
    "\n",
    "    # limiting GPU growth\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logging.info(f'detected: {len(gpus)} GPUs')\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            logging.info(e)\n",
    "\n",
    "    # tf.debugging.set_log_device_placement(True)          # logs all tf ops and their device placement;\n",
    "    os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "    os.environ['TF_GPU_THREAD_COUNT'] = f'8'               # TODO - parametrize | 1\n",
    "    os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "    \n",
    "    # ====================================================\n",
    "    # Set Device Strategy\n",
    "    # ====================================================\n",
    "    logging.info(\"Detecting devices....\")\n",
    "    logging.info('DEVICES'  + str(device_lib.list_local_devices()))\n",
    "    \n",
    "    logging.info(\"Setting device strategy...\")\n",
    "    \n",
    "    strategy = train_utils.get_train_strategy(distribute_arg=args.distribute)\n",
    "    logging.info(f'TF training strategy (main) = {strategy}')\n",
    "    \n",
    "    NUM_REPLICAS = strategy.num_replicas_in_sync\n",
    "    logging.info(f'num_replicas_in_sync = {NUM_REPLICAS}')\n",
    "    \n",
    "    # Here the batch size scales up by number of workers since\n",
    "    # `tf.data.Dataset.batch` expects the global batch size.\n",
    "    GLOBAL_BATCH_SIZE = int(args.batch_size) * int(NUM_REPLICAS)\n",
    "    logging.info(f'GLOBAL_BATCH_SIZE = {GLOBAL_BATCH_SIZE}')\n",
    "\n",
    "    # type and task of machine from strategy\n",
    "    logging.info(f'Setting task_type and task_id...')\n",
    "    if args.distribute == 'multiworker':\n",
    "        task_type, task_id = (\n",
    "            strategy.cluster_resolver.task_type,\n",
    "            strategy.cluster_resolver.task_id\n",
    "        )\n",
    "    else:\n",
    "        task_type, task_id = 'chief', None\n",
    "    \n",
    "    logging.info(f'task_type = {task_type}')\n",
    "    logging.info(f'task_id = {task_id}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # determine train job type and execute\n",
    "    # ====================================================\n",
    "    \n",
    "    if args.train_with_best_hyperparameters:\n",
    "        storage_client = storage.Client(args.project_id)\n",
    "        bucket = storage_client.bucket(args.bucket_name)\n",
    "        best_hyperparameters_blob = bucket.blob(args.best_hyperparameters_path)\n",
    "    \n",
    "    else:\n",
    "        best_hyperparameters_blob = None\n",
    "    \n",
    "    hypertune_client = hypertune.HyperTune() if args.run_hyperparameter_tuning else None\n",
    "\n",
    "    execute_task(\n",
    "        args = args\n",
    "        , best_hyperparameters_blob = best_hyperparameters_blob\n",
    "        , hypertune_client = hypertune_client\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Python Version = %s\", sys.version)\n",
    "    logging.info(\"TensorFlow Version = %s\", tf.__version__)\n",
    "    # logging.info(\"TF_CONFIG = %s\", os.environ.get(\"TF_CONFIG\", \"Not found\"))\n",
    "    # logging.info(\"DEVICES = %s\", device_lib.list_local_devices())\n",
    "    logging.info(\"Reinforcement learning task started...\")\n",
    "    \n",
    "    main()\n",
    "    \n",
    "    logging.info(\"Reinforcement learning task completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678fcb8-4b41-42aa-92ec-c3b07f7747e0",
   "metadata": {},
   "source": [
    "## Build train application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd895298-c435-414d-b84a-908cf0932ba4",
   "metadata": {},
   "source": [
    "### Vertex Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b6322612-2a66-4b05-b600-dbae4d307efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : scale-perarm-hpt\n",
      "RUN_NAME          : run-20230714-134124\n",
      "LOG_DIR           : gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230714-134124/tb-logs\n",
      "ROOT_DIR          : gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230714-134124/root\n",
      "ARTIFACTS_DIR     : gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230714-134124/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'scale-perarm-hpt'\n",
    "\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "LOG_DIR           = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/tb-logs\"\n",
    "ROOT_DIR          = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b106007a-9ed4-4237-88a9-bf4bbe459b15",
   "metadata": {},
   "source": [
    "### Create a Cloud Build YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a448db20-3021-4d07-9559-661be6a04eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile_$_DOCKERNAME']\n",
    "  env: ['AIP_STORAGE_URI=$_ARTIFACTS_DIR']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3180b65-d636-40a2-aaa2-7b6a797e43b3",
   "metadata": {},
   "source": [
    "### Write a Dockerfile\n",
    "* Use the [cloudml-hypertune](https://github.com/GoogleCloudPlatform/cloudml-hypertune) Python package to report training metrics to Vertex AI for hyperparameter tuning\n",
    "* Use the Google [Cloud Storage client library](https://cloud.google.com/storage/docs/reference/libraries) to read the best hyperparameters learned from a previous hyperarameter tuning job during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "20db1e12-1e16-4ec9-89c7-2a3b181cb715",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCKERNAME = 'train_perarm'\n",
    "# ! rm -rf Dockerfile_{DOCKERNAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "59ea1b9d-6237-4e12-9b86-9a5ec23d7459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile_train_perarm\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile_{DOCKERNAME}\n",
    "\n",
    "# Specifies base image and tag.\n",
    "# FROM gcr.io/google-appengine/python\n",
    "FROM python:3.10\n",
    "ENV PYTHONUNBUFFERED True\n",
    "\n",
    "WORKDIR /root\n",
    "\n",
    "# Installs additional packages.\n",
    "RUN pip3 install cloudml-hypertune\n",
    "RUN pip3 install google-cloud-storage\n",
    "RUN pip3 install google-cloud-aiplatform\n",
    "RUN pip3 install tensorflow==2.12.0\n",
    "RUN pip3 install tensorboard\n",
    "RUN pip3 install tensorboard-plugin-profile\n",
    "RUN pip3 install tensorboard-plugin-wit\n",
    "RUN pip3 install tensorboard-data-server\n",
    "RUN pip3 install tensorflow-io\n",
    "RUN pip3 install tf-agents==0.17.0\n",
    "RUN pip3 install matplotlib\n",
    "RUN pip3 install urllib3\n",
    "\n",
    "# Copies training code to the Docker image.\n",
    "COPY src/per_arm_rl /root/src/per_arm_rl\n",
    "\n",
    "# Sets up the entry point to invoke the task.\n",
    "ENTRYPOINT [\"python3\", \"-m\", \"src.per_arm_rl.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0092f87b-c113-4325-9696-658ee6cf5860",
   "metadata": {},
   "source": [
    "#### Build the custom container with Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9eafefba-a58f-46aa-a331-fc0abea93702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export DOCKERNAME    = train_perarm\n",
      "export IMAGE_URI     = gcr.io/hybrid-vertex/hptuning-training-custom-container\n",
      "export FILE_LOCATION = ./\n",
      "export MACHINE_TYPE  = e2-highcpu-32\n",
      "export ARTIFACTS_DIR = gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230714-134124/artifacts\n"
     ]
    }
   ],
   "source": [
    "HPTUNING_TRAINING_CONTAINER = \"hptuning-training-custom-container\"\n",
    "\n",
    "# Docker definitions for training\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{HPTUNING_TRAINING_CONTAINER}'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './'\n",
    "\n",
    "print(f\"export DOCKERNAME    = {DOCKERNAME}\")\n",
    "print(f\"export IMAGE_URI     = {IMAGE_URI}\")\n",
    "print(f\"export FILE_LOCATION = {FILE_LOCATION}\")\n",
    "print(f\"export MACHINE_TYPE  = {MACHINE_TYPE}\")\n",
    "print(f\"export ARTIFACTS_DIR = {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d2f64125-fd4a-459b-9034-2d084f85a095",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 58 file(s) totalling 57.2 MiB before compression.\n",
      "Some files were not included in the source upload.\n",
      "\n",
      "Check the gcloud log [/home/jupyter/.config/gcloud/logs/2023.07.14/13.41.31.114949.log] to see which files and the contents of the\n",
      "default gcloudignore file used (see `$ gcloud topic gcloudignore` to learn\n",
      "more).\n",
      "\n",
      "Uploading tarball of [.] to [gs://hybrid-vertex_cloudbuild/source/1689342091.211716-cc5d9e55d5e84ea380ab6431bab7c0b6.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/global/builds/938c0a4b-1db6-47aa-a5b6-ae935f9c9d7d].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/938c0a4b-1db6-47aa-a5b6-ae935f9c9d7d?project=934903580331 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"938c0a4b-1db6-47aa-a5b6-ae935f9c9d7d\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1689342091.211716-cc5d9e55d5e84ea380ab6431bab7c0b6.tgz#1689342095529012\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1689342091.211716-cc5d9e55d5e84ea380ab6431bab7c0b6.tgz#1689342095529012...\n",
      "/ [1 files][  6.2 MiB/  6.2 MiB]                                                \n",
      "Operation completed over 1 objects/6.2 MiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  60.01MB\n",
      "Step 1/17 : FROM python:3.10\n",
      "3.10: Pulling from library/python\n",
      "d52e4f012db1: Already exists\n",
      "7dd206bea61f: Already exists\n",
      "2320f9be4a9c: Already exists\n",
      "6e5565e0ba8d: Already exists\n",
      "d3797e13cc41: Already exists\n",
      "f8108bc94b5f: Pulling fs layer\n",
      "3d35965ce4db: Pulling fs layer\n",
      "4459f6d56468: Pulling fs layer\n",
      "3d35965ce4db: Verifying Checksum\n",
      "3d35965ce4db: Download complete\n",
      "f8108bc94b5f: Download complete\n",
      "4459f6d56468: Verifying Checksum\n",
      "4459f6d56468: Download complete\n",
      "f8108bc94b5f: Pull complete\n",
      "3d35965ce4db: Pull complete\n",
      "4459f6d56468: Pull complete\n",
      "Digest: sha256:1b2e0805e24189fbba4e55b9bee89e3c25533cbe4fb71ae151f3e7ae0c9b86c5\n",
      "Status: Downloaded newer image for python:3.10\n",
      " ---> d9122363988f\n",
      "Step 2/17 : ENV PYTHONUNBUFFERED True\n",
      " ---> Running in 01e676ec3390\n",
      "Removing intermediate container 01e676ec3390\n",
      " ---> 9e457d5c3f0e\n",
      "Step 3/17 : WORKDIR /root\n",
      " ---> Running in 7c6d2868e8a0\n",
      "Removing intermediate container 7c6d2868e8a0\n",
      " ---> 36f583be3447\n",
      "Step 4/17 : RUN pip3 install cloudml-hypertune\n",
      " ---> Running in 156e63559765\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: cloudml-hypertune\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3973 sha256=fa143f7353d18c641fd34b3361790e062ed8280799f7de44297fced67a6d7743\n",
      "  Stored in directory: /root/.cache/pip/wheels/c6/2d/bb/9c72de7c488cd8e60172c4920c09e404c490020162205b64ba\n",
      "Successfully built cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 156e63559765\n",
      " ---> e17539043eab\n",
      "Step 5/17 : RUN pip3 install google-cloud-storage\n",
      " ---> Running in 7879cd4fa957\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB)\n",
      "      114.6/114.6 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB)\n",
      "Collecting requests<3.0.0dev,>=2.18.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "      62.6/62.6 kB 13.8 MB/s eta 0:00:00\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB)\n",
      "      77.7/77.7 kB 17.6 MB/s eta 0:00:00\n",
      "Collecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "      181.8/181.8 kB 34.5 MB/s eta 0:00:00\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.11.1-py3-none-any.whl (120 kB)\n",
      "      120.5/120.5 kB 23.4 MB/s eta 0:00:00\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.59.1-py2.py3-none-any.whl (224 kB)\n",
      "      224.5/224.5 kB 39.3 MB/s eta 0:00:00\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "      304.5/304.5 kB 14.0 MB/s eta 0:00:00\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "      181.3/181.3 kB 34.7 MB/s eta 0:00:00\n",
      "Collecting six>=1.9.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting urllib3<2.0\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "      143.1/143.1 kB 27.4 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "      61.5/61.5 kB 13.5 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (201 kB)\n",
      "      201.8/201.8 kB 37.0 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "      157.0/157.0 kB 29.7 MB/s eta 0:00:00\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "      83.9/83.9 kB 19.2 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, six, pyasn1, protobuf, idna, google-crc32c, charset-normalizer, certifi, cachetools, rsa, requests, pyasn1-modules, googleapis-common-protos, google-resumable-media, google-auth, google-api-core, google-cloud-core, google-cloud-storage\n",
      "Successfully installed cachetools-5.3.1 certifi-2023.5.7 charset-normalizer-3.2.0 google-api-core-2.11.1 google-auth-2.22.0 google-cloud-core-2.3.3 google-cloud-storage-2.10.0 google-crc32c-1.5.0 google-resumable-media-2.5.0 googleapis-common-protos-1.59.1 idna-3.4 protobuf-4.23.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-2.31.0 rsa-4.9 six-1.16.0 urllib3-1.26.16\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 7879cd4fa957\n",
      " ---> 0388b6a4d98d\n",
      "Step 6/17 : RUN pip3 install google-cloud-aiplatform\n",
      " ---> Running in 752b343a32d9\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.28.0-py2.py3-none-any.whl (2.6 MB)\n",
      "      2.6/2.6 MB 46.0 MB/s eta 0:00:00\n",
      "Collecting packaging>=14.3\n",
      "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
      "      48.9/48.9 kB 10.7 MB/s eta 0:00:00\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.10.2-py2.py3-none-any.whl (321 kB)\n",
      "      321.3/321.3 kB 43.9 MB/s eta 0:00:00\n",
      "Collecting google-cloud-bigquery<4.0.0dev,>=1.15.0\n",
      "  Downloading google_cloud_bigquery-3.11.3-py2.py3-none-any.whl (219 kB)\n",
      "      219.5/219.5 kB 37.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/site-packages (from google-cloud-aiplatform) (4.23.4)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "  Downloading proto_plus-1.22.3-py3-none-any.whl (48 kB)\n",
      "      48.1/48.1 kB 10.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.10.0)\n",
      "Collecting shapely<2.0.0\n",
      "  Downloading Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
      "      2.0/2.0 MB 83.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.11.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.59.1)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.22.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.31.0)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2\n",
      "  Downloading grpcio_status-1.56.0-py3-none-any.whl (5.1 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2\n",
      "  Downloading grpcio-1.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "      5.2/5.2 MB 105.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
      "Collecting python-dateutil<3.0dev,>=2.7.2\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "      247.7/247.7 kB 40.2 MB/s eta 0:00:00\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "  Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.16)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n",
      "Installing collected packages: shapely, python-dateutil, proto-plus, packaging, grpcio, grpcio-status, grpc-google-iam-v1, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "Successfully installed google-cloud-aiplatform-1.28.0 google-cloud-bigquery-3.11.3 google-cloud-resource-manager-1.10.2 grpc-google-iam-v1-0.12.6 grpcio-1.56.0 grpcio-status-1.56.0 packaging-23.1 proto-plus-1.22.3 python-dateutil-2.8.2 shapely-1.8.5.post1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 752b343a32d9\n",
      " ---> 2791a9ae38de\n",
      "Step 7/17 : RUN pip3 install tensorflow==2.12.0\n",
      " ---> Running in 436a89b02c00\n",
      "Collecting tensorflow==2.12.0\n",
      "  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
      "      585.9/585.9 MB 3.6 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "      65.5/65.5 kB 14.1 MB/s eta 0:00:00\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "      4.8/4.8 MB 99.7 MB/s eta 0:00:00\n",
      "Collecting keras<2.13,>=2.12.0\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "      1.7/1.7 MB 85.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (4.23.4)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "      2.4/2.4 MB 94.8 MB/s eta 0:00:00\n",
      "Collecting numpy<1.24,>=1.22\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "      17.1/17.1 MB 95.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (23.1)\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "      5.6/5.6 MB 100.3 MB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "      440.7/440.7 kB 55.8 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "      57.5/57.5 kB 13.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (1.56.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow==2.12.0) (65.5.1)\n",
      "Collecting jax>=0.3.15\n",
      "  Downloading jax-0.4.13.tar.gz (1.3 MB)\n",
      "      1.3/1.3 MB 86.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "      77.9/77.9 kB 17.0 MB/s eta 0:00:00\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "      22.9/22.9 MB 86.1 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "      126.5/126.5 kB 26.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.40.0)\n",
      "Collecting ml-dtypes>=0.1.0\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "      1.0/1.0 MB 77.1 MB/s eta 0:00:00\n",
      "Collecting scipy>=1.7\n",
      "  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
      "      36.3/36.3 MB 58.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.22.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "      93.9/93.9 kB 19.6 MB/s eta 0:00:00\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "      242.5/242.5 kB 41.6 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "      6.6/6.6 MB 112.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.31.0)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.26.16)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.3.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.3.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2023.5.7)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.5.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "      151.7/151.7 kB 31.5 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (pyproject.toml): started\n",
      "  Building wheel for jax (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for jax: filename=jax-0.4.13-py3-none-any.whl size=1518707 sha256=a57811dd4b0a6337680bbef2c7294a705f3f0fdff2e63643f9c89d4407a0dfdd\n",
      "  Stored in directory: /root/.cache/pip/wheels/f3/7a/25/f297f69029b5e4064e4736a0c4b3996a44cc27781c120bcb99\n",
      "Successfully built jax\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, oauthlib, numpy, MarkupSafe, markdown, keras, google-pasta, gast, astunparse, absl-py, werkzeug, scipy, requests-oauthlib, opt-einsum, ml-dtypes, h5py, jax, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.3 absl-py-1.4.0 astunparse-1.6.3 flatbuffers-23.5.26 gast-0.4.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 h5py-3.9.0 jax-0.4.13 keras-2.12.0 libclang-16.0.0 markdown-3.4.3 ml-dtypes-0.2.0 numpy-1.23.5 oauthlib-3.2.2 opt-einsum-3.3.0 requests-oauthlib-1.3.1 scipy-1.11.1 tensorboard-2.12.3 tensorboard-data-server-0.7.1 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 typing-extensions-4.7.1 werkzeug-2.3.6 wrapt-1.14.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 436a89b02c00\n",
      " ---> 702684c89e39\n",
      "Step 8/17 : RUN pip3 install tensorboard\n",
      " ---> Running in 87134fa2fb0a\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/site-packages (2.12.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/site-packages (from tensorboard) (3.4.3)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/site-packages (from tensorboard) (1.56.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/site-packages (from tensorboard) (65.5.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/site-packages (from tensorboard) (0.40.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/site-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/site-packages (from tensorboard) (2.22.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/site-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/site-packages (from tensorboard) (1.23.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/site-packages (from tensorboard) (0.7.1)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/site-packages (from tensorboard) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/site-packages (from tensorboard) (2.31.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/site-packages (from tensorboard) (2.3.6)\n",
      "Requirement already satisfied: urllib3<2.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.16)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 87134fa2fb0a\n",
      " ---> 651d8aef4c4a\n",
      "Step 9/17 : RUN pip3 install tensorboard-plugin-profile\n",
      " ---> Running in 2807b3d5be96\n",
      "Collecting tensorboard-plugin-profile\n",
      "  Downloading tensorboard_plugin_profile-2.13.0-py3-none-any.whl (5.4 MB)\n",
      "      5.4/5.4 MB 67.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/site-packages (from tensorboard-plugin-profile) (65.5.1)\n",
      "Collecting gviz-api>=1.9.0\n",
      "  Downloading gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/site-packages (from tensorboard-plugin-profile) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.10/site-packages (from tensorboard-plugin-profile) (4.23.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/site-packages (from tensorboard-plugin-profile) (2.3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=0.11.15->tensorboard-plugin-profile) (2.1.3)\n",
      "Installing collected packages: gviz-api, tensorboard-plugin-profile\n",
      "Successfully installed gviz-api-1.10.0 tensorboard-plugin-profile-2.13.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 2807b3d5be96\n",
      " ---> 221ef24e7e07\n",
      "Step 10/17 : RUN pip3 install tensorboard-plugin-wit\n",
      " ---> Running in 31c3678dd444\n",
      "Collecting tensorboard-plugin-wit\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "      781.3/781.3 kB 22.1 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboard-plugin-wit\n",
      "Successfully installed tensorboard-plugin-wit-1.8.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 31c3678dd444\n",
      " ---> c402bada3616\n",
      "Step 11/17 : RUN pip3 install tensorboard-data-server\n",
      " ---> Running in 6521ac349c71\n",
      "Requirement already satisfied: tensorboard-data-server in /usr/local/lib/python3.10/site-packages (0.7.1)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 6521ac349c71\n",
      " ---> e53b44790d31\n",
      "Step 12/17 : RUN pip3 install tensorflow-io\n",
      " ---> Running in 368cfdfd00e2\n",
      "Collecting tensorflow-io\n",
      "  Downloading tensorflow_io-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (28.0 MB)\n",
      "      28.0/28.0 MB 75.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.32.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-io) (0.32.0)\n",
      "Installing collected packages: tensorflow-io\n",
      "Successfully installed tensorflow-io-0.32.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 368cfdfd00e2\n",
      " ---> 132e0b9ac9fa\n",
      "Step 13/17 : RUN pip3 install tf-agents==0.17.0\n",
      " ---> Running in 31a65cfaa8ee\n",
      "Collecting tf-agents==0.17.0\n",
      "  Downloading tf_agents-0.17.0-py3-none-any.whl (1.4 MB)\n",
      "      1.4/1.4 MB 34.3 MB/s eta 0:00:00\n",
      "Collecting tensorflow-probability~=0.20.1\n",
      "  Downloading tensorflow_probability-0.20.1-py2.py3-none-any.whl (6.9 MB)\n",
      "      6.9/6.9 MB 72.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/site-packages (from tf-agents==0.17.0) (1.4.0)\n",
      "Collecting gym<=0.23.0,>=0.17.0\n",
      "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
      "      624.4/624.4 kB 58.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "\u001b[91mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/pygame/\n",
      "\u001b[0mCollecting pygame==2.1.3\n",
      "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
      "      13.7/13.7 MB 104.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/site-packages (from tf-agents==0.17.0) (1.16.0)\n",
      "Collecting typing-extensions<4.6.0,>=3.7.4.3\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting gin-config>=0.4.0\n",
      "  Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "      61.3/61.3 kB 15.1 MB/s eta 0:00:00\n",
      "Collecting pillow\n",
      "  Downloading Pillow-10.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "      3.4/3.4 MB 73.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/site-packages (from tf-agents==0.17.0) (1.23.5)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/site-packages (from tf-agents==0.17.0) (1.14.1)\n",
      "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/site-packages (from tf-agents==0.17.0) (4.23.4)\n",
      "Collecting cloudpickle>=1.3\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Collecting decorator\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/site-packages (from tensorflow-probability~=0.20.1->tf-agents==0.17.0) (0.4.0)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "      152.8/152.8 kB 33.3 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697643 sha256=0f81d8471a2be7b8ae84144407473c7b2835e782076ff13239c489dc905f51fc\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, gin-config, dm-tree, typing-extensions, pygame, pillow, decorator, cloudpickle, tensorflow-probability, gym, tf-agents\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "Successfully installed cloudpickle-2.2.1 decorator-5.1.1 dm-tree-0.1.8 gin-config-0.5.0 gym-0.23.0 gym-notices-0.0.8 pillow-10.0.0 pygame-2.1.3 tensorflow-probability-0.20.1 tf-agents-0.17.0 typing-extensions-4.5.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 31a65cfaa8ee\n",
      " ---> dc3f0bb3c7e9\n",
      "Step 14/17 : RUN pip3 install matplotlib\n",
      " ---> Running in 5de565049a14\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "      11.6/11.6 MB 97.8 MB/s eta 0:00:00\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.41.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "      4.3/4.3 MB 99.6 MB/s eta 0:00:00\n",
      "Collecting pyparsing<3.1,>=2.3.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "      98.3/98.3 kB 21.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/site-packages (from matplotlib) (1.23.5)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/site-packages (from matplotlib) (10.0.0)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "      300.7/300.7 kB 44.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "      1.6/1.6 MB 91.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.1.0 cycler-0.11.0 fonttools-4.41.0 kiwisolver-1.4.4 matplotlib-3.7.2 pyparsing-3.0.9\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 5de565049a14\n",
      " ---> 00623a98c845\n",
      "Step 15/17 : RUN pip3 install urllib3\n",
      " ---> Running in a369e9c12819\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/site-packages (1.26.16)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container a369e9c12819\n",
      " ---> d7183632136f\n",
      "Step 16/17 : COPY src/per_arm_rl /root/src/per_arm_rl\n",
      " ---> d3b4639f67e3\n",
      "Step 17/17 : ENTRYPOINT [\"python3\", \"-m\", \"src.per_arm_rl.task\"]\n",
      " ---> Running in ccfe00b4dd35\n",
      "Removing intermediate container ccfe00b4dd35\n",
      " ---> f4508d28d68a\n",
      "Successfully built f4508d28d68a\n",
      "Successfully tagged gcr.io/hybrid-vertex/hptuning-training-custom-container:latest\n",
      "PUSH\n",
      "Pushing gcr.io/hybrid-vertex/hptuning-training-custom-container\n",
      "The push refers to repository [gcr.io/hybrid-vertex/hptuning-training-custom-container]\n",
      "e4db5069b5e0: Preparing\n",
      "96148d16178a: Preparing\n",
      "553e11a8f5cb: Preparing\n",
      "b3b0c3724449: Preparing\n",
      "9e6ea3979bdd: Preparing\n",
      "9429173f8343: Preparing\n",
      "01d4ffb14cb2: Preparing\n",
      "0423d0d9f262: Preparing\n",
      "92bb8f08441b: Preparing\n",
      "73347e062b26: Preparing\n",
      "3fc9c349f179: Preparing\n",
      "95dbb31e999c: Preparing\n",
      "b8711cf37a9b: Preparing\n",
      "a9a1dfa0b8c5: Preparing\n",
      "83026ecbbeb7: Preparing\n",
      "e49c94d01395: Preparing\n",
      "f1acaab90728: Preparing\n",
      "28218ecd8008: Preparing\n",
      "2f66f3254105: Preparing\n",
      "a72216901005: Preparing\n",
      "61581d479298: Preparing\n",
      "9429173f8343: Waiting\n",
      "01d4ffb14cb2: Waiting\n",
      "0423d0d9f262: Waiting\n",
      "92bb8f08441b: Waiting\n",
      "73347e062b26: Waiting\n",
      "3fc9c349f179: Waiting\n",
      "95dbb31e999c: Waiting\n",
      "b8711cf37a9b: Waiting\n",
      "28218ecd8008: Waiting\n",
      "a9a1dfa0b8c5: Waiting\n",
      "2f66f3254105: Waiting\n",
      "83026ecbbeb7: Waiting\n",
      "a72216901005: Waiting\n",
      "e49c94d01395: Waiting\n",
      "f1acaab90728: Waiting\n",
      "61581d479298: Waiting\n",
      "96148d16178a: Pushed\n",
      "e4db5069b5e0: Pushed\n",
      "9429173f8343: Pushed\n",
      "01d4ffb14cb2: Pushed\n",
      "92bb8f08441b: Pushed\n",
      "0423d0d9f262: Pushed\n",
      "553e11a8f5cb: Pushed\n",
      "95dbb31e999c: Pushed\n",
      "9e6ea3979bdd: Pushed\n",
      "a9a1dfa0b8c5: Layer already exists\n",
      "83026ecbbeb7: Layer already exists\n",
      "e49c94d01395: Layer already exists\n",
      "f1acaab90728: Layer already exists\n",
      "28218ecd8008: Layer already exists\n",
      "2f66f3254105: Layer already exists\n",
      "b3b0c3724449: Pushed\n",
      "a72216901005: Layer already exists\n",
      "61581d479298: Layer already exists\n",
      "3fc9c349f179: Pushed\n",
      "b8711cf37a9b: Pushed\n",
      "73347e062b26: Pushed\n",
      "latest: digest: sha256:009cf5d15992eaa768ef0fa65e0129ac2d286344bdf52b60f4acc8b0c5c2e734 size: 4742\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                                             STATUS\n",
      "938c0a4b-1db6-47aa-a5b6-ae935f9c9d7d  2023-07-14T13:41:35+00:00  4M14S     gs://hybrid-vertex_cloudbuild/source/1689342091.211716-cc5d9e55d5e84ea380ab6431bab7c0b6.tgz  gcr.io/hybrid-vertex/hptuning-training-custom-container (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --config cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION,_ARTIFACTS_DIR=$ARTIFACTS_DIR \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307bb759-0a14-4746-8d81-f846e318b1ed",
   "metadata": {},
   "source": [
    "## Submit (hpt) tuning job\n",
    "* Submit a hyperparameter training job with the custom container. Read more details for using Python packages as an alternative to using custom containers in the example shown [here](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#create)\n",
    "* Define the hyperparameter(s), max trial count, parallel trial count, parameter search algorithm, machine spec, accelerators, worker pool, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3b8fcea2-db92-42ec-98ab-5f42d91da2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPTUNING_RESULT_DIR  : hptuning\n",
      "HPTUNING_RESULT_FILE : result.json\n",
      "HPTUNING_RESULT_PATH : scale-perarm-hpt/run-20230714-130012/hptuning/result.json\n",
      "HPTUNING_RESULT_URI : gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230714-130012/hptuning/result.json\n"
     ]
    }
   ],
   "source": [
    "# Execute hyperparameter tuning instead of regular training\n",
    "RUN_HYPERPARAMETER_TUNING          = True\n",
    "TRAIN_WITH_BEST_HYPERPARAMETERS    = False  # Do not train.\n",
    "\n",
    "# Directory to store the best hyperparameter(s) in `BUCKET_NAME` and locally (temporarily)\n",
    "HPTUNING_RESULT_DIR                = \"hptuning\"\n",
    "HPTUNING_RESULT_FILE               = \"result.json\"\n",
    "HPTUNING_RESULT_PATH               = f\"{EXPERIMENT_NAME}/{RUN_NAME}/{HPTUNING_RESULT_DIR}/{HPTUNING_RESULT_FILE}\"\n",
    "HPTUNING_RESULT_URI                = f\"{BUCKET_URI}/{HPTUNING_RESULT_PATH}\"\n",
    "\n",
    "# HPTUNING_RESULT_PATH               = os.path.join(HPTUNING_RESULT_DIR, \"result.json\")\n",
    "\n",
    "print(f\"HPTUNING_RESULT_DIR  : {HPTUNING_RESULT_DIR}\")\n",
    "print(f\"HPTUNING_RESULT_FILE : {HPTUNING_RESULT_FILE}\")\n",
    "print(f\"HPTUNING_RESULT_PATH : {HPTUNING_RESULT_PATH}\")\n",
    "print(f\"HPTUNING_RESULT_URI  : {HPTUNING_RESULT_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3e816-d9f3-4b91-9327-fdc27eba54a4",
   "metadata": {},
   "source": [
    "### Accelerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1eea4d62-7fc0-4503-a47b-e4d81c9e9197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKER_MACHINE_TYPE           : n1-standard-16\n",
      "REPLICA_COUNT                 : 1\n",
      "ACCELERATOR_TYPE              : NVIDIA_TESLA_T4\n",
      "PER_MACHINE_ACCELERATOR_COUNT : 1\n",
      "DISTRIBUTE_STRATEGY           : single\n"
     ]
    }
   ],
   "source": [
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "# REPLICA_COUNT = 1\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# # REDUCTION_SERVER_COUNT = 0                                                      \n",
    "# # REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'single'\n",
    "\n",
    "WORKER_MACHINE_TYPE = 'n1-standard-16'\n",
    "REPLICA_COUNT = 1\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4' # NVIDIA_TESLA_T4 NVIDIA_TESLA_V100\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "DISTRIBUTE_STRATEGY = 'single'\n",
    "\n",
    "print(f\"WORKER_MACHINE_TYPE           : {WORKER_MACHINE_TYPE}\")\n",
    "print(f\"REPLICA_COUNT                 : {REPLICA_COUNT}\")\n",
    "print(f\"ACCELERATOR_TYPE              : {ACCELERATOR_TYPE}\")\n",
    "print(f\"PER_MACHINE_ACCELERATOR_COUNT : {PER_MACHINE_ACCELERATOR_COUNT}\")\n",
    "print(f\"DISTRIBUTE_STRATEGY           : {DISTRIBUTE_STRATEGY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5213898-3252-4eaf-a50f-1166e640a2d5",
   "metadata": {},
   "source": [
    "### TODO - Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd35da25-ef51-4fe3-8676-62d72af83a1b",
   "metadata": {},
   "source": [
    "### init Vertex SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7e30e962-435a-42d8-aefc-82c7cd2d3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID\n",
    "    , location=REGION\n",
    "    , staging_bucket=BUCKET_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4bc165-fd04-4b45-9e8a-a43702ffe9b6",
   "metadata": {},
   "source": [
    "### helper function: create training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "24a06982-6061-411b-8131-3ee3ef4373ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hyperparameter_tuning_job_sample(\n",
    "    project: str\n",
    "    , display_name: str\n",
    "    , image_uri: str\n",
    "    , args: List[str]\n",
    "    , max_trial_count: int\n",
    "    , parallel_trial_count: int\n",
    "    , location: str = \"us-central1\"\n",
    "    , api_endpoint: str = \"us-central1-aiplatform.googleapis.com\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a hyperparameter tuning job using a custom container.\n",
    "\n",
    "    Args:\n",
    "        project: GCP project ID.\n",
    "        display_name: GCP console display name for the hyperparameter tuning job in\n",
    "            Vertex AI.\n",
    "        image_uri: URI to the hyperparameter tuning container image in Container\n",
    "            Registry.\n",
    "        args: Arguments passed to the container.\n",
    "        location: Service location.\n",
    "        api_endpoint: API endpoint, eg. `<location>-aiplatform.googleapis.com`.\n",
    "\n",
    "    Returns:\n",
    "        A string of the hyperparameter tuning job ID.\n",
    "    \"\"\"\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    \n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "\n",
    "    # ====================================================\n",
    "    # study_spec\n",
    "    # ====================================================\n",
    "    # Metric based on which to evaluate which combination of hyperparameter(s) to choose\n",
    "    metric = {\n",
    "        \"metric_id\": \"final_average_return\"  # Metric you report to Vertex AI.\n",
    "        , \"goal\": aiplatform.gapic.StudySpec.MetricSpec.GoalType.MAXIMIZE,\n",
    "    }\n",
    "\n",
    "    # ====================================================\n",
    "    # Hyperparameter(s) to tune\n",
    "    # ====================================================\n",
    "    # training_loops = {\n",
    "    #     \"parameter_id\": \"training-loops\"\n",
    "    #     , \"discrete_value_spec\": {\"values\": [4, 16]}\n",
    "    #     , \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE\n",
    "    # }\n",
    "    steps_per_loop = {\n",
    "        \"parameter_id\": \"steps-per-loop\"\n",
    "        , \"discrete_value_spec\": {\"values\": [2, 4]}\n",
    "        , \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE\n",
    "    }\n",
    "    batch_size_hpt = {\n",
    "        \"parameter_id\": \"batch-size\"\n",
    "        , \"discrete_value_spec\": {\"values\": [8, 16]}\n",
    "        , \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE\n",
    "    }\n",
    "    # num_actions_hpt = {\n",
    "    #     \"parameter_id\": \"num-actions\"\n",
    "    #     , \"discrete_value_spec\": {\"values\": [5, 10, 25, 35]}\n",
    "    #     , \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE\n",
    "    # }\n",
    "\n",
    "    # ====================================================\n",
    "    # worker_pool_spec\n",
    "    # ====================================================\n",
    "    # machine_spec = {\n",
    "    #     \"machine_type\": \"n1-standard-32\"\n",
    "    #     , \"accelerator_type\": aiplatform.gapic.AcceleratorType.ACCELERATOR_TYPE_UNSPECIFIED\n",
    "    #     , \"accelerator_count\": None\n",
    "    # }\n",
    "    machine_spec = {\n",
    "        \"machine_type\": WORKER_MACHINE_TYPE    # \"n1-standard-16\"\n",
    "        , \"accelerator_type\": ACCELERATOR_TYPE # aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_T4\n",
    "        , \"accelerator_count\": PER_MACHINE_ACCELERATOR_COUNT\n",
    "    }\n",
    "    worker_pool_spec = {\n",
    "        \"machine_spec\": machine_spec\n",
    "        , \"replica_count\": REPLICA_COUNT\n",
    "        , \"container_spec\": {\n",
    "            \"image_uri\": image_uri\n",
    "            , \"args\": args\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # ====================================================\n",
    "    # hyperparameter_tuning_job\n",
    "    # ====================================================\n",
    "    hyperparameter_tuning_job = {\n",
    "        \"display_name\": display_name\n",
    "        , \"max_trial_count\": max_trial_count\n",
    "        , \"parallel_trial_count\": parallel_trial_count\n",
    "        , \"study_spec\": {\n",
    "            \"metrics\": [metric]\n",
    "            # , \"parameters\": [training_loops, steps_per_loop]\n",
    "            , \"parameters\": [batch_size_hpt, steps_per_loop] # num_actions_hpt\n",
    "            , \"algorithm\": aiplatform.gapic.StudySpec.Algorithm.RANDOM_SEARCH\n",
    "        }\n",
    "        , \"trial_job_spec\": {\"worker_pool_specs\": [worker_pool_spec]}\n",
    "    }\n",
    "    parent = f\"projects/{project}/locations/{location}\"\n",
    "\n",
    "    # ====================================================\n",
    "    # Create job via client\n",
    "    # ====================================================\n",
    "    response = client.create_hyperparameter_tuning_job(\n",
    "        parent=parent\n",
    "        , hyperparameter_tuning_job=hyperparameter_tuning_job\n",
    "    )\n",
    "    job_id = response.name.split(\"/\")[-1]\n",
    "    print(\"Job ID:\", job_id)\n",
    "    print(\"Job config:\", response)\n",
    "    \n",
    "#     # ====================================================\n",
    "#     # Create job via SDK\n",
    "#     # ====================================================\n",
    "#     metric_spec = {\"final_average_return\": \"maximize\"}\n",
    "    \n",
    "#     parameter_spec = {\n",
    "#         \"training-loops\": hpt.DiscreteParameterSpec(values=[4, 16], scale=\"linear\")\n",
    "#         , \"steps-per-loop\": hpt.DiscreteParameterSpec(values=[1, 2], scale=\"linear\")\n",
    "#     }\n",
    "#     my_custom_job = aiplatform.CustomJob(\n",
    "#         display_name=display_name\n",
    "#         , worker_pool_specs=worker_pool_spec\n",
    "#         , staging_bucket=ROOT_DIR\n",
    "#     )\n",
    "    \n",
    "#     hp_job = aiplatform.HyperparameterTuningJob(\n",
    "#         display_name=display_name\n",
    "#         , custom_job=my_custom_job\n",
    "#         , metric_spec=metric_spec\n",
    "#         , parameter_spec=parameter_spec\n",
    "#         , max_trial_count=hyperparameter_tuning_job[\"max_trial_count\"]\n",
    "#         , parallel_trial_count=hyperparameter_tuning_job[\"parallel_trial_count\"]\n",
    "#     )\n",
    "\n",
    "#     hp_job.run(sync=False)\n",
    "\n",
    "    return job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9943e85e-a149-439d-ba74-96a2492e2e3a",
   "metadata": {},
   "source": [
    "### set training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0bf22fa7-bf80-4748-8d94-8ef06b41fac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE       : 8\n",
      "TRAINING_LOOPS   : 20\n",
      "STEPS_PER_LOOP   : 6\n",
      "RANK_K           : 20\n",
      "NUM_ACTIONS      : 20\n",
      "PER_ARM          : False\n",
      "TIKHONOV_WEIGHT  : 0.001\n",
      "AGENT_ALPHA      : 10.0\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters.\n",
    "BATCH_SIZE       = 8       # Training and prediction batch size.\n",
    "TRAINING_LOOPS   = 20      # Number of training iterations.\n",
    "STEPS_PER_LOOP   = 6       # Number of driver steps per training iteration.\n",
    "\n",
    "# Set MovieLens simulation environment parameters.\n",
    "RANK_K           = 20      # Rank for matrix factorization in the MovieLens environment; also the observation dimension.\n",
    "NUM_ACTIONS      = 20      # Number of actions (movie items) to choose from.\n",
    "PER_ARM          = False   # Use the non-per-arm version of the MovieLens environment.\n",
    "\n",
    "# Set agent parameters.\n",
    "TIKHONOV_WEIGHT  = 0.001   # LinUCB Tikhonov regularization weight.\n",
    "AGENT_ALPHA      = 10.0    # LinUCB exploration parameter that multiplies the confidence intervals.\n",
    "\n",
    "print(f\"BATCH_SIZE       : {BATCH_SIZE}\")\n",
    "print(f\"TRAINING_LOOPS   : {TRAINING_LOOPS}\")\n",
    "print(f\"STEPS_PER_LOOP   : {STEPS_PER_LOOP}\")\n",
    "print(f\"RANK_K           : {RANK_K}\")\n",
    "print(f\"NUM_ACTIONS      : {NUM_ACTIONS}\")\n",
    "print(f\"PER_ARM          : {PER_ARM}\")\n",
    "print(f\"TIKHONOV_WEIGHT  : {TIKHONOV_WEIGHT}\")\n",
    "print(f\"AGENT_ALPHA      : {AGENT_ALPHA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e7e44014-daa3-4e15-9137-8de3e7517288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"args: ['--data-path=gs://mabv1-hybrid-vertex-bucket/data', \"\n",
      " \"'--bucket_name=mabv1-hybrid-vertex-bucket', '--data_gcs_prefix=data', \"\n",
      " \"'--data_path=gs://mabv1-hybrid-vertex-bucket/data', \"\n",
      " \"'--project_number=934903580331', '--batch-size=8', '--rank-k=20', \"\n",
      " \"'--num-actions=20', '--tikhonov-weight=0.001', '--agent-alpha=10.0', \"\n",
      " \"'--training-loops=20', '--steps-per-loop=6', '--distribute=single', \"\n",
      " \"'--artifacts_dir=gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230714-130012/artifacts', \"\n",
      " \"'--run-hyperparameter-tuning']\")\n"
     ]
    }
   ],
   "source": [
    "args = [\n",
    "    f\"--data-path={DATA_PATH}\"               # TODO - remove duplicate arg\n",
    "    , f\"--bucket_name={BUCKET_NAME}\"\n",
    "    , f\"--data_gcs_prefix={DATA_GCS_PREFIX}\"\n",
    "    , f\"--data_path={DATA_PATH}\"\n",
    "    , f\"--project_number={PROJECT_NUM}\"\n",
    "    , f\"--batch-size={BATCH_SIZE}\"\n",
    "    , f\"--rank-k={RANK_K}\"\n",
    "    , f\"--num-actions={NUM_ACTIONS}\"\n",
    "    , f\"--tikhonov-weight={TIKHONOV_WEIGHT}\"\n",
    "    , f\"--agent-alpha={AGENT_ALPHA}\"\n",
    "    , f\"--training-loops={TRAINING_LOOPS}\"\n",
    "    , f\"--steps-per-loop={STEPS_PER_LOOP}\"\n",
    "    , f\"--distribute={DISTRIBUTE_STRATEGY}\"\n",
    "    , f\"--artifacts_dir={ARTIFACTS_DIR}\"\n",
    "    , f\"--root_dir={ROOT_DIR}\"\n",
    "]\n",
    "\n",
    "if RUN_HYPERPARAMETER_TUNING:\n",
    "    args.append(\"--run-hyperparameter-tuning\")\n",
    "    \n",
    "elif TRAIN_WITH_BEST_HYPERPARAMETERS:\n",
    "    args.append(\"--train-with-best-hyperparameters\")\n",
    "    \n",
    "from pprint import pprint\n",
    "pprint(f\"args: {args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "77da3fea-4c70-4b85-8135-47d90b5484aa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 9007622635437162496\n",
      "Job config: name: \"projects/934903580331/locations/us-central1/hyperparameterTuningJobs/9007622635437162496\"\n",
      "display_name: \"mvl-hpt-job-run-20230714-130012\"\n",
      "study_spec {\n",
      "  metrics {\n",
      "    metric_id: \"final_average_return\"\n",
      "    goal: MAXIMIZE\n",
      "  }\n",
      "  parameters {\n",
      "    parameter_id: \"batch-size\"\n",
      "    discrete_value_spec {\n",
      "      values: 8.0\n",
      "      values: 16.0\n",
      "    }\n",
      "    scale_type: UNIT_LINEAR_SCALE\n",
      "  }\n",
      "  parameters {\n",
      "    parameter_id: \"steps-per-loop\"\n",
      "    discrete_value_spec {\n",
      "      values: 2.0\n",
      "      values: 4.0\n",
      "    }\n",
      "    scale_type: UNIT_LINEAR_SCALE\n",
      "  }\n",
      "  algorithm: RANDOM_SEARCH\n",
      "}\n",
      "max_trial_count: 4\n",
      "parallel_trial_count: 2\n",
      "trial_job_spec {\n",
      "  worker_pool_specs {\n",
      "    machine_spec {\n",
      "      machine_type: \"n1-standard-16\"\n",
      "      accelerator_type: NVIDIA_TESLA_T4\n",
      "      accelerator_count: 1\n",
      "    }\n",
      "    replica_count: 1\n",
      "    disk_spec {\n",
      "      boot_disk_type: \"pd-ssd\"\n",
      "      boot_disk_size_gb: 100\n",
      "    }\n",
      "    container_spec {\n",
      "      image_uri: \"gcr.io/hybrid-vertex/hptuning-training-custom-container:latest\"\n",
      "      args: \"--data-path=gs://mabv1-hybrid-vertex-bucket/data\"\n",
      "      args: \"--bucket_name=mabv1-hybrid-vertex-bucket\"\n",
      "      args: \"--data_gcs_prefix=data\"\n",
      "      args: \"--data_path=gs://mabv1-hybrid-vertex-bucket/data\"\n",
      "      args: \"--project_number=934903580331\"\n",
      "      args: \"--batch-size=8\"\n",
      "      args: \"--rank-k=20\"\n",
      "      args: \"--num-actions=20\"\n",
      "      args: \"--tikhonov-weight=0.001\"\n",
      "      args: \"--agent-alpha=10.0\"\n",
      "      args: \"--training-loops=20\"\n",
      "      args: \"--steps-per-loop=6\"\n",
      "      args: \"--distribute=single\"\n",
      "      args: \"--artifacts_dir=gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230714-130012/artifacts\"\n",
      "      args: \"--run-hyperparameter-tuning\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "state: JOB_STATE_PENDING\n",
      "create_time {\n",
      "  seconds: 1689340019\n",
      "  nanos: 563863000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1689340019\n",
      "  nanos: 563863000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'9007622635437162496'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_id = create_hyperparameter_tuning_job_sample(\n",
    "    project=PROJECT_ID\n",
    "    , display_name=f\"mvl-hpt-job-{RUN_NAME}\"\n",
    "    , image_uri=f\"gcr.io/{PROJECT_ID}/{HPTUNING_TRAINING_CONTAINER}:latest\"\n",
    "    , args=args\n",
    "    , max_trial_count = 4\n",
    "    , parallel_trial_count = 2\n",
    "    , location=REGION\n",
    "    , api_endpoint=f\"{REGION}-aiplatform.googleapis.com\"\n",
    ")\n",
    "\n",
    "job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a471d3-ab80-437c-a35f-b523fb2ab545",
   "metadata": {},
   "source": [
    "#### Check hyperparameter tuning job status\n",
    "* Read more about managing jobs [here](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#manage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "349823d9-3822-47f0-b2bd-0813c42f5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameter_tuning_job_sample(\n",
    "    project: str,\n",
    "    hyperparameter_tuning_job_id: str,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    ") -> aiplatform.HyperparameterTuningJob:\n",
    "    \"\"\"\n",
    "    Gets the current status of a hyperparameter tuning job.\n",
    "\n",
    "    Args:\n",
    "        project: GCP project ID.\n",
    "        hyperparameter_tuning_job_id: Hyperparameter tuning job ID.\n",
    "        location: Service location.\n",
    "        api_endpoint: API endpoint, eg. `-aiplatform.googleapis.com`.\n",
    "\n",
    "    Returns:\n",
    "        Details of the hyperparameter tuning job, such as its running status,\n",
    "        results of its trials, etc.\n",
    "    \"\"\"\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    \n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "    \n",
    "    name = client.hyperparameter_tuning_job_path(\n",
    "        project=project\n",
    "        , location=location\n",
    "        , hyperparameter_tuning_job=hyperparameter_tuning_job_id\n",
    "    )\n",
    "    \n",
    "    response = client.get_hyperparameter_tuning_job(name=name)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5bb83155-4edb-4424-b28e-a801771b0e38",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job succeeded.\n",
      "Job Time: 0:16:36.929684\n",
      "Trials: [id: \"1\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"batch-size\"\n",
      "  value {\n",
      "    number_value: 8.0\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"steps-per-loop\"\n",
      "  value {\n",
      "    number_value: 4.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"final_average_return\"\n",
      "    value: 1.2964732646942139\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1689340036\n",
      "  nanos: 175740133\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1689340194\n",
      "}\n",
      ", id: \"2\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"batch-size\"\n",
      "  value {\n",
      "    number_value: 8.0\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"steps-per-loop\"\n",
      "  value {\n",
      "    number_value: 4.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"final_average_return\"\n",
      "    value: 1.2616862058639526\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1689340036\n",
      "  nanos: 175927645\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1689340195\n",
      "}\n",
      ", id: \"3\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"batch-size\"\n",
      "  value {\n",
      "    number_value: 16.0\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"steps-per-loop\"\n",
      "  value {\n",
      "    number_value: 2.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"final_average_return\"\n",
      "    value: 0.8446623086929321\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1689340542\n",
      "  nanos: 787242052\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1689340697\n",
      "}\n",
      ", id: \"4\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"batch-size\"\n",
      "  value {\n",
      "    number_value: 16.0\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"steps-per-loop\"\n",
      "  value {\n",
      "    number_value: 2.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"final_average_return\"\n",
      "    value: 0.5608248114585876\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1689340542\n",
      "  nanos: 787388620\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1689340698\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "trials = None\n",
    "\n",
    "while True:\n",
    "    response = get_hyperparameter_tuning_job_sample(\n",
    "        project=PROJECT_ID\n",
    "        , hyperparameter_tuning_job_id=job_id\n",
    "        , location=REGION\n",
    "        , api_endpoint=f\"{REGION}-aiplatform.googleapis.com\"\n",
    "    )\n",
    "    \n",
    "    if response.state.name == 'JOB_STATE_SUCCEEDED':\n",
    "        print(\"Job succeeded.\\nJob Time:\", response.update_time - response.create_time)\n",
    "        trials = response.trials\n",
    "        print(\"Trials:\", trials)\n",
    "        break\n",
    "    elif response.state.name == \"JOB_STATE_FAILED\":\n",
    "        print(\"Job failed.\")\n",
    "        break\n",
    "    elif response.state.name == \"JOB_STATE_CANCELLED\":\n",
    "        print(\"Job cancelled.\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Current job status: {response.state.name}.\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a06fed-f0e3-4136-8e11-913455cf0485",
   "metadata": {},
   "source": [
    "#### Find the best combination(s) hyperparameter(s) for each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0f47b4ad-8a43-4ca3-b78b-d24d23ccc20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter value(s):\n",
      "Metric=final_average_return: [{'batch-size': 16.0, 'steps-per-loop': 2.0}]\n"
     ]
    }
   ],
   "source": [
    "if trials:\n",
    "    # Dict mapping from metric names to the best metric values seen so far\n",
    "    best_objective_values = dict.fromkeys(\n",
    "        [metric.metric_id for metric in trials[0].final_measurement.metrics]\n",
    "        , -np.inf\n",
    "    )\n",
    "    # Dict mapping from metric names to a list of the best combination(s) of\n",
    "    # hyperparameter(s). Each combination is a dict mapping from hyperparameter\n",
    "    # names to their values.\n",
    "    best_params = defaultdict(list)\n",
    "    for trial in trials:\n",
    "        # `final_measurement` and `parameters` are `RepeatedComposite` objects.\n",
    "        # Reference the structure above to extract the value of your interest.\n",
    "        for metric in trial.final_measurement.metrics:\n",
    "            params = {\n",
    "                param.parameter_id: param.value for param in trial.parameters\n",
    "            }\n",
    "            if metric.value > best_objective_values[metric.metric_id]:\n",
    "                best_params[metric.metric_id] = [params]\n",
    "            elif metric.value == best_objective_values[metric.metric_id]:\n",
    "                best_params[param.parameter_id].append(params)  # Handle cases where multiple hyperparameter values lead to the same performance.\n",
    "    print(\"Best hyperparameter value(s):\")\n",
    "    for metric, params in best_params.items():\n",
    "        print(f\"Metric={metric}: {sorted(params)}\")\n",
    "else:\n",
    "    print(\"No hyperparameter tuning job trials found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f615339b-fd88-4239-9c67-27157ade7ea6",
   "metadata": {},
   "source": [
    "#### Convert a combination of best hyperparameter(s) for a metric of interest to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0574bda0-f1f9-438f-b4b9-67754a769bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf $HPTUNING_RESULT_DIR\n",
    "# ! mkdir $HPTUNING_RESULT_DIR\n",
    " \n",
    "LOCAL_RESULTS_FILE = \"result.json\"  # {\"batch-size\": 8.0, \"steps-per-loop\": 2.0}\n",
    "\n",
    "with open(LOCAL_RESULTS_FILE, \"w\") as f:\n",
    "    json.dump(best_params[\"final_average_return\"][0], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a63794-61bb-415a-b08d-aebbb35abeb2",
   "metadata": {},
   "source": [
    "#### Upload the best hyperparameter(s) to GCS for use in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2a3f7eab-f1ff-4cea-8359-b17516d584ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230714-130012/hptuning/result.json\n"
     ]
    }
   ],
   "source": [
    "!gsutil -q cp $LOCAL_RESULTS_FILE $HPTUNING_RESULT_URI\n",
    "\n",
    "!gsutil ls $HPTUNING_RESULT_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc06d414-96dc-4f00-a9f9-f66f1e00669c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create custom prediction container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19025cc9-4b13-4c87-a138-395da7d92388",
   "metadata": {},
   "source": [
    "As with training, create a custom prediction container. This container handles the TF-Agents specific logic that is different from a regular TensorFlow Model. Specifically, it finds the predicted action using a trained policy. The associated source code is in `src/prediction/`.\n",
    "See other options for Vertex AI predictions [here](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions).\n",
    "\n",
    "#### Serve predictions:\n",
    "- Use [`tensorflow.saved_model.load`](https://www.tensorflow.org/agents/api_docs/python/tf_agents/policies/PolicySaver#usage), instead of [`tf_agents.policies.policy_loader.load`](https://github.com/tensorflow/agents/blob/r0.8.0/tf_agents/policies/policy_loader.py#L26), to load the trained policy, because the latter produces an object of type [`SavedModelPyTFEagerPolicy`](https://github.com/tensorflow/agents/blob/402b8aa81ca1b578ec1f687725d4ccb4115386d2/tf_agents/policies/py_tf_eager_policy.py#L137) whose `action()` is not compatible for use here.\n",
    "- Note that prediction requests contain only observation data but not reward. This is because: The prediction task is a standalone request that doesn't require prior knowledge of the system state. Meanwhile, end users only know what they observe at the moment. Reward is a piece of information that comes after the action has been made, so the end users would not have knowledge of said reward. In handling prediction requests, you create a [`TimeStep`](https://www.tensorflow.org/agents/api_docs/python/tf_agents/trajectories/TimeStep) object (consisting of `observation`, `reward`, `discount`, `step_type`) using the [`restart()`](https://www.tensorflow.org/agents/api_docs/python/tf_agents/trajectories/restart) function which takes in an `observation`. This function creates the *first* TimeStep in a trajectory of steps, where reward is 0, discount is 1 and step_type is marked as the first timestep. In other words, each prediction request forms the first `TimeStep` in a brand new trajectory.\n",
    "- For the prediction response, avoid using NumPy-typed values; instead, convert them to native Python values using methods such as [`tolist()`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.tolist.html) as opposed to `list()`.\n",
    "- There exists a prestart script in `src/prediction`. FastAPI executes this script before starting up the server. The `PORT` environment variable is set to equal `AIP_HTTP_PORT` in order to run FastAPI on the same port expected by Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "57f6f6b1-cc85-44f5-94b3-4d6ee802a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_SUBFOLDER = 'prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c454ef00-550a-4d79-84c0-e5922577b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{PRED_SUBFOLDER}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{PRED_SUBFOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "55c5c86a-0faa-4b9e-a70d-e47c8a104af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/prediction/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PRED_SUBFOLDER}/main.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Prediction server that uses a trained policy to give predicted actions.\"\"\"\n",
    "import os\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi import Request\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_agents\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "_model = tf.compat.v2.saved_model.load(os.environ[\"AIP_STORAGE_URI\"])\n",
    "\n",
    "\n",
    "@app.get(os.environ[\"AIP_HEALTH_ROUTE\"], status_code=200)\n",
    "def health():\n",
    "    \"\"\"\n",
    "    Handles server health check requests.\n",
    "\n",
    "    Returns:\n",
    "      An empty dict.\n",
    "    \"\"\"\n",
    "    return {}\n",
    "\n",
    "\n",
    "@app.post(os.environ[\"AIP_PREDICT_ROUTE\"])\n",
    "async def predict(request: Request):\n",
    "    \"\"\"\n",
    "    Handles prediction requests.\n",
    "\n",
    "    Unpacks observations in prediction requests and queries the trained policy for\n",
    "    predicted actions.\n",
    "\n",
    "    Args:\n",
    "      request: Incoming prediction requests that contain observations.\n",
    "\n",
    "    Returns:\n",
    "      A dict with the key `predictions` mapping to a list of predicted actions\n",
    "      corresponding to each observation in the prediction request.\n",
    "    \"\"\"\n",
    "    body = await request.json()\n",
    "    instances = body[\"instances\"]\n",
    "\n",
    "    predictions = []\n",
    "    for index, instance in enumerate(instances):\n",
    "        # Unpack request body and reconstruct TimeStep. Rewards default to 0.\n",
    "        batch_size = len(instance[\"observation\"])\n",
    "        \n",
    "        time_step = tf_agents.trajectories.restart(\n",
    "            observation=instance[\"observation\"]\n",
    "            , batch_size=tf.convert_to_tensor([batch_size])\n",
    "        )\n",
    "        policy_step = _model.action(time_step)\n",
    "\n",
    "        predictions.append(\n",
    "            {f\"PolicyStep {index}\": policy_step.action.numpy().tolist()}\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"predictions\": predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b8e3a99d-a3c7-414c-a074-2d1c415218cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/prediction/prestart.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PRED_SUBFOLDER}/prestart.sh\n",
    "#!/bin/bash\n",
    "export PORT=$AIP_HTTP_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b1a2fd-689d-441e-b309-b43ba36246f5",
   "metadata": {},
   "source": [
    "#### Define dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "65838998-455d-4168-be00-54b04512b59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pred_requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile pred_requirements.txt\n",
    "tf-agents==0.17.0\n",
    "tensorflow==2.12.0\n",
    "numpy\n",
    "six\n",
    "typing-extensions\n",
    "pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f58356e-938d-4199-a7be-1a384d174c5a",
   "metadata": {},
   "source": [
    "#### Write a Dockerfile\n",
    "\n",
    "Note: leave the server directory `app`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "cec9808f-cfe2-471f-826e-1b92f8696cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCKERNAME = 'pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "fe9e2c4b-5bf6-428a-bd32-196ecc30fc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile_pred\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile_{DOCKERNAME}\n",
    "\n",
    "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.10\n",
    "\n",
    "COPY src/prediction /app\n",
    "COPY pred_requirements.txt /app/requirements.txt\n",
    "\n",
    "RUN pip3 install -r /app/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba9f72-fcbb-4595-bc52-152cd3306ca3",
   "metadata": {},
   "source": [
    "#### Build the prediction container with Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "320eaaf5-b02a-4e02-88ed-6de388d75a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export DOCKERNAME=pred\n",
      "export IMAGE_URI=gcr.io/hybrid-vertex/prediction-custom-container\n",
      "export FILE_LOCATION=./\n",
      "export MACHINE_TYPE=e2-highcpu-32\n",
      "export ARTIFACTS_DIR=gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230714-130012/artifacts\n"
     ]
    }
   ],
   "source": [
    "PREDICTION_CONTAINER = \"prediction-custom-container\"\n",
    "\n",
    "# Docker definitions for training\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{PREDICTION_CONTAINER}'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './'\n",
    "\n",
    "print(f\"export DOCKERNAME={DOCKERNAME}\")\n",
    "print(f\"export IMAGE_URI={IMAGE_URI}\")\n",
    "print(f\"export FILE_LOCATION={FILE_LOCATION}\")\n",
    "print(f\"export MACHINE_TYPE={MACHINE_TYPE}\")\n",
    "print(f\"export ARTIFACTS_DIR={ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "cc09f920-69fb-4886-82ae-0e5b5fd20652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! gcloud builds submit --config cloudbuild.yaml \\\n",
    "#     --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION,_ARTIFACTS_DIR=$ARTIFACTS_DIR \\\n",
    "#     --timeout=2h \\\n",
    "#     --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe724c1-2dcb-4d10-8cec-fb56fb088120",
   "metadata": {},
   "source": [
    "## Submit custom container training job\n",
    "\n",
    "- Note again that the bucket must be in the same regional location as the service location and it should not be multi-regional.\n",
    "- Read more of CustomContainerTrainingJob's source code [here](https://github.com/googleapis/python-aiplatform/blob/v0.8.0/google/cloud/aiplatform/training_jobs.py#L2153).\n",
    "- Like with local execution, you can use TensorBoard Profiler to track the training process and resources, and visualize the corresponding artifacts using the command: `%tensorboard --logdir $PROFILER_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "421336fc-8031-45c4-afb1-3579e7b95d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : scale-perarm-hpt\n",
      "RUN_NAME          : run-20230714-134801\n",
      "LOG_DIR           : gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230714-134801/tb-logs\n",
      "ROOT_DIR          : gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230714-134801/root\n",
      "ARTIFACTS_DIR     : gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230714-134801/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'scale-perarm-hpt'\n",
    "\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "LOG_DIR           = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/tb-logs\"\n",
    "ROOT_DIR          = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "98865883-8f1e-49c8-8172-82fcfa6dfe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "59c73f27-f79b-403e-9b77-1bfdc843c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_HYPERPARAMETER_TUNING       = False  # Execute regular training instead of hyperparameter tuning.\n",
    "TRAIN_WITH_BEST_HYPERPARAMETERS = True   # @param {type:\"bool\"} Whether to use learned hyperparameters in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "ccca6c0b-26a2-479e-a386-94a39c199f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters.\n",
    "BATCH_SIZE       = 8         # Training and prediction batch size.\n",
    "TRAINING_LOOPS   = 100       # Number of training iterations.\n",
    "STEPS_PER_LOOP   = 2         # Number of driver steps per training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "c28c490b-6764-4bc7-8552-10fea73a0d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"args: ['--data-path=gs://mabv1-hybrid-vertex-bucket/data', \"\n",
      " \"'--bucket_name=mabv1-hybrid-vertex-bucket', '--data_gcs_prefix=data', \"\n",
      " \"'--data_path=gs://mabv1-hybrid-vertex-bucket/data', \"\n",
      " \"'--project_number=934903580331', '--batch-size=8', '--rank-k=20', \"\n",
      " \"'--num-actions=20', '--tikhonov-weight=0.001', '--agent-alpha=10.0', \"\n",
      " \"'--training-loops=100', '--steps-per-loop=2', '--distribute=single', \"\n",
      " \"'--artifacts_dir=gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230714-134801/artifacts', \"\n",
      " \"'--root_dir=gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230714-134801/root', \"\n",
      " \"'--train-with-best-hyperparameters', \"\n",
      " \"'--best-hyperparameters-bucket=mabv1-hybrid-vertex-bucket', \"\n",
      " \"'--best-hyperparameters-path=scale-perarm-hpt/run-20230714-130012/hptuning/result.json']\")\n"
     ]
    }
   ],
   "source": [
    "args = [\n",
    "    f\"--data-path={DATA_PATH}\"               # TODO - remove duplicate arg\n",
    "    , f\"--bucket_name={BUCKET_NAME}\"\n",
    "    , f\"--data_gcs_prefix={DATA_GCS_PREFIX}\"\n",
    "    , f\"--data_path={DATA_PATH}\"\n",
    "    , f\"--project_number={PROJECT_NUM}\"\n",
    "    , f\"--batch-size={BATCH_SIZE}\"\n",
    "    , f\"--rank-k={RANK_K}\"\n",
    "    , f\"--num-actions={NUM_ACTIONS}\"\n",
    "    , f\"--tikhonov-weight={TIKHONOV_WEIGHT}\"\n",
    "    , f\"--agent-alpha={AGENT_ALPHA}\"\n",
    "    , f\"--training-loops={TRAINING_LOOPS}\"\n",
    "    , f\"--steps-per-loop={STEPS_PER_LOOP}\"\n",
    "    , f\"--distribute={DISTRIBUTE_STRATEGY}\"\n",
    "    , f\"--artifacts_dir={ARTIFACTS_DIR}\"\n",
    "    , f\"--root_dir={ROOT_DIR}\"\n",
    "]\n",
    "\n",
    "if RUN_HYPERPARAMETER_TUNING:\n",
    "    args.append(\"--run-hyperparameter-tuning\")\n",
    "elif TRAIN_WITH_BEST_HYPERPARAMETERS:\n",
    "    args.append(\"--train-with-best-hyperparameters\")\n",
    "    args.append(f\"--best-hyperparameters-bucket={BUCKET_NAME}\")\n",
    "    args.append(f\"--best-hyperparameters-path={HPTUNING_RESULT_PATH}\")\n",
    "    \n",
    "from pprint import pprint\n",
    "pprint(f\"args: {args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "d60f019a-87c8-4e51-9178-d20ff648d216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Spec: container_spec {\n",
      "  image_uri: \"gcr.io/hybrid-vertex/prediction-custom-container:latest\"\n",
      "  predict_route: \"/predict\"\n",
      "  health_route: \"/health\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=f\"mvl-train-job-{RUN_NAME}\"\n",
    "    , container_uri=f\"gcr.io/{PROJECT_ID}/{HPTUNING_TRAINING_CONTAINER}:latest\"\n",
    "    , command=[\"python3\", \"-m\", \"src.per_arm_rl.task\"] + args  # Pass in training arguments, including hyperparameters.\n",
    "    , model_serving_container_image_uri=f\"gcr.io/{PROJECT_ID}/{PREDICTION_CONTAINER}:latest\"\n",
    "    , model_serving_container_predict_route=\"/predict\"\n",
    "    , model_serving_container_health_route=\"/health\"\n",
    ")\n",
    "\n",
    "print(\"Training Spec:\", job._managed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "1ac5e9dd-1557-4d61-8e95-b27a4200211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = job.run(\n",
    "    model_display_name = f\"{PREFIX}-perarm-model\"\n",
    "    , replica_count = 1\n",
    "    , machine_type = \"n1-standard-16\"\n",
    "    , accelerator_type = \"ACCELERATOR_TYPE_UNSPECIFIED\"     # ACCELERATOR_TYPE\n",
    "    # , tensorboard=TENSORBOARD                             # TODO\n",
    "    , accelerator_count = 0\n",
    "    , enable_web_access = True\n",
    "    , restart_job_on_worker_restart = False\n",
    "    , sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "db62f96b-b436-4342-b751-a54b5df08eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model display name: mabv1-perarm-model\n",
      "Model ID: 7981005261328351232\n"
     ]
    }
   ],
   "source": [
    "print(\"Model display name:\", model.display_name)\n",
    "print(\"Model ID:\", model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8878e9-399d-4c10-a0d6-4d6dc4ea3be7",
   "metadata": {},
   "source": [
    "### Deploy trained model to an Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "6ca9d1e0-492e-42c6-ba55-d80deaad65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "2b501650-6f91-4365-ba59-d4e84ee6f7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint display name: mabv1-perarm-model_endpoint\n",
      "Endpoint ID: 3299374910211620864\n"
     ]
    }
   ],
   "source": [
    "print(\"Endpoint display name:\", endpoint.display_name)\n",
    "print(\"Endpoint ID:\", endpoint.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb45c9-2482-4338-bd2c-c6d5e142018c",
   "metadata": {},
   "source": [
    "### Predict on the Endpoint\n",
    "- Put prediction input(s) into a list named `instances`. The observation should of dimension (BATCH_SIZE, RANK_K). Read more about the MovieLens simulation environment observation [here](https://github.com/tensorflow/agents/blob/v0.8.0/tf_agents/bandits/environments/movielens_py_environment.py#L32-L138).\n",
    "- Read more about the endpoint prediction API [here](https://cloud.google.com/sdk/gcloud/reference/ai/endpoints/predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "758e7a3b-80a6-4b74-9db1-c2675c6f37f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "\n",
    "# endpoint.predict(\n",
    "#     instances=[\n",
    "#         {\"observation\": [list(np.ones(20)) for _ in range(8)]},\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9634a-80d0-4058-a88d-aa0e566573ba",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67abc70e-6546-45e2-aba2-c1e65ba8b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete endpoint resource\n",
    "# ! gcloud ai endpoints delete $endpoint.name --quiet --region $REGION\n",
    "\n",
    "# # Delete model resource\n",
    "# ! gcloud ai models delete $model.name --quiet\n",
    "\n",
    "# # Delete Cloud Storage objects that were created\n",
    "# ! gsutil -m rm -r $ARTIFACTS_DIR"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
