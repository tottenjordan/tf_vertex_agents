{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01cffb8-8045-4314-b56a-8ac9154c6066",
   "metadata": {},
   "source": [
    "# Scale per-arm Banidt training with Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c6cb3-7970-4fab-a102-8c8749ecd3fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29580d03-390d-4d90-a5c0-14c16338ddbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Notebook Objectives:\n",
    "* Create hyperparameter tuning and training custom container\n",
    "* Submit hyperparameter tuning job (optional)\n",
    "* Create custom prediction container\n",
    "* Submit custom container training job\n",
    "* Deploy trained model to Endpoint\n",
    "* Predict on the Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64261791-880e-41c7-b6e9-a39698c66d51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**TODO** - fix vars -Create hyperparameter tuning and training custom container\n",
    "\n",
    "Create a custom container that can be used for both hyperparameter tuning and training. The associated source code is in `src/per_arm_rl/`. This serves as the inner script of the custom container.\n",
    "As before, the training function is the same as [trainer.train](https://github.com/tensorflow/agents/blob/r0.8.0/tf_agents/bandits/agents/examples/v2/trainer.py#L104), but it keeps track of intermediate metric values, supports hyperparameter tuning, and (for training) saves artifacts to different locations. The training logic for hyperparameter tuning and training is the same.\n",
    "\n",
    "**Execute hyperparameter tuning:**\n",
    "* The code does not save model artifacts. It takes in command-line arguments as hyperparameter values from the Vertex AI Hyperparameter Tuning service, and reports training result metric to Vertex AI at each trial using cloudml-hypertune.\n",
    "* Note that if you decide to save model artifacts, saving them to the same directory may cause overwriting errors if you use parallel trials in the hyperparameter tuning job. The recommended approach is to save each trial's artifacts to a different sub-directory. This would also allow you to recover all the artifacts from different trials and can potentially save you from re-training.\n",
    "* Read more about hyperparameter tuning for custom containers [here](https://cloud.google.com/vertex-ai/docs/training/containers-overview#hyperparameter_tuning_with_custom_containers); read about hyperparameter tuning support [here](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview).\n",
    "\n",
    "**Execute training:**\n",
    "* The code saves model artifacts to `os.environ[\"AIP_MODEL_DIR\"]` in addition to `ARTIFACTS_DIR`, as required [here](https://github.com/googleapis/python-aiplatform/blob/v0.8.0/google/cloud/aiplatform/training_jobs.py#L2202).\n",
    "* If you want to make changes to the function, make sure to still save the trained policy as a SavedModel to clean directories\n",
    "* avoid saving checkpoints and other artifacts, so that deploying the model to endpoints works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ea275-15ba-40b6-acb1-94d5d18996b1",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2ae96-b5b5-4b10-9d9c-0b00d533ef46",
   "metadata": {},
   "source": [
    "### set vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd715ec5-f0fb-432b-bef4-a05a57586c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'mabv1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd6fddd-38f5-4e8f-9beb-54b7cc631290",
   "metadata": {},
   "source": [
    "**run the next cell to populate env vars**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82643015-091b-444a-83a6-e24100a48a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"mabv1\"\n",
      "VERSION                  = \"v1\"\n",
      "\n",
      "BUCKET_NAME              = \"mabv1-hybrid-vertex-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://mabv1-hybrid-vertex-bucket/data\"\n",
      "BUCKET_URI               = \"gs://mabv1-hybrid-vertex-bucket\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_ID      = \"hybrid-vertex.movielens_dataset_mabv1\"\n",
      "BIGQUERY_TABLE_ID        = \"hybrid-vertex.movielens_dataset_mabv1.training_dataset\"\n",
      "\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "RL_SUB_DIR               = \"per_arm_rl\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ab55c-3ca8-4348-b902-749b19a6c57f",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e52ce64-2afe-4d2f-a43d-68775ee7cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce8b63e1-84a3-4417-adf4-ff3f0dfe9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents import TFAgent\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "from tf_agents.bandits.environments import (environment_utilities,\n",
    "                                            movielens_py_environment)\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import TFEnvironment, tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.metrics.tf_metric import TFStepMetric\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "# my project\n",
    "from src.per_arm_rl import data_utils\n",
    "from src.per_arm_rl import data_config\n",
    "\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "63cd20c0-7446-49df-9a1e-41a89065e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4dda7603-ff83-4878-af18-66da744b9648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://mabv1-hybrid-vertex-bucket/data/ml-ratings-100k-train.tfrecord\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls $DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17376b28-6faf-4e03-baf3-73126a743e52",
   "metadata": {},
   "source": [
    "## Create training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f7d34603-7d82-4207-bd6a-7713a8db3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "# RL_SUB_DIR = 'per_arm_rl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e6fc1f84-11d0-4798-90ba-69f90f6be96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training subfolder\n",
    "# ! rm -rf {REPO_DOCKER_PATH_PREFIX}/{RL_SUB_DIR}\n",
    "# ! mkdir {REPO_DOCKER_PATH_PREFIX}/{RL_SUB_DIR}\n",
    "# ! touch {REPO_DOCKER_PATH_PREFIX}/{RL_SUB_DIR}/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15557c5-2608-4451-a0c3-5d9472204f8b",
   "metadata": {},
   "source": [
    "### train_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "45a3c1ee-e5c8-47e6-b245-fe17e6769be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/per_arm_rl/train_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{RL_SUB_DIR}/train_utils.py\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "def _is_chief(task_type, task_id): \n",
    "    ''' Check for primary if multiworker training\n",
    "    '''\n",
    "    if task_type == 'chief':\n",
    "        results = 'chief'\n",
    "    else:\n",
    "        results = None\n",
    "    return results\n",
    "\n",
    "def get_train_strategy(distribute_arg):\n",
    "\n",
    "    # Single Machine, single compute device\n",
    "    if distribute_arg == 'single':\n",
    "        if tf.config.list_physical_devices('GPU'):\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "        else:\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "        logging.info(\"Single device training\")  \n",
    "    # Single Machine, multiple compute device\n",
    "    elif distribute_arg == 'mirrored':\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        logging.info(\"Mirrored Strategy distributed training\")\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    # Multi Machine, multiple compute device\n",
    "    elif distribute_arg == 'multiworker':\n",
    "        strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "        logging.info(\"Multi-worker Strategy distributed training\")\n",
    "        logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "    # Single Machine, multiple TPU devices\n",
    "    elif distribute_arg == 'tpu':\n",
    "        cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\n",
    "        tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "        strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
    "        logging.info(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "\n",
    "    return strategy\n",
    "\n",
    "def prepare_worker_pool_specs(\n",
    "    image_uri,\n",
    "    args,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-16\",\n",
    "    accelerator_count=1,\n",
    "    accelerator_type=\"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type=\"n1-highcpu-16\",\n",
    "    reduction_server_image_uri=\"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\",\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\"machine_type\": machine_type}\n",
    "\n",
    "    container_spec = {\n",
    "        \"image_uri\": image_uri,\n",
    "        \"args\": args,\n",
    "    }\n",
    "\n",
    "    chief_spec = {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"container_spec\": container_spec,\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": replica_count - 1,\n",
    "            \"machine_spec\": machine_spec,\n",
    "            \"container_spec\": container_spec,\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": reduction_server_count,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": reduction_server_machine_type,\n",
    "            },\n",
    "            \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "\n",
    "    return worker_pool_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5c9208-061b-4df3-ab6e-f20399438e1c",
   "metadata": {},
   "source": [
    "### policy_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fc82a317-4b4e-4eb9-84ca-6735fda799b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/per_arm_rl/policy_util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{RL_SUB_DIR}/policy_util.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"The utility module for reinforcement learning policy.\"\"\"\n",
    "import collections\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents import TFAgent\n",
    "from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import TFEnvironment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.metrics.tf_metric import TFStepMetric\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "def train(\n",
    "    agent: TFAgent\n",
    "    , environment: TFEnvironment\n",
    "    , training_loops: int\n",
    "    , steps_per_loop: int\n",
    "    , log_dir: str\n",
    "    , additional_metrics: Optional[List[TFStepMetric]] = None\n",
    "    , training_data_spec_transformation_fn: Optional[Callable[[T],T]] = None\n",
    "    , run_hyperparameter_tuning: bool = False\n",
    "    , root_dir: Optional[str] = None\n",
    "    , artifacts_dir: Optional[str] = None\n",
    "    , model_dir: Optional[str] = None\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Performs `training_loops` iterations of training on the agent's policy.\n",
    "\n",
    "    Uses the `environment` as the problem formulation and source of immediate\n",
    "    feedback and the agent's algorithm, to perform `training-loops` iterations\n",
    "    of on-policy training on the policy. Has hyperparameter mode and regular\n",
    "    training mode.\n",
    "    If one or more baseline_reward_fns are provided, the regret is computed\n",
    "    against each one of them. Here is example baseline_reward_fn:\n",
    "    def baseline_reward_fn(observation, per_action_reward_fns):\n",
    "     rewards = ... # compute reward for each arm\n",
    "     optimal_action_reward = ... # take the maximum reward\n",
    "     return optimal_action_reward\n",
    "\n",
    "    Args:\n",
    "      agent: An instance of `TFAgent`.\n",
    "      environment: An instance of `TFEnvironment`.\n",
    "      training_loops: An integer indicating how many training loops should be run.\n",
    "      steps_per_loop: An integer indicating how many driver steps should be\n",
    "        executed and presented to the trainer during each training loop.\n",
    "      additional_metrics: Optional; list of metric objects to log, in addition to\n",
    "        default metrics `NumberOfEpisodes`, `AverageReturnMetric`, and\n",
    "        `AverageEpisodeLengthMetric`.\n",
    "      training_data_spec_transformation_fn: Optional; function that transforms\n",
    "        the data items before they get to the replay buffer.\n",
    "      run_hyperparameter_tuning: Optional; whether this training logic is\n",
    "        executed for the purpose of hyperparameter tuning. If so, then it does\n",
    "        not save model artifacts.\n",
    "      root_dir: Optional; path to the directory where training artifacts are\n",
    "        written; usually used for a default or auto-generated location. Do not\n",
    "        specify this argument if using hyperparameter tuning instead of training.\n",
    "      artifacts_dir: Optional; path to an extra directory where training\n",
    "        artifacts are written; usually used for a mutually agreed location from\n",
    "        which artifacts will be loaded. Do not specify this argument if using\n",
    "        hyperparameter tuning instead of training.\n",
    "\n",
    "    Returns:\n",
    "      A dict mapping metric names (eg. \"AverageReturnMetric\") to a list of\n",
    "      intermediate metric values over `training_loops` iterations of training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ====================================================\n",
    "    # TB summary writer\n",
    "    # ====================================================\n",
    "    logging.info(f\" log_dir: {log_dir}\")\n",
    "    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        log_dir, flush_millis=10 * 1000\n",
    "    )\n",
    "    train_summary_writer.set_as_default()\n",
    "    \n",
    "    # ====================================================\n",
    "    # get data spec\n",
    "    # ====================================================\n",
    "    if run_hyperparameter_tuning and not (root_dir is None and artifacts_dir is None):\n",
    "        raise ValueError(\n",
    "            \"Do not specify `root_dir` or `artifacts_dir` when\" +\n",
    "            \" running hyperparameter tuning.\"\n",
    "        )\n",
    "\n",
    "    if training_data_spec_transformation_fn is None:\n",
    "        data_spec = agent.policy.trajectory_spec\n",
    "    else:\n",
    "        data_spec = training_data_spec_transformation_fn(\n",
    "            agent.policy.trajectory_spec\n",
    "        )\n",
    "        \n",
    "    # ====================================================\n",
    "    # define replay buffer\n",
    "    # ====================================================\n",
    "    replay_buffer = trainer._get_replay_buffer(\n",
    "        data_spec = data_spec\n",
    "        , batch_size = environment.batch_size\n",
    "        , steps_per_loop = steps_per_loop\n",
    "        , async_steps_per_loop = 1\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # metrics\n",
    "    # ====================================================\n",
    "    # `step_metric` records the number of individual rounds of bandit interaction;\n",
    "    # that is, (number of trajectories) * batch_size.\n",
    "    \n",
    "    step_metric = tf_metrics.EnvironmentSteps()\n",
    "    \n",
    "    metrics = [\n",
    "        tf_metrics.NumberOfEpisodes()\n",
    "        , tf_metrics.EnvironmentSteps()\n",
    "        , tf_metrics.AverageEpisodeLengthMetric(batch_size=environment.batch_size)\n",
    "    ]\n",
    "    if additional_metrics:\n",
    "        metrics += additional_metrics\n",
    "\n",
    "    if isinstance(environment.reward_spec(), dict):\n",
    "        metrics += [\n",
    "            tf_metrics.AverageReturnMultiMetric(\n",
    "                reward_spec=environment.reward_spec()\n",
    "                , batch_size=environment.batch_size\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        metrics += [\n",
    "            tf_metrics.AverageReturnMetric(batch_size=environment.batch_size)\n",
    "        ]\n",
    "\n",
    "    # Store intermediate metric results, indexed by metric names.\n",
    "    metric_results = collections.defaultdict(list)\n",
    "\n",
    "    # ====================================================\n",
    "    # Driver\n",
    "    # ====================================================\n",
    "    \n",
    "    if training_data_spec_transformation_fn is not None:\n",
    "        add_batch_fn = lambda data: replay_buffer.add_batch(\n",
    "            training_data_spec_transformation_fn(data)\n",
    "        )\n",
    "    else:\n",
    "        add_batch_fn = replay_buffer.add_batch\n",
    "\n",
    "    observers = [add_batch_fn, step_metric] + metrics\n",
    "\n",
    "    driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        env=environment\n",
    "        , policy=agent.collect_policy\n",
    "        , num_steps=steps_per_loop * environment.batch_size\n",
    "        , observers=observers\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # training_loop\n",
    "    # ====================================================\n",
    "    training_loop = trainer._get_training_loop(\n",
    "        driver = driver\n",
    "        , replay_buffer = replay_buffer\n",
    "        , agent = agent\n",
    "        , steps = steps_per_loop\n",
    "        , async_steps_per_loop = 1\n",
    "    )\n",
    "    if not run_hyperparameter_tuning:\n",
    "        saver = policy_saver.PolicySaver(agent.policy)\n",
    "\n",
    "    for train_step in range(training_loops):\n",
    "        training_loop(\n",
    "            train_step = train_step\n",
    "            , metrics = metrics\n",
    "        )\n",
    "        # log tensorboard\n",
    "        for metric in metrics:\n",
    "            metric.tf_summaries(\n",
    "                train_step=train_step\n",
    "                , step_metrics=metrics[:2]\n",
    "            )\n",
    "        \n",
    "        metric_utils.log_metrics(metrics)\n",
    "    \n",
    "        for metric in metrics:\n",
    "            metric.tf_summaries(train_step = step_metric.result())\n",
    "            metric_results[type(metric).__name__].append(metric.result().numpy())\n",
    "    \n",
    "    if not run_hyperparameter_tuning:\n",
    "        saver.save(model_dir)\n",
    "        saver.save(artifacts_dir)\n",
    "    \n",
    "    return metric_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9436ba2-20b1-4c83-9daa-5fc11c39c384",
   "metadata": {},
   "source": [
    "### train task\n",
    "\n",
    "**TODO** - add `if task_type == 'chief':` for logging experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8cab82ce-0948-40f9-920a-2711ae774b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/per_arm_rl/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{RL_SUB_DIR}/task.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"The entrypoint for training a policy.\"\"\"\n",
    "import argparse\n",
    "import functools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Union\n",
    "import time\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "import hypertune\n",
    "\n",
    "from . import policy_util\n",
    "from . import data_utils\n",
    "from . import train_utils\n",
    "from . import data_config\n",
    "from . import my_per_arm_py_env\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.environments import environment_utilities\n",
    "from tf_agents.bandits.environments import movielens_py_environment\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "PER_ARM = True  # Use the non-per-arm version of the MovieLens environment.\n",
    "\n",
    "def get_args(\n",
    "    raw_args: List[str]\n",
    ") -> argparse.Namespace:\n",
    "    \"\"\"Parses parameters and hyperparameters for training a policy.\n",
    "\n",
    "    Args:\n",
    "      raw_args: A list of command line arguments.\n",
    "\n",
    "    Returns:\n",
    "      An argpase.Namespace object mapping (hyper)parameter names to the parsed\n",
    "      values.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--project_id\", type=str, default='hybrid-vertex'\n",
    "    )\n",
    "    # Whether to execute hyperparameter tuning or training\n",
    "    parser.add_argument(\n",
    "        \"--run-hyperparameter-tuning\", action=\"store_true\"\n",
    "        , help=\"Whether to perform hyperparameter tuning instead of regular training.\"\n",
    "    )\n",
    "    # Whether to train using the best hyperparameters learned from a previous\n",
    "    # hyperparameter tuning job.\n",
    "    parser.add_argument(\n",
    "        \"--train-with-best-hyperparameters\", action=\"store_true\"\n",
    "        , help=\"Whether to train using the best hyperparameters learned from a previous hyperparameter tuning job.\"\n",
    "    )\n",
    "    # Path parameters\n",
    "    parser.add_argument(\n",
    "        \"--artifacts-dir\", type=str\n",
    "        , help=\"Extra directory where model artifacts are saved.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--profiler-dir\", default=None, type=str\n",
    "        , help=\"Directory for TensorBoard Profiler artifacts.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data-path\", type=str, help=\"Path to MovieLens 100K's 'u.data' file.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--best-hyperparameters-bucket\", type=str\n",
    "        , help=\"Path to MovieLens 100K's 'u.data' file.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--best-hyperparameters-path\", type=str\n",
    "        , help=\"Path to JSON file containing the best hyperparameters.\"\n",
    "    )\n",
    "    # Hyperparameters\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\", default=8, type=int\n",
    "        , help=\"Training and prediction batch size.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--training_loops\", default=4, type=int\n",
    "        , help=\"Number of training iterations.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--steps-per-loop\", default=2, type=int\n",
    "        , help=\"Number of driver steps per training iteration.\"\n",
    "    )\n",
    "    # MovieLens simulation environment parameters\n",
    "    parser.add_argument(\n",
    "        \"--rank-k\", default=20, type=int\n",
    "        , help=\"Rank for matrix factorization in the MovieLens environment; also the observation dimension.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-actions\", default=20, type=int\n",
    "        , help=\"Number of actions (movie items) to choose from.\"\n",
    "    )\n",
    "    # LinUCB agent parameters\n",
    "    parser.add_argument(\n",
    "        \"--tikhonov-weight\", default=0.001, type=float\n",
    "        , help=\"LinUCB Tikhonov regularization weight.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--agent-alpha\", default=10.0, type=float\n",
    "        , help=\"LinUCB exploration parameter that multiplies the confidence intervals.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--bucket_name\", default=\"tmp\", type=str\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--data_gcs_prefix\", default=\"data\", type=str\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--data_path\", default=\"gs://tmp/tmp\", type=str\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--project_number\", default=\"934903580331\", type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--distribute\", default=\"single\", type=str, help=\"\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--artifacts_dir\", default=\"gs://BUCKET/EXPERIMENT/RUN_NAME/artifacts\", type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--root_dir\", default=\"gs://BUCKET/EXPERIMENT/RUN_NAME/root\", type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--experiment_name\", default=\"tmp-experiment\", type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--experiment_run\", default=\"tmp-experiment-run\", type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log_dir\", type=str\n",
    "    )\n",
    "    return parser.parse_args(raw_args)\n",
    "\n",
    "def execute_task(\n",
    "    args: argparse.Namespace\n",
    "    , best_hyperparameters_blob: Union[storage.Blob, None]\n",
    "    , hypertune_client: Union[hypertune.HyperTune, None]\n",
    ") -> None:\n",
    "    \"\"\"Executes training, or hyperparameter tuning, for the policy.\n",
    "\n",
    "    Parses parameters and hyperparameters from the command line, reads best\n",
    "    hyperparameters if applicable, constructs the logical modules for RL, and\n",
    "    executes training or hyperparameter tuning. Tracks the training process\n",
    "    and resources using TensorBoard Profiler if applicable.\n",
    "\n",
    "    Args:\n",
    "      args: An argpase.Namespace object of (hyper)parameter values.\n",
    "      best_hyperparameters_blob: An object containing best hyperparameters in\n",
    "        Google Cloud Storage.\n",
    "      hypertune_client: Client for submitting hyperparameter tuning metrics.\n",
    "    \"\"\"\n",
    "    # ====================================================\n",
    "    # set Vertex AI env vars\n",
    "    # ====================================================\n",
    "    if 'AIP_TENSORBOARD_LOG_DIR' in os.environ:\n",
    "        log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR']\n",
    "        logging.info(f'AIP_TENSORBOARD_LOG_DIR: {log_dir}')\n",
    "    else:\n",
    "        log_dir = args.log_dir\n",
    "        logging.info(f'log_dir: {log_dir}')\n",
    "        \n",
    "    logging.info(f'TensorBoard log_dir: {log_dir}')\n",
    "    \n",
    "    # [Do Not Change] Set the root directory for training artifacts.\n",
    "    MODEL_DIR = os.environ[\"AIP_MODEL_DIR\"] if not args.run_hyperparameter_tuning else \"\"\n",
    "    logging.info(f'MODEL_DIR: {MODEL_DIR}')\n",
    "    \n",
    "    root_dir = args.root_dir if not args.run_hyperparameter_tuning else \"\"\n",
    "    logging.info(f'root_dir: {root_dir}')\n",
    "\n",
    "    # ====================================================\n",
    "    # Use best hparams learned from previous hpt job\n",
    "    # ====================================================\n",
    "    if args.train_with_best_hyperparameters:\n",
    "        logging.info(f'train_with_best_hyperparameters engaged...')\n",
    "        logging.info(f\" train_with_best_hyperparameters: {args.train_with_best_hyperparameters}\")\n",
    "        best_hyperparameters = json.loads(\n",
    "            best_hyperparameters_blob.download_as_string()\n",
    "        )\n",
    "        \n",
    "        if \"batch-size\" in best_hyperparameters:\n",
    "            args.batch_size = int(best_hyperparameters[\"batch-size\"])\n",
    "        if \"training-loops\" in best_hyperparameters:\n",
    "            args.training_loops = int(best_hyperparameters[\"training-loops\"])\n",
    "        if \"steps-per-loop\" in best_hyperparameters:\n",
    "            args.step_per_loop = int(best_hyperparameters[\"steps-per-loop\"])\n",
    "\n",
    "    # ====================================================\n",
    "    # Define RL environment\n",
    "    # ====================================================\n",
    "    env = my_per_arm_py_env.MyMovieLensPerArmPyEnvironment(\n",
    "        project_number = args.project_number\n",
    "        , data_path = args.data_path\n",
    "        , bucket_name = args.bucket_name\n",
    "        , data_gcs_prefix = args.data_gcs_prefix\n",
    "        , user_age_lookup_dict = data_config.USER_AGE_LOOKUP\n",
    "        , user_occ_lookup_dict = data_config.USER_OCC_LOOKUP\n",
    "        , movie_gen_lookup_dict = data_config.MOVIE_GEN_LOOKUP\n",
    "        , num_users = data_config.MOVIELENS_NUM_USERS\n",
    "        , num_movies = data_config.MOVIELENS_NUM_MOVIES\n",
    "        , rank_k = args.rank_k\n",
    "        , batch_size = args.batch_size\n",
    "        , num_actions = args.num_actions\n",
    "    )\n",
    "    environment = tf_py_environment.TFPyEnvironment(env)\n",
    "    \n",
    "    strategy = train_utils.get_train_strategy(distribute_arg=args.distribute)\n",
    "    logging.info(f'TF training strategy (execute task) = {strategy}')\n",
    "    \n",
    "    with strategy.scope():\n",
    "        # Define RL agent/algorithm.\n",
    "        agent = lin_ucb_agent.LinearUCBAgent(\n",
    "            time_step_spec=environment.time_step_spec()\n",
    "            , action_spec=environment.action_spec()\n",
    "            , tikhonov_weight=args.tikhonov_weight\n",
    "            , alpha=args.agent_alpha\n",
    "            , dtype=tf.float32\n",
    "            , accepts_per_arm_features=PER_ARM # TODO - streamline\n",
    "        )\n",
    "    logging.info(\"TimeStep Spec (for each batch):\\n%s\\n\", agent.time_step_spec)\n",
    "    logging.info(\"Action Spec (for each batch):\\n%s\\n\", agent.action_spec)\n",
    "    logging.info(\"Reward Spec (for each batch):\\n%s\\n\", environment.reward_spec())\n",
    "\n",
    "    # Define RL metric.\n",
    "    optimal_reward_fn = functools.partial(\n",
    "        environment_utilities.compute_optimal_reward_with_movielens_environment\n",
    "        , environment=environment\n",
    "    )\n",
    "    \n",
    "    regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n",
    "    metrics = [regret_metric]\n",
    "\n",
    "    # Perform on-policy training with the simulation MovieLens environment.\n",
    "    if args.profiler_dir is not None:\n",
    "        tf.profiler.experimental.start(args.profiler_dir)\n",
    "        \n",
    "    start_time = time.time()\n",
    "  \n",
    "    metric_results = policy_util.train(\n",
    "        agent=agent\n",
    "        , environment=environment\n",
    "        , training_loops=args.training_loops\n",
    "        , steps_per_loop=args.steps_per_loop\n",
    "        , additional_metrics=metrics\n",
    "        , run_hyperparameter_tuning=args.run_hyperparameter_tuning\n",
    "        , root_dir=root_dir if not args.run_hyperparameter_tuning else None\n",
    "        , artifacts_dir=args.artifacts_dir\n",
    "        if not args.run_hyperparameter_tuning else None\n",
    "        , model_dir = MODEL_DIR\n",
    "        , log_dir = log_dir\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    runtime_mins = int((end_time - start_time) / 60)\n",
    "    \n",
    "    if args.profiler_dir is not None:\n",
    "        tf.profiler.experimental.stop()\n",
    "\n",
    "    # Report training metrics to Vertex AI for hyperparameter tuning\n",
    "    if args.run_hyperparameter_tuning:\n",
    "        hypertune_client.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag=\"final_average_return\"\n",
    "            , metric_value=metric_results[\"AverageReturnMetric\"][-1]\n",
    "            # , global_step=args.training_loops\n",
    "        )\n",
    "        \n",
    "    if args.run_hyperparameter_tuning:\n",
    "        logging.info(\"hp-tuning engaged; not logging training output to Vertex Experiments\")\n",
    "    else:\n",
    "        logging.info(f\"Logging data to experiment run: {args.experiment_run}\")\n",
    "        \n",
    "        # gather the metrics for the last epoch to be saved in metrics\n",
    "        exp_metrics = {\n",
    "            \"AverageReturnMetric\" : float(metric_results[\"AverageReturnMetric\"][-1])\n",
    "            , \"FinalRegretMetric\" : float(metric_results[\"RegretMetric\"][-1])\n",
    "        }\n",
    "        \n",
    "        # gather the param values\n",
    "        exp_params = {\n",
    "            \"runtime\": runtime_mins\n",
    "            , \"batch_size\": args.batch_size\n",
    "            , \"training_loops\": args.training_loops\n",
    "            , \"steps_pre_loop\": args.steps_per_loop\n",
    "            , \"rank_k\": args.rank_k\n",
    "            , \"num_actions\": args.num_actions\n",
    "            , \"per_arm\": PER_ARM\n",
    "            , \"tikhonov_weight\": args.tikhonov_weight\n",
    "            , \"agent_alpha\": args.agent_alpha\n",
    "        }\n",
    "        \n",
    "        with aiplatform.start_run(\n",
    "            args.experiment_run\n",
    "        ) as my_run:\n",
    "            \n",
    "            aiplatform.log_params(exp_params)\n",
    "            \n",
    "            aiplatform.log_metrics(exp_metrics)\n",
    "            \n",
    "            aiplatform.end_run()\n",
    "            \n",
    "        logging.info(f\"EXPERIMENT RUN: '{args.experiment_run}' has ended\")\n",
    "            \n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Entry point for training or hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    args = get_args(sys.argv[1:])\n",
    "    # =============================================\n",
    "    # set GCP clients\n",
    "    # =============================================\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from google.cloud import storage\n",
    "\n",
    "    project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    storage_client = storage.Client(project=project_number)\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=project_number\n",
    "        , location='us-central1'\n",
    "        , experiment=args.experiment_name\n",
    "    )\n",
    "    \n",
    "    # =============================================\n",
    "    # GPUs\n",
    "    # =============================================\n",
    "\n",
    "    # limiting GPU growth\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logging.info(f'detected: {len(gpus)} GPUs')\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            logging.info(e)\n",
    "\n",
    "    # tf.debugging.set_log_device_placement(True)          # logs all tf ops and their device placement;\n",
    "    os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "    os.environ['TF_GPU_THREAD_COUNT'] = f'8'               # TODO - parametrize | 1\n",
    "    os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "    \n",
    "    # ====================================================\n",
    "    # Set Device Strategy\n",
    "    # ====================================================\n",
    "    logging.info(\"Detecting devices....\")\n",
    "    logging.info('DEVICES'  + str(device_lib.list_local_devices()))\n",
    "    \n",
    "    logging.info(\"Setting device strategy...\")\n",
    "    \n",
    "    strategy = train_utils.get_train_strategy(distribute_arg=args.distribute)\n",
    "    logging.info(f'TF training strategy (main) = {strategy}')\n",
    "    \n",
    "    NUM_REPLICAS = strategy.num_replicas_in_sync\n",
    "    logging.info(f'num_replicas_in_sync = {NUM_REPLICAS}')\n",
    "    \n",
    "    # Here the batch size scales up by number of workers since\n",
    "    # `tf.data.Dataset.batch` expects the global batch size.\n",
    "    GLOBAL_BATCH_SIZE = int(args.batch_size) * int(NUM_REPLICAS)\n",
    "    logging.info(f'GLOBAL_BATCH_SIZE = {GLOBAL_BATCH_SIZE}')\n",
    "\n",
    "    # type and task of machine from strategy\n",
    "    logging.info(f'Setting task_type and task_id...')\n",
    "    if args.distribute == 'multiworker':\n",
    "        task_type, task_id = (\n",
    "            strategy.cluster_resolver.task_type,\n",
    "            strategy.cluster_resolver.task_id\n",
    "        )\n",
    "    else:\n",
    "        task_type, task_id = 'chief', None\n",
    "    \n",
    "    logging.info(f'task_type = {task_type}')\n",
    "    logging.info(f'task_id = {task_id}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # determine train job type and execute\n",
    "    # ====================================================\n",
    "    \n",
    "    if args.train_with_best_hyperparameters:\n",
    "        logging.info(f\" best_hyperparameters_path: {args.best_hyperparameters_path}\")\n",
    "        storage_client = storage.Client(args.project_id)\n",
    "        bucket = storage_client.bucket(args.bucket_name)\n",
    "        best_hyperparameters_blob = bucket.blob(args.best_hyperparameters_path)\n",
    "    \n",
    "    else:\n",
    "        best_hyperparameters_blob = None\n",
    "    \n",
    "    hypertune_client = hypertune.HyperTune() if args.run_hyperparameter_tuning else None\n",
    "\n",
    "    execute_task(\n",
    "        args = args\n",
    "        , best_hyperparameters_blob = best_hyperparameters_blob\n",
    "        , hypertune_client = hypertune_client\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Python Version = %s\", sys.version)\n",
    "    logging.info(\"TensorFlow Version = %s\", tf.__version__)\n",
    "    # logging.info(\"TF_CONFIG = %s\", os.environ.get(\"TF_CONFIG\", \"Not found\"))\n",
    "    # logging.info(\"DEVICES = %s\", device_lib.list_local_devices())\n",
    "    logging.info(\"Reinforcement learning task started...\")\n",
    "    \n",
    "    main()\n",
    "    \n",
    "    logging.info(\"Reinforcement learning task completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678fcb8-4b41-42aa-92ec-c3b07f7747e0",
   "metadata": {},
   "source": [
    "## Build train application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd895298-c435-414d-b84a-908cf0932ba4",
   "metadata": {},
   "source": [
    "### Vertex Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b6322612-2a66-4b05-b600-dbae4d307efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : scale-perarm-hpt-v2\n",
      "RUN_NAME          : run-20230717-184401\n",
      "LOG_DIR           : gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt-v2/run-20230717-184401/tb-logs\n",
      "ROOT_DIR          : gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt-v2/run-20230717-184401/root\n",
      "ARTIFACTS_DIR     : gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt-v2/run-20230717-184401/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'scale-perarm-hpt-v2'\n",
    "\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "LOG_DIR           = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/tb-logs\"\n",
    "ROOT_DIR          = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b106007a-9ed4-4237-88a9-bf4bbe459b15",
   "metadata": {},
   "source": [
    "### Create a Cloud Build YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a448db20-3021-4d07-9559-661be6a04eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile_$_DOCKERNAME']\n",
    "  env: ['AIP_STORAGE_URI=$_ARTIFACTS_DIR']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3180b65-d636-40a2-aaa2-7b6a797e43b3",
   "metadata": {},
   "source": [
    "### Write a Dockerfile\n",
    "* Use the [cloudml-hypertune](https://github.com/GoogleCloudPlatform/cloudml-hypertune) Python package to report training metrics to Vertex AI for hyperparameter tuning\n",
    "* Use the Google [Cloud Storage client library](https://cloud.google.com/storage/docs/reference/libraries) to read the best hyperparameters learned from a previous hyperarameter tuning job during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "20db1e12-1e16-4ec9-89c7-2a3b181cb715",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCKERNAME = 'train_perarm'\n",
    "# ! rm -rf Dockerfile_{DOCKERNAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "59ea1b9d-6237-4e12-9b86-9a5ec23d7459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile_train_perarm\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile_{DOCKERNAME}\n",
    "\n",
    "# Specifies base image and tag.\n",
    "# FROM gcr.io/google-appengine/python\n",
    "FROM python:3.10\n",
    "ENV PYTHONUNBUFFERED True\n",
    "\n",
    "WORKDIR /root\n",
    "\n",
    "# Installs additional packages.\n",
    "RUN pip3 install cloudml-hypertune\n",
    "RUN pip3 install google-cloud-storage\n",
    "RUN pip3 install google-cloud-aiplatform\n",
    "RUN pip3 install tensorflow==2.12.0\n",
    "RUN pip3 install tensorboard\n",
    "RUN pip3 install tensorboard-plugin-profile\n",
    "RUN pip3 install tensorboard-plugin-wit\n",
    "RUN pip3 install tensorboard-data-server\n",
    "RUN pip3 install tensorflow-io\n",
    "RUN pip3 install tf-agents==0.17.0\n",
    "RUN pip3 install matplotlib\n",
    "RUN pip3 install urllib3\n",
    "\n",
    "# Copies training code to the Docker image.\n",
    "COPY src/per_arm_rl /root/src/per_arm_rl\n",
    "\n",
    "# Sets up the entry point to invoke the task.\n",
    "ENTRYPOINT [\"python3\", \"-m\", \"src.per_arm_rl.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0092f87b-c113-4325-9696-658ee6cf5860",
   "metadata": {},
   "source": [
    "#### Build the custom container with Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9eafefba-a58f-46aa-a331-fc0abea93702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export DOCKERNAME    = train_perarm\n",
      "export IMAGE_URI     = gcr.io/hybrid-vertex/hptuning-training-custom-container\n",
      "export FILE_LOCATION = ./\n",
      "export MACHINE_TYPE  = e2-highcpu-32\n",
      "export ARTIFACTS_DIR = gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt-v2/run-20230717-184401/artifacts\n"
     ]
    }
   ],
   "source": [
    "HPTUNING_TRAINING_CONTAINER = \"hptuning-training-custom-container\"\n",
    "\n",
    "# Docker definitions for training\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{HPTUNING_TRAINING_CONTAINER}'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './'\n",
    "\n",
    "print(f\"export DOCKERNAME    = {DOCKERNAME}\")\n",
    "print(f\"export IMAGE_URI     = {IMAGE_URI}\")\n",
    "print(f\"export FILE_LOCATION = {FILE_LOCATION}\")\n",
    "print(f\"export MACHINE_TYPE  = {MACHINE_TYPE}\")\n",
    "print(f\"export ARTIFACTS_DIR = {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d2f64125-fd4a-459b-9034-2d084f85a095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud builds submit --config cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION,_ARTIFACTS_DIR=$ARTIFACTS_DIR \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307bb759-0a14-4746-8d81-f846e318b1ed",
   "metadata": {},
   "source": [
    "## Prepare (hpt) training job for Vertex AI\n",
    "* Submit a hyperparameter training job with the custom container. Read more details for using Python packages as an alternative to using custom containers in the example shown [here](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#create)\n",
    "* Define the hyperparameter(s), max trial count, parallel trial count, parameter search algorithm, machine spec, accelerators, worker pool, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3b8fcea2-db92-42ec-98ab-5f42d91da2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPTUNING_RESULT_DIR  : hptuning\n",
      "HPTUNING_RESULT_FILE : result.json\n",
      "HPTUNING_RESULT_PATH : scale-perarm-hpt-v2/run-20230717-174256/hptuning/result.json\n",
      "HPTUNING_RESULT_URI  : gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt-v2/run-20230717-174256/hptuning/result.json\n"
     ]
    }
   ],
   "source": [
    "# Execute hyperparameter tuning instead of regular training\n",
    "RUN_HYPERPARAMETER_TUNING          = True\n",
    "TRAIN_WITH_BEST_HYPERPARAMETERS    = False  # Do not train.\n",
    "\n",
    "# Directory to store the best hyperparameter(s) in `BUCKET_NAME` and locally (temporarily)\n",
    "HPTUNING_RESULT_DIR                = \"hptuning\"\n",
    "HPTUNING_RESULT_FILE               = \"result.json\"\n",
    "HPTUNING_RESULT_PATH               = f\"{EXPERIMENT_NAME}/{RUN_NAME}/{HPTUNING_RESULT_DIR}/{HPTUNING_RESULT_FILE}\"\n",
    "HPTUNING_RESULT_URI                = f\"{BUCKET_URI}/{HPTUNING_RESULT_PATH}\"\n",
    "\n",
    "# HPTUNING_RESULT_PATH               = os.path.join(HPTUNING_RESULT_DIR, \"result.json\")\n",
    "\n",
    "print(f\"HPTUNING_RESULT_DIR  : {HPTUNING_RESULT_DIR}\")\n",
    "print(f\"HPTUNING_RESULT_FILE : {HPTUNING_RESULT_FILE}\")\n",
    "print(f\"HPTUNING_RESULT_PATH : {HPTUNING_RESULT_PATH}\")\n",
    "print(f\"HPTUNING_RESULT_URI  : {HPTUNING_RESULT_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3e816-d9f3-4b91-9327-fdc27eba54a4",
   "metadata": {},
   "source": [
    "### Accelerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1eea4d62-7fc0-4503-a47b-e4d81c9e9197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKER_MACHINE_TYPE            : n1-standard-16\n",
      "REPLICA_COUNT                  : 1\n",
      "ACCELERATOR_TYPE               : NVIDIA_TESLA_T4\n",
      "PER_MACHINE_ACCELERATOR_COUNT  : 1\n",
      "DISTRIBUTE_STRATEGY            : single\n",
      "REDUCTION_SERVER_COUNT         : 0\n",
      "REDUCTION_SERVER_MACHINE_TYPE  : n1-highcpu-16\n"
     ]
    }
   ],
   "source": [
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "# REPLICA_COUNT = 1\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# # REDUCTION_SERVER_COUNT = 0                                                      \n",
    "# # REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'single'\n",
    "\n",
    "WORKER_MACHINE_TYPE = 'n1-standard-16'\n",
    "REPLICA_COUNT = 1\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4' # NVIDIA_TESLA_T4 NVIDIA_TESLA_V100\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "DISTRIBUTE_STRATEGY = 'single'\n",
    "REDUCTION_SERVER_COUNT = 0                                                      \n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "\n",
    "print(f\"WORKER_MACHINE_TYPE            : {WORKER_MACHINE_TYPE}\")\n",
    "print(f\"REPLICA_COUNT                  : {REPLICA_COUNT}\")\n",
    "print(f\"ACCELERATOR_TYPE               : {ACCELERATOR_TYPE}\")\n",
    "print(f\"PER_MACHINE_ACCELERATOR_COUNT  : {PER_MACHINE_ACCELERATOR_COUNT}\")\n",
    "print(f\"DISTRIBUTE_STRATEGY            : {DISTRIBUTE_STRATEGY}\")\n",
    "print(f\"REDUCTION_SERVER_COUNT         : {REDUCTION_SERVER_COUNT}\")\n",
    "print(f\"REDUCTION_SERVER_MACHINE_TYPE  : {REDUCTION_SERVER_MACHINE_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5213898-3252-4eaf-a50f-1166e640a2d5",
   "metadata": {},
   "source": [
    "### Create Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "32e1fd75-c50c-453c-8cc6-b85416836648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME: projects/934903580331/locations/us-central1/tensorboards/408200288881606656\n",
      "TB display name: scale-perarm-hpt-v2-v1\n"
     ]
    }
   ],
   "source": [
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}-v1\"\n",
    "\n",
    "tensorboard = aiplatform.Tensorboard.create(\n",
    "    display_name=TENSORBOARD_DISPLAY_NAME\n",
    "    , project=PROJECT_ID\n",
    "    , location=REGION\n",
    ")\n",
    "\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "# use existing TB instance\n",
    "# TB_RESOURCE_NAME = 'projects/934903580331/locations/us-central1/tensorboards/6924469145035603968'\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name: {tensorboard.display_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c14bc7-7fcb-402f-adbf-8cb35e039edd",
   "metadata": {},
   "source": [
    "### Set training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d74207b8-dbb5-4baf-8c86-f8f0b594e8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE       : 8\n",
      "TRAINING_LOOPS   : 20\n",
      "STEPS_PER_LOOP   : 6\n",
      "RANK_K           : 20\n",
      "NUM_ACTIONS      : 20\n",
      "PER_ARM          : True\n",
      "TIKHONOV_WEIGHT  : 0.001\n",
      "AGENT_ALPHA      : 10.0\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters.\n",
    "BATCH_SIZE       = 8       # Training and prediction batch size.\n",
    "TRAINING_LOOPS   = 20      # Number of training iterations.\n",
    "STEPS_PER_LOOP   = 6       # Number of driver steps per training iteration.\n",
    "\n",
    "# Set MovieLens simulation environment parameters.\n",
    "RANK_K           = 20      # Rank for matrix factorization in the MovieLens environment; also the observation dimension.\n",
    "NUM_ACTIONS      = 20      # Number of actions (movie items) to choose from.\n",
    "PER_ARM          = True    # Use the non-per-arm version of the MovieLens environment.\n",
    "\n",
    "# Set agent parameters.\n",
    "TIKHONOV_WEIGHT  = 0.001   # LinUCB Tikhonov regularization weight.\n",
    "AGENT_ALPHA      = 10.0    # LinUCB exploration parameter that multiplies the confidence intervals.\n",
    "\n",
    "print(f\"BATCH_SIZE       : {BATCH_SIZE}\")\n",
    "print(f\"TRAINING_LOOPS   : {TRAINING_LOOPS}\")\n",
    "print(f\"STEPS_PER_LOOP   : {STEPS_PER_LOOP}\")\n",
    "print(f\"RANK_K           : {RANK_K}\")\n",
    "print(f\"NUM_ACTIONS      : {NUM_ACTIONS}\")\n",
    "print(f\"PER_ARM          : {PER_ARM}\")\n",
    "print(f\"TIKHONOV_WEIGHT  : {TIKHONOV_WEIGHT}\")\n",
    "print(f\"AGENT_ALPHA      : {AGENT_ALPHA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "03b3cc89-1401-480c-8d4e-1422227860f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--data-path=gs://mabv1-hybrid-vertex-bucket/data',\n",
      "                              '--bucket_name=mabv1-hybrid-vertex-bucket',\n",
      "                              '--data_gcs_prefix=data',\n",
      "                              '--data_path=gs://mabv1-hybrid-vertex-bucket/data',\n",
      "                              '--project_number=934903580331',\n",
      "                              '--batch-size=8',\n",
      "                              '--rank-k=20',\n",
      "                              '--num-actions=20',\n",
      "                              '--tikhonov-weight=0.001',\n",
      "                              '--agent-alpha=10.0',\n",
      "                              '--training_loops=20',\n",
      "                              '--steps-per-loop=6',\n",
      "                              '--distribute=single',\n",
      "                              '--artifacts_dir=gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt-v2/run-20230717-174256/artifacts',\n",
      "                              '--root_dir=gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt-v2/run-20230717-174256/root',\n",
      "                              '--experiment_name=scale-perarm-hpt-v2',\n",
      "                              '--experiment_run=run-20230717-174256',\n",
      "                              '--run-hyperparameter-tuning'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/hptuning-training-custom-container:latest'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_T4',\n",
      "                   'machine_type': 'n1-standard-16'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "WORKER_ARGS = [\n",
    "    f\"--data-path={DATA_PATH}\"               # TODO - remove duplicate arg\n",
    "    , f\"--bucket_name={BUCKET_NAME}\"\n",
    "    , f\"--data_gcs_prefix={DATA_GCS_PREFIX}\"\n",
    "    , f\"--data_path={DATA_PATH}\"\n",
    "    , f\"--project_number={PROJECT_NUM}\"\n",
    "    , f\"--batch-size={BATCH_SIZE}\"\n",
    "    , f\"--rank-k={RANK_K}\"\n",
    "    , f\"--num-actions={NUM_ACTIONS}\"\n",
    "    , f\"--tikhonov-weight={TIKHONOV_WEIGHT}\"\n",
    "    , f\"--agent-alpha={AGENT_ALPHA}\"\n",
    "    , f\"--training_loops={TRAINING_LOOPS}\"\n",
    "    , f\"--steps-per-loop={STEPS_PER_LOOP}\"\n",
    "    , f\"--distribute={DISTRIBUTE_STRATEGY}\"\n",
    "    , f\"--artifacts_dir={ARTIFACTS_DIR}\"\n",
    "    , f\"--root_dir={ROOT_DIR}\"\n",
    "    , f\"--experiment_name={EXPERIMENT_NAME}\"\n",
    "    , f\"--experiment_run={RUN_NAME}\"\n",
    "]\n",
    "\n",
    "if RUN_HYPERPARAMETER_TUNING:\n",
    "    WORKER_ARGS.append(\"--run-hyperparameter-tuning\")\n",
    "    \n",
    "elif TRAIN_WITH_BEST_HYPERPARAMETERS:\n",
    "    WORKER_ARGS.append(\"--train-with-best-hyperparameters\")\n",
    "    \n",
    "from src.per_arm_rl import train_utils\n",
    "\n",
    "WORKER_POOL_SPECS = train_utils.prepare_worker_pool_specs(\n",
    "    image_uri=f\"{IMAGE_URI}:latest\",\n",
    "    args=WORKER_ARGS,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c8ee96-89e0-45d5-a1a6-14a3997358dd",
   "metadata": {},
   "source": [
    "### Define parameter spec\n",
    "\n",
    "Next, define the 1parameter_spec1, which is a dictionary specifying the parameters you want to optimize. The **dictionary key** is the string you assigned to the command line argument for each hyperparameter, and the **dictionary value** is the parameter specification.\n",
    "\n",
    "For each hyperparameter, you need to define the `Type` as well as the bounds for the values that the tuning service will try. Hyperparameters can be of type `Double`, `Integer`, `Categorical`, or `Discrete`. If you select the type `Double` or `Integer`, you need to provide a minimum and maximum value. And if you select `Categorical` or `Discrete` you need to provide the values. For the `Double` and `Integer` types, you also need to provide the scaling value. Learn more about [Using an Appropriate Scale](https://www.youtube.com/watch?v=cSoK_6Rkbfg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3c289fe7-edb7-4b7b-ad21-49e8e8946b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary representing parameters to optimize.\n",
    "# The dictionary key is the parameter_id, which is passed into your training\n",
    "# job as a command line argument,\n",
    "# And the dictionary value is the parameter specification of the metric.\n",
    "parameter_spec = {\n",
    "    \"steps-per-loop\": hpt.DiscreteParameterSpec(values=[2, 4], scale=None)\n",
    "    , \"batch-size\": hpt.DiscreteParameterSpec(values=[8, 16], scale=None)\n",
    "    # , \"num-actions\": hpt.DiscreteParameterSpec(values=[32, 64, 128], scale=None)\n",
    "    # , \"training-loops\": hpt.DiscreteParameterSpec(values=[4, 6, 8], scale=None)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c99738c-0b2c-4a48-9025-21f82b634617",
   "metadata": {},
   "source": [
    "The final spec to define is `metric_spec`, which is a dictionary representing the metric to optimize. The dictionary key is the `hyperparameter_metric_tag` that you set in your training application code, and the value is the optimization goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b926ea3b-1631-476c-a7a5-fe2f5841f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary representing metrics to optimize.\n",
    "# The dictionary key is the metric_id, which is reported by your training job,\n",
    "# And the dictionary value is the optimization goal of the metric.\n",
    "metric_spec = {\"final_average_return\": \"maximize\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2576033c-f80f-44ed-bfa7-c2a594bef0ec",
   "metadata": {},
   "source": [
    "### submit (hpt) train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "657b600d-770c-4acd-844e-9b6620213a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME: mvl-hpt-run-20230717-174256\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID\n",
    "    , location=REGION\n",
    "    , experiment=EXPERIMENT_NAME\n",
    "    # , staging_bucket=ROOT_DIR\n",
    ")\n",
    "\n",
    "JOB_NAME = f\"mvl-hpt-{RUN_NAME}\"\n",
    "print(f\"JOB_NAME: {JOB_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a8135819-0b02-40db-a8b9-3e924660630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CustomJob\n",
    "my_custom_hpt_job = aiplatform.CustomJob(\n",
    "    display_name=JOB_NAME\n",
    "    , project=PROJECT_ID\n",
    "    , worker_pool_specs=WORKER_POOL_SPECS\n",
    "    , staging_bucket=ROOT_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e62b4a-0f73-473d-9394-ec74688a302f",
   "metadata": {},
   "source": [
    "Then, create and run a HyperparameterTuningJob.\n",
    "\n",
    "> see [source code](https://github.com/googleapis/python-aiplatform/blob/main/google/cloud/aiplatform/hyperparameter_tuning.py)\n",
    "\n",
    "There are a few arguments to note:\n",
    "\n",
    "* `max_trial_count`: Sets an upper bound on the number of trials the service will run. The recommended practice is to start with a smaller number of trials and get a sense of how impactful your chosen hyperparameters are before scaling up.\n",
    "\n",
    "* `parallel_trial_count`: If you use parallel trials, the service provisions multiple training processing clusters. The worker pool spec that you specify when creating the job is used for each individual training cluster. Increasing the number of parallel trials reduces the amount of time the hyperparameter tuning job takes to run; however, it can reduce the effectiveness of the job overall. This is because the default tuning strategy uses results of previous trials to inform the assignment of values in subsequent trials.\n",
    "\n",
    "* `search_algorithm`: The available search algorithms are grid, random, or default (None). The default option applies Bayesian optimization to search the space of possible hyperparameter values and is the recommended algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bb39f023-77fb-4624-aa67-7873a026e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run HyperparameterTuningJob\n",
    "\n",
    "hp_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=JOB_NAME,\n",
    "    custom_job=my_custom_hpt_job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=6,\n",
    "    parallel_trial_count=3,\n",
    "    project=PROJECT_ID,\n",
    "    search_algorithm=\"random\",\n",
    ")\n",
    "\n",
    "hp_job.run(\n",
    "    sync=False\n",
    "    , service_account=VERTEX_SA\n",
    "    , restart_job_on_worker_restart = False \n",
    "    , enable_web_access = True\n",
    "    , tensorboard = TB_RESOURCE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7246e29b-f37f-458c-a63a-01cfdf71224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: mvl-hpt-run-20230717-174256\n",
      "Job Resource Name: projects/934903580331/locations/us-central1/hyperparameterTuningJobs/9173068349091872768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job Name: {hp_job.display_name}\")\n",
    "print(f\"Job Resource Name: {hp_job.resource_name}\\n\")\n",
    "# print(f\"Check training progress at {custom_job._dashboard_uri()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "77da3fea-4c70-4b85-8135-47d90b5484aa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 9007622635437162496\n",
      "Job config: name: \"projects/934903580331/locations/us-central1/hyperparameterTuningJobs/9007622635437162496\"\n",
      "display_name: \"mvl-hpt-job-run-20230714-130012\"\n",
      "study_spec {\n",
      "  metrics {\n",
      "    metric_id: \"final_average_return\"\n",
      "    goal: MAXIMIZE\n",
      "  }\n",
      "  parameters {\n",
      "    parameter_id: \"batch-size\"\n",
      "    discrete_value_spec {\n",
      "      values: 8.0\n",
      "      values: 16.0\n",
      "    }\n",
      "    scale_type: UNIT_LINEAR_SCALE\n",
      "  }\n",
      "  parameters {\n",
      "    parameter_id: \"steps-per-loop\"\n",
      "    discrete_value_spec {\n",
      "      values: 2.0\n",
      "      values: 4.0\n",
      "    }\n",
      "    scale_type: UNIT_LINEAR_SCALE\n",
      "  }\n",
      "  algorithm: RANDOM_SEARCH\n",
      "}\n",
      "max_trial_count: 4\n",
      "parallel_trial_count: 2\n",
      "trial_job_spec {\n",
      "  worker_pool_specs {\n",
      "    machine_spec {\n",
      "      machine_type: \"n1-standard-16\"\n",
      "      accelerator_type: NVIDIA_TESLA_T4\n",
      "      accelerator_count: 1\n",
      "    }\n",
      "    replica_count: 1\n",
      "    disk_spec {\n",
      "      boot_disk_type: \"pd-ssd\"\n",
      "      boot_disk_size_gb: 100\n",
      "    }\n",
      "    container_spec {\n",
      "      image_uri: \"gcr.io/hybrid-vertex/hptuning-training-custom-container:latest\"\n",
      "      args: \"--data-path=gs://mabv1-hybrid-vertex-bucket/data\"\n",
      "      args: \"--bucket_name=mabv1-hybrid-vertex-bucket\"\n",
      "      args: \"--data_gcs_prefix=data\"\n",
      "      args: \"--data_path=gs://mabv1-hybrid-vertex-bucket/data\"\n",
      "      args: \"--project_number=934903580331\"\n",
      "      args: \"--batch-size=8\"\n",
      "      args: \"--rank-k=20\"\n",
      "      args: \"--num-actions=20\"\n",
      "      args: \"--tikhonov-weight=0.001\"\n",
      "      args: \"--agent-alpha=10.0\"\n",
      "      args: \"--training-loops=20\"\n",
      "      args: \"--steps-per-loop=6\"\n",
      "      args: \"--distribute=single\"\n",
      "      args: \"--artifacts_dir=gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230714-130012/artifacts\"\n",
      "      args: \"--run-hyperparameter-tuning\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "state: JOB_STATE_PENDING\n",
      "create_time {\n",
      "  seconds: 1689340019\n",
      "  nanos: 563863000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1689340019\n",
      "  nanos: 563863000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'9007622635437162496'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# job_id = create_hyperparameter_tuning_job_sample(\n",
    "#     project=PROJECT_ID\n",
    "#     , display_name=f\"mvl-hpt-job-{RUN_NAME}\"\n",
    "#     , image_uri=f\"gcr.io/{PROJECT_ID}/{HPTUNING_TRAINING_CONTAINER}:latest\"\n",
    "#     , args=args\n",
    "#     , max_trial_count = 4\n",
    "#     , parallel_trial_count = 2\n",
    "#     , location=REGION\n",
    "#     , api_endpoint=f\"{REGION}-aiplatform.googleapis.com\"\n",
    "# )\n",
    "\n",
    "# job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a471d3-ab80-437c-a35f-b523fb2ab545",
   "metadata": {},
   "source": [
    "#### Check hyperparameter tuning job status\n",
    "* Read more about managing jobs [here](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#manage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "349823d9-3822-47f0-b2bd-0813c42f5c93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_hyperparameter_tuning_job_sample(\n",
    "#     project: str,\n",
    "#     hyperparameter_tuning_job_id: str,\n",
    "#     location: str = \"us-central1\",\n",
    "#     api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "# ) -> aiplatform.HyperparameterTuningJob:\n",
    "#     \"\"\"\n",
    "#     Gets the current status of a hyperparameter tuning job.\n",
    "\n",
    "#     Args:\n",
    "#         project: GCP project ID.\n",
    "#         hyperparameter_tuning_job_id: Hyperparameter tuning job ID.\n",
    "#         location: Service location.\n",
    "#         api_endpoint: API endpoint, eg. `-aiplatform.googleapis.com`.\n",
    "\n",
    "#     Returns:\n",
    "#         Details of the hyperparameter tuning job, such as its running status,\n",
    "#         results of its trials, etc.\n",
    "#     \"\"\"\n",
    "#     # The AI Platform services require regional API endpoints.\n",
    "#     client_options = {\"api_endpoint\": api_endpoint}\n",
    "    \n",
    "#     # Initialize client that will be used to create and send requests.\n",
    "#     # This client only needs to be created once, and can be reused for multiple requests.\n",
    "#     client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "    \n",
    "#     name = client.hyperparameter_tuning_job_path(\n",
    "#         project=project\n",
    "#         , location=location\n",
    "#         , hyperparameter_tuning_job=hyperparameter_tuning_job_id\n",
    "#     )\n",
    "    \n",
    "#     response = client.get_hyperparameter_tuning_job(name=name)\n",
    "    \n",
    "#     return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5bb83155-4edb-4424-b28e-a801771b0e38",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job succeeded.\n",
      "Job Time: 0:16:36.929684\n",
      "Trials: [id: \"1\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"batch-size\"\n",
      "  value {\n",
      "    number_value: 8.0\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"steps-per-loop\"\n",
      "  value {\n",
      "    number_value: 4.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"final_average_return\"\n",
      "    value: 1.2964732646942139\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1689340036\n",
      "  nanos: 175740133\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1689340194\n",
      "}\n",
      ", id: \"2\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"batch-size\"\n",
      "  value {\n",
      "    number_value: 8.0\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"steps-per-loop\"\n",
      "  value {\n",
      "    number_value: 4.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"final_average_return\"\n",
      "    value: 1.2616862058639526\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1689340036\n",
      "  nanos: 175927645\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1689340195\n",
      "}\n",
      ", id: \"3\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"batch-size\"\n",
      "  value {\n",
      "    number_value: 16.0\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"steps-per-loop\"\n",
      "  value {\n",
      "    number_value: 2.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"final_average_return\"\n",
      "    value: 0.8446623086929321\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1689340542\n",
      "  nanos: 787242052\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1689340697\n",
      "}\n",
      ", id: \"4\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"batch-size\"\n",
      "  value {\n",
      "    number_value: 16.0\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"steps-per-loop\"\n",
      "  value {\n",
      "    number_value: 2.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"final_average_return\"\n",
      "    value: 0.5608248114585876\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1689340542\n",
      "  nanos: 787388620\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1689340698\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# trials = None\n",
    "\n",
    "# while True:\n",
    "#     response = get_hyperparameter_tuning_job_sample(\n",
    "#         project=PROJECT_ID\n",
    "#         , hyperparameter_tuning_job_id=job_id\n",
    "#         , location=REGION\n",
    "#         , api_endpoint=f\"{REGION}-aiplatform.googleapis.com\"\n",
    "#     )\n",
    "    \n",
    "#     if response.state.name == 'JOB_STATE_SUCCEEDED':\n",
    "#         print(\"Job succeeded.\\nJob Time:\", response.update_time - response.create_time)\n",
    "#         trials = response.trials\n",
    "#         print(\"Trials:\", trials)\n",
    "#         break\n",
    "#     elif response.state.name == \"JOB_STATE_FAILED\":\n",
    "#         print(\"Job failed.\")\n",
    "#         break\n",
    "#     elif response.state.name == \"JOB_STATE_CANCELLED\":\n",
    "#         print(\"Job cancelled.\")\n",
    "#         break\n",
    "#     else:\n",
    "#         print(f\"Current job status: {response.state.name}.\")\n",
    "#     time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a06fed-f0e3-4136-8e11-913455cf0485",
   "metadata": {},
   "source": [
    "#### Find the best combination(s) hyperparameter(s) for each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0f47b4ad-8a43-4ca3-b78b-d24d23ccc20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter value(s):\n",
      "Metric=final_average_return: [{'batch-size': 16.0, 'steps-per-loop': 2.0}]\n"
     ]
    }
   ],
   "source": [
    "# if trials:\n",
    "#     # Dict mapping from metric names to the best metric values seen so far\n",
    "#     best_objective_values = dict.fromkeys(\n",
    "#         [metric.metric_id for metric in trials[0].final_measurement.metrics]\n",
    "#         , -np.inf\n",
    "#     )\n",
    "#     # Dict mapping from metric names to a list of the best combination(s) of\n",
    "#     # hyperparameter(s). Each combination is a dict mapping from hyperparameter\n",
    "#     # names to their values.\n",
    "#     best_params = defaultdict(list)\n",
    "#     for trial in trials:\n",
    "#         # `final_measurement` and `parameters` are `RepeatedComposite` objects.\n",
    "#         # Reference the structure above to extract the value of your interest.\n",
    "#         for metric in trial.final_measurement.metrics:\n",
    "#             params = {\n",
    "#                 param.parameter_id: param.value for param in trial.parameters\n",
    "#             }\n",
    "#             if metric.value > best_objective_values[metric.metric_id]:\n",
    "#                 best_params[metric.metric_id] = [params]\n",
    "#             elif metric.value == best_objective_values[metric.metric_id]:\n",
    "#                 best_params[param.parameter_id].append(params)  # Handle cases where multiple hyperparameter values lead to the same performance.\n",
    "#     print(\"Best hyperparameter value(s):\")\n",
    "#     for metric, params in best_params.items():\n",
    "#         print(f\"Metric={metric}: {sorted(params)}\")\n",
    "# else:\n",
    "#     print(\"No hyperparameter tuning job trials found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "eaf59c25-606f-4686-ae73-744285da0c3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter value(s):\n",
      "Metric=final_average_return: [{'batch-size': 8.0, 'steps-per-loop': 2.0}]\n"
     ]
    }
   ],
   "source": [
    "if hp_job.trials:\n",
    "    best_objective_values = dict.fromkeys(\n",
    "        [metric.metric_id for metric in hp_job.trials[0].final_measurement.metrics]\n",
    "        , -np.inf\n",
    "    )\n",
    "    best_params = defaultdict(list)\n",
    "    for trial in hp_job.trials:\n",
    "        for metric in trial.final_measurement.metrics:\n",
    "            # here\n",
    "            params = {\n",
    "                param.parameter_id: param.value for param in trial.parameters\n",
    "            }\n",
    "            if metric.value > best_objective_values[metric.metric_id]:\n",
    "                best_params[metric.metric_id] = [params]\n",
    "            elif metric.value == best_objective_values[metric.metric_id]:\n",
    "                best_params[param.parameter_id].append(params)  # Handle cases where multiple hyperparameter values lead to the same performance.\n",
    "    print(\"Best hyperparameter value(s):\")\n",
    "    for metric, params in best_params.items():\n",
    "        print(f\"Metric={metric}: {sorted(params)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "00ccabdd-57c6-4be3-94d9-09e3c39daa0a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# best = (None, None, None, 0.0)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      4\u001b[0m best_objective_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys(\n\u001b[0;32m----> 5\u001b[0m     [metric\u001b[38;5;241m.\u001b[39mmetric_id \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfinal_measurement\u001b[38;5;241m.\u001b[39mmetrics]\n\u001b[1;32m      6\u001b[0m     , \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m hp_job\u001b[38;5;241m.\u001b[39mtrials:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m trial\u001b[38;5;241m.\u001b[39mfinal_measurement\u001b[38;5;241m.\u001b[39mmetrics:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# here\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# best = (None, None, None, 0.0)\n",
    "# trials = None\n",
    "\n",
    "best_objective_values = dict.fromkeys(\n",
    "    [metric.metric_id for metric in trials[0].final_measurement.metrics]\n",
    "    , -np.inf\n",
    ")\n",
    "for trial in hp_job.trials:\n",
    "    for metric in trial.final_measurement.metrics:\n",
    "        # here\n",
    "        params = {\n",
    "            param.parameter_id: param.value for param in trial.parameters\n",
    "        }\n",
    "        if metric.value > best_objective_values[metric.metric_id]:\n",
    "            best_params[metric.metric_id] = [params]\n",
    "        elif metric.value == best_objective_values[metric.metric_id]:\n",
    "            best_params[param.parameter_id].append(params)  # Handle cases where multiple hyperparameter values lead to the same performance.\n",
    "print(\"Best hyperparameter value(s):\")\n",
    "for metric, params in best_params.items():\n",
    "    print(f\"Metric={metric}: {sorted(params)}\")\n",
    "            \n",
    "            \n",
    "#     # Keep track of the best outcome\n",
    "#     if float(trial.final_measurement.metrics[0].value) > best[3]:\n",
    "#         try:\n",
    "#             best = (\n",
    "#                 trial.id,\n",
    "#                 float(trial.parameters[0].value),\n",
    "#                 float(trial.parameters[1].value),\n",
    "#                 float(trial.final_measurement.metrics[0].value),\n",
    "#             )\n",
    "#         except:\n",
    "#             best = (\n",
    "#                 trial.id,\n",
    "#                 float(trial.parameters[0].value),\n",
    "#                 None,\n",
    "#                 float(trial.final_measurement.metrics[0].value),\n",
    "#             )\n",
    "\n",
    "# print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f615339b-fd88-4239-9c67-27157ade7ea6",
   "metadata": {},
   "source": [
    "#### Convert a combination of best hyperparameter(s) for a metric of interest to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0574bda0-f1f9-438f-b4b9-67754a769bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf $HPTUNING_RESULT_DIR\n",
    "# ! mkdir $HPTUNING_RESULT_DIR\n",
    " \n",
    "LOCAL_RESULTS_FILE = \"result.json\"  # {\"batch-size\": 8.0, \"steps-per-loop\": 2.0}\n",
    "\n",
    "with open(LOCAL_RESULTS_FILE, \"w\") as f:\n",
    "    json.dump(best_params[\"final_average_return\"][0], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a63794-61bb-415a-b08d-aebbb35abeb2",
   "metadata": {},
   "source": [
    "#### Upload the best hyperparameter(s) to GCS for use in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2a3f7eab-f1ff-4cea-8359-b17516d584ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt-v2/run-20230717-174256/hptuning/result.json\n"
     ]
    }
   ],
   "source": [
    "!gsutil -q cp $LOCAL_RESULTS_FILE $HPTUNING_RESULT_URI\n",
    "\n",
    "!gsutil ls $HPTUNING_RESULT_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc06d414-96dc-4f00-a9f9-f66f1e00669c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create custom prediction container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19025cc9-4b13-4c87-a138-395da7d92388",
   "metadata": {},
   "source": [
    "As with training, create a custom prediction container. This container handles the TF-Agents specific logic that is different from a regular TensorFlow Model. Specifically, it finds the predicted action using a trained policy. The associated source code is in `src/prediction/`.\n",
    "See other options for Vertex AI predictions [here](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions).\n",
    "\n",
    "#### Serve predictions:\n",
    "- Use [`tensorflow.saved_model.load`](https://www.tensorflow.org/agents/api_docs/python/tf_agents/policies/PolicySaver#usage), instead of [`tf_agents.policies.policy_loader.load`](https://github.com/tensorflow/agents/blob/r0.8.0/tf_agents/policies/policy_loader.py#L26), to load the trained policy, because the latter produces an object of type [`SavedModelPyTFEagerPolicy`](https://github.com/tensorflow/agents/blob/402b8aa81ca1b578ec1f687725d4ccb4115386d2/tf_agents/policies/py_tf_eager_policy.py#L137) whose `action()` is not compatible for use here.\n",
    "- Note that prediction requests contain only observation data but not reward. This is because: The prediction task is a standalone request that doesn't require prior knowledge of the system state. Meanwhile, end users only know what they observe at the moment. Reward is a piece of information that comes after the action has been made, so the end users would not have knowledge of said reward. In handling prediction requests, you create a [`TimeStep`](https://www.tensorflow.org/agents/api_docs/python/tf_agents/trajectories/TimeStep) object (consisting of `observation`, `reward`, `discount`, `step_type`) using the [`restart()`](https://www.tensorflow.org/agents/api_docs/python/tf_agents/trajectories/restart) function which takes in an `observation`. This function creates the *first* TimeStep in a trajectory of steps, where reward is 0, discount is 1 and step_type is marked as the first timestep. In other words, each prediction request forms the first `TimeStep` in a brand new trajectory.\n",
    "- For the prediction response, avoid using NumPy-typed values; instead, convert them to native Python values using methods such as [`tolist()`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.tolist.html) as opposed to `list()`.\n",
    "- There exists a prestart script in `src/prediction`. FastAPI executes this script before starting up the server. The `PORT` environment variable is set to equal `AIP_HTTP_PORT` in order to run FastAPI on the same port expected by Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "57f6f6b1-cc85-44f5-94b3-4d6ee802a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_SUBFOLDER = 'prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c454ef00-550a-4d79-84c0-e5922577b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{PRED_SUBFOLDER}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{PRED_SUBFOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "55c5c86a-0faa-4b9e-a70d-e47c8a104af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/prediction/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PRED_SUBFOLDER}/main.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Prediction server that uses a trained policy to give predicted actions.\"\"\"\n",
    "import os\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi import Request\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_agents\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "_model = tf.compat.v2.saved_model.load(os.environ[\"AIP_STORAGE_URI\"])\n",
    "\n",
    "\n",
    "@app.get(os.environ[\"AIP_HEALTH_ROUTE\"], status_code=200)\n",
    "def health():\n",
    "    \"\"\"\n",
    "    Handles server health check requests.\n",
    "\n",
    "    Returns:\n",
    "      An empty dict.\n",
    "    \"\"\"\n",
    "    return {}\n",
    "\n",
    "\n",
    "@app.post(os.environ[\"AIP_PREDICT_ROUTE\"])\n",
    "async def predict(request: Request):\n",
    "    \"\"\"\n",
    "    Handles prediction requests.\n",
    "\n",
    "    Unpacks observations in prediction requests and queries the trained policy for\n",
    "    predicted actions.\n",
    "\n",
    "    Args:\n",
    "      request: Incoming prediction requests that contain observations.\n",
    "\n",
    "    Returns:\n",
    "      A dict with the key `predictions` mapping to a list of predicted actions\n",
    "      corresponding to each observation in the prediction request.\n",
    "    \"\"\"\n",
    "    body = await request.json()\n",
    "    instances = body[\"instances\"]\n",
    "\n",
    "    predictions = []\n",
    "    for index, instance in enumerate(instances):\n",
    "        # Unpack request body and reconstruct TimeStep. Rewards default to 0.\n",
    "        batch_size = len(instance[\"observation\"])\n",
    "        \n",
    "        time_step = tf_agents.trajectories.restart(\n",
    "            observation=instance[\"observation\"]\n",
    "            , batch_size=tf.convert_to_tensor([batch_size])\n",
    "        )\n",
    "        policy_step = _model.action(time_step)\n",
    "\n",
    "        predictions.append(\n",
    "            {f\"PolicyStep {index}\": policy_step.action.numpy().tolist()}\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"predictions\": predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b8e3a99d-a3c7-414c-a074-2d1c415218cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/prediction/prestart.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PRED_SUBFOLDER}/prestart.sh\n",
    "#!/bin/bash\n",
    "export PORT=$AIP_HTTP_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b1a2fd-689d-441e-b309-b43ba36246f5",
   "metadata": {},
   "source": [
    "#### Define dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "65838998-455d-4168-be00-54b04512b59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pred_requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile pred_requirements.txt\n",
    "tf-agents==0.17.0\n",
    "tensorflow==2.12.0\n",
    "numpy\n",
    "six\n",
    "typing-extensions\n",
    "pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f58356e-938d-4199-a7be-1a384d174c5a",
   "metadata": {},
   "source": [
    "#### Write a Dockerfile\n",
    "\n",
    "Note: leave the server directory `app`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cec9808f-cfe2-471f-826e-1b92f8696cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCKERNAME = 'pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fe9e2c4b-5bf6-428a-bd32-196ecc30fc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile_pred\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile_{DOCKERNAME}\n",
    "\n",
    "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.10\n",
    "\n",
    "COPY src/prediction /app\n",
    "COPY pred_requirements.txt /app/requirements.txt\n",
    "\n",
    "RUN pip3 install -r /app/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba9f72-fcbb-4595-bc52-152cd3306ca3",
   "metadata": {},
   "source": [
    "#### Build the prediction container with Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "320eaaf5-b02a-4e02-88ed-6de388d75a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export DOCKERNAME=pred\n",
      "export IMAGE_URI=gcr.io/hybrid-vertex/prediction-custom-container\n",
      "export FILE_LOCATION=./\n",
      "export MACHINE_TYPE=e2-highcpu-32\n",
      "export ARTIFACTS_DIR=gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt-v2/run-20230717-174256/artifacts\n"
     ]
    }
   ],
   "source": [
    "PREDICTION_CONTAINER = \"prediction-custom-container\"\n",
    "\n",
    "# Docker definitions for training\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{PREDICTION_CONTAINER}'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './'\n",
    "\n",
    "print(f\"export DOCKERNAME={DOCKERNAME}\")\n",
    "print(f\"export IMAGE_URI={IMAGE_URI}\")\n",
    "print(f\"export FILE_LOCATION={FILE_LOCATION}\")\n",
    "print(f\"export MACHINE_TYPE={MACHINE_TYPE}\")\n",
    "print(f\"export ARTIFACTS_DIR={ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cc09f920-69fb-4886-82ae-0e5b5fd20652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! gcloud builds submit --config cloudbuild.yaml \\\n",
    "#     --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION,_ARTIFACTS_DIR=$ARTIFACTS_DIR \\\n",
    "#     --timeout=2h \\\n",
    "#     --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe724c1-2dcb-4d10-8cec-fb56fb088120",
   "metadata": {},
   "source": [
    "## Submit custom container training job\n",
    "\n",
    "- Note again that the bucket must be in the same regional location as the service location and it should not be multi-regional.\n",
    "- Read more of CustomContainerTrainingJob's source code [here](https://github.com/googleapis/python-aiplatform/blob/v0.8.0/google/cloud/aiplatform/training_jobs.py#L2153).\n",
    "- Like with local execution, you can use TensorBoard Profiler to track the training process and resources, and visualize the corresponding artifacts using the command: `%tensorboard --logdir $PROFILER_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "421336fc-8031-45c4-afb1-3579e7b95d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : scale-perarm-hpt\n",
      "RUN_NAME          : run-20230717-185029\n",
      "LOG_DIR           : gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230717-185029/tb-logs\n",
      "ROOT_DIR          : gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230717-185029/root\n",
      "ARTIFACTS_DIR     : gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230717-185029/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'scale-perarm-hpt'\n",
    "\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "LOG_DIR           = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/tb-logs\"\n",
    "ROOT_DIR          = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "98865883-8f1e-49c8-8172-82fcfa6dfe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "59c73f27-f79b-403e-9b77-1bfdc843c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_HYPERPARAMETER_TUNING       = False  # Execute regular training instead of hyperparameter tuning.\n",
    "TRAIN_WITH_BEST_HYPERPARAMETERS = True   # @param {type:\"bool\"} Whether to use learned hyperparameters in training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b1dc2e-ec7b-424b-9a4a-de40cfdd4b51",
   "metadata": {},
   "source": [
    "### set Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fe2866e1-3f4e-4bf0-a97e-760cf5c58cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME: projects/934903580331/locations/us-central1/tensorboards/5127972698365886464\n",
      "TB display name: scale-perarm-hpt-v2\n"
     ]
    }
   ],
   "source": [
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}-v2\"\n",
    "\n",
    "tensorboard = aiplatform.Tensorboard.create(\n",
    "    display_name=TENSORBOARD_DISPLAY_NAME\n",
    "    , project=PROJECT_ID\n",
    "    , location=REGION\n",
    ")\n",
    "\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "# use existing TB instance\n",
    "# TB_RESOURCE_NAME = 'projects/934903580331/locations/us-central1/tensorboards/6924469145035603968'\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name: {tensorboard.display_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71603efe-bfa5-4ec6-8103-cec850fb839e",
   "metadata": {},
   "source": [
    "### set training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ccca6c0b-26a2-479e-a386-94a39c199f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters.\n",
    "BATCH_SIZE       = 8         # Training and prediction batch size.\n",
    "TRAINING_LOOPS   = 100       # Number of training iterations.\n",
    "STEPS_PER_LOOP   = 2         # Number of driver steps per training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c28c490b-6764-4bc7-8552-10fea73a0d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"args: ['--data-path=gs://mabv1-hybrid-vertex-bucket/data', \"\n",
      " \"'--bucket_name=mabv1-hybrid-vertex-bucket', '--data_gcs_prefix=data', \"\n",
      " \"'--data_path=gs://mabv1-hybrid-vertex-bucket/data', \"\n",
      " \"'--project_number=934903580331', '--batch-size=8', '--rank-k=20', \"\n",
      " \"'--num-actions=20', '--tikhonov-weight=0.001', '--agent-alpha=10.0', \"\n",
      " \"'--training_loops=100', '--steps-per-loop=2', '--distribute=single', \"\n",
      " \"'--artifacts_dir=gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230717-185029/artifacts', \"\n",
      " \"'--root_dir=gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230717-185029/root', \"\n",
      " \"'--experiment_name=scale-perarm-hpt', \"\n",
      " \"'--experiment_run=run-20230717-185029', '--train-with-best-hyperparameters', \"\n",
      " \"'--best-hyperparameters-bucket=mabv1-hybrid-vertex-bucket', \"\n",
      " \"'--best-hyperparameters-path=scale-perarm-hpt-v2/run-20230717-174256/hptuning/result.json']\")\n"
     ]
    }
   ],
   "source": [
    "args = [\n",
    "    f\"--data-path={DATA_PATH}\"               # TODO - remove duplicate arg\n",
    "    , f\"--bucket_name={BUCKET_NAME}\"\n",
    "    , f\"--data_gcs_prefix={DATA_GCS_PREFIX}\"\n",
    "    , f\"--data_path={DATA_PATH}\"\n",
    "    , f\"--project_number={PROJECT_NUM}\"\n",
    "    , f\"--batch-size={BATCH_SIZE}\"\n",
    "    , f\"--rank-k={RANK_K}\"\n",
    "    , f\"--num-actions={NUM_ACTIONS}\"\n",
    "    , f\"--tikhonov-weight={TIKHONOV_WEIGHT}\"\n",
    "    , f\"--agent-alpha={AGENT_ALPHA}\"\n",
    "    , f\"--training_loops={TRAINING_LOOPS}\"\n",
    "    , f\"--steps-per-loop={STEPS_PER_LOOP}\"\n",
    "    , f\"--distribute={DISTRIBUTE_STRATEGY}\"\n",
    "    , f\"--artifacts_dir={ARTIFACTS_DIR}\"\n",
    "    , f\"--root_dir={ROOT_DIR}\"\n",
    "    , f\"--experiment_name={EXPERIMENT_NAME}\"\n",
    "    , f\"--experiment_run={RUN_NAME}\"\n",
    "]\n",
    "\n",
    "if RUN_HYPERPARAMETER_TUNING:\n",
    "    args.append(\"--run-hyperparameter-tuning\")\n",
    "elif TRAIN_WITH_BEST_HYPERPARAMETERS:\n",
    "    args.append(\"--train-with-best-hyperparameters\")\n",
    "    args.append(f\"--best-hyperparameters-bucket={BUCKET_NAME}\")\n",
    "    args.append(f\"--best-hyperparameters-path={HPTUNING_RESULT_PATH}\")\n",
    "    \n",
    "from pprint import pprint\n",
    "pprint(f\"args: {args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c437f0d1-f5e5-4dcc-b08c-efd964bd4f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aiplatform.init(\n",
    "#     project=PROJECT_ID\n",
    "#     , location=REGION\n",
    "#     , experiment=EXPERIMENT_NAME\n",
    "#     # , staging_bucket=ROOT_DIR\n",
    "# )\n",
    "\n",
    "# JOB_NAME = f\"mvl-hpt-{RUN_NAME}\"\n",
    "# print(f\"JOB_NAME: {JOB_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "45b07bef-7efa-4cfa-981a-59a75eaefeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a CustomJob\n",
    "# my_custom_hpt_job = aiplatform.CustomJob(\n",
    "#     display_name=JOB_NAME\n",
    "#     , project=PROJECT_ID\n",
    "#     , worker_pool_specs=WORKER_POOL_SPECS\n",
    "#     , staging_bucket=ROOT_DIR\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d60f019a-87c8-4e51-9178-d20ff648d216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Spec: container_spec {\n",
      "  image_uri: \"gcr.io/hybrid-vertex/prediction-custom-container:latest\"\n",
      "  predict_route: \"/predict\"\n",
      "  health_route: \"/health\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=f\"mvl-train-job-{RUN_NAME}\"\n",
    "    , container_uri=f\"gcr.io/{PROJECT_ID}/{HPTUNING_TRAINING_CONTAINER}:latest\"\n",
    "    , command=[\"python3\", \"-m\", \"src.per_arm_rl.task\"] + args  # Pass in training arguments, including hyperparameters.\n",
    "    , model_serving_container_image_uri=f\"gcr.io/{PROJECT_ID}/{PREDICTION_CONTAINER}:latest\"\n",
    "    , model_serving_container_predict_route=\"/predict\"\n",
    "    , model_serving_container_health_route=\"/health\"\n",
    ")\n",
    "\n",
    "print(\"Training Spec:\", job._managed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1ac5e9dd-1557-4d61-8e95-b27a4200211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = job.run(\n",
    "    model_display_name = f\"{PREFIX}-perarm-model\"\n",
    "    , replica_count = 1\n",
    "    , machine_type = \"n1-standard-16\"\n",
    "    , accelerator_type = ACCELERATOR_TYPE\n",
    "    , tensorboard=TB_RESOURCE_NAME\n",
    "    , accelerator_count = PER_MACHINE_ACCELERATOR_COUNT\n",
    "    , enable_web_access = True\n",
    "    , restart_job_on_worker_restart = False\n",
    "    , sync=False\n",
    "    , service_account=VERTEX_SA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "db62f96b-b436-4342-b751-a54b5df08eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model display name: mabv1-perarm-model\n",
      "Model ID: 7766802804051542016\n"
     ]
    }
   ],
   "source": [
    "print(\"Model display name:\", model.display_name)\n",
    "print(\"Model ID:\", model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8878e9-399d-4c10-a0d6-4d6dc4ea3be7",
   "metadata": {},
   "source": [
    "### Deploy trained model to an Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6ca9d1e0-492e-42c6-ba55-d80deaad65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2b501650-6f91-4365-ba59-d4e84ee6f7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint display name: mabv1-perarm-model_endpoint\n",
      "Endpoint ID: 1696656392821145600\n"
     ]
    }
   ],
   "source": [
    "print(\"Endpoint display name:\", endpoint.display_name)\n",
    "print(\"Endpoint ID:\", endpoint.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb45c9-2482-4338-bd2c-c6d5e142018c",
   "metadata": {},
   "source": [
    "### Predict on the Endpoint\n",
    "- Put prediction input(s) into a list named `instances`. The observation should of dimension (BATCH_SIZE, RANK_K). Read more about the MovieLens simulation environment observation [here](https://github.com/tensorflow/agents/blob/v0.8.0/tf_agents/bandits/environments/movielens_py_environment.py#L32-L138).\n",
    "- Read more about the endpoint prediction API [here](https://cloud.google.com/sdk/gcloud/reference/ai/endpoints/predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "758e7a3b-80a6-4b74-9db1-c2675c6f37f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "500 Internal Server Error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:65\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/grpc/_channel.py:1030\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1028\u001b[0m state, call, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[1;32m   1029\u001b[0m                               wait_for_ready, compression)\n\u001b[0;32m-> 1030\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/grpc/_channel.py:910\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"Internal Server Error\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.251.171.95:443 {created_time:\"2023-07-17T19:20:25.314563449+00:00\", grpc_status:13, grpc_message:\"Internal Server Error\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## TODO\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m22\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:1559\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict)\u001b[0m\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1547\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mjson_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1548\u001b[0m         deployed_model_id\u001b[38;5;241m=\u001b[39mraw_predict_response\u001b[38;5;241m.\u001b[39mheaders[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1556\u001b[0m         ),\n\u001b[1;32m   1557\u001b[0m     )\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1559\u001b[0m     prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gca_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1567\u001b[0m         predictions\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   1568\u001b[0m             json_format\u001b[38;5;241m.\u001b[39mMessageToDict(item)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1573\u001b[0m         model_resource_name\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m   1574\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:602\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    597\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(metadata) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m    598\u001b[0m     gapic_v1\u001b[38;5;241m.\u001b[39mrouting_header\u001b[38;5;241m.\u001b[39mto_grpc_metadata(((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mendpoint),)),\n\u001b[1;32m    599\u001b[0m )\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata)\n\u001b[1;32m    111\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:67\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mInternalServerError\u001b[0m: 500 Internal Server Error"
     ]
    }
   ],
   "source": [
    "## TODO\n",
    "\n",
    "endpoint.predict(\n",
    "    instances=[\n",
    "        {\"observation\": [list(np.ones(22)) for _ in range(8)]},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9634a-80d0-4058-a88d-aa0e566573ba",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67abc70e-6546-45e2-aba2-c1e65ba8b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete endpoint resource\n",
    "# ! gcloud ai endpoints delete $endpoint.name --quiet --region $REGION\n",
    "\n",
    "# # Delete model resource\n",
    "# ! gcloud ai models delete $model.name --quiet\n",
    "\n",
    "# # Delete Cloud Storage objects that were created\n",
    "# ! gsutil -m rm -r $ARTIFACTS_DIR"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
