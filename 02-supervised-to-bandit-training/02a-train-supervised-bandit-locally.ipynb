{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385307d6-2058-47ec-8da0-57f7ef5c43d6",
   "metadata": {},
   "source": [
    "# Train Bandits with per-arm features\n",
    "\n",
    "**Exploring linear and nonlinear** (e.g., those with neural network-based value functions) bandit methods for recommendations using TF-Agents\n",
    "\n",
    "> Neural linear bandits provide a nice way to leverage the representation power of deep learning and the bandit approach for uncertainty measure and efficient exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e9956-66cd-4bf4-9b4d-8c2c646f0313",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "In this notebook, we explore the following topics for training contextual bandits with per-arm features:\n",
    "\n",
    "1. Data preperation\n",
    "2. Sampling functions\n",
    "3. TensorSpecs\n",
    "4. Agent, Network, training policy\n",
    "5. Reward function\n",
    "6. Trajectory function\n",
    "7. Train & Eval loops\n",
    "8. Getting predictions -\n",
    "9. Preparing the training application - abstracting all steps above to be used in subsequent notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fd257-b98b-426a-a2cd-024429b014f1",
   "metadata": {},
   "source": [
    "## Load notebook config\n",
    "\n",
    "* use the prefix defined in `00-env-setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39c9d08-d118-4013-a47f-88450f49f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX: rec-bandits-v2\n"
     ]
    }
   ],
   "source": [
    "# PREFIX = 'mabv1'\n",
    "VERSION        = \"v2\"                       # TODO\n",
    "PREFIX         = f'rec-bandits-{VERSION}'   # TODO\n",
    "\n",
    "print(f\"PREFIX: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908f6b95-b539-4a9f-a836-840d26ea3b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"rec-bandits-v2\"\n",
      "VERSION                  = \"v2\"\n",
      "\n",
      "BUCKET_NAME              = \"rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "BUCKET_URI               = \"gs://rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_NAME    = \"mvlens_rec_bandits_v2\"\n",
      "BIGQUERY_TABLE_NAME      = \"training_dataset\"\n",
      "\n",
      "REPOSITORY               = \"rl-movielens-rec-bandits-v2\"\n",
      "\n",
      "DOCKERNAME_01            = \"Dockerfile_train_my_perarm_env\"\n",
      "IMAGE_NAME_01            = \"train-my-perarm-env-v2\"\n",
      "IMAGE_URI_01             = \"gcr.io/hybrid-vertex/train-my-perarm-env-v2\"\n",
      "\n",
      "DOCKERNAME_02            = \"Dockerfile_perarm_feats\"\n",
      "IMAGE_NAME_02            = \"train-perarm-feats-v2\"\n",
      "IMAGE_URI_02             = \"gcr.io/hybrid-vertex/train-perarm-feats-v2\"\n",
      "\n",
      "DOCKERNAME_03            = \"Dockerfile_ranking_bandit\"\n",
      "IMAGE_NAME_03            = \"train-rank-bandit-v2\"\n",
      "IMAGE_URI_03             = \"gcr.io/hybrid-vertex/train-rank-bandit-v2\"\n",
      "\n",
      "DOCKERNAME_04            = \"Dockerfile_train_bandit_e2e\"\n",
      "IMAGE_NAME_04            = \"train-mab-e2e-v2\"\n",
      "IMAGE_URI_04             = \"gcr.io/hybrid-vertex/train-mab-e2e-v2\"\n",
      "\n",
      "DOCKERNAME_04_pred       = \"Dockerfile_pred_bandit_e2e\"\n",
      "IMAGE_NAME_04_pred       = \"pred-mab-e2e-v2\"\n",
      "IMAGE_URI_04_pred        = \"gcr.io/hybrid-vertex/pred-mab-e2e-v2\"\n",
      "\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-rec-bandits-v2/local_docker_tfa\"\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c690a9-e2bd-4759-ba41-4e2469098aee",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d0dfe4-695c-4dd4-9f24-67f7488ce1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c669f1a1-1af7-4efb-ab2d-6bf3b3847991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import functools\n",
    "import collections\n",
    "import pickle as pkl\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf-agents\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.metrics import export_utils\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.bandits.policies import policy_utilities\n",
    "from tf_agents.bandits.agents import neural_linucb_agent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "from tf_agents.bandits.agents import neural_epsilon_greedy_agent\n",
    "from tf_agents.bandits.networks import global_and_arm_feature_network\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "\n",
    "# GPU\n",
    "from numba import cuda \n",
    "import gc\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# this repo\n",
    "from src import train_utils as train_utils\n",
    "from src import reward_factory as reward_factory\n",
    "from src.data import data_utils as data_utils\n",
    "from src.data import data_config as data_config\n",
    "from src.trainer import eval_perarm as eval_perarm\n",
    "from src.trainer import train_perarm as train_perarm\n",
    "from src.agents import agent_factory as agent_factory\n",
    "from src.networks import encoding_network as emb_features\n",
    "\n",
    "# tf exceptions and vars\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e58dd7-ab2b-419f-9771-bf1e98db758b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4969d3e-1fc0-45db-8a69-aa6b342019de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "274e7f4a-1802-4946-888e-876638f5c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a866b1-85b9-43e6-9546-edfbbf886bce",
   "metadata": {},
   "source": [
    "# [1] Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d4ed28-23d7-4785-b327-e5b543b0edb9",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "* Load train and eval datasets from TFRecords created in the `01-movielens-data-prep.ipynb` notebook\n",
    "* training examples represent historical (previously collected) interaction data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc3fcebe-818b-4767-afdc-cfb65b3b953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCS_DATA_PATH: gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/mv_b128_g12_a16/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/mv_b128_g12_a16_v4/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/mv_b128_g12_a16_v5/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/train/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/val/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/vocabs/\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_GEN_GCS_PATH = data_config.EXAMPLE_GEN_GCS_PATH\n",
    "GCS_DATA_PATH = f\"{BUCKET_URI}/{EXAMPLE_GEN_GCS_PATH}\"\n",
    "\n",
    "print(f\"GCS_DATA_PATH: {GCS_DATA_PATH}\")\n",
    "\n",
    "!gsutil ls $GCS_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5b953-14c0-42ed-a511-77147a1bc0ac",
   "metadata": {},
   "source": [
    "### Read TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d0157c8-a04c-4dbd-b6d9-a1ede97687a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da0cdac-76af-4608-b70e-be7dbe0584b5",
   "metadata": {},
   "source": [
    "**Train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c934c06-bf08-4c7f-b0cc-0de04ef3515c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/train/ml-1m-gen-001-of-008.tfrecord',\n",
       " 'gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/train/ml-1m-gen-002-of-008.tfrecord',\n",
       " 'gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/train/ml-1m-gen-003-of-008.tfrecord']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT = \"train\" # \"train\" | \"val\"\n",
    "\n",
    "train_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{EXAMPLE_GEN_GCS_PATH}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "train_files = train_files[:3]\n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7511e4d-bf81-4800-bde7-8b16dec9aeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_movie_genres': <tf.Tensor: shape=(1, 10), dtype=string, numpy=\n",
      "array([[b'Drama', b'UNK', b'UNK', b'UNK', b'UNK', b'UNK', b'UNK', b'UNK',\n",
      "        b'UNK', b'UNK']], dtype=object)>,\n",
      " 'target_movie_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'1775'], dtype=object)>,\n",
      " 'target_movie_rating': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([4.], dtype=float32)>,\n",
      " 'target_movie_title': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Live Flesh (1997)'], dtype=object)>,\n",
      " 'target_movie_year': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1997])>,\n",
      " 'target_rating_timestamp': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([974612615])>,\n",
      " 'user_age': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([50])>,\n",
      " 'user_gender': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'M'], dtype=object)>,\n",
      " 'user_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'2173'], dtype=object)>,\n",
      " 'user_occupation_text': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'programmer'], dtype=object)>,\n",
      " 'user_zip_code': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'87505'], dtype=object)>}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "train_dataset = train_dataset.map(data_utils._parse_function)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452750c1-46a0-4ee1-84d4-0c3b227de38f",
   "metadata": {},
   "source": [
    "**Val data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7b5376f-9438-4304-84aa-de09c30f5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = \"val\"\n",
    "\n",
    "val_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{EXAMPLE_GEN_GCS_PATH}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        val_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "val_dataset = tf.data.TFRecordDataset(val_files)\n",
    "val_dataset = val_dataset.map(data_utils._parse_function, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8959967-522e-41c8-9a1b-050ca8bc191f",
   "metadata": {},
   "source": [
    "### get vocab\n",
    "\n",
    "**TODO:** \n",
    "* streamline vocab calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3ccf137-7a72-42e7-aa89-3c81a99cf40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vocab...\n",
      "Downloaded vocab from: gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/vocabs/vocab_dict.pkl\n",
      "\n",
      "'movie_id'\n",
      "'movie_year'\n",
      "'movie_genre'\n",
      "'movie_title'\n",
      "'user_id'\n",
      "'user_gender_vocab'\n",
      "'user_age_vocab'\n",
      "'user_occ_vocab'\n",
      "'user_zip_vocab'\n",
      "'min_timestamp'\n",
      "'max_timestamp'\n",
      "'timestamp_buckets'\n"
     ]
    }
   ],
   "source": [
    "EXISTING_VOCAB_FILE = f'gs://{BUCKET_NAME}/{EXAMPLE_GEN_GCS_PATH}/vocabs/{VOCAB_FILENAME}'\n",
    "print(f\"Downloading vocab...\")\n",
    "\n",
    "os.system(f'gsutil -q cp {EXISTING_VOCAB_FILE} .')\n",
    "print(f\"Downloaded vocab from: {EXISTING_VOCAB_FILE}\\n\")\n",
    "\n",
    "filehandler = open(VOCAB_FILENAME, 'rb')\n",
    "vocab_dict = pkl.load(filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "for key in vocab_dict.keys():\n",
    "    pprint(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfda012c-a2c3-4384-a5a4-54f5c6649006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocab_dict['user_occupation_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0138a295-2b0d-4359-8496-af8552ec8cff",
   "metadata": {},
   "source": [
    "# [2] Preprocessing layers for global and arm features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28c002-8d3a-4cf4-b69b-de56a6abab90",
   "metadata": {},
   "source": [
    "The preproccesing layers will ultimately feed the two sampling functions described below. These sampling functions will be used to create [trajectories](https://github.com/tensorflow/agents/blob/master/tf_agents/trajectories/trajectory.py#L36) (i.e., the training examples for our model)\n",
    "\n",
    "`global_context_sampling_fn`: \n",
    "* A function that outputs a random 1d array or list of ints or floats\n",
    "* This output is the global context. Its shape and type must be consistent across calls.\n",
    "\n",
    "`arm_context_sampling_fn`: \n",
    "* A function that outputs a random 1 array or list of ints or floats (same type as the output of `global_context_sampling_fn`). * This output is the per-arm context. Its shape must be consistent across calls.\n",
    "\n",
    "See `debug-and-learn-emb-models.ipynb` for more details and debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ea1f7-0621-44b8-b062-1f59b0964d72",
   "metadata": {},
   "source": [
    "## Global & Per-Arm feature embedding models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a1063-15d4-472e-b5a7-d92dcdea3c0f",
   "metadata": {},
   "source": [
    "**get expected dimensions**\n",
    "\n",
    "**common layers**\n",
    "* layer sizes for the final tower\n",
    "* The network that takes as input the concatenation of \n",
    "  the outputs of the global and the arm networks\n",
    "*  hidden layers ideally divisible by 8, e.g., [16, 32, 64, 128, 256, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8d77956-635c-438a-916a-185eec52f9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPECTED_GLOBAL_DIM: 72\n",
      "EXPECTED_PER_ARM_DIM: 64\n",
      "EXPECTED_GLOBAL_LAYERS      : [72, 36, 18]\n",
      "EXPECTED_ARM_LAYERS         : [64, 32, 16]\n",
      "EXPECTED_COMMON_LAYERS      : [34, 17, 8]\n"
     ]
    }
   ],
   "source": [
    "NUM_OOV_BUCKETS        = 1\n",
    "GLOBAL_EMBEDDING_SIZE  = 12\n",
    "MV_EMBEDDING_SIZE      = 16\n",
    "\n",
    "NUM_GLOBAL_FEATURES = len(data_utils.USER_FEATURE_NAMES)     # 6\n",
    "NUM_ARM_FEATURES    = len(data_utils.MOVIE_FEATURE_NAMES)    # 5\n",
    "EXPECTED_GLOBAL_DIM  = GLOBAL_EMBEDDING_SIZE * NUM_GLOBAL_FEATURES\n",
    "EXPECTED_PER_ARM_DIM = MV_EMBEDDING_SIZE * NUM_ARM_FEATURES\n",
    "print(f\"EXPECTED_GLOBAL_DIM: {EXPECTED_GLOBAL_DIM}\")\n",
    "print(f\"EXPECTED_PER_ARM_DIM: {EXPECTED_PER_ARM_DIM}\")\n",
    "\n",
    "EXPECTED_GLOBAL_LAYERS   = [\n",
    "    EXPECTED_GLOBAL_DIM, \n",
    "    int(EXPECTED_GLOBAL_DIM/2), \n",
    "    int(EXPECTED_GLOBAL_DIM/4)\n",
    "]\n",
    "EXPECTED_ARM_LAYERS      = [\n",
    "    EXPECTED_PER_ARM_DIM, \n",
    "    int(EXPECTED_PER_ARM_DIM/2), \n",
    "    int(EXPECTED_PER_ARM_DIM/4)\n",
    "]\n",
    "EXPECTED_FIRST_COMMON_LAYER = EXPECTED_GLOBAL_LAYERS[-1] + EXPECTED_ARM_LAYERS[-1]\n",
    "EXPECTED_COMMON_LAYERS = [\n",
    "    int(EXPECTED_FIRST_COMMON_LAYER), \n",
    "    int(EXPECTED_FIRST_COMMON_LAYER/2), \n",
    "    int(EXPECTED_FIRST_COMMON_LAYER/4)\n",
    "]\n",
    "print(f\"EXPECTED_GLOBAL_LAYERS      : {EXPECTED_GLOBAL_LAYERS}\")\n",
    "print(f\"EXPECTED_ARM_LAYERS         : {EXPECTED_ARM_LAYERS}\")\n",
    "print(f\"EXPECTED_COMMON_LAYERS      : {EXPECTED_COMMON_LAYERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c33673e-6069-477a-af80-0d2c436099bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.perarm_features import emb_feature_v2 as emb_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea335991-dcec-40b3-ba0d-a0fdfb9c2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test emb models\n",
    "for i in range(1):\n",
    "    \n",
    "    iterator = iter(train_dataset.batch(1))\n",
    "    data = next(iterator)\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48178fff-491f-4f03-b156-7765d1fa9707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.networks.encoding_network.EmbeddingModel at 0x7fb184fe9990>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = emb_features.EmbeddingModel(\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    "    max_genre_length = data_config.MAX_GENRE_LENGTH\n",
    ")\n",
    "\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "903c63c5-2468-4443-bcb8-5ff6d69810dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL_DIM: 72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 72), dtype=float32, numpy=\n",
       "array([[-0.00954443, -0.02403196,  0.04878663, -0.00168763, -0.02886732,\n",
       "         0.02556446,  0.01497078,  0.00508047,  0.04433241, -0.02069455,\n",
       "         0.02263496,  0.04296675,  0.03206808,  0.03713808, -0.00061151,\n",
       "        -0.0239444 , -0.00398848,  0.02038134,  0.02925194, -0.03548796,\n",
       "        -0.04818512, -0.04563278,  0.00500185, -0.00052773,  0.04147378,\n",
       "         0.01062421, -0.0488079 ,  0.03812608,  0.02836689, -0.01253514,\n",
       "         0.00256438, -0.0349028 , -0.0311203 , -0.00767165, -0.00801482,\n",
       "        -0.0308045 ,  0.0373571 , -0.00265469,  0.00124051, -0.00683895,\n",
       "        -0.02101296, -0.00094485,  0.03022655, -0.01573528, -0.02096498,\n",
       "        -0.01417056,  0.04562125, -0.00982308,  0.0391654 ,  0.03760383,\n",
       "         0.03286931, -0.03916093,  0.04995331, -0.00295743, -0.02619722,\n",
       "        -0.04954935,  0.02836919,  0.02686154, -0.00949102,  0.01771566,\n",
       "         0.02565399,  0.00791726,  0.00750047,  0.02091935, -0.0129848 ,\n",
       "        -0.02602668, -0.04123507, -0.02540704,  0.01971879,  0.02493007,\n",
       "        -0.01434119, -0.0371298 ]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_globals = embs._get_global_context_features(data)\n",
    "\n",
    "GLOBAL_DIM = test_globals.shape[1]            \n",
    "# shape checks out at batch_dim, nactions, arm feats\n",
    "print(f\"GLOBAL_DIM: {GLOBAL_DIM}\")\n",
    "\n",
    "test_globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebec4863-c614-4248-b680-8e29ec5b730d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER_ARM_DIM: 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 64), dtype=float32, numpy=\n",
       "array([[-0.01567669,  0.00467674,  0.0147797 ,  0.04148023,  0.00753425,\n",
       "         0.0108854 , -0.02002864, -0.01253166, -0.00871527, -0.01548574,\n",
       "        -0.00943429, -0.00079099,  0.03119593,  0.04229933, -0.01335735,\n",
       "        -0.00392927,  0.00333258,  0.02858807,  0.04337367, -0.00730864,\n",
       "        -0.03244675,  0.01234595,  0.04639426,  0.00474442,  0.01491665,\n",
       "        -0.0315549 , -0.01064789, -0.01711632,  0.01663466, -0.04666859,\n",
       "         0.007291  , -0.01495687, -0.04950098, -0.00944309,  0.02771567,\n",
       "        -0.02391213, -0.03713242, -0.02827439,  0.00476832, -0.03760714,\n",
       "        -0.02368922,  0.02933912,  0.0096979 , -0.00493972, -0.04697755,\n",
       "        -0.04118456, -0.03631474,  0.02600015,  0.17577843,  0.17099933,\n",
       "        -0.05267937, -0.0255987 , -0.09529316, -0.04079174, -0.04933178,\n",
       "         0.13072246,  0.04872721,  0.22688693,  0.17985678,  0.14281774,\n",
       "         0.00101691,  0.1542656 ,  0.15358084, -0.16454875]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arms = embs._get_per_arm_features(data)\n",
    "\n",
    "PER_ARM_DIM = test_arms.shape[1]            \n",
    "# shape checks out at batch_dim, nactions, arm feats\n",
    "print(f\"PER_ARM_DIM: {PER_ARM_DIM}\")\n",
    "\n",
    "test_arms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb4902-5bb9-4587-8d29-c01d78b006be",
   "metadata": {},
   "source": [
    "# [3] TensorSpecs\n",
    "\n",
    "* say you have a global observation spec of `[17]`. And you have two batch dimensions `[4, 5]`. Then your observation has to have the shape `[4, 5, 17]`\n",
    "* and then if you have arm_obs_spec with shape `[9, 13]`, then the arm obs shape has to be exactly `[4, 5, 9, 13]`\n",
    "* and this has to be true for every single tensor in your tensor nest\n",
    "* the first 2 dims are the outer dims that are the same for all tensors, the rest of the dimensions have to follow the spec for each tensor\n",
    "\n",
    "**TODO:**\n",
    "* explain relationship between Tensor Specs and their Tensor counterparts\n",
    "* highlight the errors, lessons learned, and utility functions to address these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ff9baaf-987d-448d-a981-742a79b581e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE      : 128\n",
      "EVAL_BATCH_SIZE : 1\n",
      "NUM_ACTIONS     : 2\n",
      "GLOBAL_DIM      : 72\n",
      "PER_ARM_DIM     : 64\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE      = 128\n",
    "EVAL_BATCH_SIZE = 1\n",
    "\n",
    "NUM_ACTIONS     = 2 #this is kinda deceptive - \n",
    "#our approach is to learn by \"flashing\" one movie rating at a time per user context. \n",
    "#The n_actions = show/don't show the movie with one degree of freedom (n-1)\n",
    "\n",
    "\n",
    "print(f\"BATCH_SIZE      : {BATCH_SIZE}\")\n",
    "print(f\"EVAL_BATCH_SIZE : {EVAL_BATCH_SIZE}\")\n",
    "print(f\"NUM_ACTIONS     : {NUM_ACTIONS}\")\n",
    "print(f\"GLOBAL_DIM      : {GLOBAL_DIM}\")\n",
    "print(f\"PER_ARM_DIM     : {PER_ARM_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20eca8d-8c73-4ec8-9d0f-f2b428055ac2",
   "metadata": {},
   "source": [
    "## Implementing MAB with TF-Agents\n",
    "\n",
    "In TF-Agents, the *per-arm features* implementation differs from the *global-only* feature examples in the following aspects:\n",
    "* Reward is modeled not per-arm, but globally.\n",
    "* The arms are permutation invariant: it doesn’t matter which arm is arm 1 or arm 2, only their features.\n",
    "* One can have a different number of arms to choose from in every step (note that unspecified/dynamically changing number of arms will have a problem with XLA compatibility).\n",
    "\n",
    "When implementing per-arm features in TF-Bandits, the following details have to be discussed:\n",
    "* Observation spec and observations,\n",
    "* Action spec and actions,\n",
    "* Implementation of specific policies and agents.\n",
    "\n",
    "\n",
    "**TODO:**\n",
    "* outline the components and highlight their interactions, dependencies on eachother, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68ebe8-116d-43b3-a6e1-4f5a5c7f4741",
   "metadata": {},
   "source": [
    "### Observation spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c4746-d01b-4ca4-aa53-ab68da54d37a",
   "metadata": {},
   "source": [
    "**This observation spec allows the user to have a global observation of fixed dimension**, and an unspecified number of *per-arm* features (also of fixed dimension)\n",
    "* The actions output by the policy are still integers as usual, and they indicate which row of the arm-features it has chosen \n",
    "* The action spec must be a single integer value without boundaries:\n",
    "\n",
    "```python\n",
    "global_spec = tensor_spec.TensorSpec([GLOBAL_DIM], tf.float32)\n",
    "per_arm_spec = tensor_spec.TensorSpec([None, PER_ARM_DIM], tf.float32)\n",
    "observation_spec = {'global': global_spec, 'per_arm': per_arm_spec}\n",
    "\n",
    "action_spec = tensor_spec.TensorSpec((), tf.int32)\n",
    "```\n",
    "> Here the only difference compared to the action spec with global features only is that the tensor spec is not bounded, as we don’t know how many arms there will be at any time step\n",
    "\n",
    "**XLA compatibility:**\n",
    "* Since dynamic tensor shapes are not compatible with XLA, the number of arm features (and consequently, number of arms for a step) cannot be dynamic. \n",
    "* One workaround is to fix the maximum number of arms for a problem, then pad the arm features in steps with fewer arms, and use action masking to indicate how many arms are actually active.\n",
    "\n",
    "```python\n",
    "per_arm_spec = tensor_spec.TensorSpec([NUM_ACTIONS, PER_ARM_DIM], tf.float32)\n",
    "\n",
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=(), dtype=tf.int32, minimum = 0, maximum = NUM_ACTIONS - 1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36bd3b33-635a-4274-8b9e-7172696ebb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_spec = {\n",
    "    'global': tf.TensorSpec([GLOBAL_DIM], tf.float32),\n",
    "    'per_arm': tf.TensorSpec([NUM_ACTIONS, PER_ARM_DIM], tf.float32) #excluding action dim here\n",
    "}\n",
    "observation_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2da92-7db2-4f42-94a7-b7bad1c8fc42",
   "metadata": {},
   "source": [
    "### Action spec\n",
    "\n",
    "> The time_step_spec and action_spec are specifications for the input time step and the output action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af42c7-18d7-480f-a7f3-d7da3f4840eb",
   "metadata": {},
   "source": [
    "```python\n",
    "    if (\n",
    "        not tensor_spec.is_bounded(action_spec)\n",
    "        or not tensor_spec.is_discrete(action_spec)\n",
    "        or action_spec.shape.rank > 1\n",
    "        or action_spec.shape.num_elements() != 1\n",
    "    ):\n",
    "      raise NotImplementedError(\n",
    "          'action_spec must be a BoundedTensorSpec of type int32 and shape (). '\n",
    "          'Found {}.'.format(action_spec)\n",
    "      )\n",
    "```\n",
    "\n",
    "* [src](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/policies/reward_prediction_base_policy.py#L97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "549a123c-349a-4103-b39a-4502f47d1e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=[], \n",
    "    dtype=tf.int32,\n",
    "    minimum=tf.constant(0),            \n",
    "    maximum=NUM_ACTIONS-1, # n degrees of freedom and will dictate the expected mean reward spec shape\n",
    "    name=\"action_spec\"\n",
    ")\n",
    "\n",
    "action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36ee2635-d4e3-4468-886c-ae9c62e3c80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected_num_actions: 2\n",
      "predicted_rewards_mean: TensorSpec(shape=(2,), dtype=tf.float32, name=None)\n"
     ]
    }
   ],
   "source": [
    "expected_num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "print(f\"expected_num_actions: {expected_num_actions}\")\n",
    "\n",
    "predicted_rewards_mean = tensor_spec.TensorSpec([expected_num_actions])\n",
    "print(f\"predicted_rewards_mean: {predicted_rewards_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a51efce-9f1b-42d1-bec4-7b788e3fd7e0",
   "metadata": {},
   "source": [
    "### TimeStep spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95f05860-0fbf-4a5a-8273-9c81761e0ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': {'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None),\n",
       "                 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)},\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec = ts.time_step_spec(\n",
    "    observation_spec = observation_spec, \n",
    "    # reward_spec = _reward_spec\n",
    ")\n",
    "time_step_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b129a-6d19-4b3d-a2e7-e27070f57ac0",
   "metadata": {},
   "source": [
    "### Reward Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b48e89aa-e010-4bd9-a7e0-ad62dd4c5949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': TensorSpec(shape=(128,), dtype=tf.float32, name='reward')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.specs import array_spec\n",
    "\n",
    "reward_spec = {\n",
    "    \"reward\": array_spec.ArraySpec(shape=[BATCH_SIZE], dtype=np.float32, name=\"reward\")\n",
    "}\n",
    "\n",
    "reward_tensor_spec = train_utils.from_spec(reward_spec)\n",
    "reward_tensor_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9181303-6565-45f5-a293-08d50420a805",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Inspect chosen arm features spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6b526a5-997c-4621-b21c-82c6ca1d6a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90d3e03e-619e-4fa4-b817-f02b4cefd6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(64,), dtype=tf.float32, name=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_arm_features_info = (\n",
    "  policy_utilities.create_chosen_arm_features_info_spec(\n",
    "      time_step_spec.observation,\n",
    "  )\n",
    ")\n",
    "chosen_arm_features_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc427092-beb7-4baf-a2e9-b8d10629182c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_policy_type = (\n",
    "    policy_utilities.BanditPolicyType.GREEDY\n",
    ")\n",
    "bandit_policy_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a4a2a33-c7d4-4d81-8082-80dfc3596741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_policy_type = (\n",
    "    policy_utilities.create_bandit_policy_type_tensor_spec(shape=[1])\n",
    ")\n",
    "bandit_policy_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e7ac02b-18bd-4519-92fe-5541b1a8ddc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorSpec(shape=(2,), dtype=tf.float32, name=None), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32)), chosen_arm_features=TensorSpec(shape=(64,), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_spec = policy_utilities.PerArmPolicyInfo(\n",
    "  predicted_rewards_mean=predicted_rewards_mean,\n",
    "  bandit_policy_type=bandit_policy_type,\n",
    "  chosen_arm_features=chosen_arm_features_info,\n",
    ")\n",
    "info_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21f28b9b-8183-495a-89b6-a01f30ea8637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PerArmPolicyInfo(\n",
    "#     log_probability=(), \n",
    "#     predicted_rewards_mean=TensorSpec(shape=(2,), \n",
    "#                                       dtype=tf.float32, name=None), \n",
    "#     multiobjective_scalarized_predicted_rewards_mean=(), \n",
    "#     predicted_rewards_optimistic=(), \n",
    "#     predicted_rewards_sampled=(), \n",
    "#     bandit_policy_type=BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32)), \n",
    "#     chosen_arm_features=TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9c197-ae9b-461d-8956-f078b929ac12",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [4] The Agent\n",
    "\n",
    "**Note** that contextual bandits form a special case of RL, where the actions taken by the agent do not alter the state of the environment \n",
    "\n",
    "> “Contextual” refers to the fact that the agent chooses among a set of actions while having knowledge of the context (environment observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d6e84-688e-48c5-aea1-77df031348c9",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Agent & Network defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aadb01-eb5c-4870-ae14-9e66624ba594",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Agent types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075e8a6-11a9-4346-8725-3653fba4bac4",
   "metadata": {},
   "source": [
    "> `AGENT_TYPE = ['LinUCB', 'LinTS', 'epsGreedy', 'NeuralLinUCB']`\n",
    "\n",
    "1. **LinearUCBAgent**: (`LinUCB`) - An agent implementing the Linear UCB bandit algorithm ([docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/lin_ucb_agent/LinearUCBAgent))\n",
    "\n",
    "2. **LinearThompsonSamplingAgent**: (`LinTS`) - Implements the Linear Thompson Sampling Agent from the [paper](https://arxiv.org/abs/1209.3352): *Thompson Sampling for Contextual Bandits with Linear Payoffs* ([docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/linear_thompson_sampling_agent/LinearThompsonSamplingAgent))\n",
    "\n",
    "3. **NeuralEpsilonGreedyAgent**: (`epsGreedy`) - A neural network based epsilon greedy agent ([docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_epsilon_greedy_agent/NeuralEpsilonGreedyAgent))\n",
    "\n",
    "4. **NeuralLinUCBAgent**: (`NeuralLinUCB`) - An agent implementing the LinUCB algorithm on top of a neural network ([docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_linucb_agent/NeuralLinUCBAgent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8e88e-c8ea-4193-a911-0d974ef3b1a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Network types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547087d-5fad-4521-a584-cb46ce52897d",
   "metadata": {},
   "source": [
    "Which network architecture to use for the `epsGreedy` or `NeuralLinUCB` agents\n",
    "\n",
    "```\n",
    "NETWORK_TYPE = ['commontower', 'dotproduct']\n",
    "```\n",
    "\n",
    "**GlobalAndArmCommonTowerNetwork:** (`commontower`)\n",
    "* This network takes the output of the global and per-arm networks, and leads them through a common network, that in turn outputs reward estimates\n",
    "> * `GLOBAL_LAYERS` - Iterable of ints. Specifies the layers of the global tower\n",
    "> * `ARM_LAYERS` - Iterable of ints. Specifies the layers of the arm tower\n",
    "> * `COMMON_LAYERS` - Iterable of ints. Specifies the layers of the common tower\n",
    "* The network produced by this function can be used either in `GreedyRewardPredictionPolicy`, or `NeuralLinUCBPolicy`\n",
    "> * In the former case, the network must have `output_dim=1`, it is going to be an instance of `QNetwork`, and used in the policy as a reward prediction network\n",
    "> * In the latter case, the network will be an encoding network with its output consumed by a reward layer or a `LinUCB` method. The specified `output_dim` will be the encoding dimension\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmCommonTowerNetwork)\n",
    "\n",
    "**GlobalAndArmDotProductNetwork:** (`dotproduct`)\n",
    "* This network calculates the **dot product** of the output of the global and per-arm networks and returns them as reward estimates\n",
    "> * `GLOBAL_LAYERS` - Iterable of ints. Specifies the layers of the global tower\n",
    "> * `ARM_LAYERS` - Iterable of ints. Specifies the layers of the arm tower\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmDotProductNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3dc270-deb5-4e96-8276-74759a06c318",
   "metadata": {},
   "source": [
    "### Agent config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbe29263-453d-405e-ac50-fee7f379f335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick check on the inputs of the agent - this can be used to diagnose spec shape inputs\n",
      "\n",
      "time_step_spec:  TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': {'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None),\n",
      "                 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)},\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n",
      "\n",
      "action_spec:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))\n",
      "\n",
      "observation_spec:  {'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Quick check on the inputs of the agent - this can be used to diagnose spec shape inputs\")\n",
    "print(\"\\ntime_step_spec: \", time_step_spec)\n",
    "print(\"\\naction_spec: \", action_spec)\n",
    "print(\"\\nobservation_spec: \", observation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d817bf2-1fa6-4bae-af90-745fad996b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128,\n",
      " 'common_layers': [34, 8],\n",
      " 'encoding_dim': 1,\n",
      " 'epsilon': 0.01,\n",
      " 'eval_batch_size': 1,\n",
      " 'global_layers': [72, 36, 18],\n",
      " 'learning_rate': 0.05,\n",
      " 'model_type': 'epsGreedy',\n",
      " 'network_type': 'commontower',\n",
      " 'num_actions': 2,\n",
      " 'per_arm_layers': [64, 32, 16]}\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Agents\n",
    "# ================================\n",
    "AGENT_TYPE      = 'epsGreedy' # 'LinUCB' | 'LinTS |, 'epsGreedy' | 'NeuralLinUCB'\n",
    "\n",
    "# Parameters for linear agents (LinUCB and LinTS).\n",
    "AGENT_ALPHA     = 0.1\n",
    "\n",
    "# Parameters for neural agents (NeuralEpsGreedy and NerualLinUCB).\n",
    "EPSILON         = 0.01\n",
    "LR              = 0.05\n",
    "\n",
    "# Parameters for NeuralLinUCB\n",
    "ENCODING_DIM    = 1\n",
    "EPS_PHASE_STEPS = 1000\n",
    "\n",
    "# ================================\n",
    "# Agent's Preprocess Network\n",
    "# ================================\n",
    " # beginning should be of size: GLOBAL_DIM\n",
    "GLOBAL_LAYERS   = [GLOBAL_DIM, int(GLOBAL_DIM/2), int(GLOBAL_DIM/4)]\n",
    "\n",
    "# beginning should be of size: PER_ARM_DIM\n",
    "ARM_LAYERS      = [PER_ARM_DIM, int(PER_ARM_DIM/2), int(PER_ARM_DIM/4)]\n",
    "\n",
    "# ================================\n",
    "# common layers\n",
    "# ================================\n",
    "\"\"\"\n",
    "> layer sizes for the final tower\n",
    "> The network that takes as input the concatenation of \n",
    "  the outputs of the global and the arm networks\n",
    "> hidden layers ideally divisible by 8, e.g., [16, 32, 64, 128, 256, 512]\n",
    "\"\"\"\n",
    "FIRST_COMMON_LAYER = GLOBAL_LAYERS[-1] + ARM_LAYERS[-1] # min(GLOBAL_LAYERS[-1], ARM_LAYERS[-1])\n",
    "\n",
    "COMMON_LAYERS = [\n",
    "    int(FIRST_COMMON_LAYER),\n",
    "    # int(FIRST_COMMON_LAYER/2),\n",
    "    int(FIRST_COMMON_LAYER/4)\n",
    "]\n",
    "\n",
    "NETWORK_TYPE    = \"commontower\" # 'commontower' | 'dotproduct'\n",
    "\n",
    "if AGENT_TYPE == 'NeuralLinUCB':\n",
    "    NETWORK_TYPE = 'commontower'\n",
    "    ENCODING_DIM = COMMON_LAYERS[-1]\n",
    "    \n",
    "if NETWORK_TYPE == 'dotproduct':\n",
    "    assert GLOBAL_LAYERS[0] == ARM_LAYERS[0]\n",
    "\n",
    "\n",
    "HPARAMS = {  # TODO - streamline and consolidate\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"eval_batch_size\" : EVAL_BATCH_SIZE,\n",
    "    \"num_actions\": NUM_ACTIONS,\n",
    "    \"model_type\": AGENT_TYPE,\n",
    "    \"network_type\": NETWORK_TYPE,\n",
    "    \"global_layers\": GLOBAL_LAYERS,\n",
    "    \"per_arm_layers\": ARM_LAYERS,\n",
    "    \"common_layers\": COMMON_LAYERS,\n",
    "    \"learning_rate\": LR,\n",
    "    \"epsilon\": EPSILON,\n",
    "    \"encoding_dim\": ENCODING_DIM\n",
    "}\n",
    "pprint(HPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bf060bb-9880-40b4-8583-dd7a3f915bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent: NeuralEpsGreedyAgent\n",
      "Network: commontower\n"
     ]
    }
   ],
   "source": [
    "# from src.perarm_features import agent_factory as agent_factory\n",
    "\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "agent = agent_factory.PerArmAgentFactory._get_agent(\n",
    "    agent_type = AGENT_TYPE,\n",
    "    network_type = NETWORK_TYPE,\n",
    "    time_step_spec = time_step_spec,\n",
    "    action_spec = action_spec,\n",
    "    observation_spec=observation_spec,\n",
    "    global_layers = GLOBAL_LAYERS,\n",
    "    arm_layers = ARM_LAYERS,\n",
    "    common_layers = COMMON_LAYERS,\n",
    "    agent_alpha = AGENT_ALPHA,\n",
    "    learning_rate = LR,\n",
    "    epsilon = EPSILON,\n",
    "    train_step_counter = global_step,\n",
    "    output_dim = ENCODING_DIM,\n",
    "    eps_phase_steps = EPS_PHASE_STEPS,\n",
    "    summarize_grads_and_vars = True,\n",
    "    debug_summaries = True\n",
    ")\n",
    "      \n",
    "agent.initialize()\n",
    "print(f'agent: {agent.name}')\n",
    "\n",
    "# if network:\n",
    "#     print(f\"Network: {network}\")\n",
    "\n",
    "if NETWORK_TYPE:\n",
    "    print(f\"Network: {NETWORK_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d77d1-a0b3-4898-adf0-0b32bfaf5bd7",
   "metadata": {},
   "source": [
    "#### Inspect the specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78ca56e7-a386-4bf4-8c2b-3d1665521f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41483f5a-bda6-48a4-9e1c-88d8b4afeea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TupleWrapper(TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': DictWrapper({'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.time_step_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f205716-f265-4ca8-81a7-4799de8f60a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TupleWrapper(Trajectory(\n",
       "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32)),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': DictWrapper({'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None)}),\n",
       " 'policy_info': PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorSpec(shape=(2,), dtype=tf.float32, name=None), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32)), chosen_arm_features=TensorSpec(shape=(64,), dtype=tf.float32, name=None)),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.training_data_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d404dae-3cc3-4f81-962b-4641455ca4f2",
   "metadata": {},
   "source": [
    "# [5] Reward function\n",
    "\n",
    "> see `reward_factory.py`\n",
    "\n",
    "* Since we are training a policy with previously collected interaction data, we model the reward function from actual rewards\n",
    "* We will simply pass the `user_rating` (values 0-5) as rewards to the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10a4f40a-b4ec-4fc2-9f73-3f830b21f351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'target_movie_rating'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.TARGET_FEATURE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df9aecd6-d20a-48a7-9ead-4da3bbcfdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_rewards(element):\n",
    "    \"\"\"Calculates reward for the actions.\"\"\"\n",
    "\n",
    "    def _calc_reward(x):\n",
    "        \"\"\"\n",
    "        Calculates reward for a single action.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### uncomment for linear rewards\n",
    "        r0 = lambda: tf.constant(0.0)\n",
    "        r1 = lambda: tf.constant(1.0)\n",
    "        r2 = lambda: tf.constant(2.0)\n",
    "        r3 = lambda: tf.constant(3.0)\n",
    "        r4 = lambda: tf.constant(4.0)\n",
    "        r5 = lambda: tf.constant(5.0)\n",
    "        \n",
    "        ### uncomment for binary rewards\n",
    "        # r0 = lambda: tf.constant(0.0) # 0.0\n",
    "        # r1 = lambda: tf.constant(0.0) # 1.0\n",
    "        # r2 = lambda: tf.constant(0.0) # 2.0\n",
    "        # r3 = lambda: tf.constant(0.0) # 3.0\n",
    "        # r4 = lambda: tf.constant(1.0) # 4.0\n",
    "        # r5 = lambda: tf.constant(1.0) # 5.0\n",
    "        \n",
    "        c1 = tf.equal(x, 1.0)\n",
    "        c2 = tf.equal(x, 2.0)\n",
    "        c3 = tf.equal(x, 3.0)\n",
    "        c4 = tf.equal(x, 4.0)\n",
    "        c5 = tf.equal(x, 5.0)\n",
    "        return tf.case(\n",
    "            [(c1, r1), (c2, r2), (c3, r3),(c4, r4),(c5, r5)], \n",
    "            default=r0, exclusive=True\n",
    "        )\n",
    "\n",
    "    return tf.map_fn(\n",
    "        fn=_calc_reward, \n",
    "        # elems=element['user_rating'],\n",
    "        elems=element[data_utils.TARGET_FEATURE_NAME],\n",
    "        dtype=tf.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59703a-9a00-4eba-a3b7-45b21b738dd4",
   "metadata": {},
   "source": [
    "# [6] Trajectory function\n",
    "\n",
    "> This function will convert training samples from the TF Records to `trajectories` which the Agent interprets as training records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3605d9f-d239-4871-9a61-6e9b21f9c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _trajectory_fn(element): # hparams\n",
    "    \n",
    "    \"\"\"Converts a dataset element into a trajectory.\"\"\"\n",
    "    global_features = embs._get_global_context_features(element)\n",
    "    arm_features = embs._get_per_arm_features(element)\n",
    "\n",
    "    # Adds a time dimension.\n",
    "    arm_features = train_utils._add_outer_dimension(arm_features)\n",
    "\n",
    "    # obs spec\n",
    "    observation = {\n",
    "        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n",
    "            train_utils._add_outer_dimension(global_features),\n",
    "    }\n",
    "\n",
    "    reward = train_utils._add_outer_dimension(_get_rewards(element))\n",
    "    # reward = train_utils._add_outer_dimension(reward_factory._get_rewards(element))\n",
    "\n",
    "    # To emit the predicted rewards in policy_info, we need to create dummy\n",
    "    # rewards to match the definition in TensorSpec for the ones specified in\n",
    "    # emit_policy_info set.\n",
    "    dummy_rewards = tf.zeros([HPARAMS['batch_size'], 1, HPARAMS['num_actions']])\n",
    "    policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "        chosen_arm_features=arm_features,\n",
    "        # Pass dummy mean rewards here to match the model_spec for emitting\n",
    "        # mean rewards in policy info\n",
    "        predicted_rewards_mean=dummy_rewards,\n",
    "        bandit_policy_type=tf.zeros([HPARAMS['batch_size'], 1, 1], dtype=tf.int32)\n",
    "        # policy_utilities.create_bandit_policy_type_tensor_spec(shape=[1]) # policy_utilities.BanditPolicyType.GREEDY\n",
    "        # tf.zeros([batch_size, 1, 1], dtype=tf.int32)\n",
    "    )\n",
    "    \n",
    "    if HPARAMS['model_type'] == 'neural_ucb':\n",
    "        policy_info = policy_info._replace(\n",
    "            predicted_rewards_optimistic=dummy_rewards\n",
    "        )\n",
    "        \n",
    "    return trajectory.single_step(\n",
    "        observation=observation,\n",
    "        action=tf.zeros_like(\n",
    "            reward, dtype=tf.int32\n",
    "        ),  # Arm features are copied from policy info, put dummy zeros here\n",
    "        policy_info=policy_info,\n",
    "        reward=reward,\n",
    "        discount=tf.zeros_like(reward)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a0989c-5a06-4fd5-a12d-d92dbbe71a76",
   "metadata": {},
   "source": [
    "Inspect the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "257791d1-c97c-48c0-bc92-f5a4737b6c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_traj.action.shape      : (128, 1)\n",
      "test_traj.discount.shape    : (128, 1)\n",
      "test_traj.reward.shape      : (128, 1)\n",
      "test_traj.observation.shape : (128, 1, 72)\n"
     ]
    }
   ],
   "source": [
    "for x in train_dataset.batch(HPARAMS['batch_size']).take(1):\n",
    "    test_traj = _trajectory_fn(x)\n",
    "    \n",
    "print(f\"test_traj.action.shape      : {test_traj.action.shape}\") \n",
    "print(f\"test_traj.discount.shape    : {test_traj.discount.shape}\")\n",
    "print(f\"test_traj.reward.shape      : {test_traj.reward.shape}\")\n",
    "print(f\"test_traj.observation.shape : {test_traj.observation['global'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3dace3d1-ce61-48cf-82a4-f701d3fe337c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_traj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbfc9a-049e-49b9-b2a3-ecb9fd4b9964",
   "metadata": {},
   "source": [
    "# [7] Training the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13adc1ef-b50e-41a3-9b95-c128d0c55a2b",
   "metadata": {},
   "source": [
    "## set Vertex Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e861fb27-df24-4821-8448-cf1346b186f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : 02-supervised-bandits-v1\n",
      "RUN_NAME          : run-20240313-192115\n",
      "\n",
      "CHECKPT_DIR       : gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/chkpoint\n",
      "BASE_OUTPUT_DIR   : gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-192115\n",
      "LOG_DIR           : gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-192115/logs\n",
      "ROOT_DIR          : gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-192115/root\n",
      "ARTIFACTS_DIR     : gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-192115/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'02-supervised-bandits-v1'\n",
    "\n",
    "# new experiment\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "CHECKPT_DIR       = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/chkpoint\"\n",
    "BASE_OUTPUT_DIR   = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}\"\n",
    "LOG_DIR           = f\"{BASE_OUTPUT_DIR}/logs\"\n",
    "ROOT_DIR          = f\"{BASE_OUTPUT_DIR}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BASE_OUTPUT_DIR}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "# vertex_ai.init(\n",
    "#     project=PROJECT_ID,\n",
    "#     location=REGION,\n",
    "#     experiment=EXPERIMENT_NAME\n",
    "# )\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\\n\")\n",
    "print(f\"CHECKPT_DIR       : {CHECKPT_DIR}\")\n",
    "print(f\"BASE_OUTPUT_DIR   : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de729bad-0bc9-429e-b4cb-7b24bf615aa1",
   "metadata": {},
   "source": [
    "### Create TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2570564-71f4-4dda-8d8a-59784db67632",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_TENSORBOARD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db63052a-7eea-4982-964d-1f7ecab0665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME: projects/934903580331/locations/us-central1/tensorboards/9081900074732093440\n",
      "TB display name: 02-supervised-bandits-v1\n"
     ]
    }
   ],
   "source": [
    "if NEW_TENSORBOARD:\n",
    "    # create new TB instance\n",
    "    TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}\"\n",
    "\n",
    "    tensorboard = aiplatform.Tensorboard.create(\n",
    "        display_name=TENSORBOARD_DISPLAY_NAME\n",
    "        , project=PROJECT_ID\n",
    "        , location=REGION\n",
    "    )\n",
    "\n",
    "    TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "else:\n",
    "    # use existing TB instance\n",
    "    # TB_RESOURCE_NAME = 'projects/934903580331/locations/us-central1/tensorboards/TODO' # bandit-rewards\n",
    "    tensorboard = aiplatform.Tensorboard(\n",
    "        tensorboard_name=TB_RESOURCE_NAME\n",
    "    )\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name: {tensorboard.display_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a0b708d-990e-468b-a1b1-a8ba8f71d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete Tensorboard\n",
    "# vertex_ai_tb.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c891d27-d9d1-4e64-8981-1a1ae343c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aiplatform.init(\n",
    "#     project=PROJECT_ID,\n",
    "#     location=REGION,\n",
    "#     experiment=EXPERIMENT_NAME,\n",
    "#     experiment_tensorboard=TB_ID\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7a5e4-efff-490b-b431-53037a2f4a16",
   "metadata": {},
   "source": [
    "## eval loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba0392e-90c8-4dc8-8df2-6d2af1ade490",
   "metadata": {},
   "source": [
    "> When evaluating an epsilon-greedy bandit, we need to remove those predictions that come from the `uniform_random` policy. We only want the predictions from the `GREEDY` policy, becasue we want to know how the model performs when it is actually used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b3d3c-75f7-46f4-9a1b-6329e419b7f5",
   "metadata": {},
   "source": [
    "## train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb7ffc-00df-4d46-857b-86c87b78f597",
   "metadata": {},
   "source": [
    "`agent.train(experience=...)`\n",
    "\n",
    "where `experience` is a batch of trajectories data in the form of a Trajectory. \n",
    "* The structure of experience must match that of `self.training_data_spec`. \n",
    "* All tensors in experience must be shaped [batch, time, ...] where time must be equal to self.train_step_length if that property is not None."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d6d45-c71e-4fe4-9af8-af9a8bda4e4f",
   "metadata": {},
   "source": [
    "### logs & checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "049ee49b-3b1a-4b12-a360-15e759dff7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('metrics: [<tf_agents.metrics.tf_metrics.AverageReturnMetric object at '\n",
      " '0x7fb184feaec0>]')\n",
      "setting checkpoint_manager: gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/chkpoint\n",
      "\n",
      "'saver: <tf_agents.policies.policy_saver.PolicySaver object at 0x7fb0f997f7f0>'\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# TB summary writer\n",
    "# ====================================================\n",
    "train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    f\"{LOG_DIR}\", flush_millis=10 * 1000\n",
    ")\n",
    "# ====================================================\n",
    "# metrics\n",
    "# ====================================================\n",
    "step_metric = tf_metrics.EnvironmentSteps()\n",
    "metrics = [\n",
    "    # tf_metrics.NumberOfEpisodes(),\n",
    "    # tf_metrics.AverageEpisodeLengthMetric(batch_size=HPARAMS['batch_size']),\n",
    "    tf_metrics.AverageReturnMetric(batch_size=HPARAMS['batch_size'])\n",
    "]\n",
    "\n",
    "pprint(f\"metrics: {metrics}\")\n",
    "\n",
    "# ====================================================\n",
    "# get checkpoint manager\n",
    "# ====================================================\n",
    "print(f\"setting checkpoint_manager: {CHECKPT_DIR}\\n\")\n",
    "\n",
    "checkpoint_manager = train_utils.restore_and_get_checkpoint_manager(\n",
    "    root_dir=CHECKPT_DIR, \n",
    "    agent=agent, \n",
    "    metrics=metrics, \n",
    "    step_metric=step_metric\n",
    ")\n",
    "# ====================================================\n",
    "# policy saver\n",
    "# ====================================================\n",
    "saver = policy_saver.PolicySaver(\n",
    "    agent.policy, \n",
    "    train_step=global_step\n",
    ")\n",
    "pprint(f\"saver: {saver}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d155f1f4-0d95-40a8-a37c-c608a64af803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7fb0c80f76a0>,\n",
       " 'get_initial_state': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7fb0c80f7b80>,\n",
       " 'get_train_step': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7fb0c80f4490>,\n",
       " 'get_metadata': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7fb0c80f7d00>}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83683f4b-8a1a-44b9-8d5f-7ee3304c794c",
   "metadata": {},
   "source": [
    "### config\n",
    "\n",
    "* calculate train & val dataset sizes\n",
    "* define logging and chkpt intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9432613-6583-4828-8a2a-dd939adbc3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_DATA_SIZE : 300000\n",
      "NUM_TRAIN_STEPS : 50\n",
      "EVAL_DATA_SIZE  : 90000\n",
      "NUM_EVAL_STEPS  : 1000\n",
      "CHKPT_INTERVAL  : 50\n",
      "LOG_INTERVAL    : 10\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_SIZE = 300_000       # len(list(train_dataset))\n",
    "NUM_TRAIN_STEPS = 50            # TRAIN_DATA_SIZE // HPARAMS['batch_size']\n",
    "\n",
    "EVAL_DATA_SIZE  = 90_000        # len(list(val_dataset))\n",
    "NUM_EVAL_STEPS  = 1_000         # EVAL_DATA_SIZE // HPARAMS['eval_batch_size']\n",
    "\n",
    "CHKPT_INTERVAL  = NUM_TRAIN_STEPS # // 5\n",
    "LOG_INTERVAL    = 10\n",
    "# EVAL_INTERVAL = NUM_TRAIN_STEPS // 2\n",
    "\n",
    "print(f\"TRAIN_DATA_SIZE : {TRAIN_DATA_SIZE}\")\n",
    "print(f\"NUM_TRAIN_STEPS : {NUM_TRAIN_STEPS}\")\n",
    "print(f\"EVAL_DATA_SIZE  : {EVAL_DATA_SIZE}\")\n",
    "print(f\"NUM_EVAL_STEPS  : {NUM_EVAL_STEPS}\")\n",
    "print(f\"CHKPT_INTERVAL  : {CHKPT_INTERVAL}\")\n",
    "print(f\"LOG_INTERVAL    : {LOG_INTERVAL}\")\n",
    "# print(f\"EVAL_INTERVAL : {EVAL_INTERVAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "379ffc65-4eec-4af1-8fd4-912ef65a9808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TakeDataset element_spec={'target_movie_genres': TensorSpec(shape=(None, 10), dtype=tf.string, name=None), 'target_movie_id': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'target_movie_rating': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'target_movie_title': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'target_movie_year': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'target_rating_timestamp': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'user_age': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'user_gender': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_id': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_occupation_text': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_zip_code': TensorSpec(shape=(None,), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data\n",
    "train_ds_iterator = iter(train_dataset.batch(HPARAMS['batch_size']).repeat())\n",
    "\n",
    "# eval dataset\n",
    "eval_ds = val_dataset.batch(HPARAMS[\"eval_batch_size\"])\n",
    "\n",
    "if NUM_EVAL_STEPS > 0:\n",
    "    eval_ds = eval_ds.take(NUM_EVAL_STEPS)\n",
    "\n",
    "# eval_ds = eval_ds.prefetch(tf.data.AUTOTUNE)\n",
    "# eval_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498d7e3-d166-4e1d-94a5-f721de58694a",
   "metadata": {},
   "source": [
    "### run train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da4680f3-693b-4530-8e4d-88bc43036c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating pre-trained Agent...\n",
      "pre-train val_loss     : 15.020397186279297\n",
      "pre-train eval runtime : 0\n",
      "starting train loop...\n",
      "step = 0: train loss = 15.989999771118164\n",
      "step = 10: train loss = 2.9800000190734863\n",
      "step = 20: train loss = 3.0999999046325684\n",
      "step = 30: train loss = 2.240000009536743\n",
      "step = 40: train loss = 0.9200000166893005\n",
      "train runtime_mins: 5\n",
      "saved trained policy to: gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-192115/artifacts\n",
      "evaluating trained Agent...\n",
      "post-train val_loss     : 2.30190110206604\n",
      "post-train eval runtime : 0\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "# agent.train = common.function(agent.train)\n",
    "\n",
    "list_o_loss = []\n",
    "\n",
    "# ====================================================\n",
    "# Evaluate the agent's policy once before training\n",
    "# ====================================================\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "pre_policy_tf = py_tf_eager_policy.PyTFEagerPolicy(agent.policy, use_tf_function=True)\n",
    "\n",
    "print(f\"evaluating pre-trained Agent...\")\n",
    "start_time = time.time()\n",
    "\n",
    "pre_val_loss, pre_preds, pre_tr_rewards = eval_perarm._run_bandit_eval(\n",
    "    policy = pre_policy_tf,\n",
    "    data = eval_ds,\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"pre-train val_loss     : {pre_val_loss}\")\n",
    "print(f\"pre-train eval runtime : {runtime_mins}\")\n",
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "print(f\"starting train loop...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(NUM_TRAIN_STEPS):\n",
    "    \n",
    "    with train_summary_writer.as_default():\n",
    "\n",
    "        data = next(train_ds_iterator)\n",
    "        trajectories = _trajectory_fn(data)\n",
    "\n",
    "        # All tensors in experience must be shaped [batch, time, ...] \n",
    "        step = agent.train_step_counter.numpy()\n",
    "        loss = agent.train(experience=trajectories)\n",
    "        list_o_loss.append(loss.loss.numpy())\n",
    "\n",
    "        train_utils._export_metrics_and_summaries(\n",
    "            step=i, \n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        # print step loss\n",
    "        if step % LOG_INTERVAL == 0:\n",
    "            print(\n",
    "                'step = {0}: train loss = {1}'.format(\n",
    "                    step, round(loss.loss.numpy(), 2)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if i > 0 and i % CHKPT_INTERVAL == 0:\n",
    "            saver.save(os.path.join(CHKPOINT_DIR, 'policy_%d' % step_metric.result()))\n",
    "            print(f\"saved policy to: {CHKPOINT_DIR}\")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"train runtime_mins: {runtime_mins}\")\n",
    "\n",
    "saver.save(ARTIFACTS_DIR)\n",
    "print(f\"saved trained policy to: {ARTIFACTS_DIR}\")\n",
    "\n",
    "# ====================================================\n",
    "# Evaluate the agent's policy once after training\n",
    "# ====================================================\n",
    "print(f\"evaluating trained Agent...\")\n",
    "\n",
    "post_policy_tf = py_tf_eager_policy.PyTFEagerPolicy(agent.policy, use_tf_function=True)\n",
    "start_time = time.time()\n",
    "\n",
    "val_loss, preds, tr_rewards = eval_perarm._run_bandit_eval(\n",
    "    policy = post_policy_tf,\n",
    "    data = eval_ds,\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"post-train val_loss     : {val_loss}\")\n",
    "print(f\"post-train eval runtime : {runtime_mins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "17b2b0d2-9795-4367-b57b-c331e94d1000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.301901"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb1ed8-f67b-4e7b-9cbc-70edd50e49ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate train job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31fe28-2f05-499f-b1cc-505f3c9074e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### train loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41937f9a-6c28-48d2-bc7c-bfe4372c7cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_o_loss)\n",
    "# plt.ylim([0, 10])\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e947e1-67f4-4205-99af-6921c6e9c896",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65627011-48eb-4e8e-981e-8b61b0f427c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0f77a33-0962-4af0-af97-cf05f895cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d0509cb-0777-4d35-86c6-28c8acbcc793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f30e9adf9e7e69a8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f30e9adf9e7e69a8\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=$LOG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc40143-30b1-479f-b9b1-fa4c07b1690e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [8] Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c94a0-122f-4ba6-acef-5653a3cc7850",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b1c382b5-e2e0-45a2-9c5f-ef2c468d0f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-192115/artifacts\n"
     ]
    }
   ],
   "source": [
    "POLICY_URI = ARTIFACTS_DIR\n",
    "\n",
    "print(POLICY_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "68592d97-1272-4c6d-a4bd-43a6f7fe6c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.py_tf_eager_policy.SavedModelPyTFEagerPolicy at 0x7fb0c8122020>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy\n",
    "\n",
    "trained_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n",
    "    POLICY_URI, load_specs_from_pbtxt=True\n",
    ")\n",
    "trained_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d910a06f-678f-4bfa-a132-10b885f871c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762c936-3c49-4ec5-a9eb-4df5816dc888",
   "metadata": {},
   "source": [
    "TODO - explain bandit_policy_type()\n",
    "\n",
    "**Dummy arm values?**\n",
    "* We set `chosen_arm_features` to dummy values of all zeros. We need to save dummy chosen arm features to make the returned policy step have the same structure as the policy state spec.\n",
    "* `emit_policy_info = ('predicted_rewards_mean', 'bandit_policy_type')` defines what side information we want to get as part of the policy info when we call policy network \n",
    "* This makes it so that the model always returns the expected rewards even if the model is exploring\n",
    "* This means that the largest predicted rewards may not match the selected action when the model is exploring (i.e. bandit_policy == UNIFORM == 2)\n",
    "\n",
    "**UNIFORM Random policy:**\n",
    "```\n",
    "PolicyStep(action=array(1, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.637535 , 3.5557823], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([2], dtype=int32), chosen_arm_features=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)))\n",
    "```\n",
    "\n",
    "**GREEDY policy:**\n",
    "```\n",
    "PolicyStep(action=array(0, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.8278818, 3.607565 ], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([ 0.04054676, -0.04735027,  0.02202327,  0.01276748, -0.01367382,\n",
    "       -0.04838754,  0.00104916, -0.01956806,  0.01028793, -0.00256665,\n",
    "       -0.04376553,  0.03900594,  0.02388967, -0.04016995, -0.04945569,\n",
    "       -0.04539652,  0.04423274, -0.04630332,  0.00390794,  0.03986299,\n",
    "       -0.0398974 ,  0.01655747, -0.0471294 , -0.00690235, -0.03643382,\n",
    "       -0.0437116 ,  0.04757959, -0.00991895,  0.0281055 , -0.04633433,\n",
    "        0.01897702, -0.00824345,  0.03232259, -0.04980658, -0.01144745,\n",
    "        0.01690939,  0.02536928,  0.03384003, -0.01558131,  0.04731432,\n",
    "        0.04579857, -0.03305101,  0.03521084, -0.03915765,  0.04927064,\n",
    "        0.04695194,  0.03792156,  0.040496  ,  0.02574866, -0.00665367,\n",
    "       -0.03839222, -0.01564004, -0.04710373,  0.03557512, -0.01392462,\n",
    "        0.01368621,  0.02792176, -0.01799218,  0.00246101,  0.02263175,\n",
    "        0.0170817 ,  0.01388383, -0.03048695,  0.01198485], dtype=float32)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "567f64e1-8149-4364-b561-1a9d12581b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aff30396-e0fd-4f43-acaa-6af8b20b12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFER_SIZE = 1\n",
    "dummy_arm = tf.zeros([INFER_SIZE, PER_ARM_DIM], dtype=tf.float32)\n",
    "\n",
    "SKIP_NUM = 10\n",
    "\n",
    "for x in eval_ds.skip(SKIP_NUM).take(INFER_SIZE):\n",
    "    # get feature tensors\n",
    "    \n",
    "    global_feat_infer = embs._get_global_context_features(x)\n",
    "    arm_feat_infer = embs._get_per_arm_features(x)\n",
    "    \n",
    "    rewards = _get_rewards(x)\n",
    "    # rewards = reward_factory._get_rewards(x)\n",
    "    \n",
    "    # reshape arm features\n",
    "    arm_feat_infer = tf.reshape(arm_feat_infer, [HPARAMS['eval_batch_size'], PER_ARM_DIM]) # perarm_dim\n",
    "    concat_arm = tf.concat([arm_feat_infer, dummy_arm], axis=0)\n",
    "    \n",
    "    # flatten global\n",
    "    flat_global_infer = tf.reshape(global_feat_infer, [GLOBAL_DIM])\n",
    "    feature = {'global': flat_global_infer, 'per_arm': concat_arm}\n",
    "    \n",
    "    # get actual reward\n",
    "    actual_reward = rewards.numpy()[0]\n",
    "    \n",
    "    # build trajectory step\n",
    "    trajectory_step = train_utils._get_eval_step(feature, actual_reward)\n",
    "    \n",
    "    prediction = trained_policy.action(trajectory_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2bcd1e82-168e-4df3-92bd-4cd34ecd3a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
       " 'observation': {'global': <tf.Tensor: shape=(72,), dtype=float32, numpy=\n",
       "array([-0.04180052, -0.01288463, -0.03763759, -0.04910102,  0.01293076,\n",
       "        0.04238378,  0.00375085, -0.04270661, -0.02033963, -0.01652616,\n",
       "       -0.03559557, -0.0035817 ,  0.03255766,  0.00961917,  0.04013762,\n",
       "        0.01646889,  0.01545901,  0.01313862,  0.04104466,  0.02343484,\n",
       "        0.0113572 ,  0.01218323, -0.03393515,  0.037779  ,  0.02740731,\n",
       "       -0.0280769 , -0.03349806, -0.02822067,  0.03202895,  0.04132451,\n",
       "       -0.00319894,  0.01823548,  0.030741  ,  0.00584149,  0.0047561 ,\n",
       "        0.00417908,  0.0373571 , -0.00265469,  0.00124051, -0.00683895,\n",
       "       -0.02101296, -0.00094485,  0.03022655, -0.01573528, -0.02096498,\n",
       "       -0.01417056,  0.04562125, -0.00982308,  0.0391654 ,  0.03760383,\n",
       "        0.03286931, -0.03916093,  0.04995331, -0.00295743, -0.02619722,\n",
       "       -0.04954935,  0.02836919,  0.02686154, -0.00949102,  0.01771566,\n",
       "        0.00033338, -0.02991512, -0.00921457, -0.01202456,  0.04430324,\n",
       "        0.04629269, -0.00958859,  0.01260105, -0.01952093,  0.02465925,\n",
       "       -0.03532827,  0.02468587], dtype=float32)>,\n",
       "                 'per_arm': <tf.Tensor: shape=(2, 64), dtype=float32, numpy=\n",
       "array([[ 0.0261127 ,  0.02215729,  0.01524419,  0.03614006,  0.02269845,\n",
       "        -0.01079534,  0.00610126, -0.0298137 , -0.0023763 , -0.04295554,\n",
       "         0.00944187, -0.03426471,  0.03427717, -0.03977456, -0.03377431,\n",
       "         0.00923246,  0.00333258,  0.02858807,  0.04337367, -0.00730864,\n",
       "        -0.03244675,  0.01234595,  0.04639426,  0.00474442,  0.01491665,\n",
       "        -0.0315549 , -0.01064789, -0.01711632,  0.01663466, -0.04666859,\n",
       "         0.007291  , -0.01495687,  0.01144717, -0.04056351, -0.00503174,\n",
       "        -0.01027514,  0.03554142, -0.01782044, -0.01763439, -0.04968584,\n",
       "        -0.01514242,  0.0253898 ,  0.00585924, -0.04071265,  0.02620872,\n",
       "         0.01534164, -0.04550194, -0.02735262,  0.10546707,  0.10259961,\n",
       "        -0.03160762, -0.01535922, -0.0571759 , -0.02447505, -0.02959907,\n",
       "         0.07843348,  0.02923632,  0.13613215,  0.10791406,  0.08569065,\n",
       "         0.00061015,  0.09255935,  0.09214851, -0.09872925],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "      dtype=float32)>},\n",
       " 'reward': <tf.Tensor: shape=(), dtype=float32, numpy=3.0>,\n",
       " 'step_type': <tf.Tensor: shape=(), dtype=int32, numpy=0>})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e58af9ab-f4da-4857-afe5-6b8de26ed0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=array(0, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([2.6660724, 2.44228  ], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([ 0.0261127 ,  0.02215729,  0.01524419,  0.03614006,  0.02269845,\n",
       "       -0.01079534,  0.00610126, -0.0298137 , -0.0023763 , -0.04295554,\n",
       "        0.00944187, -0.03426471,  0.03427717, -0.03977456, -0.03377431,\n",
       "        0.00923246,  0.00333258,  0.02858807,  0.04337367, -0.00730864,\n",
       "       -0.03244675,  0.01234595,  0.04639426,  0.00474442,  0.01491665,\n",
       "       -0.0315549 , -0.01064789, -0.01711632,  0.01663466, -0.04666859,\n",
       "        0.007291  , -0.01495687,  0.01144717, -0.04056351, -0.00503174,\n",
       "       -0.01027514,  0.03554142, -0.01782044, -0.01763439, -0.04968584,\n",
       "       -0.01514242,  0.0253898 ,  0.00585924, -0.04071265,  0.02620872,\n",
       "        0.01534164, -0.04550194, -0.02735262,  0.10546707,  0.10259961,\n",
       "       -0.03160762, -0.01535922, -0.0571759 , -0.02447505, -0.02959907,\n",
       "        0.07843348,  0.02923632,  0.13613215,  0.10791406,  0.08569065,\n",
       "        0.00061015,  0.09255935,  0.09214851, -0.09872925], dtype=float32)))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ec286d78-dd56-455d-90f8-4ffa88f3ac50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.6660724, 2.44228  ], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.info.predicted_rewards_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c85a0087-7388-4f03-9278-aace31121233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0, dtype=int32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a8266c7c-3688-46a5-a1c8-a63bb7146128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.info.bandit_policy_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9712bc00-7ea0-4003-a3c7-81a0be5bc995",
   "metadata": {},
   "source": [
    "# [9] Prepare training application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f62966-123f-410e-9610-832f20566635",
   "metadata": {},
   "source": [
    "To scale this training in Vertex AI's managed training service, we need to package all our code into a python application\n",
    "\n",
    "* This means, we need to modularize all steps executed up until this point. For example, the training loop we ran above is further parameterized in `train_perarm.py`, which will be called from our `task.py` when we submit the job to Vertex AI\n",
    "* This training package will eventually be used to create a custom training image (more on that in `04b-build-training-image` notebook)\n",
    "* For additional details on preparing the training application for Vertex AI, check out [the documentation](https://cloud.google.com/vertex-ai/docs/training/code-requirements) and [this code example](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_training.ipynb) (which also has good, related commentary)\n",
    "\n",
    "**The remaining cells will repeat steps executed above**\n",
    "> we are just illustrating / validating that they are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c3d30-bef4-4ec5-a978-116856a70e7e",
   "metadata": {},
   "source": [
    "### Distribution strategy\n",
    "\n",
    "Use `strategy_utils` to generate a strategy. Under the hood, passing the parameter:\n",
    "\n",
    "* `use_gpu = False` returns `tf.distribute.get_strategy()`, which uses CPU\n",
    "* `use_gpu = True` returns `tf.distribute.MirroredStrategy()`, which uses all GPUs that are visible to TensorFlow on one machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "68140c4d-12ff-4758-89ca-44fd710ca0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy at 0x7fae88447c70>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.train.utils import strategy_utils\n",
    "\n",
    "use_gpu = True\n",
    "use_tpu = False\n",
    "\n",
    "distribution_strategy = strategy_utils.get_strategy(tpu=use_tpu, use_gpu=use_gpu)\n",
    "distribution_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e80724b1-6525-4986-b832-4af8b49d923c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_REPLICAS = distribution_strategy.num_replicas_in_sync\n",
    "NUM_REPLICAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129ee41-b43c-4d5a-b656-cdac6b79688c",
   "metadata": {},
   "source": [
    "### Vertex Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ef509680-4d19-4efa-9f01-21039e0d573b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : 02-supervised-bandits-v1\n"
     ]
    }
   ],
   "source": [
    "# using same experiment name \n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7ad98430-d836-40f1-8e4f-eb46f0c91fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : 02-supervised-bandits-v1\n",
      "RUN_NAME          : run-20240313-193640\n",
      "\n",
      "CHECKPT_DIR       : gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/chkpoint\n",
      "BASE_OUTPUT_DIR   : gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640\n",
      "LOG_DIR           : gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/logs\n",
      "ROOT_DIR          : gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/root\n",
      "ARTIFACTS_DIR     : gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/artifacts\n"
     ]
    }
   ],
   "source": [
    "# new experiment\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "CHECKPT_DIR       = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/chkpoint\"\n",
    "BASE_OUTPUT_DIR   = f'{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "LOG_DIR           = f\"{BASE_OUTPUT_DIR}/logs\"\n",
    "ROOT_DIR          = f\"{BASE_OUTPUT_DIR}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BASE_OUTPUT_DIR}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "# aiplatform.init(\n",
    "#     project=PROJECT_ID,\n",
    "#     location=REGION,\n",
    "#     experiment=EXPERIMENT_NAME\n",
    "# )\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\\n\")\n",
    "print(f\"CHECKPT_DIR       : {CHECKPT_DIR}\")\n",
    "print(f\"BASE_OUTPUT_DIR   : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83de7c4-f7c7-4290-b44a-9e9194bac882",
   "metadata": {},
   "source": [
    "### Create TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "17576ce0-727d-4297-a52d-f64fb75ca78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME : projects/934903580331/locations/us-central1/tensorboards/2625990008896487424\n",
      "TB display name  : 02-supervised-bandits-v1-run-20240313-193640\n",
      "TB_ID            : 2625990008896487424\n"
     ]
    }
   ],
   "source": [
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}-{RUN_NAME}\"\n",
    "\n",
    "tensorboard = aiplatform.Tensorboard.create(\n",
    "    display_name=TENSORBOARD_DISPLAY_NAME\n",
    "    , project=PROJECT_ID\n",
    "    , location=REGION\n",
    ")\n",
    "\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "TB_ID = TB_RESOURCE_NAME.split('/')[-1]\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME : {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name  : {tensorboard.display_name}\")\n",
    "print(f\"TB_ID            : {TB_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "71d43cf9-db3f-437e-98ee-3791ac0c5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    experiment_tensorboard=TB_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2e082-c0f6-4792-a279-e827c48b5895",
   "metadata": {},
   "source": [
    "### trajectory function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "55e21068-a7a5-44c8-a16f-d8c41d976c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src import reward_factory as reward_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b9ce410e-ac03-48b2-8006-4591b38297a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with distribution_strategy.scope():\n",
    "    \n",
    "    embs = emb_features.EmbeddingModel(\n",
    "        vocab_dict = vocab_dict,\n",
    "        num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "        global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "        mv_emb_size = MV_EMBEDDING_SIZE,\n",
    "        max_genre_length = data_config.MAX_GENRE_LENGTH\n",
    "    )\n",
    "    \n",
    "    def _trajectory_fn(element): # hparams\n",
    "    \n",
    "        \"\"\"Converts a dataset element into a trajectory.\"\"\"\n",
    "        # global_features = _get_global_context_features(element)\n",
    "        # arm_features = _get_per_arm_features(element)\n",
    "\n",
    "        global_features = embs._get_global_context_features(element)\n",
    "        arm_features = embs._get_per_arm_features(element)\n",
    "\n",
    "        # Adds a time dimension.\n",
    "        arm_features = train_utils._add_outer_dimension(arm_features)\n",
    "\n",
    "        # obs spec\n",
    "        observation = {\n",
    "            bandit_spec_utils.GLOBAL_FEATURE_KEY:\n",
    "                train_utils._add_outer_dimension(global_features),\n",
    "        }\n",
    "\n",
    "        # reward = train_utils._add_outer_dimension(reward_factory._get_binary_rewards(element))\n",
    "        reward = train_utils._add_outer_dimension(reward_factory._get_rewards(element))\n",
    "\n",
    "        # To emit the predicted rewards in policy_info, we need to create dummy\n",
    "        # rewards to match the definition in TensorSpec for the ones specified in\n",
    "        # emit_policy_info set.\n",
    "        dummy_rewards = tf.zeros([HPARAMS['batch_size'], 1, HPARAMS['num_actions']])\n",
    "        policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "            chosen_arm_features=arm_features,\n",
    "            # Pass dummy mean rewards here to match the model_spec for emitting\n",
    "            # mean rewards in policy info\n",
    "            predicted_rewards_mean=dummy_rewards,\n",
    "            bandit_policy_type=tf.zeros([HPARAMS['batch_size'], 1, 1], dtype=tf.int32)\n",
    "            # policy_utilities.create_bandit_policy_type_tensor_spec(shape=[1]) \n",
    "            # policy_utilities.BanditPolicyType.GREEDY\n",
    "            # tf.zeros([batch_size, 1, 1], dtype=tf.int32)\n",
    "        )\n",
    "\n",
    "        if HPARAMS['model_type'] == 'neural_ucb':\n",
    "            policy_info = policy_info._replace(\n",
    "                predicted_rewards_optimistic=dummy_rewards\n",
    "            )\n",
    "\n",
    "        return trajectory.single_step(\n",
    "            observation=observation,\n",
    "            action=tf.zeros_like(\n",
    "                reward, \n",
    "                dtype=tf.int32\n",
    "            ),  # Arm features are copied from policy info, put dummy zeros here\n",
    "            policy_info=policy_info,\n",
    "            reward=reward,\n",
    "            discount=tf.zeros_like(reward)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b759404-b282-4f55-add8-7d795867c99e",
   "metadata": {},
   "source": [
    "### get agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3be40320-a73f-45f7-9fc4-0bd64df0ae5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 128,\n",
       " 'eval_batch_size': 1,\n",
       " 'num_actions': 2,\n",
       " 'model_type': 'epsGreedy',\n",
       " 'network_type': 'commontower',\n",
       " 'global_layers': [72, 36, 18],\n",
       " 'per_arm_layers': [64, 32, 16],\n",
       " 'common_layers': [34, 8],\n",
       " 'learning_rate': 0.05,\n",
       " 'epsilon': 0.01,\n",
       " 'encoding_dim': 1}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HPARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fbdecf78-94c5-4f8c-a6c0-86f2eace4663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: NeuralEpsGreedyAgent\n",
      "Network: commontower\n"
     ]
    }
   ],
   "source": [
    "with distribution_strategy.scope():\n",
    "\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "    agent = agent_factory.PerArmAgentFactory._get_agent(\n",
    "        agent_type = HPARAMS['model_type'],\n",
    "        network_type = HPARAMS['network_type'],\n",
    "        time_step_spec = time_step_spec,\n",
    "        action_spec = action_spec,\n",
    "        observation_spec=observation_spec,\n",
    "        global_layers = HPARAMS['global_layers'],\n",
    "        arm_layers = HPARAMS['per_arm_layers'],\n",
    "        common_layers = HPARAMS['common_layers'],\n",
    "        agent_alpha = AGENT_ALPHA,\n",
    "        learning_rate = HPARAMS['learning_rate'],\n",
    "        epsilon = HPARAMS['epsilon'],\n",
    "        train_step_counter = global_step,\n",
    "        output_dim = HPARAMS['encoding_dim'],\n",
    "        eps_phase_steps = EPS_PHASE_STEPS,\n",
    "        summarize_grads_and_vars = True,\n",
    "        debug_summaries = True\n",
    "    )\n",
    "    \n",
    "    agent.initialize()\n",
    "\n",
    "print(f\"Agent: {agent.name}\")\n",
    "\n",
    "# if network:\n",
    "#     print(f\"Network: {network}\")\n",
    "\n",
    "if NETWORK_TYPE:\n",
    "    print(f\"Network: {NETWORK_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "108ed33c-d8ef-4524-97e9-ced9e01c154b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHKPT_INTERVAL    : 200\n",
      "eval_batch_size   : 1\n",
      "EVAL_DATA_SIZE    : 90000\n",
      "NUM_EVAL_STEPS    : 100\n",
      "TOTAL_TRAIN_TAKE  : 10000\n"
     ]
    }
   ],
   "source": [
    "IS_TESTING = True\n",
    "\n",
    "# train args\n",
    "NUM_EPOCHS            = 3\n",
    "TRAINING_LOOPS        = 500\n",
    "STEPS_PER_LOOP        = 1\n",
    "\n",
    "drop_arm_feature_fn   = None\n",
    "ASYNC_STEPS_PER_LOOP  = 1\n",
    "\n",
    "LOG_INTERVAL          = 100\n",
    "CHKPT_INTERVAL        = 200\n",
    "\n",
    "# eval args\n",
    "NUM_EVAL_STEPS = 100\n",
    "\n",
    "TOTAL_TRAIN_TAKE = 10000 #TRAINING_LOOPS * HPARAMS['batch_size']\n",
    "\n",
    "print(f\"CHKPT_INTERVAL    : {CHKPT_INTERVAL}\")\n",
    "print(f\"eval_batch_size   : {EVAL_BATCH_SIZE}\")\n",
    "print(f\"EVAL_DATA_SIZE    : {EVAL_DATA_SIZE}\")\n",
    "print(f\"NUM_EVAL_STEPS    : {NUM_EVAL_STEPS}\")\n",
    "print(f\"TOTAL_TRAIN_TAKE  : {TOTAL_TRAIN_TAKE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f66ff927-9a39-4abf-b247-845f0dc7721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval dataset\n",
    "eval_ds = val_dataset.batch(HPARAMS['eval_batch_size'])\n",
    "\n",
    "if NUM_EVAL_STEPS >= 0:\n",
    "    with distribution_strategy.scope():\n",
    "        eval_ds = eval_ds.take(NUM_EVAL_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "528441f5-64ec-4f09-bd50-b2ae85b553bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# TB summary writer\n",
    "# ====================================================\n",
    "with distribution_strategy.scope():\n",
    "    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        f\"{LOG_DIR}\", flush_millis=10 * 1000\n",
    "    )\n",
    "    train_summary_writer.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f6c98398-d545-4dcc-a71f-32249b5f0336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution_strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fae88447c70>\n",
      "number of train_files: 2\n",
      "Inpsecting agent policy from train_peram file...\n",
      "agent.policy: <tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x7fae883d1ba0>\n",
      "Inpsecting agent policy from train_peram file: Complete\n",
      "setting checkpoint_manager: gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/chkpoint\n",
      "agent.train_step_counter: 50\n",
      "starting train loop...\n",
      "epoch: 1\n",
      "step = 100: loss = 1.2000000476837158\n",
      "step = 200: loss = 1.4299999475479126\n",
      "step = 300: loss = 1.190000057220459\n",
      "step = 400: loss = 1.1399999856948853\n",
      "step = 500: loss = 1.2799999713897705\n",
      "epoch: 2\n",
      "step = 600: loss = 1.2100000381469727\n",
      "step = 700: loss = 1.4500000476837158\n",
      "step = 800: loss = 1.0800000429153442\n",
      "step = 900: loss = 1.309999942779541\n",
      "step = 1000: loss = 1.2100000381469727\n",
      "epoch: 3\n",
      "step = 1100: loss = 1.2000000476837158\n",
      "step = 1200: loss = 1.25\n",
      "step = 1300: loss = 1.2100000381469727\n",
      "step = 1400: loss = 1.190000057220459\n",
      "step = 1500: loss = 1.4299999475479126\n",
      "runtime_mins: 1\n",
      "saved trained policy to: gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/artifacts\n",
      "saved trained policy to: gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/chkpoint\n",
      "complete train job in 2 minutes\n"
     ]
    }
   ],
   "source": [
    "#start the timer and training\n",
    "start_time = time.time()\n",
    "\n",
    "metric_results, agent = train_perarm.train_perarm(\n",
    "    agent = agent,\n",
    "    reward_spec = reward_tensor_spec,\n",
    "    epsilon = HPARAMS['epsilon'],\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    num_iterations = TRAINING_LOOPS,\n",
    "    steps_per_loop = STEPS_PER_LOOP,\n",
    "    num_eval_steps = NUM_EVAL_STEPS,\n",
    "    # data\n",
    "    batch_size = HPARAMS['batch_size'],\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    # functions\n",
    "    _trajectory_fn = _trajectory_fn,\n",
    "    # _run_bandit_eval_fn = _run_bandit_eval,\n",
    "    # train intervals\n",
    "    chkpt_interval = CHKPT_INTERVAL,\n",
    "    log_interval = LOG_INTERVAL,\n",
    "    # dirs\n",
    "    bucket_name = BUCKET_NAME,\n",
    "    data_dir_prefix_path = f\"{EXAMPLE_GEN_GCS_PATH}\",\n",
    "    log_dir = LOG_DIR,\n",
    "    model_dir = ARTIFACTS_DIR,\n",
    "    chkpoint_dir = CHECKPT_DIR,\n",
    "    async_steps_per_loop = ASYNC_STEPS_PER_LOOP,\n",
    "    resume_training_loops = False,\n",
    "    use_gpu = True,\n",
    "    use_tpu = False,\n",
    "    profiler = False,\n",
    "    global_step = global_step,\n",
    "    total_train_take = TOTAL_TRAIN_TAKE,\n",
    "    train_summary_writer = train_summary_writer,\n",
    "    strategy = distribution_strategy,\n",
    "    is_testing = IS_TESTING,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "runtime_mins = int((end_time - start_time) / 60)\n",
    "print(f\"complete train job in {runtime_mins} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "177aa3ab-05b4-48f9-b434-9335bd9eb54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2931468"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(metric_results)\n",
    "metric_results[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ba65417f-54e0-436e-b0d4-93cffb981ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(metric_results)\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61464938-a3e7-4ab0-9149-4a9124199dc1",
   "metadata": {},
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9108f8b6-7aea-48d6-a763-461b30671c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "083c2351-5ac8-4218-9bef-e249777aee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=$LOG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ba747-00d5-4908-8d0b-334ae9d2c791",
   "metadata": {},
   "source": [
    "#### eval trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c5cf9366-d9cd-4c9d-951a-99a50877a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82506e-95d7-4a4e-a821-e08512615db7",
   "metadata": {},
   "source": [
    "After training, our `agent.policy` is used as the deployment \"model\" that will generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6c82396a-7822-41fb-830f-1f6eb9c9b213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy at 0x7fae883d1ba0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment_agent = agent.policy\n",
    "deployment_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f6706224-ef5b-4185-bde4-bccb7c8e7ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating trained Agent...\n",
      "post-train val_loss     : 1.2780730724334717\n",
      "post-train eval runtime : 0\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Evaluate the agent's policy once after training\n",
    "# ====================================================\n",
    "print(f\"evaluating trained Agent...\")\n",
    "\n",
    "post_policy_tf = py_tf_eager_policy.PyTFEagerPolicy(deployment_agent, use_tf_function=True)\n",
    "start_time = time.time()\n",
    "\n",
    "val_loss, preds, tr_rewards = eval_perarm._run_bandit_eval(\n",
    "    policy = post_policy_tf,\n",
    "    data = eval_ds,\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"post-train val_loss     : {val_loss}\")\n",
    "print(f\"post-train eval runtime : {runtime_mins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f899120f-3706-4b72-8f2c-2169221dd66b",
   "metadata": {},
   "source": [
    "## Using Trained Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b0f691-860e-4853-928e-4c7ece714dd0",
   "metadata": {},
   "source": [
    "### load trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ea029f73-0638-462a-8f00-54b05c54d69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/artifacts\n"
     ]
    }
   ],
   "source": [
    "POLICY_URI = ARTIFACTS_DIR\n",
    "\n",
    "print(POLICY_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "66d1d819-0152-437c-aefc-45ffd34d808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/artifacts/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/artifacts/fingerprint.pb\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/artifacts/policy_specs.pbtxt\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/artifacts/saved_model.pb\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/artifacts/assets/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/artifacts/variables/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $POLICY_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6aad948e-e346-4da3-8cb2-747435c8dc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.py_tf_eager_policy.SavedModelPyTFEagerPolicy at 0x7fae806f7be0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy\n",
    "\n",
    "trained_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n",
    "    POLICY_URI, load_specs_from_pbtxt=True\n",
    ")\n",
    "\n",
    "trained_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "483f8fa5-cbd1-4e3c-8e62-47f91f42e137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action_spec', minimum=0, maximum=1)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_policy.action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8f658891-bc09-49b8-bc58-000a7926df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFER_SIZE = 1\n",
    "dummy_arm = tf.zeros([INFER_SIZE, PER_ARM_DIM], dtype=tf.float32)\n",
    "\n",
    "SKIP_NUM = 10\n",
    "\n",
    "for x in eval_ds.skip(SKIP_NUM).take(INFER_SIZE):\n",
    "    # get feature tensors\n",
    "    \n",
    "    # global_feat_infer = _get_global_context_features(x)\n",
    "    # arm_feat_infer = _get_per_arm_features(x)\n",
    "    \n",
    "    global_feat_infer = embs._get_global_context_features(x)\n",
    "    arm_feat_infer = embs._get_per_arm_features(x)\n",
    "    \n",
    "    # rewards = _get_rewards(x)\n",
    "    rewards = reward_factory._get_rewards(x)\n",
    "    \n",
    "    # reshape arm features\n",
    "    arm_feat_infer = tf.reshape(arm_feat_infer, [HPARAMS['eval_batch_size'], PER_ARM_DIM]) # perarm_dim\n",
    "    concat_arm = tf.concat([arm_feat_infer, dummy_arm], axis=0)\n",
    "    \n",
    "    # flatten global\n",
    "    flat_global_infer = tf.reshape(global_feat_infer, [GLOBAL_DIM])\n",
    "    feature = {'global': flat_global_infer, 'per_arm': concat_arm}\n",
    "    \n",
    "    # get actual reward\n",
    "    actual_reward = rewards.numpy()[0]\n",
    "    \n",
    "    # build trajectory step\n",
    "    trajectory_step = train_utils._get_eval_step(feature, actual_reward)\n",
    "    \n",
    "    prediction = trained_policy.action(trajectory_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "49ce41ed-41b7-404d-9796-1658e7955894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
       " 'observation': {'global': <tf.Tensor: shape=(72,), dtype=float32, numpy=\n",
       "array([ 0.02686488,  0.04024548,  0.02308411, -0.02001072, -0.04564461,\n",
       "       -0.01644728,  0.01685527,  0.03973028, -0.03262532,  0.02532138,\n",
       "       -0.00777791, -0.02473327, -0.01311972, -0.03418459, -0.00583006,\n",
       "       -0.01100175, -0.04789207, -0.01735733, -0.0450286 ,  0.03365994,\n",
       "       -0.02977235,  0.0157761 , -0.01726376,  0.04136987, -0.00549601,\n",
       "        0.04077182, -0.00583255, -0.01910355, -0.00927299,  0.03418962,\n",
       "        0.02432555, -0.03775217,  0.01009493,  0.02008145, -0.03067734,\n",
       "        0.0117029 ,  0.03952468,  0.04534792, -0.02857381, -0.02307111,\n",
       "       -0.00746765,  0.03818199,  0.04199522,  0.03592339,  0.00161424,\n",
       "       -0.03737838,  0.04256186,  0.00249221, -0.04535607,  0.02894286,\n",
       "       -0.01138566, -0.02463037, -0.03271401, -0.04997932,  0.01932846,\n",
       "        0.01135371,  0.01531801,  0.01694382, -0.04856749,  0.03487397,\n",
       "        0.03813579, -0.026586  ,  0.0091952 ,  0.0355577 ,  0.00053481,\n",
       "        0.0023469 ,  0.00912814,  0.02631464, -0.03674991,  0.04484452,\n",
       "        0.02695623, -0.01823224], dtype=float32)>,\n",
       "                 'per_arm': <tf.Tensor: shape=(2, 64), dtype=float32, numpy=\n",
       "array([[-0.03819902, -0.00963265,  0.02579476, -0.00280404, -0.00252074,\n",
       "         0.00483309, -0.03345426, -0.04098202, -0.03691676, -0.04223515,\n",
       "         0.00595496, -0.02698987,  0.02475912,  0.0415615 ,  0.03490516,\n",
       "         0.03919195, -0.03368784, -0.04664968, -0.04624866,  0.01436097,\n",
       "         0.02693458, -0.03615444, -0.02702515,  0.01146175,  0.03049691,\n",
       "         0.030593  , -0.01151949, -0.01228634,  0.04001122, -0.00657036,\n",
       "        -0.04044862,  0.00084016,  0.04357857,  0.03752908,  0.00150322,\n",
       "        -0.04452466, -0.03336992,  0.00519491,  0.01589949, -0.04609166,\n",
       "         0.03201555, -0.03351036, -0.02021856,  0.01854335, -0.00607276,\n",
       "         0.0152379 , -0.0168758 ,  0.03579767,  0.11524488,  0.04350803,\n",
       "         0.02395778,  0.05809326, -0.00364802, -0.07442126,  0.02335799,\n",
       "        -0.07629991,  0.13602136,  0.04747006,  0.02962836, -0.08189921,\n",
       "         0.07996084, -0.08975591,  0.08651204, -0.09866363],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "      dtype=float32)>},\n",
       " 'reward': <tf.Tensor: shape=(), dtype=float32, numpy=3.0>,\n",
       " 'step_type': <tf.Tensor: shape=(), dtype=int32, numpy=0>})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f515fcd0-2228-4f68-9ef9-665ce1fc3fb2",
   "metadata": {},
   "source": [
    "### view prediction response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6af4c297-d2e8-4f7b-bed5-e1da997dfe34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=array(1, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.425938, 3.507738], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd58d4b-6749-4177-8342-83ee969408c9",
   "metadata": {},
   "source": [
    "#### chosen action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d49adbbb-f747-4e4c-b82d-df88373c644f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1, dtype=int32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd12013-1d12-4197-8254-cef9138df42c",
   "metadata": {},
   "source": [
    "#### Per Arm Policy Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d6126290-546b-4702-8a1e-91f18adb05c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.425938, 3.507738], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f66961-ebb5-4ae5-8e6f-ad73114d8353",
   "metadata": {},
   "source": [
    "**Finished**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
