{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385307d6-2058-47ec-8da0-57f7ef5c43d6",
   "metadata": {},
   "source": [
    "# Train Bandits with per-arm features\n",
    "\n",
    "**Exploring linear and nonlinear** (e.g., those with neural network-based value functions) bandit methods for recommendations using TF-Agents\n",
    "\n",
    "> Neural linear bandits provide a nice way to leverage the representation power of deep learning and the bandit approach for uncertainty measure and efficient exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e9956-66cd-4bf4-9b4d-8c2c646f0313",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "In this notebook, we explore the following topics for training contextual bandits with per-arm features:\n",
    "\n",
    "1. Data preperation\n",
    "2. Sampling functions\n",
    "3. TensorSpecs\n",
    "4. Agent, Network, training policy\n",
    "5. Reward function\n",
    "6. Trajectory function\n",
    "7. Train & Eval loops\n",
    "8. Getting predictions -\n",
    "9. Preparing the training application - abstracting all steps above to be used in subsequent notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fd257-b98b-426a-a2cd-024429b014f1",
   "metadata": {},
   "source": [
    "## Load notebook config\n",
    "\n",
    "* use the prefix defined in `00-env-setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39c9d08-d118-4013-a47f-88450f49f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX: rec-bandits-v2\n"
     ]
    }
   ],
   "source": [
    "# PREFIX = 'mabv1'\n",
    "VERSION        = \"v2\"                       # TODO\n",
    "PREFIX         = f'rec-bandits-{VERSION}'   # TODO\n",
    "\n",
    "print(f\"PREFIX: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908f6b95-b539-4a9f-a836-840d26ea3b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"rec-bandits-v2\"\n",
      "VERSION                  = \"v2\"\n",
      "\n",
      "BUCKET_NAME              = \"rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "BUCKET_URI               = \"gs://rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "DATA_PATH_KFP_DEMO       = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data/kfp_demo_data/u.data\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_NAME    = \"mvlens_rec_bandits_v2\"\n",
      "BIGQUERY_TABLE_NAME      = \"training_dataset\"\n",
      "\n",
      "REPOSITORY               = \"rl-movielens-rec-bandits-v2\"\n",
      "\n",
      "DOCKERNAME_01            = \"Dockerfile_train_my_perarm_env\"\n",
      "IMAGE_NAME_01            = \"train-my-perarm-env-v2\"\n",
      "IMAGE_URI_01             = \"gcr.io/hybrid-vertex/train-my-perarm-env-v2\"\n",
      "\n",
      "DOCKERNAME_02            = \"Dockerfile_perarm_feats\"\n",
      "IMAGE_NAME_02            = \"train-perarm-feats-v2\"\n",
      "IMAGE_URI_02             = \"gcr.io/hybrid-vertex/train-perarm-feats-v2\"\n",
      "\n",
      "DOCKERNAME_03            = \"Dockerfile_ranking_bandit\"\n",
      "IMAGE_NAME_03            = \"train-rank-bandit-v2\"\n",
      "IMAGE_URI_03             = \"gcr.io/hybrid-vertex/train-rank-bandit-v2\"\n",
      "\n",
      "DOCKERNAME_04            = \"Dockerfile_train_mab_e2e\"\n",
      "IMAGE_NAME_04            = \"train-mab-e2e-v2\"\n",
      "IMAGE_URI_04             = \"gcr.io/hybrid-vertex/train-mab-e2e-v2\"\n",
      "\n",
      "DOCKERNAME_04_pred       = \"Dockerfile_pred_mab_e2e\"\n",
      "IMAGE_NAME_04_pred       = \"pred-mab-e2e-v2\"\n",
      "IMAGE_URI_04_pred        = \"gcr.io/hybrid-vertex/pred-mab-e2e-v2\"\n",
      "\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-rec-bandits-v2/local_docker_tfa\"\n",
      "\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "RL_SUB_DIR               = \"per_arm_rl\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c690a9-e2bd-4759-ba41-4e2469098aee",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d0dfe4-695c-4dd4-9f24-67f7488ce1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c669f1a1-1af7-4efb-ab2d-6bf3b3847991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pprint import pprint\n",
    "import pickle as pkl\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "# from tf_agents.agents import TFAgent\n",
    "\n",
    "# from tf_agents.bandits.environments import stationary_stochastic_per_arm_py_environment as p_a_env\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "# from tf_agents.drivers import dynamic_step_driver\n",
    "# from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "# from tf_agents.bandits.agents import lin_ucb_agent\n",
    "# from tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\n",
    "from tf_agents.bandits.agents import neural_epsilon_greedy_agent\n",
    "from tf_agents.bandits.agents import neural_linucb_agent\n",
    "from tf_agents.bandits.networks import global_and_arm_feature_network\n",
    "from tf_agents.bandits.policies import policy_utilities\n",
    "\n",
    "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "nest = tf.nest\n",
    "\n",
    "# GPU\n",
    "from numba import cuda \n",
    "import gc\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# this repo\n",
    "from src import train_utils as train_utils\n",
    "from src import reward_factory as reward_factory\n",
    "from src.data import data_utils as data_utils\n",
    "from src.data import data_config as data_config\n",
    "from src.trainer import eval_perarm as eval_perarm\n",
    "from src.trainer import train_perarm as train_perarm\n",
    "from src.agents import agent_factory as agent_factory\n",
    "from src.networks import encoding_network as emb_features\n",
    "\n",
    "# tf exceptions and vars\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e58dd7-ab2b-419f-9771-bf1e98db758b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4969d3e-1fc0-45db-8a69-aa6b342019de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "274e7f4a-1802-4946-888e-876638f5c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a866b1-85b9-43e6-9546-edfbbf886bce",
   "metadata": {},
   "source": [
    "# [1] Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d4ed28-23d7-4785-b327-e5b543b0edb9",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "* Load train and eval datasets from TFRecords created in the `01-movielens-data-prep.ipynb` notebook\n",
    "* training examples represent historical (previously collected) interaction data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc3fcebe-818b-4767-afdc-cfb65b3b953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCS_DATA_PATH: gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/mv_b128_g12_a16/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/mv_b128_g12_a16_v4/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/train/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/val/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/vocabs/\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_GEN_GCS_PATH = data_config.EXAMPLE_GEN_GCS_PATH\n",
    "GCS_DATA_PATH = f\"{BUCKET_URI}/{EXAMPLE_GEN_GCS_PATH}\"\n",
    "\n",
    "print(f\"GCS_DATA_PATH: {GCS_DATA_PATH}\")\n",
    "\n",
    "!gsutil ls $GCS_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5b953-14c0-42ed-a511-77147a1bc0ac",
   "metadata": {},
   "source": [
    "### Read TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d0157c8-a04c-4dbd-b6d9-a1ede97687a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da0cdac-76af-4608-b70e-be7dbe0584b5",
   "metadata": {},
   "source": [
    "**Train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c934c06-bf08-4c7f-b0cc-0de04ef3515c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/train/ml-1m-gen-001-of-008.tfrecord',\n",
       " 'gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/train/ml-1m-gen-002-of-008.tfrecord',\n",
       " 'gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/train/ml-1m-gen-003-of-008.tfrecord']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT = \"train\" # \"train\" | \"val\"\n",
    "\n",
    "train_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{EXAMPLE_GEN_GCS_PATH}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "train_files = train_files[:3]\n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7511e4d-bf81-4800-bde7-8b16dec9aeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_movie_genres': <tf.Tensor: shape=(1, 10), dtype=string, numpy=\n",
      "array([[b'Drama', b'UNK', b'UNK', b'UNK', b'UNK', b'UNK', b'UNK', b'UNK',\n",
      "        b'UNK', b'UNK']], dtype=object)>,\n",
      " 'target_movie_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'1775'], dtype=object)>,\n",
      " 'target_movie_rating': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([4.], dtype=float32)>,\n",
      " 'target_movie_title': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Live Flesh (1997)'], dtype=object)>,\n",
      " 'target_movie_year': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1997])>,\n",
      " 'target_rating_timestamp': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([974612615])>,\n",
      " 'user_age': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([50])>,\n",
      " 'user_gender': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'M'], dtype=object)>,\n",
      " 'user_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'2173'], dtype=object)>,\n",
      " 'user_occupation_text': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'programmer'], dtype=object)>,\n",
      " 'user_zip_code': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'87505'], dtype=object)>}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "train_dataset = train_dataset.map(data_utils._parse_function)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452750c1-46a0-4ee1-84d4-0c3b227de38f",
   "metadata": {},
   "source": [
    "**Val data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7b5376f-9438-4304-84aa-de09c30f5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = \"val\"\n",
    "\n",
    "val_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{EXAMPLE_GEN_GCS_PATH}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        val_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "val_dataset = tf.data.TFRecordDataset(val_files)\n",
    "val_dataset = val_dataset.map(data_utils._parse_function, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8959967-522e-41c8-9a1b-050ca8bc191f",
   "metadata": {},
   "source": [
    "### get vocab\n",
    "\n",
    "**TODO:** \n",
    "* streamline vocab calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3ccf137-7a72-42e7-aa89-3c81a99cf40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vocab...\n",
      "Downloaded vocab from: gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/vocabs/vocab_dict.pkl\n",
      "\n",
      "'movie_id'\n",
      "'movie_year'\n",
      "'movie_genre'\n",
      "'movie_title'\n",
      "'user_id'\n",
      "'user_gender_vocab'\n",
      "'user_age_vocab'\n",
      "'user_occ_vocab'\n",
      "'user_zip_vocab'\n",
      "'min_timestamp'\n",
      "'max_timestamp'\n",
      "'timestamp_buckets'\n"
     ]
    }
   ],
   "source": [
    "EXISTING_VOCAB_FILE = f'gs://{BUCKET_NAME}/{EXAMPLE_GEN_GCS_PATH}/vocabs/{VOCAB_FILENAME}'\n",
    "print(f\"Downloading vocab...\")\n",
    "\n",
    "os.system(f'gsutil -q cp {EXISTING_VOCAB_FILE} .')\n",
    "print(f\"Downloaded vocab from: {EXISTING_VOCAB_FILE}\\n\")\n",
    "\n",
    "filehandler = open(VOCAB_FILENAME, 'rb')\n",
    "vocab_dict = pkl.load(filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "for key in vocab_dict.keys():\n",
    "    pprint(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfda012c-a2c3-4384-a5a4-54f5c6649006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocab_dict['user_occupation_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0138a295-2b0d-4359-8496-af8552ec8cff",
   "metadata": {},
   "source": [
    "# [2] Preprocessing layers for global and arm features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28c002-8d3a-4cf4-b69b-de56a6abab90",
   "metadata": {},
   "source": [
    "The preproccesing layers will ultimately feed the two sampling functions described below. These sampling functions will be used to create [trajectories](https://github.com/tensorflow/agents/blob/master/tf_agents/trajectories/trajectory.py#L36) (i.e., the training examples for our model)\n",
    "\n",
    "`global_context_sampling_fn`: \n",
    "* A function that outputs a random 1d array or list of ints or floats\n",
    "* This output is the global context. Its shape and type must be consistent across calls.\n",
    "\n",
    "`arm_context_sampling_fn`: \n",
    "* A function that outputs a random 1 array or list of ints or floats (same type as the output of `global_context_sampling_fn`). * This output is the per-arm context. Its shape must be consistent across calls.\n",
    "\n",
    "See `debug-and-learn-emb-models.ipynb` for more details and debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ea1f7-0621-44b8-b062-1f59b0964d72",
   "metadata": {},
   "source": [
    "## Global & Per-Arm feature embedding models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a1063-15d4-472e-b5a7-d92dcdea3c0f",
   "metadata": {},
   "source": [
    "**get expected dimensions**\n",
    "\n",
    "**common layers**\n",
    "* layer sizes for the final tower\n",
    "* The network that takes as input the concatenation of \n",
    "  the outputs of the global and the arm networks\n",
    "*  hidden layers ideally divisible by 8, e.g., [16, 32, 64, 128, 256, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8d77956-635c-438a-916a-185eec52f9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPECTED_GLOBAL_DIM: 72\n",
      "EXPECTED_PER_ARM_DIM: 64\n",
      "EXPECTED_GLOBAL_LAYERS      : [72, 36, 18]\n",
      "EXPECTED_ARM_LAYERS         : [64, 32, 16]\n",
      "EXPECTED_COMMON_LAYERS      : [34, 17, 8]\n"
     ]
    }
   ],
   "source": [
    "NUM_OOV_BUCKETS        = 1\n",
    "GLOBAL_EMBEDDING_SIZE  = 12\n",
    "MV_EMBEDDING_SIZE      = 16\n",
    "\n",
    "NUM_GLOBAL_FEATURES = len(data_utils.USER_FEATURE_NAMES)     # 6\n",
    "NUM_ARM_FEATURES    = len(data_utils.MOVIE_FEATURE_NAMES)    # 5\n",
    "EXPECTED_GLOBAL_DIM  = GLOBAL_EMBEDDING_SIZE * NUM_GLOBAL_FEATURES\n",
    "EXPECTED_PER_ARM_DIM = MV_EMBEDDING_SIZE * NUM_ARM_FEATURES\n",
    "print(f\"EXPECTED_GLOBAL_DIM: {EXPECTED_GLOBAL_DIM}\")\n",
    "print(f\"EXPECTED_PER_ARM_DIM: {EXPECTED_PER_ARM_DIM}\")\n",
    "\n",
    "EXPECTED_GLOBAL_LAYERS   = [\n",
    "    EXPECTED_GLOBAL_DIM, \n",
    "    int(EXPECTED_GLOBAL_DIM/2), \n",
    "    int(EXPECTED_GLOBAL_DIM/4)\n",
    "]\n",
    "EXPECTED_ARM_LAYERS      = [\n",
    "    EXPECTED_PER_ARM_DIM, \n",
    "    int(EXPECTED_PER_ARM_DIM/2), \n",
    "    int(EXPECTED_PER_ARM_DIM/4)\n",
    "]\n",
    "EXPECTED_FIRST_COMMON_LAYER = EXPECTED_GLOBAL_LAYERS[-1] + EXPECTED_ARM_LAYERS[-1]\n",
    "EXPECTED_COMMON_LAYERS = [\n",
    "    int(EXPECTED_FIRST_COMMON_LAYER), \n",
    "    int(EXPECTED_FIRST_COMMON_LAYER/2), \n",
    "    int(EXPECTED_FIRST_COMMON_LAYER/4)\n",
    "]\n",
    "print(f\"EXPECTED_GLOBAL_LAYERS      : {EXPECTED_GLOBAL_LAYERS}\")\n",
    "print(f\"EXPECTED_ARM_LAYERS         : {EXPECTED_ARM_LAYERS}\")\n",
    "print(f\"EXPECTED_COMMON_LAYERS      : {EXPECTED_COMMON_LAYERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c33673e-6069-477a-af80-0d2c436099bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.perarm_features import emb_feature_v2 as emb_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea335991-dcec-40b3-ba0d-a0fdfb9c2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test emb models\n",
    "for i in range(1):\n",
    "    \n",
    "    iterator = iter(train_dataset.batch(1))\n",
    "    data = next(iterator)\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48178fff-491f-4f03-b156-7765d1fa9707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.networks.encoding_network.EmbeddingModel at 0x7f9c350e6620>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = emb_features.EmbeddingModel(\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "903c63c5-2468-4443-bcb8-5ff6d69810dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL_DIM: 72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 72), dtype=float32, numpy=\n",
       "array([[ 0.03427093,  0.02444058, -0.00908045, -0.00497157, -0.03092518,\n",
       "         0.03691265,  0.04367464,  0.02046332, -0.03355793,  0.04679419,\n",
       "         0.0416473 , -0.04423703,  0.02943542, -0.0439412 ,  0.03132882,\n",
       "         0.04058753,  0.02676168,  0.01436519, -0.00680351,  0.03707686,\n",
       "        -0.0394115 ,  0.00050453,  0.00964335,  0.0324119 ,  0.01405532,\n",
       "        -0.048514  ,  0.01631901,  0.0049826 ,  0.00306644, -0.00274923,\n",
       "         0.0278886 ,  0.03749123,  0.02555528, -0.00764818,  0.04942514,\n",
       "        -0.03802457,  0.00680869, -0.04238832,  0.02525747, -0.00554125,\n",
       "         0.04364064,  0.00983664, -0.00790104,  0.02039062, -0.00756299,\n",
       "         0.02883396,  0.0384622 , -0.03334291, -0.03127741, -0.00939306,\n",
       "         0.01489509,  0.00713561, -0.03823937,  0.00486313,  0.02849397,\n",
       "        -0.00206742,  0.0132013 , -0.02648675, -0.01663002,  0.03477401,\n",
       "        -0.03565244,  0.01965222,  0.0084461 , -0.01130296,  0.00509446,\n",
       "         0.03235698,  0.02708601, -0.01190995,  0.02254989, -0.02232662,\n",
       "         0.00716916, -0.04197664]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_globals = embs._get_global_context_features(data)\n",
    "\n",
    "GLOBAL_DIM = test_globals.shape[1]            \n",
    "# shape checks out at batch_dim, nactions, arm feats\n",
    "print(f\"GLOBAL_DIM: {GLOBAL_DIM}\")\n",
    "\n",
    "test_globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebec4863-c614-4248-b680-8e29ec5b730d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER_ARM_DIM: 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 64), dtype=float32, numpy=\n",
       "array([[-5.40733337e-03,  4.48495857e-02, -4.64750789e-02,\n",
       "        -1.22782104e-02, -1.67412162e-02, -3.85383256e-02,\n",
       "         3.13885100e-02, -1.90032367e-02, -2.88875587e-02,\n",
       "        -4.15520743e-03,  4.26864289e-02, -4.79623191e-02,\n",
       "        -2.69930605e-02,  1.65897645e-02, -3.70720625e-02,\n",
       "         2.82901414e-02, -2.16893218e-02, -3.62938344e-02,\n",
       "        -2.40176395e-02, -4.35819402e-02, -7.72870285e-03,\n",
       "        -3.07578556e-02, -3.59451920e-02,  3.70066874e-02,\n",
       "         3.74156758e-02, -1.01232370e-02,  1.56152081e-02,\n",
       "        -2.29099728e-02,  2.03718953e-02, -2.61931773e-02,\n",
       "        -4.03303988e-02,  1.23524778e-02,  1.45035647e-02,\n",
       "         2.29690559e-02, -1.00304261e-02,  1.88382901e-02,\n",
       "         4.28435244e-02, -3.57904918e-02, -4.99218591e-02,\n",
       "        -4.85492833e-02, -2.72154342e-02, -3.15414295e-02,\n",
       "        -1.21942982e-02, -4.42124121e-02, -4.01362777e-05,\n",
       "        -2.70093568e-02, -1.54081360e-02, -9.47571918e-03,\n",
       "         1.07487679e-01,  1.00651443e-01,  3.11542116e-02,\n",
       "         1.00823283e-01, -8.45716000e-02,  4.05299105e-02,\n",
       "        -1.49599195e-01, -1.44305944e-01, -1.65189028e-01,\n",
       "        -2.46012509e-01, -2.11418234e-02, -1.20026946e-01,\n",
       "         9.15870667e-02,  2.45437041e-01,  7.61030763e-02,\n",
       "        -1.87543929e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arms = embs._get_per_arm_features(data)\n",
    "\n",
    "PER_ARM_DIM = test_arms.shape[1]            \n",
    "# shape checks out at batch_dim, nactions, arm feats\n",
    "print(f\"PER_ARM_DIM: {PER_ARM_DIM}\")\n",
    "\n",
    "test_arms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb4902-5bb9-4587-8d29-c01d78b006be",
   "metadata": {},
   "source": [
    "# [3] TensorSpecs\n",
    "\n",
    "* say you have a global observation spec of `[17]`. And you have two batch dimensions `[4, 5]`. Then your observation has to have the shape `[4, 5, 17]`\n",
    "* and then if you have arm_obs_spec with shape `[9, 13]`, then the arm obs shape has to be exactly `[4, 5, 9, 13]`\n",
    "* and this has to be true for every single tensor in your tensor nest\n",
    "* the first 2 dims are the outer dims that are the same for all tensors, the rest of the dimensions have to follow the spec for each tensor\n",
    "\n",
    "**TODO:**\n",
    "* explain relationship between Tensor Specs and their Tensor counterparts\n",
    "* highlight the errors, lessons learned, and utility functions to address these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ff9baaf-987d-448d-a981-742a79b581e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE      : 128\n",
      "EVAL_BATCH_SIZE : 1\n",
      "NUM_ACTIONS     : 2\n",
      "GLOBAL_DIM      : 72\n",
      "PER_ARM_DIM     : 64\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE      = 128\n",
    "EVAL_BATCH_SIZE = 1\n",
    "\n",
    "NUM_ACTIONS     = 2 #this is kinda deceptive - \n",
    "#our approach is to learn by \"flashing\" one movie rating at a time per user context. \n",
    "#The n_actions = show/don't show the movie with one degree of freedom (n-1)\n",
    "\n",
    "\n",
    "print(f\"BATCH_SIZE      : {BATCH_SIZE}\")\n",
    "print(f\"EVAL_BATCH_SIZE : {EVAL_BATCH_SIZE}\")\n",
    "print(f\"NUM_ACTIONS     : {NUM_ACTIONS}\")\n",
    "print(f\"GLOBAL_DIM      : {GLOBAL_DIM}\")\n",
    "print(f\"PER_ARM_DIM     : {PER_ARM_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20eca8d-8c73-4ec8-9d0f-f2b428055ac2",
   "metadata": {},
   "source": [
    "## Implementing MAB with TF-Agents\n",
    "\n",
    "In TF-Agents, the *per-arm features* implementation differs from the *global-only* feature examples in the following aspects:\n",
    "* Reward is modeled not per-arm, but globally.\n",
    "* The arms are permutation invariant: it doesn’t matter which arm is arm 1 or arm 2, only their features.\n",
    "* One can have a different number of arms to choose from in every step (note that unspecified/dynamically changing number of arms will have a problem with XLA compatibility).\n",
    "\n",
    "When implementing per-arm features in TF-Bandits, the following details have to be discussed:\n",
    "* Observation spec and observations,\n",
    "* Action spec and actions,\n",
    "* Implementation of specific policies and agents.\n",
    "\n",
    "\n",
    "**TODO:**\n",
    "* outline the components and highlight their interactions, dependencies on eachother, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68ebe8-116d-43b3-a6e1-4f5a5c7f4741",
   "metadata": {},
   "source": [
    "### Observation spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c4746-d01b-4ca4-aa53-ab68da54d37a",
   "metadata": {},
   "source": [
    "**This observation spec allows the user to have a global observation of fixed dimension**, and an unspecified number of *per-arm* features (also of fixed dimension)\n",
    "* The actions output by the policy are still integers as usual, and they indicate which row of the arm-features it has chosen \n",
    "* The action spec must be a single integer value without boundaries:\n",
    "\n",
    "```python\n",
    "global_spec = tensor_spec.TensorSpec([GLOBAL_DIM], tf.float32)\n",
    "per_arm_spec = tensor_spec.TensorSpec([None, PER_ARM_DIM], tf.float32)\n",
    "observation_spec = {'global': global_spec, 'per_arm': per_arm_spec}\n",
    "\n",
    "action_spec = tensor_spec.TensorSpec((), tf.int32)\n",
    "```\n",
    "> Here the only difference compared to the action spec with global features only is that the tensor spec is not bounded, as we don’t know how many arms there will be at any time step\n",
    "\n",
    "**XLA compatibility:**\n",
    "* Since dynamic tensor shapes are not compatible with XLA, the number of arm features (and consequently, number of arms for a step) cannot be dynamic. \n",
    "* One workaround is to fix the maximum number of arms for a problem, then pad the arm features in steps with fewer arms, and use action masking to indicate how many arms are actually active.\n",
    "\n",
    "```python\n",
    "per_arm_spec = tensor_spec.TensorSpec([NUM_ACTIONS, PER_ARM_DIM], tf.float32)\n",
    "\n",
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=(), dtype=tf.int32, minimum = 0, maximum = NUM_ACTIONS - 1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36bd3b33-635a-4274-8b9e-7172696ebb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_spec = {\n",
    "    'global': tf.TensorSpec([GLOBAL_DIM], tf.float32),\n",
    "    'per_arm': tf.TensorSpec([NUM_ACTIONS, PER_ARM_DIM], tf.float32) #excluding action dim here\n",
    "}\n",
    "observation_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2da92-7db2-4f42-94a7-b7bad1c8fc42",
   "metadata": {},
   "source": [
    "### Action spec\n",
    "\n",
    "> The time_step_spec and action_spec are specifications for the input time step and the output action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af42c7-18d7-480f-a7f3-d7da3f4840eb",
   "metadata": {},
   "source": [
    "```python\n",
    "    if (\n",
    "        not tensor_spec.is_bounded(action_spec)\n",
    "        or not tensor_spec.is_discrete(action_spec)\n",
    "        or action_spec.shape.rank > 1\n",
    "        or action_spec.shape.num_elements() != 1\n",
    "    ):\n",
    "      raise NotImplementedError(\n",
    "          'action_spec must be a BoundedTensorSpec of type int32 and shape (). '\n",
    "          'Found {}.'.format(action_spec)\n",
    "      )\n",
    "```\n",
    "\n",
    "* [src](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/policies/reward_prediction_base_policy.py#L97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "549a123c-349a-4103-b39a-4502f47d1e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=[], \n",
    "    dtype=tf.int32,\n",
    "    minimum=tf.constant(0),            \n",
    "    maximum=NUM_ACTIONS-1, # n degrees of freedom and will dictate the expected mean reward spec shape\n",
    "    name=\"action_spec\"\n",
    ")\n",
    "\n",
    "action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36ee2635-d4e3-4468-886c-ae9c62e3c80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected_num_actions: 2\n",
      "predicted_rewards_mean: TensorSpec(shape=(2,), dtype=tf.float32, name=None)\n"
     ]
    }
   ],
   "source": [
    "expected_num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "print(f\"expected_num_actions: {expected_num_actions}\")\n",
    "\n",
    "predicted_rewards_mean = tensor_spec.TensorSpec([expected_num_actions])\n",
    "print(f\"predicted_rewards_mean: {predicted_rewards_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a51efce-9f1b-42d1-bec4-7b788e3fd7e0",
   "metadata": {},
   "source": [
    "### TimeStep spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95f05860-0fbf-4a5a-8273-9c81761e0ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': {'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None),\n",
       "                 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)},\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec = ts.time_step_spec(\n",
    "    observation_spec = observation_spec, \n",
    "    # reward_spec = _reward_spec\n",
    ")\n",
    "time_step_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b129a-6d19-4b3d-a2e7-e27070f57ac0",
   "metadata": {},
   "source": [
    "### Reward Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b48e89aa-e010-4bd9-a7e0-ad62dd4c5949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': TensorSpec(shape=(128,), dtype=tf.float32, name='reward')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.specs import array_spec\n",
    "\n",
    "reward_spec = {\n",
    "    \"reward\": array_spec.ArraySpec(shape=[BATCH_SIZE], dtype=np.float32, name=\"reward\")\n",
    "}\n",
    "\n",
    "reward_tensor_spec = train_utils.from_spec(reward_spec)\n",
    "reward_tensor_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9181303-6565-45f5-a293-08d50420a805",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Inspect chosen arm features spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6b526a5-997c-4621-b21c-82c6ca1d6a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90d3e03e-619e-4fa4-b817-f02b4cefd6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(64,), dtype=tf.float32, name=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_arm_features_info = (\n",
    "  policy_utilities.create_chosen_arm_features_info_spec(\n",
    "      time_step_spec.observation,\n",
    "  )\n",
    ")\n",
    "chosen_arm_features_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc427092-beb7-4baf-a2e9-b8d10629182c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_policy_type = (\n",
    "    policy_utilities.BanditPolicyType.GREEDY\n",
    ")\n",
    "bandit_policy_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a4a2a33-c7d4-4d81-8082-80dfc3596741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_policy_type = (\n",
    "    policy_utilities.create_bandit_policy_type_tensor_spec(shape=[1])\n",
    ")\n",
    "bandit_policy_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e7ac02b-18bd-4519-92fe-5541b1a8ddc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorSpec(shape=(2,), dtype=tf.float32, name=None), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32)), chosen_arm_features=TensorSpec(shape=(64,), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_spec = policy_utilities.PerArmPolicyInfo(\n",
    "  predicted_rewards_mean=predicted_rewards_mean,\n",
    "  bandit_policy_type=bandit_policy_type,\n",
    "  chosen_arm_features=chosen_arm_features_info,\n",
    ")\n",
    "info_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21f28b9b-8183-495a-89b6-a01f30ea8637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PerArmPolicyInfo(\n",
    "#     log_probability=(), \n",
    "#     predicted_rewards_mean=TensorSpec(shape=(2,), \n",
    "#                                       dtype=tf.float32, name=None), \n",
    "#     multiobjective_scalarized_predicted_rewards_mean=(), \n",
    "#     predicted_rewards_optimistic=(), \n",
    "#     predicted_rewards_sampled=(), \n",
    "#     bandit_policy_type=BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32)), \n",
    "#     chosen_arm_features=TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9c197-ae9b-461d-8956-f078b929ac12",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [4] The Agent\n",
    "\n",
    "**Note** that contextual bandits form a special case of RL, where the actions taken by the agent do not alter the state of the environment \n",
    "\n",
    "> “Contextual” refers to the fact that the agent chooses among a set of actions while having knowledge of the context (environment observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d6e84-688e-48c5-aea1-77df031348c9",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Agent & Network defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aadb01-eb5c-4870-ae14-9e66624ba594",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Agent types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075e8a6-11a9-4346-8725-3653fba4bac4",
   "metadata": {},
   "source": [
    "> `AGENT_TYPE = ['LinUCB', 'LinTS', 'epsGreedy', 'NeuralLinUCB']`\n",
    "\n",
    "1. **LinearUCBAgent**: (`LinUCB`) - An agent implementing the Linear UCB bandit algorithm ([docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/lin_ucb_agent/LinearUCBAgent))\n",
    "\n",
    "2. **LinearThompsonSamplingAgent**: (`LinTS`) - Implements the Linear Thompson Sampling Agent from the [paper](https://arxiv.org/abs/1209.3352): *Thompson Sampling for Contextual Bandits with Linear Payoffs* ([docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/linear_thompson_sampling_agent/LinearThompsonSamplingAgent))\n",
    "\n",
    "3. **NeuralEpsilonGreedyAgent**: (`epsGreedy`) - A neural network based epsilon greedy agent ([docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_epsilon_greedy_agent/NeuralEpsilonGreedyAgent))\n",
    "\n",
    "4. **NeuralLinUCBAgent**: (`NeuralLinUCB`) - An agent implementing the LinUCB algorithm on top of a neural network ([docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_linucb_agent/NeuralLinUCBAgent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8e88e-c8ea-4193-a911-0d974ef3b1a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Network types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547087d-5fad-4521-a584-cb46ce52897d",
   "metadata": {},
   "source": [
    "Which network architecture to use for the `epsGreedy` or `NeuralLinUCB` agents\n",
    "\n",
    "```\n",
    "NETWORK_TYPE = ['commontower', 'dotproduct']\n",
    "```\n",
    "\n",
    "**GlobalAndArmCommonTowerNetwork:** (`commontower`)\n",
    "* This network takes the output of the global and per-arm networks, and leads them through a common network, that in turn outputs reward estimates\n",
    "> * `GLOBAL_LAYERS` - Iterable of ints. Specifies the layers of the global tower\n",
    "> * `ARM_LAYERS` - Iterable of ints. Specifies the layers of the arm tower\n",
    "> * `COMMON_LAYERS` - Iterable of ints. Specifies the layers of the common tower\n",
    "* The network produced by this function can be used either in `GreedyRewardPredictionPolicy`, or `NeuralLinUCBPolicy`\n",
    "> * In the former case, the network must have `output_dim=1`, it is going to be an instance of `QNetwork`, and used in the policy as a reward prediction network\n",
    "> * In the latter case, the network will be an encoding network with its output consumed by a reward layer or a `LinUCB` method. The specified `output_dim` will be the encoding dimension\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmCommonTowerNetwork)\n",
    "\n",
    "**GlobalAndArmDotProductNetwork:** (`dotproduct`)\n",
    "* This network calculates the **dot product** of the output of the global and per-arm networks and returns them as reward estimates\n",
    "> * `GLOBAL_LAYERS` - Iterable of ints. Specifies the layers of the global tower\n",
    "> * `ARM_LAYERS` - Iterable of ints. Specifies the layers of the arm tower\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmDotProductNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3dc270-deb5-4e96-8276-74759a06c318",
   "metadata": {},
   "source": [
    "### Agent config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbe29263-453d-405e-ac50-fee7f379f335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick check on the inputs of the agent - this can be used to diagnose spec shape inputs\n",
      "\n",
      "time_step_spec:  TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': {'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None),\n",
      "                 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)},\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n",
      "\n",
      "action_spec:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))\n",
      "\n",
      "observation_spec:  {'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Quick check on the inputs of the agent - this can be used to diagnose spec shape inputs\")\n",
    "print(\"\\ntime_step_spec: \", time_step_spec)\n",
    "print(\"\\naction_spec: \", action_spec)\n",
    "print(\"\\nobservation_spec: \", observation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d817bf2-1fa6-4bae-af90-745fad996b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128,\n",
      " 'common_layers': [34, 8],\n",
      " 'encoding_dim': 1,\n",
      " 'epsilon': 0.01,\n",
      " 'eval_batch_size': 1,\n",
      " 'global_layers': [72, 36, 18],\n",
      " 'learning_rate': 0.05,\n",
      " 'model_type': 'epsGreedy',\n",
      " 'network_type': 'commontower',\n",
      " 'num_actions': 2,\n",
      " 'per_arm_layers': [64, 32, 16]}\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Agents\n",
    "# ================================\n",
    "AGENT_TYPE      = 'epsGreedy' # 'LinUCB' | 'LinTS |, 'epsGreedy' | 'NeuralLinUCB'\n",
    "\n",
    "# Parameters for linear agents (LinUCB and LinTS).\n",
    "AGENT_ALPHA     = 0.1\n",
    "\n",
    "# Parameters for neural agents (NeuralEpsGreedy and NerualLinUCB).\n",
    "EPSILON         = 0.01\n",
    "LR              = 0.05\n",
    "\n",
    "# Parameters for NeuralLinUCB\n",
    "ENCODING_DIM    = 1\n",
    "EPS_PHASE_STEPS = 1000\n",
    "\n",
    "# ================================\n",
    "# Agent's Preprocess Network\n",
    "# ================================\n",
    " # beginning should be of size: GLOBAL_DIM\n",
    "GLOBAL_LAYERS   = [GLOBAL_DIM, int(GLOBAL_DIM/2), int(GLOBAL_DIM/4)]\n",
    "\n",
    "# beginning should be of size: PER_ARM_DIM\n",
    "ARM_LAYERS      = [PER_ARM_DIM, int(PER_ARM_DIM/2), int(PER_ARM_DIM/4)]\n",
    "\n",
    "# ================================\n",
    "# common layers\n",
    "# ================================\n",
    "\"\"\"\n",
    "> layer sizes for the final tower\n",
    "> The network that takes as input the concatenation of \n",
    "  the outputs of the global and the arm networks\n",
    "> hidden layers ideally divisible by 8, e.g., [16, 32, 64, 128, 256, 512]\n",
    "\"\"\"\n",
    "FIRST_COMMON_LAYER = GLOBAL_LAYERS[-1] + ARM_LAYERS[-1] # min(GLOBAL_LAYERS[-1], ARM_LAYERS[-1])\n",
    "\n",
    "COMMON_LAYERS = [\n",
    "    int(FIRST_COMMON_LAYER),\n",
    "    # int(FIRST_COMMON_LAYER/2),\n",
    "    int(FIRST_COMMON_LAYER/4)\n",
    "]\n",
    "\n",
    "NETWORK_TYPE    = \"commontower\" # 'commontower' | 'dotproduct'\n",
    "\n",
    "if AGENT_TYPE == 'NeuralLinUCB':\n",
    "    NETWORK_TYPE = 'commontower'\n",
    "    ENCODING_DIM = COMMON_LAYERS[-1]\n",
    "    \n",
    "if NETWORK_TYPE == 'dotproduct':\n",
    "    assert GLOBAL_LAYERS[0] == ARM_LAYERS[0]\n",
    "\n",
    "\n",
    "HPARAMS = {  # TODO - streamline and consolidate\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"eval_batch_size\" : EVAL_BATCH_SIZE,\n",
    "    \"num_actions\": NUM_ACTIONS,\n",
    "    \"model_type\": AGENT_TYPE,\n",
    "    \"network_type\": NETWORK_TYPE,\n",
    "    \"global_layers\": GLOBAL_LAYERS,\n",
    "    \"per_arm_layers\": ARM_LAYERS,\n",
    "    \"common_layers\": COMMON_LAYERS,\n",
    "    \"learning_rate\": LR,\n",
    "    \"epsilon\": EPSILON,\n",
    "    \"encoding_dim\": ENCODING_DIM\n",
    "}\n",
    "pprint(HPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bf060bb-9880-40b4-8583-dd7a3f915bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent: NeuralEpsGreedyAgent\n",
      "Network: commontower\n"
     ]
    }
   ],
   "source": [
    "# from src.perarm_features import agent_factory as agent_factory\n",
    "\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "agent = agent_factory.PerArmAgentFactory._get_agent(\n",
    "    agent_type = AGENT_TYPE,\n",
    "    network_type = NETWORK_TYPE,\n",
    "    time_step_spec = time_step_spec,\n",
    "    action_spec = action_spec,\n",
    "    observation_spec=observation_spec,\n",
    "    global_layers = GLOBAL_LAYERS,\n",
    "    arm_layers = ARM_LAYERS,\n",
    "    common_layers = COMMON_LAYERS,\n",
    "    agent_alpha = AGENT_ALPHA,\n",
    "    learning_rate = LR,\n",
    "    epsilon = EPSILON,\n",
    "    train_step_counter = global_step,\n",
    "    output_dim = ENCODING_DIM,\n",
    "    eps_phase_steps = EPS_PHASE_STEPS,\n",
    "    summarize_grads_and_vars = True,\n",
    "    debug_summaries = True\n",
    ")\n",
    "      \n",
    "agent.initialize()\n",
    "print(f'agent: {agent.name}')\n",
    "\n",
    "# if network:\n",
    "#     print(f\"Network: {network}\")\n",
    "\n",
    "if NETWORK_TYPE:\n",
    "    print(f\"Network: {NETWORK_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d77d1-a0b3-4898-adf0-0b32bfaf5bd7",
   "metadata": {},
   "source": [
    "#### Inspect the specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78ca56e7-a386-4bf4-8c2b-3d1665521f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41483f5a-bda6-48a4-9e1c-88d8b4afeea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TupleWrapper(TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': DictWrapper({'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.time_step_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f205716-f265-4ca8-81a7-4799de8f60a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TupleWrapper(Trajectory(\n",
       "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32)),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': DictWrapper({'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None)}),\n",
       " 'policy_info': PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorSpec(shape=(2,), dtype=tf.float32, name=None), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32)), chosen_arm_features=TensorSpec(shape=(64,), dtype=tf.float32, name=None)),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.training_data_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d404dae-3cc3-4f81-962b-4641455ca4f2",
   "metadata": {},
   "source": [
    "# [5] Reward function\n",
    "\n",
    "> see `reward_factory.py`\n",
    "\n",
    "* Since we are training a policy with previously collected interaction data, we model the reward function from actual rewards\n",
    "* We will simply pass the `user_rating` (values 0-5) as rewards to the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10a4f40a-b4ec-4fc2-9f73-3f830b21f351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'target_movie_rating'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.TARGET_FEATURE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df9aecd6-d20a-48a7-9ead-4da3bbcfdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_rewards(element):\n",
    "    \"\"\"Calculates reward for the actions.\"\"\"\n",
    "\n",
    "    def _calc_reward(x):\n",
    "        \"\"\"\n",
    "        Calculates reward for a single action.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### uncomment for linear rewards\n",
    "        r0 = lambda: tf.constant(0.0)\n",
    "        r1 = lambda: tf.constant(1.0)\n",
    "        r2 = lambda: tf.constant(2.0)\n",
    "        r3 = lambda: tf.constant(3.0)\n",
    "        r4 = lambda: tf.constant(4.0)\n",
    "        r5 = lambda: tf.constant(5.0)\n",
    "        \n",
    "        ### uncomment for binary rewards\n",
    "        # r0 = lambda: tf.constant(0.0) # 0.0\n",
    "        # r1 = lambda: tf.constant(0.0) # 1.0\n",
    "        # r2 = lambda: tf.constant(0.0) # 2.0\n",
    "        # r3 = lambda: tf.constant(0.0) # 3.0\n",
    "        # r4 = lambda: tf.constant(1.0) # 4.0\n",
    "        # r5 = lambda: tf.constant(1.0) # 5.0\n",
    "        \n",
    "        c1 = tf.equal(x, 1.0)\n",
    "        c2 = tf.equal(x, 2.0)\n",
    "        c3 = tf.equal(x, 3.0)\n",
    "        c4 = tf.equal(x, 4.0)\n",
    "        c5 = tf.equal(x, 5.0)\n",
    "        return tf.case(\n",
    "            [(c1, r1), (c2, r2), (c3, r3),(c4, r4),(c5, r5)], \n",
    "            default=r0, exclusive=True\n",
    "        )\n",
    "\n",
    "    return tf.map_fn(\n",
    "        fn=_calc_reward, \n",
    "        # elems=element['user_rating'],\n",
    "        elems=element[data_utils.TARGET_FEATURE_NAME],\n",
    "        dtype=tf.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59703a-9a00-4eba-a3b7-45b21b738dd4",
   "metadata": {},
   "source": [
    "# [6] Trajectory function\n",
    "\n",
    "> This function will convert training samples from the TF Records to `trajectories` which the Agent interprets as training records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3605d9f-d239-4871-9a61-6e9b21f9c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _trajectory_fn(element): # hparams\n",
    "    \n",
    "    \"\"\"Converts a dataset element into a trajectory.\"\"\"\n",
    "    global_features = embs._get_global_context_features(element)\n",
    "    arm_features = embs._get_per_arm_features(element)\n",
    "\n",
    "    # Adds a time dimension.\n",
    "    arm_features = train_utils._add_outer_dimension(arm_features)\n",
    "\n",
    "    # obs spec\n",
    "    observation = {\n",
    "        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n",
    "            train_utils._add_outer_dimension(global_features),\n",
    "    }\n",
    "\n",
    "    reward = train_utils._add_outer_dimension(_get_rewards(element))\n",
    "    # reward = train_utils._add_outer_dimension(reward_factory._get_rewards(element))\n",
    "\n",
    "    # To emit the predicted rewards in policy_info, we need to create dummy\n",
    "    # rewards to match the definition in TensorSpec for the ones specified in\n",
    "    # emit_policy_info set.\n",
    "    dummy_rewards = tf.zeros([HPARAMS['batch_size'], 1, HPARAMS['num_actions']])\n",
    "    policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "        chosen_arm_features=arm_features,\n",
    "        # Pass dummy mean rewards here to match the model_spec for emitting\n",
    "        # mean rewards in policy info\n",
    "        predicted_rewards_mean=dummy_rewards,\n",
    "        bandit_policy_type=tf.zeros([HPARAMS['batch_size'], 1, 1], dtype=tf.int32)\n",
    "        # policy_utilities.create_bandit_policy_type_tensor_spec(shape=[1]) # policy_utilities.BanditPolicyType.GREEDY\n",
    "        # tf.zeros([batch_size, 1, 1], dtype=tf.int32)\n",
    "    )\n",
    "    \n",
    "    if HPARAMS['model_type'] == 'neural_ucb':\n",
    "        policy_info = policy_info._replace(\n",
    "            predicted_rewards_optimistic=dummy_rewards\n",
    "        )\n",
    "        \n",
    "    return trajectory.single_step(\n",
    "        observation=observation,\n",
    "        action=tf.zeros_like(\n",
    "            reward, dtype=tf.int32\n",
    "        ),  # Arm features are copied from policy info, put dummy zeros here\n",
    "        policy_info=policy_info,\n",
    "        reward=reward,\n",
    "        discount=tf.zeros_like(reward)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a0989c-5a06-4fd5-a12d-d92dbbe71a76",
   "metadata": {},
   "source": [
    "Inspect the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "257791d1-c97c-48c0-bc92-f5a4737b6c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_traj.action.shape      : (128, 1)\n",
      "test_traj.discount.shape    : (128, 1)\n",
      "test_traj.reward.shape      : (128, 1)\n",
      "test_traj.observation.shape : (128, 1, 72)\n"
     ]
    }
   ],
   "source": [
    "for x in train_dataset.batch(HPARAMS['batch_size']).take(1):\n",
    "    test_traj = _trajectory_fn(x)\n",
    "    \n",
    "print(f\"test_traj.action.shape      : {test_traj.action.shape}\") \n",
    "print(f\"test_traj.discount.shape    : {test_traj.discount.shape}\")\n",
    "print(f\"test_traj.reward.shape      : {test_traj.reward.shape}\")\n",
    "print(f\"test_traj.observation.shape : {test_traj.observation['global'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3dace3d1-ce61-48cf-82a4-f701d3fe337c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_traj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbfc9a-049e-49b9-b2a3-ecb9fd4b9964",
   "metadata": {},
   "source": [
    "# [7] Training the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13adc1ef-b50e-41a3-9b95-c128d0c55a2b",
   "metadata": {},
   "source": [
    "## set Vertex Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e861fb27-df24-4821-8448-cf1346b186f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : 02-big-context-bandits-v5-rec-bandits-v2\n",
      "RUN_NAME          : run-20240313-144121\n",
      "\n",
      "CHECKPT_DIR       : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/chkpoint\n",
      "BASE_OUTPUT_DIR   : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-144121\n",
      "LOG_DIR           : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-144121/logs\n",
      "ROOT_DIR          : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-144121/root\n",
      "ARTIFACTS_DIR     : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-144121/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'02-big-context-bandits-v5-{PREFIX}'\n",
    "\n",
    "# new experiment\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "CHECKPT_DIR       = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/chkpoint\"\n",
    "BASE_OUTPUT_DIR   = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}\"\n",
    "LOG_DIR           = f\"{BASE_OUTPUT_DIR}/logs\"\n",
    "ROOT_DIR          = f\"{BASE_OUTPUT_DIR}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BASE_OUTPUT_DIR}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "# vertex_ai.init(\n",
    "#     project=PROJECT_ID,\n",
    "#     location=REGION,\n",
    "#     experiment=EXPERIMENT_NAME\n",
    "# )\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\\n\")\n",
    "print(f\"CHECKPT_DIR       : {CHECKPT_DIR}\")\n",
    "print(f\"BASE_OUTPUT_DIR   : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de729bad-0bc9-429e-b4cb-7b24bf615aa1",
   "metadata": {},
   "source": [
    "### Create TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2570564-71f4-4dda-8d8a-59784db67632",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_TENSORBOARD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db63052a-7eea-4982-964d-1f7ecab0665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME: projects/934903580331/locations/us-central1/tensorboards/7854669176273633280\n",
      "TB display name: 02-big-context-bandits-v5-rec-bandits-v2\n"
     ]
    }
   ],
   "source": [
    "if NEW_TENSORBOARD:\n",
    "    # create new TB instance\n",
    "    TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}\"\n",
    "\n",
    "    tensorboard = aiplatform.Tensorboard.create(\n",
    "        display_name=TENSORBOARD_DISPLAY_NAME\n",
    "        , project=PROJECT_ID\n",
    "        , location=REGION\n",
    "    )\n",
    "\n",
    "    TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "else:\n",
    "    # use existing TB instance\n",
    "    TB_RESOURCE_NAME = 'projects/934903580331/locations/us-central1/tensorboards/10313419068538880' # bandit-rewards\n",
    "    tensorboard = aiplatform.Tensorboard(\n",
    "        tensorboard_name=TB_RESOURCE_NAME\n",
    "    )\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name: {tensorboard.display_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a0b708d-990e-468b-a1b1-a8ba8f71d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete Tensorboard\n",
    "# vertex_ai_tb.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c891d27-d9d1-4e64-8981-1a1ae343c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aiplatform.init(\n",
    "#     project=PROJECT_ID,\n",
    "#     location=REGION,\n",
    "#     experiment=EXPERIMENT_NAME,\n",
    "#     experiment_tensorboard=TB_ID\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7a5e4-efff-490b-b431-53037a2f4a16",
   "metadata": {},
   "source": [
    "## eval loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba0392e-90c8-4dc8-8df2-6d2af1ade490",
   "metadata": {},
   "source": [
    "> When evaluating an epsilon-greedy bandit, we need to remove those predictions that come from the `uniform_random` policy. We only want the predictions from the `GREEDY` policy, becasue we want to know how the model performs when it is actually used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b3d3c-75f7-46f4-9a1b-6329e419b7f5",
   "metadata": {},
   "source": [
    "## train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97cfe7c3-4022-47a9-83c2-df0ad8a2abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.metrics import export_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.utils import common\n",
    "\n",
    "# from src.per_arm_rl import train_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb7ffc-00df-4d46-857b-86c87b78f597",
   "metadata": {},
   "source": [
    "`agent.train(experience=...)`\n",
    "\n",
    "where `experience` is a batch of trajectories data in the form of a Trajectory. \n",
    "* The structure of experience must match that of `self.training_data_spec`. \n",
    "* All tensors in experience must be shaped [batch, time, ...] where time must be equal to self.train_step_length if that property is not None."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d6d45-c71e-4fe4-9af8-af9a8bda4e4f",
   "metadata": {},
   "source": [
    "### logs & checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "049ee49b-3b1a-4b12-a360-15e759dff7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('metrics: [<tf_agents.metrics.tf_metrics.AverageReturnMetric object at '\n",
      " '0x7f9c25024bb0>]')\n",
      "setting checkpoint_manager: gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/chkpoint\n",
      "\n",
      "'saver: <tf_agents.policies.policy_saver.PolicySaver object at 0x7f9ba09b1990>'\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# TB summary writer\n",
    "# ====================================================\n",
    "train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    f\"{LOG_DIR}\", flush_millis=10 * 1000\n",
    ")\n",
    "# ====================================================\n",
    "# metrics\n",
    "# ====================================================\n",
    "step_metric = tf_metrics.EnvironmentSteps()\n",
    "metrics = [\n",
    "    # tf_metrics.NumberOfEpisodes(),\n",
    "    # tf_metrics.AverageEpisodeLengthMetric(batch_size=HPARAMS['batch_size']),\n",
    "    tf_metrics.AverageReturnMetric(batch_size=HPARAMS['batch_size'])\n",
    "]\n",
    "\n",
    "pprint(f\"metrics: {metrics}\")\n",
    "\n",
    "# ====================================================\n",
    "# get checkpoint manager\n",
    "# ====================================================\n",
    "print(f\"setting checkpoint_manager: {CHECKPT_DIR}\\n\")\n",
    "\n",
    "checkpoint_manager = train_utils.restore_and_get_checkpoint_manager(\n",
    "    root_dir=CHECKPT_DIR, \n",
    "    agent=agent, \n",
    "    metrics=metrics, \n",
    "    step_metric=step_metric\n",
    ")\n",
    "# ====================================================\n",
    "# policy saver\n",
    "# ====================================================\n",
    "saver = policy_saver.PolicySaver(\n",
    "    agent.policy, \n",
    "    train_step=global_step\n",
    ")\n",
    "pprint(f\"saver: {saver}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d155f1f4-0d95-40a8-a37c-c608a64af803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7f9ba0026b90>,\n",
       " 'get_initial_state': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7f9ba0026cb0>,\n",
       " 'get_train_step': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7f9b9ffea170>,\n",
       " 'get_metadata': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7f9b9ffe85b0>}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83683f4b-8a1a-44b9-8d5f-7ee3304c794c",
   "metadata": {},
   "source": [
    "### config\n",
    "\n",
    "* calculate train & val dataset sizes\n",
    "* define logging and chkpt intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9432613-6583-4828-8a2a-dd939adbc3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_DATA_SIZE : 300000\n",
      "NUM_TRAIN_STEPS : 50\n",
      "EVAL_DATA_SIZE  : 90000\n",
      "NUM_EVAL_STEPS  : 1000\n",
      "CHKPT_INTERVAL  : 50\n",
      "LOG_INTERVAL    : 10\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_SIZE = 300_000       # len(list(train_dataset))\n",
    "NUM_TRAIN_STEPS = 50            # TRAIN_DATA_SIZE // HPARAMS['batch_size']\n",
    "\n",
    "EVAL_DATA_SIZE  = 90_000        # len(list(val_dataset))\n",
    "NUM_EVAL_STEPS  = 1_000         # EVAL_DATA_SIZE // HPARAMS['eval_batch_size']\n",
    "\n",
    "CHKPT_INTERVAL  = NUM_TRAIN_STEPS # // 5\n",
    "LOG_INTERVAL    = 10\n",
    "# EVAL_INTERVAL = NUM_TRAIN_STEPS // 2\n",
    "\n",
    "print(f\"TRAIN_DATA_SIZE : {TRAIN_DATA_SIZE}\")\n",
    "print(f\"NUM_TRAIN_STEPS : {NUM_TRAIN_STEPS}\")\n",
    "print(f\"EVAL_DATA_SIZE  : {EVAL_DATA_SIZE}\")\n",
    "print(f\"NUM_EVAL_STEPS  : {NUM_EVAL_STEPS}\")\n",
    "print(f\"CHKPT_INTERVAL  : {CHKPT_INTERVAL}\")\n",
    "print(f\"LOG_INTERVAL    : {LOG_INTERVAL}\")\n",
    "# print(f\"EVAL_INTERVAL : {EVAL_INTERVAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "379ffc65-4eec-4af1-8fd4-912ef65a9808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TakeDataset element_spec={'target_movie_genres': TensorSpec(shape=(None, 10), dtype=tf.string, name=None), 'target_movie_id': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'target_movie_rating': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'target_movie_title': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'target_movie_year': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'target_rating_timestamp': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'user_age': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'user_gender': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_id': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_occupation_text': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_zip_code': TensorSpec(shape=(None,), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data\n",
    "train_ds_iterator = iter(train_dataset.batch(HPARAMS['batch_size']).repeat())\n",
    "\n",
    "# eval dataset\n",
    "eval_ds = val_dataset.batch(HPARAMS[\"eval_batch_size\"])\n",
    "\n",
    "if NUM_EVAL_STEPS > 0:\n",
    "    eval_ds = eval_ds.take(NUM_EVAL_STEPS)\n",
    "\n",
    "# eval_ds = eval_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "eval_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498d7e3-d166-4e1d-94a5-f721de58694a",
   "metadata": {},
   "source": [
    "### run train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da4680f3-693b-4530-8e4d-88bc43036c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating pre-trained Agent...\n",
      "pre-train val_loss     : 15.014739990234375\n",
      "pre-train eval runtime : 0\n",
      "starting train loop...\n",
      "step = 0: train loss = 15.979999542236328\n",
      "step = 10: train loss = 6.559999942779541\n",
      "step = 20: train loss = 1.4299999475479126\n",
      "step = 30: train loss = 1.7599999904632568\n",
      "step = 40: train loss = 1.0099999904632568\n",
      "train runtime_mins: 5\n",
      "saved trained policy to: gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-144121/artifacts\n",
      "evaluating trained Agent...\n",
      "post-train val_loss     : 2.6361613273620605\n",
      "post-train eval runtime : 0\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "# agent.train = common.function(agent.train)\n",
    "\n",
    "list_o_loss = []\n",
    "\n",
    "# ====================================================\n",
    "# Evaluate the agent's policy once before training\n",
    "# ====================================================\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "pre_policy_tf = py_tf_eager_policy.PyTFEagerPolicy(agent.policy, use_tf_function=True)\n",
    "\n",
    "print(f\"evaluating pre-trained Agent...\")\n",
    "start_time = time.time()\n",
    "\n",
    "pre_val_loss, pre_preds, pre_tr_rewards = eval_perarm._run_bandit_eval(\n",
    "    policy = pre_policy_tf,\n",
    "    data = eval_ds,\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"pre-train val_loss     : {pre_val_loss}\")\n",
    "print(f\"pre-train eval runtime : {runtime_mins}\")\n",
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "print(f\"starting train loop...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(NUM_TRAIN_STEPS):\n",
    "    \n",
    "    with train_summary_writer.as_default():\n",
    "\n",
    "        data = next(train_ds_iterator)\n",
    "        trajectories = _trajectory_fn(data)\n",
    "\n",
    "        # All tensors in experience must be shaped [batch, time, ...] \n",
    "        step = agent.train_step_counter.numpy()\n",
    "        loss = agent.train(experience=trajectories)\n",
    "        list_o_loss.append(loss.loss.numpy())\n",
    "\n",
    "        train_utils._export_metrics_and_summaries(\n",
    "            step=i, \n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        # print step loss\n",
    "        if step % LOG_INTERVAL == 0:\n",
    "            print(\n",
    "                'step = {0}: train loss = {1}'.format(\n",
    "                    step, round(loss.loss.numpy(), 2)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if i > 0 and i % CHKPT_INTERVAL == 0:\n",
    "            saver.save(os.path.join(CHKPOINT_DIR, 'policy_%d' % step_metric.result()))\n",
    "            print(f\"saved policy to: {CHKPOINT_DIR}\")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"train runtime_mins: {runtime_mins}\")\n",
    "\n",
    "saver.save(ARTIFACTS_DIR)\n",
    "print(f\"saved trained policy to: {ARTIFACTS_DIR}\")\n",
    "\n",
    "# ====================================================\n",
    "# Evaluate the agent's policy once after training\n",
    "# ====================================================\n",
    "print(f\"evaluating trained Agent...\")\n",
    "\n",
    "post_policy_tf = py_tf_eager_policy.PyTFEagerPolicy(agent.policy, use_tf_function=True)\n",
    "start_time = time.time()\n",
    "\n",
    "val_loss, preds, tr_rewards = eval_perarm._run_bandit_eval(\n",
    "    policy = post_policy_tf,\n",
    "    data = eval_ds,\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"post-train val_loss     : {val_loss}\")\n",
    "print(f\"post-train eval runtime : {runtime_mins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17b2b0d2-9795-4367-b57b-c331e94d1000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6361613"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb1ed8-f67b-4e7b-9cbc-70edd50e49ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate train job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31fe28-2f05-499f-b1cc-505f3c9074e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### train loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41937f9a-6c28-48d2-bc7c-bfe4372c7cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_o_loss)\n",
    "# plt.ylim([0, 10])\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e947e1-67f4-4205-99af-6921c6e9c896",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "65627011-48eb-4e8e-981e-8b61b0f427c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6006: logdir gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-local-bandits-rec-bandits-v2/run-20240301-042649/logs (started 12 days, 10:13:47 ago; pid 3837675)\n",
      "  - port 6006: logdir gs://rec-bandits-v2-hybrid-vertex-bucket/02x-new-data-loading-v3/run-20240307-012852/logs (started 6 days, 13:20:09 ago; pid 1590886)\n",
      "  - port 6006: logdir gs://rec-bandits-v2-hybrid-vertex-bucket/02b-deep-bandits-rec-bandits-v2/run-20240214-180454/logs (started 27 days, 20:29:39 ago; pid 3029265)\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0f77a33-0962-4af0-af97-cf05f895cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d0509cb-0777-4d35-86c6-28c8acbcc793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f30e9adf9e7e69a8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f30e9adf9e7e69a8\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=$LOG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc40143-30b1-479f-b9b1-fa4c07b1690e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [8] Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c94a0-122f-4ba6-acef-5653a3cc7850",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1c382b5-e2e0-45a2-9c5f-ef2c468d0f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-144121/artifacts\n"
     ]
    }
   ],
   "source": [
    "POLICY_URI = ARTIFACTS_DIR\n",
    "\n",
    "print(POLICY_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "68592d97-1272-4c6d-a4bd-43a6f7fe6c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.py_tf_eager_policy.SavedModelPyTFEagerPolicy at 0x7f9b9ffe8d90>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy\n",
    "\n",
    "trained_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n",
    "    POLICY_URI, load_specs_from_pbtxt=True\n",
    ")\n",
    "trained_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d910a06f-678f-4bfa-a132-10b885f871c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762c936-3c49-4ec5-a9eb-4df5816dc888",
   "metadata": {},
   "source": [
    "TODO - explain bandit_policy_type()\n",
    "\n",
    "**Dummy arm values?**\n",
    "* We set `chosen_arm_features` to dummy values of all zeros. We need to save dummy chosen arm features to make the returned policy step have the same structure as the policy state spec.\n",
    "* `emit_policy_info = ('predicted_rewards_mean', 'bandit_policy_type')` defines what side information we want to get as part of the policy info when we call policy network \n",
    "* This makes it so that the model always returns the expected rewards even if the model is exploring\n",
    "* This means that the largest predicted rewards may not match the selected action when the model is exploring (i.e. bandit_policy == UNIFORM == 2)\n",
    "\n",
    "**UNIFORM Random policy:**\n",
    "```\n",
    "PolicyStep(action=array(1, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.637535 , 3.5557823], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([2], dtype=int32), chosen_arm_features=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)))\n",
    "```\n",
    "\n",
    "**GREEDY policy:**\n",
    "```\n",
    "PolicyStep(action=array(0, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.8278818, 3.607565 ], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([ 0.04054676, -0.04735027,  0.02202327,  0.01276748, -0.01367382,\n",
    "       -0.04838754,  0.00104916, -0.01956806,  0.01028793, -0.00256665,\n",
    "       -0.04376553,  0.03900594,  0.02388967, -0.04016995, -0.04945569,\n",
    "       -0.04539652,  0.04423274, -0.04630332,  0.00390794,  0.03986299,\n",
    "       -0.0398974 ,  0.01655747, -0.0471294 , -0.00690235, -0.03643382,\n",
    "       -0.0437116 ,  0.04757959, -0.00991895,  0.0281055 , -0.04633433,\n",
    "        0.01897702, -0.00824345,  0.03232259, -0.04980658, -0.01144745,\n",
    "        0.01690939,  0.02536928,  0.03384003, -0.01558131,  0.04731432,\n",
    "        0.04579857, -0.03305101,  0.03521084, -0.03915765,  0.04927064,\n",
    "        0.04695194,  0.03792156,  0.040496  ,  0.02574866, -0.00665367,\n",
    "       -0.03839222, -0.01564004, -0.04710373,  0.03557512, -0.01392462,\n",
    "        0.01368621,  0.02792176, -0.01799218,  0.00246101,  0.02263175,\n",
    "        0.0170817 ,  0.01388383, -0.03048695,  0.01198485], dtype=float32)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "567f64e1-8149-4364-b561-1a9d12581b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aff30396-e0fd-4f43-acaa-6af8b20b12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFER_SIZE = 1\n",
    "dummy_arm = tf.zeros([INFER_SIZE, PER_ARM_DIM], dtype=tf.float32)\n",
    "\n",
    "SKIP_NUM = 10\n",
    "\n",
    "for x in eval_ds.skip(SKIP_NUM).take(INFER_SIZE):\n",
    "    # get feature tensors\n",
    "    \n",
    "    global_feat_infer = embs._get_global_context_features(x)\n",
    "    arm_feat_infer = embs._get_per_arm_features(x)\n",
    "    \n",
    "    rewards = _get_rewards(x)\n",
    "    # rewards = reward_factory._get_rewards(x)\n",
    "    \n",
    "    # reshape arm features\n",
    "    arm_feat_infer = tf.reshape(arm_feat_infer, [HPARAMS['eval_batch_size'], PER_ARM_DIM]) # perarm_dim\n",
    "    concat_arm = tf.concat([arm_feat_infer, dummy_arm], axis=0)\n",
    "    \n",
    "    # flatten global\n",
    "    flat_global_infer = tf.reshape(global_feat_infer, [GLOBAL_DIM])\n",
    "    feature = {'global': flat_global_infer, 'per_arm': concat_arm}\n",
    "    \n",
    "    # get actual reward\n",
    "    actual_reward = rewards.numpy()[0]\n",
    "    \n",
    "    # build trajectory step\n",
    "    trajectory_step = train_utils._get_eval_step(feature, actual_reward)\n",
    "    \n",
    "    prediction = trained_policy.action(trajectory_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2bcd1e82-168e-4df3-92bd-4cd34ecd3a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
       " 'observation': {'global': <tf.Tensor: shape=(72,), dtype=float32, numpy=\n",
       "array([-0.03181489, -0.03150504,  0.01586967, -0.03423699,  0.02468583,\n",
       "       -0.02089763,  0.00090661, -0.03285823,  0.01283187, -0.02559625,\n",
       "        0.01428579,  0.00261404,  0.03205058, -0.01217639, -0.04342364,\n",
       "        0.00607739, -0.00856508,  0.00967627, -0.0204155 ,  0.01912046,\n",
       "        0.00766133,  0.02384528,  0.00028539, -0.00129075,  0.0039437 ,\n",
       "       -0.04848684, -0.00826347,  0.04584655, -0.02666476, -0.00137733,\n",
       "        0.0288277 ,  0.02119733,  0.02753642, -0.00672438, -0.00840075,\n",
       "       -0.04080123,  0.00680869, -0.04238832,  0.02525747, -0.00554125,\n",
       "        0.04364064,  0.00983664, -0.00790104,  0.02039062, -0.00756299,\n",
       "        0.02883396,  0.0384622 , -0.03334291, -0.03127741, -0.00939306,\n",
       "        0.01489509,  0.00713561, -0.03823937,  0.00486313,  0.02849397,\n",
       "       -0.00206742,  0.0132013 , -0.02648675, -0.01663002,  0.03477401,\n",
       "       -0.0339587 ,  0.01062815, -0.01629164, -0.00711859,  0.00557371,\n",
       "       -0.01477247,  0.04940696, -0.04959068,  0.02431491,  0.01431907,\n",
       "       -0.03684079,  0.00318494], dtype=float32)>,\n",
       "                 'per_arm': <tf.Tensor: shape=(2, 64), dtype=float32, numpy=\n",
       "array([[ 0.01546891,  0.03901565, -0.03869734,  0.02990491,  0.01326268,\n",
       "         0.0376035 ,  0.03045154,  0.03173845, -0.02372525,  0.03941197,\n",
       "        -0.0338662 , -0.03101235, -0.00853217,  0.00802015,  0.0430473 ,\n",
       "         0.01540356, -0.02168932, -0.03629383, -0.02401764, -0.04358194,\n",
       "        -0.0077287 , -0.03075786, -0.03594519,  0.03700669,  0.03741568,\n",
       "        -0.01012324,  0.01561521, -0.02290997,  0.0203719 , -0.02619318,\n",
       "        -0.0403304 ,  0.01235248, -0.03126605,  0.02974543,  0.0222582 ,\n",
       "        -0.04646191, -0.03931414, -0.04322553, -0.00944998,  0.01204463,\n",
       "         0.02464309,  0.0406023 ,  0.04995011, -0.03614325, -0.03751718,\n",
       "        -0.045487  ,  0.04200497, -0.00375426,  0.06449261,  0.06039086,\n",
       "         0.01869253,  0.06049397, -0.05074296,  0.02431795, -0.08975951,\n",
       "        -0.08658357, -0.09911341, -0.1476075 , -0.01268509, -0.07201616,\n",
       "         0.05495224,  0.14726223,  0.04566185, -0.11252636],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "      dtype=float32)>},\n",
       " 'reward': <tf.Tensor: shape=(), dtype=float32, numpy=3.0>,\n",
       " 'step_type': <tf.Tensor: shape=(), dtype=int32, numpy=0>})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e58af9ab-f4da-4857-afe5-6b8de26ed0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=array(0, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([2.5652807, 2.1232865], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([ 0.01546891,  0.03901565, -0.03869734,  0.02990491,  0.01326268,\n",
       "        0.0376035 ,  0.03045154,  0.03173845, -0.02372525,  0.03941197,\n",
       "       -0.0338662 , -0.03101235, -0.00853217,  0.00802015,  0.0430473 ,\n",
       "        0.01540356, -0.02168932, -0.03629383, -0.02401764, -0.04358194,\n",
       "       -0.0077287 , -0.03075786, -0.03594519,  0.03700669,  0.03741568,\n",
       "       -0.01012324,  0.01561521, -0.02290997,  0.0203719 , -0.02619318,\n",
       "       -0.0403304 ,  0.01235248, -0.03126605,  0.02974543,  0.0222582 ,\n",
       "       -0.04646191, -0.03931414, -0.04322553, -0.00944998,  0.01204463,\n",
       "        0.02464309,  0.0406023 ,  0.04995011, -0.03614325, -0.03751718,\n",
       "       -0.045487  ,  0.04200497, -0.00375426,  0.06449261,  0.06039086,\n",
       "        0.01869253,  0.06049397, -0.05074296,  0.02431795, -0.08975951,\n",
       "       -0.08658357, -0.09911341, -0.1476075 , -0.01268509, -0.07201616,\n",
       "        0.05495224,  0.14726223,  0.04566185, -0.11252636], dtype=float32)))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ec286d78-dd56-455d-90f8-4ffa88f3ac50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.5652807, 2.1232865], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.info.predicted_rewards_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c85a0087-7388-4f03-9278-aace31121233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0, dtype=int32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a8266c7c-3688-46a5-a1c8-a63bb7146128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.info.bandit_policy_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9712bc00-7ea0-4003-a3c7-81a0be5bc995",
   "metadata": {},
   "source": [
    "# [9] Prepare training application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f62966-123f-410e-9610-832f20566635",
   "metadata": {},
   "source": [
    "To scale this training in Vertex AI's managed training service, we need to package all our code into a python application\n",
    "\n",
    "* This means, we need to modularize all steps executed up until this point. For example, the training loop we ran above is further parameterized in `train_perarm.py`, which will be called from our `task.py` when we submit the job to Vertex AI\n",
    "* This training package will eventually be used to create a custom training image (more on that in `04b-build-training-image` notebook)\n",
    "* For additional details on preparing the training application for Vertex AI, check out [the documentation](https://cloud.google.com/vertex-ai/docs/training/code-requirements) and [this code example](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_training.ipynb) (which also has good, related commentary)\n",
    "\n",
    "**The remaining cells will repeat steps executed above**\n",
    "> we are just illustrating / validating that they are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c3d30-bef4-4ec5-a978-116856a70e7e",
   "metadata": {},
   "source": [
    "### Distribution strategy\n",
    "\n",
    "Use `strategy_utils` to generate a strategy. Under the hood, passing the parameter:\n",
    "\n",
    "* `use_gpu = False` returns `tf.distribute.get_strategy()`, which uses CPU\n",
    "* `use_gpu = True` returns `tf.distribute.MirroredStrategy()`, which uses all GPUs that are visible to TensorFlow on one machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68140c4d-12ff-4758-89ca-44fd710ca0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy at 0x7f95dcc13be0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.train.utils import strategy_utils\n",
    "\n",
    "use_gpu = True\n",
    "use_tpu = False\n",
    "\n",
    "distribution_strategy = strategy_utils.get_strategy(tpu=use_tpu, use_gpu=use_gpu)\n",
    "distribution_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e80724b1-6525-4986-b832-4af8b49d923c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_REPLICAS = distribution_strategy.num_replicas_in_sync\n",
    "NUM_REPLICAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129ee41-b43c-4d5a-b656-cdac6b79688c",
   "metadata": {},
   "source": [
    "### Vertex Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ef509680-4d19-4efa-9f01-21039e0d573b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : 02-big-context-bandits-v5-rec-bandits-v2\n"
     ]
    }
   ],
   "source": [
    "# using same experiment name \n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7ad98430-d836-40f1-8e4f-eb46f0c91fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : 02-big-context-bandits-v5-rec-bandits-v2\n",
      "RUN_NAME          : run-20240313-145006\n",
      "\n",
      "CHECKPT_DIR       : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/chkpoint\n",
      "BASE_OUTPUT_DIR   : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-145006\n",
      "LOG_DIR           : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-145006/logs\n",
      "ROOT_DIR          : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-145006/root\n",
      "ARTIFACTS_DIR     : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-145006/artifacts\n"
     ]
    }
   ],
   "source": [
    "# new experiment\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "CHECKPT_DIR       = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/chkpoint\"\n",
    "BASE_OUTPUT_DIR   = f'{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "LOG_DIR           = f\"{BASE_OUTPUT_DIR}/logs\"\n",
    "ROOT_DIR          = f\"{BASE_OUTPUT_DIR}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BASE_OUTPUT_DIR}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "# aiplatform.init(\n",
    "#     project=PROJECT_ID,\n",
    "#     location=REGION,\n",
    "#     experiment=EXPERIMENT_NAME\n",
    "# )\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\\n\")\n",
    "print(f\"CHECKPT_DIR       : {CHECKPT_DIR}\")\n",
    "print(f\"BASE_OUTPUT_DIR   : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83de7c4-f7c7-4290-b44a-9e9194bac882",
   "metadata": {},
   "source": [
    "### Create TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "17576ce0-727d-4297-a52d-f64fb75ca78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME : projects/934903580331/locations/us-central1/tensorboards/1742440057001738240\n",
      "TB display name  : 02-big-context-bandits-v5-rec-bandits-v2-run-20240313-145006\n",
      "TB_ID            : 1742440057001738240\n"
     ]
    }
   ],
   "source": [
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}-{RUN_NAME}\"\n",
    "\n",
    "tensorboard = aiplatform.Tensorboard.create(\n",
    "    display_name=TENSORBOARD_DISPLAY_NAME\n",
    "    , project=PROJECT_ID\n",
    "    , location=REGION\n",
    ")\n",
    "\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "TB_ID = TB_RESOURCE_NAME.split('/')[-1]\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME : {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name  : {tensorboard.display_name}\")\n",
    "print(f\"TB_ID            : {TB_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "71d43cf9-db3f-437e-98ee-3791ac0c5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    experiment_tensorboard=TB_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2e082-c0f6-4792-a279-e827c48b5895",
   "metadata": {},
   "source": [
    "### trajectory function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "55e21068-a7a5-44c8-a16f-d8c41d976c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src import reward_factory as reward_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b9ce410e-ac03-48b2-8006-4591b38297a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with distribution_strategy.scope():\n",
    "    \n",
    "    embs = emb_features.EmbeddingModel(\n",
    "        vocab_dict = vocab_dict,\n",
    "        num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "        global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "        mv_emb_size = MV_EMBEDDING_SIZE,\n",
    "    )\n",
    "    \n",
    "    def _trajectory_fn(element): # hparams\n",
    "    \n",
    "        \"\"\"Converts a dataset element into a trajectory.\"\"\"\n",
    "        # global_features = _get_global_context_features(element)\n",
    "        # arm_features = _get_per_arm_features(element)\n",
    "\n",
    "        global_features = embs._get_global_context_features(element)\n",
    "        arm_features = embs._get_per_arm_features(element)\n",
    "\n",
    "        # Adds a time dimension.\n",
    "        arm_features = train_utils._add_outer_dimension(arm_features)\n",
    "\n",
    "        # obs spec\n",
    "        observation = {\n",
    "            bandit_spec_utils.GLOBAL_FEATURE_KEY:\n",
    "                train_utils._add_outer_dimension(global_features),\n",
    "        }\n",
    "\n",
    "        # reward = train_utils._add_outer_dimension(reward_factory._get_binary_rewards(element))\n",
    "        reward = train_utils._add_outer_dimension(reward_factory._get_rewards(element))\n",
    "\n",
    "        # To emit the predicted rewards in policy_info, we need to create dummy\n",
    "        # rewards to match the definition in TensorSpec for the ones specified in\n",
    "        # emit_policy_info set.\n",
    "        dummy_rewards = tf.zeros([HPARAMS['batch_size'], 1, HPARAMS['num_actions']])\n",
    "        policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "            chosen_arm_features=arm_features,\n",
    "            # Pass dummy mean rewards here to match the model_spec for emitting\n",
    "            # mean rewards in policy info\n",
    "            predicted_rewards_mean=dummy_rewards,\n",
    "            bandit_policy_type=tf.zeros([HPARAMS['batch_size'], 1, 1], dtype=tf.int32)\n",
    "            # policy_utilities.create_bandit_policy_type_tensor_spec(shape=[1]) \n",
    "            # policy_utilities.BanditPolicyType.GREEDY\n",
    "            # tf.zeros([batch_size, 1, 1], dtype=tf.int32)\n",
    "        )\n",
    "\n",
    "        if HPARAMS['model_type'] == 'neural_ucb':\n",
    "            policy_info = policy_info._replace(\n",
    "                predicted_rewards_optimistic=dummy_rewards\n",
    "            )\n",
    "\n",
    "        return trajectory.single_step(\n",
    "            observation=observation,\n",
    "            action=tf.zeros_like(\n",
    "                reward, \n",
    "                dtype=tf.int32\n",
    "            ),  # Arm features are copied from policy info, put dummy zeros here\n",
    "            policy_info=policy_info,\n",
    "            reward=reward,\n",
    "            discount=tf.zeros_like(reward)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b759404-b282-4f55-add8-7d795867c99e",
   "metadata": {},
   "source": [
    "### get agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3be40320-a73f-45f7-9fc4-0bd64df0ae5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 128,\n",
       " 'eval_batch_size': 1,\n",
       " 'num_actions': 2,\n",
       " 'model_type': 'epsGreedy',\n",
       " 'network_type': 'commontower',\n",
       " 'global_layers': [72, 36, 18],\n",
       " 'per_arm_layers': [64, 32, 16],\n",
       " 'common_layers': [34, 8],\n",
       " 'learning_rate': 0.05,\n",
       " 'epsilon': 0.01,\n",
       " 'encoding_dim': 1}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HPARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fbdecf78-94c5-4f8c-a6c0-86f2eace4663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: NeuralEpsGreedyAgent\n",
      "Network: commontower\n"
     ]
    }
   ],
   "source": [
    "with distribution_strategy.scope():\n",
    "\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "    agent = agent_factory.PerArmAgentFactory._get_agent(\n",
    "        agent_type = HPARAMS['model_type'],\n",
    "        network_type = HPARAMS['network_type'],\n",
    "        time_step_spec = time_step_spec,\n",
    "        action_spec = action_spec,\n",
    "        observation_spec=observation_spec,\n",
    "        global_layers = HPARAMS['global_layers'],\n",
    "        arm_layers = HPARAMS['per_arm_layers'],\n",
    "        common_layers = HPARAMS['common_layers'],\n",
    "        agent_alpha = AGENT_ALPHA,\n",
    "        learning_rate = HPARAMS['learning_rate'],\n",
    "        epsilon = HPARAMS['epsilon'],\n",
    "        train_step_counter = global_step,\n",
    "        output_dim = HPARAMS['encoding_dim'],\n",
    "        eps_phase_steps = EPS_PHASE_STEPS,\n",
    "        summarize_grads_and_vars = True,\n",
    "        debug_summaries = True\n",
    "    )\n",
    "    \n",
    "    agent.initialize()\n",
    "\n",
    "print(f\"Agent: {agent.name}\")\n",
    "\n",
    "# if network:\n",
    "#     print(f\"Network: {network}\")\n",
    "\n",
    "if NETWORK_TYPE:\n",
    "    print(f\"Network: {NETWORK_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "108ed33c-d8ef-4524-97e9-ced9e01c154b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHKPT_INTERVAL    : 200\n",
      "eval_batch_size   : 1\n",
      "EVAL_DATA_SIZE    : 90000\n",
      "NUM_EVAL_STEPS    : 100\n",
      "TOTAL_TRAIN_TAKE  : 10000\n"
     ]
    }
   ],
   "source": [
    "IS_TESTING = True\n",
    "\n",
    "# train args\n",
    "NUM_EPOCHS            = 3\n",
    "TRAINING_LOOPS        = 500\n",
    "STEPS_PER_LOOP        = 1\n",
    "\n",
    "drop_arm_feature_fn   = None\n",
    "ASYNC_STEPS_PER_LOOP  = 1\n",
    "\n",
    "LOG_INTERVAL          = 100\n",
    "CHKPT_INTERVAL        = 200\n",
    "\n",
    "# eval args\n",
    "NUM_EVAL_STEPS = 100\n",
    "\n",
    "TOTAL_TRAIN_TAKE = 10000 #TRAINING_LOOPS * HPARAMS['batch_size']\n",
    "\n",
    "print(f\"CHKPT_INTERVAL    : {CHKPT_INTERVAL}\")\n",
    "print(f\"eval_batch_size   : {EVAL_BATCH_SIZE}\")\n",
    "print(f\"EVAL_DATA_SIZE    : {EVAL_DATA_SIZE}\")\n",
    "print(f\"NUM_EVAL_STEPS    : {NUM_EVAL_STEPS}\")\n",
    "print(f\"TOTAL_TRAIN_TAKE  : {TOTAL_TRAIN_TAKE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f66ff927-9a39-4abf-b247-845f0dc7721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval dataset\n",
    "eval_ds = val_dataset.batch(HPARAMS['eval_batch_size'])\n",
    "\n",
    "if NUM_EVAL_STEPS >= 0:\n",
    "    with distribution_strategy.scope():\n",
    "        eval_ds = eval_ds.take(NUM_EVAL_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "528441f5-64ec-4f09-bd50-b2ae85b553bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# TB summary writer\n",
    "# ====================================================\n",
    "with distribution_strategy.scope():\n",
    "    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        f\"{LOG_DIR}\", flush_millis=10 * 1000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f6c98398-d545-4dcc-a71f-32249b5f0336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution_strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f95dcc13be0>\n",
      "number of train_files: 2\n",
      "Inpsecting agent policy from train_peram file...\n",
      "agent.policy: <tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x7f95dcba9900>\n",
      "Inpsecting agent policy from train_peram file: Complete\n",
      "setting checkpoint_manager: gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/chkpoint\n",
      "agent.train_step_counter: 50\n",
      "starting train loop...\n",
      "epoch: 1\n",
      "step = 100: loss = 1.340000033378601\n",
      "step = 200: loss = 1.5\n",
      "step = 300: loss = 1.149999976158142\n",
      "step = 400: loss = 1.1299999952316284\n",
      "step = 500: loss = 1.350000023841858\n",
      "epoch: 2\n",
      "step = 600: loss = 1.2000000476837158\n",
      "step = 700: loss = 1.4199999570846558\n",
      "step = 800: loss = 1.100000023841858\n",
      "step = 900: loss = 1.3300000429153442\n",
      "step = 1000: loss = 1.2400000095367432\n",
      "epoch: 3\n",
      "step = 1100: loss = 1.2100000381469727\n",
      "step = 1200: loss = 1.2699999809265137\n",
      "step = 1300: loss = 1.2799999713897705\n",
      "step = 1400: loss = 1.1699999570846558\n",
      "step = 1500: loss = 1.4600000381469727\n",
      "runtime_mins: 1\n",
      "saved trained policy to: gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-145006/artifacts\n",
      "saved trained policy to: gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/chkpoint\n",
      "complete train job in 2 minutes\n"
     ]
    }
   ],
   "source": [
    "#start the timer and training\n",
    "start_time = time.time()\n",
    "\n",
    "metric_results, agent = train_perarm.train_perarm(\n",
    "    agent = agent,\n",
    "    reward_spec = reward_tensor_spec,\n",
    "    epsilon = HPARAMS['epsilon'],\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    num_iterations = TRAINING_LOOPS,\n",
    "    steps_per_loop = STEPS_PER_LOOP,\n",
    "    num_eval_steps = NUM_EVAL_STEPS,\n",
    "    # data\n",
    "    batch_size = HPARAMS['batch_size'],\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    # functions\n",
    "    _trajectory_fn = _trajectory_fn,\n",
    "    # _run_bandit_eval_fn = _run_bandit_eval,\n",
    "    # train intervals\n",
    "    chkpt_interval = CHKPT_INTERVAL,\n",
    "    log_interval = LOG_INTERVAL,\n",
    "    # dirs\n",
    "    bucket_name = BUCKET_NAME,\n",
    "    data_dir_prefix_path = f\"{EXAMPLE_GEN_GCS_PATH}\",\n",
    "    log_dir = LOG_DIR,\n",
    "    model_dir = ARTIFACTS_DIR,\n",
    "    chkpoint_dir = CHECKPT_DIR,\n",
    "    async_steps_per_loop = ASYNC_STEPS_PER_LOOP,\n",
    "    resume_training_loops = False,\n",
    "    use_gpu = True,\n",
    "    use_tpu = False,\n",
    "    profiler = False,\n",
    "    global_step = global_step,\n",
    "    total_train_take = TOTAL_TRAIN_TAKE,\n",
    "    train_summary_writer = train_summary_writer,\n",
    "    strategy = distribution_strategy,\n",
    "    is_testing = IS_TESTING,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "runtime_mins = int((end_time - start_time) / 60)\n",
    "print(f\"complete train job in {runtime_mins} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "177aa3ab-05b4-48f9-b434-9335bd9eb54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.302479"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(metric_results)\n",
    "metric_results[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ba65417f-54e0-436e-b0d4-93cffb981ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(metric_results)\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61464938-a3e7-4ab0-9149-4a9124199dc1",
   "metadata": {},
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9108f8b6-7aea-48d6-a763-461b30671c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "083c2351-5ac8-4218-9bef-e249777aee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=$LOG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ba747-00d5-4908-8d0b-334ae9d2c791",
   "metadata": {},
   "source": [
    "#### eval trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c5cf9366-d9cd-4c9d-951a-99a50877a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82506e-95d7-4a4e-a821-e08512615db7",
   "metadata": {},
   "source": [
    "After training, our `agent.policy` is used as the deployment \"model\" that will generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6c82396a-7822-41fb-830f-1f6eb9c9b213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy at 0x7f95dcba9900>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment_agent = agent.policy\n",
    "deployment_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f6706224-ef5b-4185-bde4-bccb7c8e7ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating trained Agent...\n",
      "post-train val_loss     : 1.292786717414856\n",
      "post-train eval runtime : 0\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Evaluate the agent's policy once after training\n",
    "# ====================================================\n",
    "print(f\"evaluating trained Agent...\")\n",
    "\n",
    "post_policy_tf = py_tf_eager_policy.PyTFEagerPolicy(deployment_agent, use_tf_function=True)\n",
    "start_time = time.time()\n",
    "\n",
    "val_loss, preds, tr_rewards = eval_perarm._run_bandit_eval(\n",
    "    policy = post_policy_tf,\n",
    "    data = eval_ds,\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"post-train val_loss     : {val_loss}\")\n",
    "print(f\"post-train eval runtime : {runtime_mins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f899120f-3706-4b72-8f2c-2169221dd66b",
   "metadata": {},
   "source": [
    "## Using Trained Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b0f691-860e-4853-928e-4c7ece714dd0",
   "metadata": {},
   "source": [
    "### load trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ea029f73-0638-462a-8f00-54b05c54d69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-145006/artifacts\n"
     ]
    }
   ],
   "source": [
    "POLICY_URI = ARTIFACTS_DIR\n",
    "\n",
    "print(POLICY_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "66d1d819-0152-437c-aefc-45ffd34d808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-145006/artifacts/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-145006/artifacts/fingerprint.pb\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-145006/artifacts/policy_specs.pbtxt\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-145006/artifacts/saved_model.pb\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-145006/artifacts/assets/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v5-rec-bandits-v2/run-20240313-145006/artifacts/variables/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $POLICY_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6aad948e-e346-4da3-8cb2-747435c8dc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.py_tf_eager_policy.SavedModelPyTFEagerPolicy at 0x7f95b87ce800>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy\n",
    "\n",
    "trained_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n",
    "    POLICY_URI, load_specs_from_pbtxt=True\n",
    ")\n",
    "\n",
    "trained_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "483f8fa5-cbd1-4e3c-8e62-47f91f42e137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action_spec', minimum=0, maximum=1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_policy.action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8f658891-bc09-49b8-bc58-000a7926df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFER_SIZE = 1\n",
    "dummy_arm = tf.zeros([INFER_SIZE, PER_ARM_DIM], dtype=tf.float32)\n",
    "\n",
    "SKIP_NUM = 10\n",
    "\n",
    "for x in eval_ds.skip(SKIP_NUM).take(INFER_SIZE):\n",
    "    # get feature tensors\n",
    "    \n",
    "    # global_feat_infer = _get_global_context_features(x)\n",
    "    # arm_feat_infer = _get_per_arm_features(x)\n",
    "    \n",
    "    global_feat_infer = embs._get_global_context_features(x)\n",
    "    arm_feat_infer = embs._get_per_arm_features(x)\n",
    "    \n",
    "    # rewards = _get_rewards(x)\n",
    "    rewards = reward_factory._get_rewards(x)\n",
    "    \n",
    "    # reshape arm features\n",
    "    arm_feat_infer = tf.reshape(arm_feat_infer, [HPARAMS['eval_batch_size'], PER_ARM_DIM]) # perarm_dim\n",
    "    concat_arm = tf.concat([arm_feat_infer, dummy_arm], axis=0)\n",
    "    \n",
    "    # flatten global\n",
    "    flat_global_infer = tf.reshape(global_feat_infer, [GLOBAL_DIM])\n",
    "    feature = {'global': flat_global_infer, 'per_arm': concat_arm}\n",
    "    \n",
    "    # get actual reward\n",
    "    actual_reward = rewards.numpy()[0]\n",
    "    \n",
    "    # build trajectory step\n",
    "    trajectory_step = train_utils._get_eval_step(feature, actual_reward)\n",
    "    \n",
    "    prediction = trained_policy.action(trajectory_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "49ce41ed-41b7-404d-9796-1658e7955894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
       " 'observation': {'global': <tf.Tensor: shape=(72,), dtype=float32, numpy=\n",
       "array([ 0.03634105, -0.02280818,  0.02708255, -0.00824574, -0.04827409,\n",
       "        0.00856422, -0.03433117, -0.02448149,  0.03787032,  0.00102285,\n",
       "        0.00581681, -0.02979258,  0.02028104,  0.00265091,  0.01721741,\n",
       "        0.00509389,  0.00907559, -0.01964155, -0.02387047, -0.03362812,\n",
       "        0.02229828, -0.02407413, -0.00672073, -0.00142486,  0.00319892,\n",
       "       -0.0004467 ,  0.00390351, -0.0369692 ,  0.03309306, -0.0028758 ,\n",
       "        0.04218576, -0.02528499,  0.01202533,  0.02394516,  0.03323077,\n",
       "        0.04732381, -0.02995162,  0.02597247, -0.0036668 ,  0.0204771 ,\n",
       "        0.01990895,  0.04975117, -0.00975417,  0.03996885, -0.04559312,\n",
       "        0.04126066, -0.03744272,  0.01142378, -0.01320666,  0.04710834,\n",
       "       -0.04355972,  0.04625926, -0.00807767, -0.02726229,  0.00522984,\n",
       "       -0.02223763,  0.02111321, -0.04266765,  0.00216252,  0.03832426,\n",
       "       -0.01327248, -0.00987834,  0.00406607, -0.02919067,  0.01524535,\n",
       "        0.00087684,  0.01795359,  0.04174394, -0.00368375,  0.04014163,\n",
       "       -0.00711173,  0.00685298], dtype=float32)>,\n",
       "                 'per_arm': <tf.Tensor: shape=(2, 64), dtype=float32, numpy=\n",
       "array([[-0.03879542,  0.03226275, -0.02357858, -0.01037304, -0.00297999,\n",
       "         0.04811803, -0.02336609, -0.00336276,  0.0202581 , -0.0084527 ,\n",
       "         0.01727679,  0.04385204, -0.04328888,  0.03786583,  0.00890929,\n",
       "         0.01534132, -0.01922024,  0.02032164,  0.02985385,  0.01918969,\n",
       "         0.00680706,  0.02482397,  0.00941877, -0.03043364, -0.00753353,\n",
       "        -0.01047317, -0.02461664, -0.00370162, -0.03391642, -0.03019607,\n",
       "         0.00663633, -0.02279399, -0.03977058,  0.00864974, -0.02387491,\n",
       "        -0.01763676, -0.00099308, -0.01723137,  0.04398564, -0.02812533,\n",
       "         0.02614185, -0.04913936,  0.03103432, -0.02357408, -0.04463016,\n",
       "         0.0214722 ,  0.02040354, -0.00701165,  0.09718235, -0.02543563,\n",
       "         0.08365364, -0.13950011,  0.11098821, -0.07731131, -0.08125913,\n",
       "         0.13837692, -0.00471404, -0.09402649, -0.08641978,  0.11699529,\n",
       "        -0.14604339,  0.04837694, -0.13218406,  0.13109486],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "      dtype=float32)>},\n",
       " 'reward': <tf.Tensor: shape=(), dtype=float32, numpy=3.0>,\n",
       " 'step_type': <tf.Tensor: shape=(), dtype=int32, numpy=0>})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f515fcd0-2228-4f68-9ef9-665ce1fc3fb2",
   "metadata": {},
   "source": [
    "### view prediction response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6af4c297-d2e8-4f7b-bed5-e1da997dfe34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=array(0, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.4621954, 3.4621954], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([-0.03879542,  0.03226275, -0.02357858, -0.01037304, -0.00297999,\n",
       "        0.04811803, -0.02336609, -0.00336276,  0.0202581 , -0.0084527 ,\n",
       "        0.01727679,  0.04385204, -0.04328888,  0.03786583,  0.00890929,\n",
       "        0.01534132, -0.01922024,  0.02032164,  0.02985385,  0.01918969,\n",
       "        0.00680706,  0.02482397,  0.00941877, -0.03043364, -0.00753353,\n",
       "       -0.01047317, -0.02461664, -0.00370162, -0.03391642, -0.03019607,\n",
       "        0.00663633, -0.02279399, -0.03977058,  0.00864974, -0.02387491,\n",
       "       -0.01763676, -0.00099308, -0.01723137,  0.04398564, -0.02812533,\n",
       "        0.02614185, -0.04913936,  0.03103432, -0.02357408, -0.04463016,\n",
       "        0.0214722 ,  0.02040354, -0.00701165,  0.09718235, -0.02543563,\n",
       "        0.08365364, -0.13950011,  0.11098821, -0.07731131, -0.08125913,\n",
       "        0.13837692, -0.00471404, -0.09402649, -0.08641978,  0.11699529,\n",
       "       -0.14604339,  0.04837694, -0.13218406,  0.13109486], dtype=float32)))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd58d4b-6749-4177-8342-83ee969408c9",
   "metadata": {},
   "source": [
    "#### chosen action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d49adbbb-f747-4e4c-b82d-df88373c644f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0, dtype=int32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd12013-1d12-4197-8254-cef9138df42c",
   "metadata": {},
   "source": [
    "#### Per Arm Policy Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d6126290-546b-4702-8a1e-91f18adb05c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.4621954, 3.4621954], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([-0.03879542,  0.03226275, -0.02357858, -0.01037304, -0.00297999,\n",
       "        0.04811803, -0.02336609, -0.00336276,  0.0202581 , -0.0084527 ,\n",
       "        0.01727679,  0.04385204, -0.04328888,  0.03786583,  0.00890929,\n",
       "        0.01534132, -0.01922024,  0.02032164,  0.02985385,  0.01918969,\n",
       "        0.00680706,  0.02482397,  0.00941877, -0.03043364, -0.00753353,\n",
       "       -0.01047317, -0.02461664, -0.00370162, -0.03391642, -0.03019607,\n",
       "        0.00663633, -0.02279399, -0.03977058,  0.00864974, -0.02387491,\n",
       "       -0.01763676, -0.00099308, -0.01723137,  0.04398564, -0.02812533,\n",
       "        0.02614185, -0.04913936,  0.03103432, -0.02357408, -0.04463016,\n",
       "        0.0214722 ,  0.02040354, -0.00701165,  0.09718235, -0.02543563,\n",
       "        0.08365364, -0.13950011,  0.11098821, -0.07731131, -0.08125913,\n",
       "        0.13837692, -0.00471404, -0.09402649, -0.08641978,  0.11699529,\n",
       "       -0.14604339,  0.04837694, -0.13218406,  0.13109486], dtype=float32))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f66961-ebb5-4ae5-8e6f-ad73114d8353",
   "metadata": {},
   "source": [
    "**Finished**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
