{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385307d6-2058-47ec-8da0-57f7ef5c43d6",
   "metadata": {},
   "source": [
    "# Train Bandits with per-arm features\n",
    "\n",
    "**Exploring linear and nonlinear** (e.g., those with neural network-based value functions) bandit methods for recommendations using TF-Agents\n",
    "\n",
    "> Neural linear bandits provide a nice way to leverage the representation power of deep learning and the bandit approach for uncertainty measure and efficient exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e9956-66cd-4bf4-9b4d-8c2c646f0313",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "In this notebook, we explore the following topics for training contextual bandits with per-arm features:\n",
    "\n",
    "1. Data preperation\n",
    "2. Sampling functions\n",
    "3. TensorSpecs\n",
    "4. Agent, Network, training policy\n",
    "5. Reward function\n",
    "6. Trajectory function\n",
    "7. Train & Eval loops\n",
    "8. Getting predictions -\n",
    "9. Preparing the training application - abstracting all steps above to be used in subsequent notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fd257-b98b-426a-a2cd-024429b014f1",
   "metadata": {},
   "source": [
    "## Load notebook config\n",
    "\n",
    "* use the prefix defined in `00-env-setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39c9d08-d118-4013-a47f-88450f49f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX: rec-bandits-v2\n"
     ]
    }
   ],
   "source": [
    "VERSION        = \"v2\"                       # TODO\n",
    "PREFIX         = f'rec-bandits-{VERSION}'   # TODO\n",
    "\n",
    "print(f\"PREFIX: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908f6b95-b539-4a9f-a836-840d26ea3b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"rec-bandits-v2\"\n",
      "VERSION                  = \"v2\"\n",
      "\n",
      "BUCKET_NAME              = \"rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "BUCKET_URI               = \"gs://rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_NAME    = \"mvlens_rec_bandits_v2\"\n",
      "BIGQUERY_TABLE_NAME      = \"training_dataset\"\n",
      "\n",
      "REPOSITORY               = \"rl-movielens-rec-bandits-v2\"\n",
      "\n",
      "DOCKERNAME_01            = \"Dockerfile_train_my_perarm_env\"\n",
      "IMAGE_NAME_01            = \"train-my-perarm-env-v2\"\n",
      "IMAGE_URI_01             = \"gcr.io/hybrid-vertex/train-my-perarm-env-v2\"\n",
      "\n",
      "DOCKERNAME_02            = \"Dockerfile_perarm_feats\"\n",
      "IMAGE_NAME_02            = \"train-perarm-feats-v2\"\n",
      "IMAGE_URI_02             = \"gcr.io/hybrid-vertex/train-perarm-feats-v2\"\n",
      "\n",
      "DOCKERNAME_03            = \"Dockerfile_ranking_bandit\"\n",
      "IMAGE_NAME_03            = \"train-rank-bandit-v2\"\n",
      "IMAGE_URI_03             = \"gcr.io/hybrid-vertex/train-rank-bandit-v2\"\n",
      "\n",
      "DOCKERNAME_04            = \"Dockerfile_train_bandit_e2e\"\n",
      "IMAGE_NAME_04            = \"train-mab-e2e-v2\"\n",
      "IMAGE_URI_04             = \"gcr.io/hybrid-vertex/train-mab-e2e-v2\"\n",
      "\n",
      "DOCKERNAME_04_pred       = \"Dockerfile_pred_bandit_e2e\"\n",
      "IMAGE_NAME_04_pred       = \"pred-mab-e2e-v2\"\n",
      "IMAGE_URI_04_pred        = \"gcr.io/hybrid-vertex/pred-mab-e2e-v2\"\n",
      "\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-rec-bandits-v2/local_docker_tfa\"\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c690a9-e2bd-4759-ba41-4e2469098aee",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d0dfe4-695c-4dd4-9f24-67f7488ce1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c669f1a1-1af7-4efb-ab2d-6bf3b3847991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import functools\n",
    "import collections\n",
    "import pickle as pkl\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf-agents\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.metrics import export_utils\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.bandits.policies import policy_utilities\n",
    "from tf_agents.bandits.agents import neural_linucb_agent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "from tf_agents.bandits.agents import neural_epsilon_greedy_agent\n",
    "from tf_agents.bandits.networks import global_and_arm_feature_network\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "\n",
    "# GPU\n",
    "from numba import cuda \n",
    "import gc\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# this repo\n",
    "from src.utils import train_utils, reward_factory\n",
    "from src.data import data_utils, data_config\n",
    "from src.trainer import eval_perarm as eval_perarm, train_perarm\n",
    "from src.agents import agent_factory as agent_factory\n",
    "from src.networks import encoding_network as emb_features\n",
    "\n",
    "# tf exceptions and vars\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e58dd7-ab2b-419f-9771-bf1e98db758b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4969d3e-1fc0-45db-8a69-aa6b342019de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "274e7f4a-1802-4946-888e-876638f5c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a866b1-85b9-43e6-9546-edfbbf886bce",
   "metadata": {},
   "source": [
    "# [1] Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d4ed28-23d7-4785-b327-e5b543b0edb9",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "* Load train and eval datasets from TFRecords created in the `01-movielens-data-prep.ipynb` notebook\n",
    "* training examples represent historical (previously collected) interaction data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc3fcebe-818b-4767-afdc-cfb65b3b953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCS_DATA_PATH: gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/mv_b128_g12_a16/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/mv_b128_g12_a16_v4/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/mv_b128_g12_a16_v5/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/mv_b128_g12_a16_v6/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/train/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/val/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/vocabs/\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_GEN_GCS_PATH = data_config.EXAMPLE_GEN_GCS_PATH\n",
    "GCS_DATA_PATH = f\"{BUCKET_URI}/{EXAMPLE_GEN_GCS_PATH}\"\n",
    "\n",
    "print(f\"GCS_DATA_PATH: {GCS_DATA_PATH}\")\n",
    "\n",
    "!gsutil ls $GCS_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5b953-14c0-42ed-a511-77147a1bc0ac",
   "metadata": {},
   "source": [
    "### Read TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d0157c8-a04c-4dbd-b6d9-a1ede97687a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da0cdac-76af-4608-b70e-be7dbe0584b5",
   "metadata": {},
   "source": [
    "**Train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c934c06-bf08-4c7f-b0cc-0de04ef3515c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/train/ml-1m-gen-001-of-008.tfrecord',\n",
       " 'gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/train/ml-1m-gen-002-of-008.tfrecord',\n",
       " 'gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/train/ml-1m-gen-003-of-008.tfrecord']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT = \"train\" # \"train\" | \"val\"\n",
    "\n",
    "train_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{EXAMPLE_GEN_GCS_PATH}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "train_files = train_files[:3]\n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7511e4d-bf81-4800-bde7-8b16dec9aeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_movie_genres': <tf.Tensor: shape=(1, 10), dtype=string, numpy=\n",
      "array([[b'Drama', b'UNK', b'UNK', b'UNK', b'UNK', b'UNK', b'UNK', b'UNK',\n",
      "        b'UNK', b'UNK']], dtype=object)>,\n",
      " 'target_movie_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'1775'], dtype=object)>,\n",
      " 'target_movie_rating': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([4.], dtype=float32)>,\n",
      " 'target_movie_title': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Live Flesh (1997)'], dtype=object)>,\n",
      " 'target_movie_year': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1997])>,\n",
      " 'target_rating_timestamp': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([974612615])>,\n",
      " 'user_age': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([50])>,\n",
      " 'user_gender': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'M'], dtype=object)>,\n",
      " 'user_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'2173'], dtype=object)>,\n",
      " 'user_occupation_text': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'programmer'], dtype=object)>,\n",
      " 'user_zip_code': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'87505'], dtype=object)>}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "train_dataset = train_dataset.map(data_utils._parse_function)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452750c1-46a0-4ee1-84d4-0c3b227de38f",
   "metadata": {},
   "source": [
    "**Val data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7b5376f-9438-4304-84aa-de09c30f5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = \"val\"\n",
    "\n",
    "val_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{EXAMPLE_GEN_GCS_PATH}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        val_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "val_dataset = tf.data.TFRecordDataset(val_files)\n",
    "val_dataset = val_dataset.map(data_utils._parse_function, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8959967-522e-41c8-9a1b-050ca8bc191f",
   "metadata": {},
   "source": [
    "### get vocab\n",
    "\n",
    "**TODO:** \n",
    "* streamline vocab calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3ccf137-7a72-42e7-aa89-3c81a99cf40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vocab...\n",
      "Downloaded vocab from: gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/vocabs/vocab_dict.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXISTING_VOCAB_FILE = f'gs://{BUCKET_NAME}/{EXAMPLE_GEN_GCS_PATH}/vocabs/{VOCAB_FILENAME}'\n",
    "print(f\"Downloading vocab...\")\n",
    "\n",
    "os.system(f'gsutil -q cp {EXISTING_VOCAB_FILE} .')\n",
    "print(f\"Downloaded vocab from: {EXISTING_VOCAB_FILE}\\n\")\n",
    "\n",
    "filehandler = open(VOCAB_FILENAME, 'rb')\n",
    "vocab_dict = pkl.load(filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "# for key in vocab_dict.keys():\n",
    "#     pprint(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfda012c-a2c3-4384-a5a4-54f5c6649006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocab_dict['user_occupation_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0138a295-2b0d-4359-8496-af8552ec8cff",
   "metadata": {},
   "source": [
    "# [2] Preprocessing layers for global and arm features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28c002-8d3a-4cf4-b69b-de56a6abab90",
   "metadata": {},
   "source": [
    "The preproccesing layers will ultimately feed the two sampling functions described below. These sampling functions will be used to create [trajectories](https://github.com/tensorflow/agents/blob/master/tf_agents/trajectories/trajectory.py#L36) (i.e., the training examples for our model)\n",
    "\n",
    "`global_context_sampling_fn`: \n",
    "* A function that outputs a random 1d array or list of ints or floats\n",
    "* This output is the global context. Its shape and type must be consistent across calls.\n",
    "\n",
    "`arm_context_sampling_fn`: \n",
    "* A function that outputs a random 1 array or list of ints or floats (same type as the output of `global_context_sampling_fn`). * This output is the per-arm context. Its shape must be consistent across calls.\n",
    "\n",
    "See `debug-and-learn-emb-models.ipynb` for more details and debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ea1f7-0621-44b8-b062-1f59b0964d72",
   "metadata": {},
   "source": [
    "## Global & Per-Arm feature embedding models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a1063-15d4-472e-b5a7-d92dcdea3c0f",
   "metadata": {},
   "source": [
    "**get expected dimensions**\n",
    "\n",
    "**common layers**\n",
    "* layer sizes for the final tower\n",
    "* The network that takes as input the concatenation of \n",
    "  the outputs of the global and the arm networks\n",
    "*  hidden layers ideally divisible by 8, e.g., [16, 32, 64, 128, 256, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8d77956-635c-438a-916a-185eec52f9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPECTED_GLOBAL_DIM: 72\n",
      "EXPECTED_PER_ARM_DIM: 64\n",
      "EXPECTED_GLOBAL_LAYERS      : [72, 36, 18]\n",
      "EXPECTED_ARM_LAYERS         : [64, 32, 16]\n",
      "EXPECTED_COMMON_LAYERS      : [34, 17, 8]\n"
     ]
    }
   ],
   "source": [
    "NUM_OOV_BUCKETS        = 1\n",
    "GLOBAL_EMBEDDING_SIZE  = 12\n",
    "MV_EMBEDDING_SIZE      = 16\n",
    "\n",
    "NUM_GLOBAL_FEATURES = len(data_utils.USER_FEATURE_NAMES)     # 6\n",
    "NUM_ARM_FEATURES    = len(data_utils.MOVIE_FEATURE_NAMES)    # 5\n",
    "EXPECTED_GLOBAL_DIM  = GLOBAL_EMBEDDING_SIZE * NUM_GLOBAL_FEATURES\n",
    "EXPECTED_PER_ARM_DIM = MV_EMBEDDING_SIZE * NUM_ARM_FEATURES\n",
    "print(f\"EXPECTED_GLOBAL_DIM: {EXPECTED_GLOBAL_DIM}\")\n",
    "print(f\"EXPECTED_PER_ARM_DIM: {EXPECTED_PER_ARM_DIM}\")\n",
    "\n",
    "EXPECTED_GLOBAL_LAYERS   = [\n",
    "    EXPECTED_GLOBAL_DIM, \n",
    "    int(EXPECTED_GLOBAL_DIM/2), \n",
    "    int(EXPECTED_GLOBAL_DIM/4)\n",
    "]\n",
    "EXPECTED_ARM_LAYERS      = [\n",
    "    EXPECTED_PER_ARM_DIM, \n",
    "    int(EXPECTED_PER_ARM_DIM/2), \n",
    "    int(EXPECTED_PER_ARM_DIM/4)\n",
    "]\n",
    "EXPECTED_FIRST_COMMON_LAYER = EXPECTED_GLOBAL_LAYERS[-1] + EXPECTED_ARM_LAYERS[-1]\n",
    "EXPECTED_COMMON_LAYERS = [\n",
    "    int(EXPECTED_FIRST_COMMON_LAYER), \n",
    "    int(EXPECTED_FIRST_COMMON_LAYER/2), \n",
    "    int(EXPECTED_FIRST_COMMON_LAYER/4)\n",
    "]\n",
    "print(f\"EXPECTED_GLOBAL_LAYERS      : {EXPECTED_GLOBAL_LAYERS}\")\n",
    "print(f\"EXPECTED_ARM_LAYERS         : {EXPECTED_ARM_LAYERS}\")\n",
    "print(f\"EXPECTED_COMMON_LAYERS      : {EXPECTED_COMMON_LAYERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c33673e-6069-477a-af80-0d2c436099bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.perarm_features import emb_feature_v2 as emb_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea335991-dcec-40b3-ba0d-a0fdfb9c2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test emb models\n",
    "for i in range(1):\n",
    "    \n",
    "    iterator = iter(train_dataset.batch(1))\n",
    "    data = next(iterator)\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48178fff-491f-4f03-b156-7765d1fa9707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.networks.encoding_network.EmbeddingModel at 0x7fc3784ff640>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = emb_features.EmbeddingModel(\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    "    max_genre_length = data_config.MAX_GENRE_LENGTH\n",
    ")\n",
    "\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "903c63c5-2468-4443-bcb8-5ff6d69810dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL_DIM: 72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 72), dtype=float32, numpy=\n",
       "array([[ 0.00731398, -0.0136763 , -0.04041263, -0.02670314,  0.01631968,\n",
       "        -0.04038107,  0.00028306, -0.01482489, -0.00759079, -0.00962334,\n",
       "        -0.0314153 , -0.00756546,  0.04809726, -0.01716544, -0.0269835 ,\n",
       "         0.03002367, -0.02603165, -0.00863469,  0.03278402,  0.02853569,\n",
       "        -0.00016142, -0.01631402,  0.03956812, -0.02243693,  0.04483045,\n",
       "         0.00400126, -0.01622275, -0.00868951,  0.02648267, -0.04559091,\n",
       "         0.02729172, -0.0202024 , -0.01677945, -0.01731074, -0.02108399,\n",
       "         0.03882679,  0.02431991,  0.0070347 ,  0.02457161, -0.01536963,\n",
       "        -0.04204952,  0.03619448,  0.028123  , -0.03171634,  0.04010076,\n",
       "        -0.04999392,  0.03570152,  0.04195454,  0.032514  , -0.04602477,\n",
       "        -0.01869357, -0.04476898, -0.04559103, -0.0238757 , -0.04479721,\n",
       "         0.00878171, -0.01440529, -0.00934676,  0.04387155, -0.00718099,\n",
       "         0.01533658,  0.01052434, -0.00029897,  0.0288724 ,  0.03409344,\n",
       "         0.03252195,  0.00213705,  0.01344169, -0.01489104,  0.01605071,\n",
       "         0.00278874, -0.0052791 ]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_globals = embs._get_global_context_features(data)\n",
    "\n",
    "GLOBAL_DIM = test_globals.shape[1]            \n",
    "# shape checks out at batch_dim, nactions, arm feats\n",
    "print(f\"GLOBAL_DIM: {GLOBAL_DIM}\")\n",
    "\n",
    "test_globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebec4863-c614-4248-b680-8e29ec5b730d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER_ARM_DIM: 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 64), dtype=float32, numpy=\n",
       "array([[-0.00887288,  0.03619725,  0.01700417,  0.03442718,  0.04108962,\n",
       "        -0.00731521, -0.024377  ,  0.0272311 ,  0.03656573,  0.01044219,\n",
       "         0.03878045,  0.03579852, -0.04248155,  0.01155361, -0.00463337,\n",
       "        -0.04181756, -0.0078347 , -0.01975232,  0.00852995,  0.01284253,\n",
       "        -0.00909581, -0.02728211, -0.03728725, -0.02742556, -0.00621618,\n",
       "         0.03099342,  0.03457431,  0.03430482,  0.00351389,  0.0278071 ,\n",
       "         0.00036978, -0.00294856,  0.03097058,  0.01153038,  0.00892768,\n",
       "        -0.01024436, -0.00184669,  0.02408041, -0.04955539, -0.00164948,\n",
       "         0.04363689, -0.04397154,  0.03556773,  0.00729598, -0.01966389,\n",
       "         0.03107779, -0.03748571, -0.04980398,  0.15518023,  0.17571644,\n",
       "        -0.07523721, -0.09462744, -0.15551513,  0.13351454,  0.16215248,\n",
       "         0.12875529, -0.10442048,  0.08552039,  0.09006248,  0.15442313,\n",
       "         0.14638378, -0.09060036, -0.02820443,  0.14137255]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arms = embs._get_per_arm_features(data)\n",
    "\n",
    "PER_ARM_DIM = test_arms.shape[1]            \n",
    "# shape checks out at batch_dim, nactions, arm feats\n",
    "print(f\"PER_ARM_DIM: {PER_ARM_DIM}\")\n",
    "\n",
    "test_arms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb4902-5bb9-4587-8d29-c01d78b006be",
   "metadata": {},
   "source": [
    "# [3] TensorSpecs\n",
    "\n",
    "* say you have a global observation spec of `[17]`. And you have two batch dimensions `[4, 5]`. Then your observation has to have the shape `[4, 5, 17]`\n",
    "* and then if you have arm_obs_spec with shape `[9, 13]`, then the arm obs shape has to be exactly `[4, 5, 9, 13]`\n",
    "* and this has to be true for every single tensor in your tensor nest\n",
    "* the first 2 dims are the outer dims that are the same for all tensors, the rest of the dimensions have to follow the spec for each tensor\n",
    "\n",
    "**TODO:**\n",
    "* explain relationship between Tensor Specs and their Tensor counterparts\n",
    "* highlight the errors, lessons learned, and utility functions to address these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ff9baaf-987d-448d-a981-742a79b581e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE      : 128\n",
      "EVAL_BATCH_SIZE : 1\n",
      "NUM_ACTIONS     : 2\n",
      "GLOBAL_DIM      : 72\n",
      "PER_ARM_DIM     : 64\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE      = 128\n",
    "EVAL_BATCH_SIZE = 1\n",
    "\n",
    "NUM_ACTIONS     = 2 #this is kinda deceptive - \n",
    "#our approach is to learn by \"flashing\" one movie rating at a time per user context. \n",
    "#The n_actions = show/don't show the movie with one degree of freedom (n-1)\n",
    "\n",
    "\n",
    "print(f\"BATCH_SIZE      : {BATCH_SIZE}\")\n",
    "print(f\"EVAL_BATCH_SIZE : {EVAL_BATCH_SIZE}\")\n",
    "print(f\"NUM_ACTIONS     : {NUM_ACTIONS}\")\n",
    "print(f\"GLOBAL_DIM      : {GLOBAL_DIM}\")\n",
    "print(f\"PER_ARM_DIM     : {PER_ARM_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20eca8d-8c73-4ec8-9d0f-f2b428055ac2",
   "metadata": {},
   "source": [
    "## Implementing MAB with TF-Agents\n",
    "\n",
    "In TF-Agents, the *per-arm features* implementation differs from the *global-only* feature examples in the following aspects:\n",
    "* Reward is modeled not per-arm, but globally.\n",
    "* The arms are permutation invariant: it doesn’t matter which arm is arm 1 or arm 2, only their features.\n",
    "* One can have a different number of arms to choose from in every step (note that unspecified/dynamically changing number of arms will have a problem with XLA compatibility).\n",
    "\n",
    "When implementing per-arm features in TF-Bandits, the following details have to be discussed:\n",
    "* Observation spec and observations,\n",
    "* Action spec and actions,\n",
    "* Implementation of specific policies and agents.\n",
    "\n",
    "\n",
    "**TODO:**\n",
    "* outline the components and highlight their interactions, dependencies on eachother, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68ebe8-116d-43b3-a6e1-4f5a5c7f4741",
   "metadata": {},
   "source": [
    "### Observation spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c4746-d01b-4ca4-aa53-ab68da54d37a",
   "metadata": {},
   "source": [
    "**This observation spec allows the user to have a global observation of fixed dimension**, and an unspecified number of *per-arm* features (also of fixed dimension)\n",
    "* The actions output by the policy are still integers as usual, and they indicate which row of the arm-features it has chosen \n",
    "* The action spec must be a single integer value without boundaries:\n",
    "\n",
    "```python\n",
    "global_spec = tensor_spec.TensorSpec([GLOBAL_DIM], tf.float32)\n",
    "per_arm_spec = tensor_spec.TensorSpec([None, PER_ARM_DIM], tf.float32)\n",
    "observation_spec = {'global': global_spec, 'per_arm': per_arm_spec}\n",
    "\n",
    "action_spec = tensor_spec.TensorSpec((), tf.int32)\n",
    "```\n",
    "> Here the only difference compared to the action spec with global features only is that the tensor spec is not bounded, as we don’t know how many arms there will be at any time step\n",
    "\n",
    "**XLA compatibility:**\n",
    "* Since dynamic tensor shapes are not compatible with XLA, the number of arm features (and consequently, number of arms for a step) cannot be dynamic. \n",
    "* One workaround is to fix the maximum number of arms for a problem, then pad the arm features in steps with fewer arms, and use action masking to indicate how many arms are actually active.\n",
    "\n",
    "```python\n",
    "per_arm_spec = tensor_spec.TensorSpec([NUM_ACTIONS, PER_ARM_DIM], tf.float32)\n",
    "\n",
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=(), dtype=tf.int32, minimum = 0, maximum = NUM_ACTIONS - 1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36bd3b33-635a-4274-8b9e-7172696ebb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_spec = {\n",
    "    'global': tf.TensorSpec([GLOBAL_DIM], tf.float32),\n",
    "    'per_arm': tf.TensorSpec([NUM_ACTIONS, PER_ARM_DIM], tf.float32) #excluding action dim here\n",
    "}\n",
    "observation_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2da92-7db2-4f42-94a7-b7bad1c8fc42",
   "metadata": {},
   "source": [
    "### Action spec\n",
    "\n",
    "> The time_step_spec and action_spec are specifications for the input time step and the output action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af42c7-18d7-480f-a7f3-d7da3f4840eb",
   "metadata": {},
   "source": [
    "```python\n",
    "    if (\n",
    "        not tensor_spec.is_bounded(action_spec)\n",
    "        or not tensor_spec.is_discrete(action_spec)\n",
    "        or action_spec.shape.rank > 1\n",
    "        or action_spec.shape.num_elements() != 1\n",
    "    ):\n",
    "      raise NotImplementedError(\n",
    "          'action_spec must be a BoundedTensorSpec of type int32 and shape (). '\n",
    "          'Found {}.'.format(action_spec)\n",
    "      )\n",
    "```\n",
    "\n",
    "* [src](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/policies/reward_prediction_base_policy.py#L97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "549a123c-349a-4103-b39a-4502f47d1e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=[], \n",
    "    dtype=tf.int32,\n",
    "    minimum=tf.constant(0),            \n",
    "    maximum=NUM_ACTIONS-1, # n degrees of freedom and will dictate the expected mean reward spec shape\n",
    "    name=\"action_spec\"\n",
    ")\n",
    "\n",
    "action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36ee2635-d4e3-4468-886c-ae9c62e3c80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected_num_actions: 2\n",
      "predicted_rewards_mean: TensorSpec(shape=(2,), dtype=tf.float32, name=None)\n"
     ]
    }
   ],
   "source": [
    "expected_num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "print(f\"expected_num_actions: {expected_num_actions}\")\n",
    "\n",
    "predicted_rewards_mean = tensor_spec.TensorSpec([expected_num_actions])\n",
    "print(f\"predicted_rewards_mean: {predicted_rewards_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a51efce-9f1b-42d1-bec4-7b788e3fd7e0",
   "metadata": {},
   "source": [
    "### TimeStep spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95f05860-0fbf-4a5a-8273-9c81761e0ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': {'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None),\n",
       "                 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)},\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec = ts.time_step_spec(\n",
    "    observation_spec = observation_spec, \n",
    "    # reward_spec = _reward_spec\n",
    ")\n",
    "time_step_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b129a-6d19-4b3d-a2e7-e27070f57ac0",
   "metadata": {},
   "source": [
    "### Reward Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b48e89aa-e010-4bd9-a7e0-ad62dd4c5949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': TensorSpec(shape=(128,), dtype=tf.float32, name='reward')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.specs import array_spec\n",
    "\n",
    "reward_spec = {\n",
    "    \"reward\": array_spec.ArraySpec(shape=[BATCH_SIZE], dtype=np.float32, name=\"reward\")\n",
    "}\n",
    "\n",
    "reward_tensor_spec = train_utils.from_spec(reward_spec)\n",
    "reward_tensor_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9181303-6565-45f5-a293-08d50420a805",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Inspect chosen arm features spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6b526a5-997c-4621-b21c-82c6ca1d6a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90d3e03e-619e-4fa4-b817-f02b4cefd6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(64,), dtype=tf.float32, name=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_arm_features_info = (\n",
    "  policy_utilities.create_chosen_arm_features_info_spec(\n",
    "      time_step_spec.observation,\n",
    "  )\n",
    ")\n",
    "chosen_arm_features_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc427092-beb7-4baf-a2e9-b8d10629182c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_policy_type = (\n",
    "    policy_utilities.BanditPolicyType.GREEDY\n",
    ")\n",
    "bandit_policy_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a4a2a33-c7d4-4d81-8082-80dfc3596741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_policy_type = (\n",
    "    policy_utilities.create_bandit_policy_type_tensor_spec(shape=[1])\n",
    ")\n",
    "bandit_policy_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e7ac02b-18bd-4519-92fe-5541b1a8ddc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorSpec(shape=(2,), dtype=tf.float32, name=None), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32)), chosen_arm_features=TensorSpec(shape=(64,), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_spec = policy_utilities.PerArmPolicyInfo(\n",
    "  predicted_rewards_mean=predicted_rewards_mean,\n",
    "  bandit_policy_type=bandit_policy_type,\n",
    "  chosen_arm_features=chosen_arm_features_info,\n",
    ")\n",
    "info_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21f28b9b-8183-495a-89b6-a01f30ea8637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PerArmPolicyInfo(\n",
    "#     log_probability=(), \n",
    "#     predicted_rewards_mean=TensorSpec(shape=(2,), \n",
    "#                                       dtype=tf.float32, name=None), \n",
    "#     multiobjective_scalarized_predicted_rewards_mean=(), \n",
    "#     predicted_rewards_optimistic=(), \n",
    "#     predicted_rewards_sampled=(), \n",
    "#     bandit_policy_type=BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32)), \n",
    "#     chosen_arm_features=TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9c197-ae9b-461d-8956-f078b929ac12",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [4] The Agent\n",
    "\n",
    "**Note** that contextual bandits form a special case of RL, where the actions taken by the agent do not alter the state of the environment \n",
    "\n",
    "> “Contextual” refers to the fact that the agent chooses among a set of actions while having knowledge of the context (environment observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d6e84-688e-48c5-aea1-77df031348c9",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Agent & Network defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aadb01-eb5c-4870-ae14-9e66624ba594",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Agent types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075e8a6-11a9-4346-8725-3653fba4bac4",
   "metadata": {},
   "source": [
    "> `AGENT_TYPE = ['LinUCB', 'LinTS', 'epsGreedy', 'NeuralLinUCB']`\n",
    "\n",
    "1. **LinearUCBAgent**: (`LinUCB`) - An agent implementing the Linear UCB bandit algorithm ([docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/lin_ucb_agent/LinearUCBAgent))\n",
    "\n",
    "2. **LinearThompsonSamplingAgent**: (`LinTS`) - Implements the Linear Thompson Sampling Agent from the [paper](https://arxiv.org/abs/1209.3352): *Thompson Sampling for Contextual Bandits with Linear Payoffs* ([docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/linear_thompson_sampling_agent/LinearThompsonSamplingAgent))\n",
    "\n",
    "3. **NeuralEpsilonGreedyAgent**: (`epsGreedy`) - A neural network based epsilon greedy agent ([docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_epsilon_greedy_agent/NeuralEpsilonGreedyAgent))\n",
    "\n",
    "4. **NeuralLinUCBAgent**: (`NeuralLinUCB`) - An agent implementing the LinUCB algorithm on top of a neural network ([docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_linucb_agent/NeuralLinUCBAgent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8e88e-c8ea-4193-a911-0d974ef3b1a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Network types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547087d-5fad-4521-a584-cb46ce52897d",
   "metadata": {},
   "source": [
    "Which network architecture to use for the `epsGreedy` or `NeuralLinUCB` agents\n",
    "\n",
    "```\n",
    "NETWORK_TYPE = ['commontower', 'dotproduct']\n",
    "```\n",
    "\n",
    "**GlobalAndArmCommonTowerNetwork:** (`commontower`)\n",
    "* This network takes the output of the global and per-arm networks, and leads them through a common network, that in turn outputs reward estimates\n",
    "> * `GLOBAL_LAYERS` - Iterable of ints. Specifies the layers of the global tower\n",
    "> * `ARM_LAYERS` - Iterable of ints. Specifies the layers of the arm tower\n",
    "> * `COMMON_LAYERS` - Iterable of ints. Specifies the layers of the common tower\n",
    "* The network produced by this function can be used either in `GreedyRewardPredictionPolicy`, or `NeuralLinUCBPolicy`\n",
    "> * In the former case, the network must have `output_dim=1`, it is going to be an instance of `QNetwork`, and used in the policy as a reward prediction network\n",
    "> * In the latter case, the network will be an encoding network with its output consumed by a reward layer or a `LinUCB` method. The specified `output_dim` will be the encoding dimension\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmCommonTowerNetwork)\n",
    "\n",
    "**GlobalAndArmDotProductNetwork:** (`dotproduct`)\n",
    "* This network calculates the **dot product** of the output of the global and per-arm networks and returns them as reward estimates\n",
    "> * `GLOBAL_LAYERS` - Iterable of ints. Specifies the layers of the global tower\n",
    "> * `ARM_LAYERS` - Iterable of ints. Specifies the layers of the arm tower\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmDotProductNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3dc270-deb5-4e96-8276-74759a06c318",
   "metadata": {},
   "source": [
    "### Agent config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbe29263-453d-405e-ac50-fee7f379f335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick check on the inputs of the agent - this can be used to diagnose spec shape inputs\n",
      "\n",
      "time_step_spec:  TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': {'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None),\n",
      "                 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)},\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n",
      "\n",
      "action_spec:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))\n",
      "\n",
      "observation_spec:  {'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Quick check on the inputs of the agent - this can be used to diagnose spec shape inputs\")\n",
    "print(\"\\ntime_step_spec: \", time_step_spec)\n",
    "print(\"\\naction_spec: \", action_spec)\n",
    "print(\"\\nobservation_spec: \", observation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d817bf2-1fa6-4bae-af90-745fad996b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128,\n",
      " 'common_layers': [34, 8],\n",
      " 'encoding_dim': 1,\n",
      " 'epsilon': 0.01,\n",
      " 'eval_batch_size': 1,\n",
      " 'global_layers': [72, 36, 18],\n",
      " 'learning_rate': 0.05,\n",
      " 'model_type': 'epsGreedy',\n",
      " 'network_type': 'commontower',\n",
      " 'num_actions': 2,\n",
      " 'per_arm_layers': [64, 32, 16]}\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Agents\n",
    "# ================================\n",
    "AGENT_TYPE      = 'epsGreedy' # 'LinUCB' | 'LinTS |, 'epsGreedy' | 'NeuralLinUCB'\n",
    "\n",
    "# Parameters for linear agents (LinUCB and LinTS).\n",
    "AGENT_ALPHA     = 0.1\n",
    "\n",
    "# Parameters for neural agents (NeuralEpsGreedy and NerualLinUCB).\n",
    "EPSILON         = 0.01\n",
    "LR              = 0.05\n",
    "\n",
    "# Parameters for NeuralLinUCB\n",
    "ENCODING_DIM    = 1\n",
    "EPS_PHASE_STEPS = 1000\n",
    "\n",
    "# ================================\n",
    "# Agent's Preprocess Network\n",
    "# ================================\n",
    " # beginning should be of size: GLOBAL_DIM\n",
    "GLOBAL_LAYERS   = [GLOBAL_DIM, int(GLOBAL_DIM/2), int(GLOBAL_DIM/4)]\n",
    "\n",
    "# beginning should be of size: PER_ARM_DIM\n",
    "ARM_LAYERS      = [PER_ARM_DIM, int(PER_ARM_DIM/2), int(PER_ARM_DIM/4)]\n",
    "\n",
    "# ================================\n",
    "# common layers\n",
    "# ================================\n",
    "\"\"\"\n",
    "> layer sizes for the final tower\n",
    "> The network that takes as input the concatenation of \n",
    "  the outputs of the global and the arm networks\n",
    "> hidden layers ideally divisible by 8, e.g., [16, 32, 64, 128, 256, 512]\n",
    "\"\"\"\n",
    "FIRST_COMMON_LAYER = GLOBAL_LAYERS[-1] + ARM_LAYERS[-1] # min(GLOBAL_LAYERS[-1], ARM_LAYERS[-1])\n",
    "\n",
    "COMMON_LAYERS = [\n",
    "    int(FIRST_COMMON_LAYER),\n",
    "    # int(FIRST_COMMON_LAYER/2),\n",
    "    int(FIRST_COMMON_LAYER/4)\n",
    "]\n",
    "\n",
    "NETWORK_TYPE    = \"commontower\" # 'commontower' | 'dotproduct'\n",
    "\n",
    "if AGENT_TYPE == 'NeuralLinUCB':\n",
    "    NETWORK_TYPE = 'commontower'\n",
    "    ENCODING_DIM = COMMON_LAYERS[-1]\n",
    "    \n",
    "if NETWORK_TYPE == 'dotproduct':\n",
    "    assert GLOBAL_LAYERS[0] == ARM_LAYERS[0]\n",
    "\n",
    "\n",
    "HPARAMS = {  # TODO - streamline and consolidate\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"eval_batch_size\" : EVAL_BATCH_SIZE,\n",
    "    \"num_actions\": NUM_ACTIONS,\n",
    "    \"model_type\": AGENT_TYPE,\n",
    "    \"network_type\": NETWORK_TYPE,\n",
    "    \"global_layers\": GLOBAL_LAYERS,\n",
    "    \"per_arm_layers\": ARM_LAYERS,\n",
    "    \"common_layers\": COMMON_LAYERS,\n",
    "    \"learning_rate\": LR,\n",
    "    \"epsilon\": EPSILON,\n",
    "    \"encoding_dim\": ENCODING_DIM\n",
    "}\n",
    "pprint(HPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bf060bb-9880-40b4-8583-dd7a3f915bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent: NeuralEpsGreedyAgent\n",
      "Network: commontower\n"
     ]
    }
   ],
   "source": [
    "# from src.perarm_features import agent_factory as agent_factory\n",
    "\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "agent = agent_factory.PerArmAgentFactory._get_agent(\n",
    "    agent_type = AGENT_TYPE,\n",
    "    network_type = NETWORK_TYPE,\n",
    "    time_step_spec = time_step_spec,\n",
    "    action_spec = action_spec,\n",
    "    observation_spec=observation_spec,\n",
    "    global_layers = GLOBAL_LAYERS,\n",
    "    arm_layers = ARM_LAYERS,\n",
    "    common_layers = COMMON_LAYERS,\n",
    "    agent_alpha = AGENT_ALPHA,\n",
    "    learning_rate = LR,\n",
    "    epsilon = EPSILON,\n",
    "    train_step_counter = global_step,\n",
    "    output_dim = ENCODING_DIM,\n",
    "    eps_phase_steps = EPS_PHASE_STEPS,\n",
    "    summarize_grads_and_vars = True,\n",
    "    debug_summaries = True\n",
    ")\n",
    "      \n",
    "agent.initialize()\n",
    "print(f'agent: {agent.name}')\n",
    "\n",
    "# if network:\n",
    "#     print(f\"Network: {network}\")\n",
    "\n",
    "if NETWORK_TYPE:\n",
    "    print(f\"Network: {NETWORK_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d77d1-a0b3-4898-adf0-0b32bfaf5bd7",
   "metadata": {},
   "source": [
    "#### Inspect the specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78ca56e7-a386-4bf4-8c2b-3d1665521f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41483f5a-bda6-48a4-9e1c-88d8b4afeea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TupleWrapper(TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': DictWrapper({'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.time_step_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f205716-f265-4ca8-81a7-4799de8f60a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TupleWrapper(Trajectory(\n",
       "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32)),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': DictWrapper({'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None)}),\n",
       " 'policy_info': PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorSpec(shape=(2,), dtype=tf.float32, name=None), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32)), chosen_arm_features=TensorSpec(shape=(64,), dtype=tf.float32, name=None)),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.training_data_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d404dae-3cc3-4f81-962b-4641455ca4f2",
   "metadata": {},
   "source": [
    "# [5] Reward function\n",
    "\n",
    "> see `reward_factory.py`\n",
    "\n",
    "* Since we are training a policy with previously collected interaction data, we model the reward function from actual rewards\n",
    "* We will simply pass the `user_rating` (values 0-5) as rewards to the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10a4f40a-b4ec-4fc2-9f73-3f830b21f351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'target_movie_rating'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.TARGET_FEATURE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df9aecd6-d20a-48a7-9ead-4da3bbcfdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_rewards(element):\n",
    "    \"\"\"Calculates reward for the actions.\"\"\"\n",
    "\n",
    "    def _calc_reward(x):\n",
    "        \"\"\"\n",
    "        Calculates reward for a single action.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### uncomment for linear rewards\n",
    "        r0 = lambda: tf.constant(0.0)\n",
    "        r1 = lambda: tf.constant(1.0)\n",
    "        r2 = lambda: tf.constant(2.0)\n",
    "        r3 = lambda: tf.constant(3.0)\n",
    "        r4 = lambda: tf.constant(4.0)\n",
    "        r5 = lambda: tf.constant(5.0)\n",
    "        \n",
    "        ### uncomment for binary rewards\n",
    "        # r0 = lambda: tf.constant(0.0) # 0.0\n",
    "        # r1 = lambda: tf.constant(0.0) # 1.0\n",
    "        # r2 = lambda: tf.constant(0.0) # 2.0\n",
    "        # r3 = lambda: tf.constant(0.0) # 3.0\n",
    "        # r4 = lambda: tf.constant(1.0) # 4.0\n",
    "        # r5 = lambda: tf.constant(1.0) # 5.0\n",
    "        \n",
    "        c1 = tf.equal(x, 1.0)\n",
    "        c2 = tf.equal(x, 2.0)\n",
    "        c3 = tf.equal(x, 3.0)\n",
    "        c4 = tf.equal(x, 4.0)\n",
    "        c5 = tf.equal(x, 5.0)\n",
    "        return tf.case(\n",
    "            [(c1, r1), (c2, r2), (c3, r3),(c4, r4),(c5, r5)], \n",
    "            default=r0, exclusive=True\n",
    "        )\n",
    "\n",
    "    return tf.map_fn(\n",
    "        fn=_calc_reward, \n",
    "        # elems=element['user_rating'],\n",
    "        elems=element[data_utils.TARGET_FEATURE_NAME],\n",
    "        dtype=tf.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59703a-9a00-4eba-a3b7-45b21b738dd4",
   "metadata": {},
   "source": [
    "# [6] Trajectory function\n",
    "\n",
    "> This function will convert training samples from the TF Records to `trajectories` which the Agent interprets as training records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3605d9f-d239-4871-9a61-6e9b21f9c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _trajectory_fn(element): # hparams\n",
    "    \n",
    "    \"\"\"Converts a dataset element into a trajectory.\"\"\"\n",
    "    global_features = embs._get_global_context_features(element)\n",
    "    arm_features = embs._get_per_arm_features(element)\n",
    "\n",
    "    # Adds a time dimension.\n",
    "    arm_features = train_utils._add_outer_dimension(arm_features)\n",
    "\n",
    "    # obs spec\n",
    "    observation = {\n",
    "        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n",
    "            train_utils._add_outer_dimension(global_features),\n",
    "    }\n",
    "\n",
    "    reward = train_utils._add_outer_dimension(_get_rewards(element))\n",
    "\n",
    "    # To emit the predicted rewards in policy_info, we need to create dummy\n",
    "    # rewards to match the definition in TensorSpec for the ones specified in\n",
    "    # emit_policy_info set.\n",
    "    dummy_rewards = tf.zeros([HPARAMS['batch_size'], 1, HPARAMS['num_actions']])\n",
    "    policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "        chosen_arm_features=arm_features,\n",
    "        # Pass dummy mean rewards here to match the model_spec for emitting\n",
    "        # mean rewards in policy info\n",
    "        predicted_rewards_mean=dummy_rewards,\n",
    "        bandit_policy_type=tf.zeros([HPARAMS['batch_size'], 1, 1], dtype=tf.int32)\n",
    "    )\n",
    "    \n",
    "    if HPARAMS['model_type'] == 'neural_ucb':\n",
    "        policy_info = policy_info._replace(\n",
    "            predicted_rewards_optimistic=dummy_rewards\n",
    "        )\n",
    "        \n",
    "    return trajectory.single_step(\n",
    "        observation=observation,\n",
    "        action=tf.zeros_like(\n",
    "            reward, dtype=tf.int32\n",
    "        ),  # Arm features are copied from policy info, put dummy zeros here\n",
    "        policy_info=policy_info,\n",
    "        reward=reward,\n",
    "        discount=tf.zeros_like(reward)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a0989c-5a06-4fd5-a12d-d92dbbe71a76",
   "metadata": {},
   "source": [
    "Inspect the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "257791d1-c97c-48c0-bc92-f5a4737b6c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_traj.action.shape      : (128, 1)\n",
      "test_traj.discount.shape    : (128, 1)\n",
      "test_traj.reward.shape      : (128, 1)\n",
      "test_traj.observation.shape : (128, 1, 72)\n"
     ]
    }
   ],
   "source": [
    "for x in train_dataset.batch(HPARAMS['batch_size']).take(1):\n",
    "    test_traj = _trajectory_fn(x)\n",
    "    \n",
    "print(f\"test_traj.action.shape      : {test_traj.action.shape}\") \n",
    "print(f\"test_traj.discount.shape    : {test_traj.discount.shape}\")\n",
    "print(f\"test_traj.reward.shape      : {test_traj.reward.shape}\")\n",
    "print(f\"test_traj.observation.shape : {test_traj.observation['global'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3dace3d1-ce61-48cf-82a4-f701d3fe337c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_traj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbfc9a-049e-49b9-b2a3-ecb9fd4b9964",
   "metadata": {},
   "source": [
    "# [7] Training the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13adc1ef-b50e-41a3-9b95-c128d0c55a2b",
   "metadata": {},
   "source": [
    "## set Vertex Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e861fb27-df24-4821-8448-cf1346b186f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : 02a-supervised-bandit-v3\n",
      "RUN_NAME          : run-20240314-024303\n",
      "\n",
      "CHECKPT_DIR       : gs://rec-bandits-v2-hybrid-vertex-bucket/02a-supervised-bandit-v3/chkpoint\n",
      "BASE_OUTPUT_DIR   : gs://rec-bandits-v2-hybrid-vertex-bucket/02a-supervised-bandit-v3/run-20240314-024303\n",
      "LOG_DIR           : gs://rec-bandits-v2-hybrid-vertex-bucket/02a-supervised-bandit-v3/run-20240314-024303/logs\n",
      "ROOT_DIR          : gs://rec-bandits-v2-hybrid-vertex-bucket/02a-supervised-bandit-v3/run-20240314-024303/root\n",
      "ARTIFACTS_DIR     : gs://rec-bandits-v2-hybrid-vertex-bucket/02a-supervised-bandit-v3/run-20240314-024303/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'02a-supervised-bandit-v3'\n",
    "\n",
    "# new experiment\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "CHECKPT_DIR       = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/chkpoint\"\n",
    "BASE_OUTPUT_DIR   = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}\"\n",
    "LOG_DIR           = f\"{BASE_OUTPUT_DIR}/logs\"\n",
    "ROOT_DIR          = f\"{BASE_OUTPUT_DIR}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BASE_OUTPUT_DIR}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\\n\")\n",
    "print(f\"CHECKPT_DIR       : {CHECKPT_DIR}\")\n",
    "print(f\"BASE_OUTPUT_DIR   : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de729bad-0bc9-429e-b4cb-7b24bf615aa1",
   "metadata": {},
   "source": [
    "### Create TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2570564-71f4-4dda-8d8a-59784db67632",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_TENSORBOARD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db63052a-7eea-4982-964d-1f7ecab0665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME: projects/934903580331/locations/us-central1/tensorboards/58938221295304704\n",
      "TB display name: 02a-supervised-bandit-v3\n"
     ]
    }
   ],
   "source": [
    "if NEW_TENSORBOARD:\n",
    "    # create new TB instance\n",
    "    TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}\"\n",
    "\n",
    "    tensorboard = aiplatform.Tensorboard.create(\n",
    "        display_name=TENSORBOARD_DISPLAY_NAME\n",
    "        , project=PROJECT_ID\n",
    "        , location=REGION\n",
    "    )\n",
    "\n",
    "    TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "else:\n",
    "    # use existing TB instance\n",
    "    # TB_RESOURCE_NAME = f'projects/{PROJECT_NUM}/locations/{LOCATION}/tensorboards/XXXXXX_TODO_XXXXX'\n",
    "    tensorboard = aiplatform.Tensorboard(\n",
    "        tensorboard_name=TB_RESOURCE_NAME\n",
    "    )\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name: {tensorboard.display_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a0b708d-990e-468b-a1b1-a8ba8f71d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete Tensorboard\n",
    "# vertex_ai_tb.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c891d27-d9d1-4e64-8981-1a1ae343c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aiplatform.init(\n",
    "#     project=PROJECT_ID,\n",
    "#     location=REGION,\n",
    "#     experiment=EXPERIMENT_NAME,\n",
    "#     experiment_tensorboard=TB_ID\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7a5e4-efff-490b-b431-53037a2f4a16",
   "metadata": {},
   "source": [
    "## eval loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba0392e-90c8-4dc8-8df2-6d2af1ade490",
   "metadata": {},
   "source": [
    "> When evaluating an epsilon-greedy bandit, we need to remove those predictions that come from the `uniform_random` policy. We only want the predictions from the `GREEDY` policy, becasue we want to know how the model performs when it is actually used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b3d3c-75f7-46f4-9a1b-6329e419b7f5",
   "metadata": {},
   "source": [
    "## train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb7ffc-00df-4d46-857b-86c87b78f597",
   "metadata": {},
   "source": [
    "`agent.train(experience=...)`\n",
    "\n",
    "where `experience` is a batch of trajectories data in the form of a Trajectory. \n",
    "* The structure of experience must match that of `self.training_data_spec`. \n",
    "* All tensors in experience must be shaped [batch, time, ...] where time must be equal to self.train_step_length if that property is not None."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d6d45-c71e-4fe4-9af8-af9a8bda4e4f",
   "metadata": {},
   "source": [
    "### logs & checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "049ee49b-3b1a-4b12-a360-15e759dff7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('metrics: [<tf_agents.metrics.tf_metrics.AverageReturnMetric object at '\n",
      " '0x7fc3784fe1d0>]')\n",
      "setting checkpoint_manager: gs://rec-bandits-v2-hybrid-vertex-bucket/02a-supervised-bandit-v3/chkpoint\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7fc2b5852440>,\n",
       " 'get_initial_state': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7fc2b5851b10>,\n",
       " 'get_train_step': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7fc2b5851b70>,\n",
       " 'get_metadata': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7fc2b5853be0>}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# TB summary writer\n",
    "# ====================================================\n",
    "train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    f\"{LOG_DIR}\", flush_millis=10 * 1000\n",
    ")\n",
    "# ====================================================\n",
    "# metrics\n",
    "# ====================================================\n",
    "step_metric = tf_metrics.EnvironmentSteps()\n",
    "metrics = [\n",
    "    # tf_metrics.NumberOfEpisodes(),\n",
    "    # tf_metrics.AverageEpisodeLengthMetric(batch_size=HPARAMS['batch_size']),\n",
    "    tf_metrics.AverageReturnMetric(batch_size=HPARAMS['batch_size'])\n",
    "]\n",
    "\n",
    "pprint(f\"metrics: {metrics}\")\n",
    "\n",
    "# ====================================================\n",
    "# get checkpoint manager\n",
    "# ====================================================\n",
    "print(f\"setting checkpoint_manager: {CHECKPT_DIR}\\n\")\n",
    "\n",
    "checkpoint_manager = train_utils.restore_and_get_checkpoint_manager(\n",
    "    root_dir=CHECKPT_DIR, \n",
    "    agent=agent, \n",
    "    metrics=metrics, \n",
    "    step_metric=step_metric\n",
    ")\n",
    "# ====================================================\n",
    "# policy saver\n",
    "# ====================================================\n",
    "saver = policy_saver.PolicySaver(\n",
    "    agent.policy, \n",
    "    train_step=global_step\n",
    ")\n",
    "saver.signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83683f4b-8a1a-44b9-8d5f-7ee3304c794c",
   "metadata": {},
   "source": [
    "### config\n",
    "\n",
    "* calculate train & val dataset sizes\n",
    "* define logging and chkpt intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f9432613-6583-4828-8a2a-dd939adbc3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_TRAIN_STEPS : 50\n",
      "NUM_EVAL_STEPS  : 1000\n",
      "CHKPT_INTERVAL  : 50\n",
      "LOG_INTERVAL    : 10\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN_STEPS = 50\n",
    "NUM_EVAL_STEPS  = 1_000\n",
    "CHKPT_INTERVAL  = NUM_TRAIN_STEPS # // 5\n",
    "LOG_INTERVAL    = 10\n",
    "\n",
    "print(f\"NUM_TRAIN_STEPS : {NUM_TRAIN_STEPS}\")\n",
    "print(f\"NUM_EVAL_STEPS  : {NUM_EVAL_STEPS}\")\n",
    "print(f\"CHKPT_INTERVAL  : {CHKPT_INTERVAL}\")\n",
    "print(f\"LOG_INTERVAL    : {LOG_INTERVAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "379ffc65-4eec-4af1-8fd4-912ef65a9808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_ds_iterator = iter(train_dataset.batch(HPARAMS['batch_size']).repeat())\n",
    "\n",
    "# eval dataset\n",
    "eval_ds = val_dataset.batch(HPARAMS[\"eval_batch_size\"])\n",
    "\n",
    "if NUM_EVAL_STEPS > 0:\n",
    "    eval_ds = eval_ds.take(NUM_EVAL_STEPS)\n",
    "\n",
    "# eval_ds = eval_ds.prefetch(tf.data.AUTOTUNE)\n",
    "# eval_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498d7e3-d166-4e1d-94a5-f721de58694a",
   "metadata": {},
   "source": [
    "### run train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da4680f3-693b-4530-8e4d-88bc43036c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating pre-trained Agent...\n",
      "pre-train val_loss     : 14.987306594848633\n",
      "pre-train eval runtime : 0\n",
      "starting train loop...\n",
      "step = 0: train loss = 15.960000038146973\n",
      "step = 10: train loss = 10.0\n",
      "step = 20: train loss = 4.420000076293945\n",
      "step = 30: train loss = 1.7799999713897705\n",
      "step = 40: train loss = 1.0499999523162842\n",
      "train runtime_mins: 5\n",
      "saved to checkpoint_manager: gs://rec-bandits-v2-hybrid-vertex-bucket/02a-supervised-bandit-v3/chkpoint\n",
      "saved trained policy to: gs://rec-bandits-v2-hybrid-vertex-bucket/02a-supervised-bandit-v3/run-20240314-024303/artifacts\n",
      "evaluating trained Agent...\n",
      "post-train val_loss     : 1.5380302667617798\n",
      "post-train eval runtime : 0\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "# agent.train = common.function(agent.train)\n",
    "\n",
    "list_o_loss = []\n",
    "\n",
    "# ====================================================\n",
    "# Evaluate the agent's policy once before training\n",
    "# ====================================================\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "pre_policy_tf = py_tf_eager_policy.PyTFEagerPolicy(agent.policy, use_tf_function=True)\n",
    "\n",
    "print(f\"evaluating pre-trained Agent...\")\n",
    "start_time = time.time()\n",
    "\n",
    "pre_val_loss, pre_preds, pre_tr_rewards = eval_perarm._run_bandit_eval(\n",
    "    policy = pre_policy_tf,\n",
    "    data = eval_ds,\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"pre-train val_loss     : {pre_val_loss}\")\n",
    "print(f\"pre-train eval runtime : {runtime_mins}\")\n",
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "print(f\"starting train loop...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(NUM_TRAIN_STEPS):\n",
    "    \n",
    "    with train_summary_writer.as_default():\n",
    "\n",
    "        data = next(train_ds_iterator)\n",
    "        trajectories = _trajectory_fn(data)\n",
    "\n",
    "        # All tensors in experience must be shaped [batch, time, ...] \n",
    "        step = agent.train_step_counter.numpy()\n",
    "        loss = agent.train(experience=trajectories)\n",
    "        list_o_loss.append(loss.loss.numpy())\n",
    "\n",
    "        train_utils._export_metrics_and_summaries(\n",
    "            step=i, \n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        # print step loss\n",
    "        if step % LOG_INTERVAL == 0:\n",
    "            print(\n",
    "                'step = {0}: train loss = {1}'.format(\n",
    "                    step, round(loss.loss.numpy(), 2)\n",
    "                )\n",
    "            )\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"train runtime_mins: {runtime_mins}\")\n",
    "checkpoint_manager.save(global_step)\n",
    "print(f\"saved to checkpoint_manager: {CHECKPT_DIR}\")\n",
    "saver.save(ARTIFACTS_DIR)\n",
    "print(f\"saved trained policy to: {ARTIFACTS_DIR}\")\n",
    "\n",
    "# ====================================================\n",
    "# Evaluate the agent's policy once after training\n",
    "# ====================================================\n",
    "print(f\"evaluating trained Agent...\")\n",
    "\n",
    "post_policy_tf = py_tf_eager_policy.PyTFEagerPolicy(agent.policy, use_tf_function=True)\n",
    "start_time = time.time()\n",
    "\n",
    "val_loss, preds, tr_rewards = eval_perarm._run_bandit_eval(\n",
    "    policy = post_policy_tf,\n",
    "    data = eval_ds,\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"post-train val_loss     : {val_loss}\")\n",
    "print(f\"post-train eval runtime : {runtime_mins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17b2b0d2-9795-4367-b57b-c331e94d1000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5380303"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb1ed8-f67b-4e7b-9cbc-70edd50e49ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate train job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31fe28-2f05-499f-b1cc-505f3c9074e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### train loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "41937f9a-6c28-48d2-bc7c-bfe4372c7cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_o_loss)\n",
    "# plt.ylim([0, 10])\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e947e1-67f4-4205-99af-6921c6e9c896",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65627011-48eb-4e8e-981e-8b61b0f427c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0f77a33-0962-4af0-af97-cf05f895cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d0509cb-0777-4d35-86c6-28c8acbcc793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f30e9adf9e7e69a8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f30e9adf9e7e69a8\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=$LOG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc40143-30b1-479f-b9b1-fa4c07b1690e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [8] Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c94a0-122f-4ba6-acef-5653a3cc7850",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1c382b5-e2e0-45a2-9c5f-ef2c468d0f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02a-supervised-bandit-v3/run-20240314-024303/artifacts\n"
     ]
    }
   ],
   "source": [
    "POLICY_URI = ARTIFACTS_DIR\n",
    "\n",
    "print(POLICY_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "68592d97-1272-4c6d-a4bd-43a6f7fe6c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.py_tf_eager_policy.SavedModelPyTFEagerPolicy at 0x7fc2b69ab820>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy\n",
    "\n",
    "trained_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n",
    "    POLICY_URI, load_specs_from_pbtxt=True\n",
    ")\n",
    "trained_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d910a06f-678f-4bfa-a132-10b885f871c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762c936-3c49-4ec5-a9eb-4df5816dc888",
   "metadata": {},
   "source": [
    "TODO - explain bandit_policy_type()\n",
    "\n",
    "**Dummy arm values?**\n",
    "* We set `chosen_arm_features` to dummy values of all zeros. We need to save dummy chosen arm features to make the returned policy step have the same structure as the policy state spec.\n",
    "* `emit_policy_info = ('predicted_rewards_mean', 'bandit_policy_type')` defines what side information we want to get as part of the policy info when we call policy network \n",
    "* This makes it so that the model always returns the expected rewards even if the model is exploring\n",
    "* This means that the largest predicted rewards may not match the selected action when the model is exploring (i.e. bandit_policy == UNIFORM == 2)\n",
    "\n",
    "**UNIFORM Random policy:**\n",
    "```\n",
    "PolicyStep(action=array(1, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.637535 , 3.5557823], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([2], dtype=int32), chosen_arm_features=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)))\n",
    "```\n",
    "\n",
    "**GREEDY policy:**\n",
    "```\n",
    "PolicyStep(action=array(0, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.8278818, 3.607565 ], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([ 0.04054676, -0.04735027,  0.02202327,  0.01276748, -0.01367382,\n",
    "       -0.04838754,  0.00104916, -0.01956806,  0.01028793, -0.00256665,\n",
    "       -0.04376553,  0.03900594,  0.02388967, -0.04016995, -0.04945569,\n",
    "       -0.04539652,  0.04423274, -0.04630332,  0.00390794,  0.03986299,\n",
    "       -0.0398974 ,  0.01655747, -0.0471294 , -0.00690235, -0.03643382,\n",
    "       -0.0437116 ,  0.04757959, -0.00991895,  0.0281055 , -0.04633433,\n",
    "        0.01897702, -0.00824345,  0.03232259, -0.04980658, -0.01144745,\n",
    "        0.01690939,  0.02536928,  0.03384003, -0.01558131,  0.04731432,\n",
    "        0.04579857, -0.03305101,  0.03521084, -0.03915765,  0.04927064,\n",
    "        0.04695194,  0.03792156,  0.040496  ,  0.02574866, -0.00665367,\n",
    "       -0.03839222, -0.01564004, -0.04710373,  0.03557512, -0.01392462,\n",
    "        0.01368621,  0.02792176, -0.01799218,  0.00246101,  0.02263175,\n",
    "        0.0170817 ,  0.01388383, -0.03048695,  0.01198485], dtype=float32)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "567f64e1-8149-4364-b561-1a9d12581b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aff30396-e0fd-4f43-acaa-6af8b20b12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFER_SIZE = 1\n",
    "dummy_arm = tf.zeros([INFER_SIZE, PER_ARM_DIM], dtype=tf.float32)\n",
    "\n",
    "SKIP_NUM = 10\n",
    "\n",
    "for x in eval_ds.skip(SKIP_NUM).take(INFER_SIZE):\n",
    "    # get feature tensors\n",
    "    \n",
    "    global_feat_infer = embs._get_global_context_features(x)\n",
    "    arm_feat_infer = embs._get_per_arm_features(x)\n",
    "    \n",
    "    rewards = _get_rewards(x)\n",
    "    # rewards = reward_factory._get_rewards(x)\n",
    "    \n",
    "    # reshape arm features\n",
    "    arm_feat_infer = tf.reshape(arm_feat_infer, [HPARAMS['eval_batch_size'], PER_ARM_DIM]) # perarm_dim\n",
    "    concat_arm = tf.concat([arm_feat_infer, dummy_arm], axis=0)\n",
    "    \n",
    "    # flatten global\n",
    "    flat_global_infer = tf.reshape(global_feat_infer, [GLOBAL_DIM])\n",
    "    feature = {'global': flat_global_infer, 'per_arm': concat_arm}\n",
    "    \n",
    "    # get actual reward\n",
    "    actual_reward = rewards.numpy()[0]\n",
    "    \n",
    "    # build trajectory step\n",
    "    trajectory_step = train_utils._get_eval_step(feature, actual_reward)\n",
    "    \n",
    "    prediction = trained_policy.action(trajectory_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2bcd1e82-168e-4df3-92bd-4cd34ecd3a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
       " 'observation': {'global': <tf.Tensor: shape=(72,), dtype=float32, numpy=\n",
       "array([-0.01937   , -0.01148008,  0.04564018, -0.00891905,  0.02754474,\n",
       "       -0.01083482, -0.00704963,  0.03862441, -0.04456897, -0.03509618,\n",
       "       -0.00037976,  0.0133433 ,  0.00627569,  0.0282287 ,  0.01014861,\n",
       "        0.00185812, -0.01657658,  0.00610872, -0.00344028,  0.01255758,\n",
       "       -0.03804554, -0.02399547,  0.04724855, -0.00948345,  0.0012117 ,\n",
       "       -0.01687596,  0.03302559, -0.04819508, -0.01989744, -0.0479574 ,\n",
       "        0.00310808, -0.02024279, -0.01765497,  0.00557814,  0.03363528,\n",
       "       -0.00258738,  0.02431991,  0.0070347 ,  0.02457161, -0.01536963,\n",
       "       -0.04204952,  0.03619448,  0.028123  , -0.03171634,  0.04010076,\n",
       "       -0.04999392,  0.03570152,  0.04195454,  0.032514  , -0.04602477,\n",
       "       -0.01869357, -0.04476898, -0.04559103, -0.0238757 , -0.04479721,\n",
       "        0.00878171, -0.01440529, -0.00934676,  0.04387155, -0.00718099,\n",
       "       -0.01606951, -0.01950452, -0.01540834,  0.04660899,  0.04248348,\n",
       "        0.00167714,  0.00291684,  0.02328638,  0.04280323,  0.02029267,\n",
       "        0.04909423, -0.0128206 ], dtype=float32)>,\n",
       "                 'per_arm': <tf.Tensor: shape=(2, 64), dtype=float32, numpy=\n",
       "array([[-0.04905591,  0.03016118, -0.00855867, -0.04545943,  0.0267613 ,\n",
       "        -0.03762635, -0.03010865,  0.0006574 , -0.04785687, -0.03800324,\n",
       "         0.01027825, -0.02211777, -0.02931494, -0.0294627 ,  0.00912614,\n",
       "         0.04256069, -0.0078347 , -0.01975232,  0.00852995,  0.01284253,\n",
       "        -0.00909581, -0.02728211, -0.03728725, -0.02742556, -0.00621618,\n",
       "         0.03099342,  0.03457431,  0.03430482,  0.00351389,  0.0278071 ,\n",
       "         0.00036978, -0.00294856,  0.03283571,  0.02467329,  0.02102164,\n",
       "        -0.0105974 ,  0.03344886, -0.01781635,  0.0456937 ,  0.04280451,\n",
       "        -0.01295062, -0.03528979, -0.0238891 ,  0.01655625, -0.03554971,\n",
       "         0.04133953, -0.01434727,  0.02707123,  0.09310813,  0.10542987,\n",
       "        -0.04514233, -0.05677646, -0.09330908,  0.08010872,  0.09729148,\n",
       "         0.07725318, -0.06265229,  0.05131223,  0.05403749,  0.09265389,\n",
       "         0.08783026, -0.05436022, -0.01692266,  0.08482353],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "      dtype=float32)>},\n",
       " 'reward': <tf.Tensor: shape=(), dtype=float32, numpy=3.0>,\n",
       " 'step_type': <tf.Tensor: shape=(), dtype=int32, numpy=0>})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e58af9ab-f4da-4857-afe5-6b8de26ed0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=array(0, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.140486 , 2.9922621], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([-0.04905591,  0.03016118, -0.00855867, -0.04545943,  0.0267613 ,\n",
       "       -0.03762635, -0.03010865,  0.0006574 , -0.04785687, -0.03800324,\n",
       "        0.01027825, -0.02211777, -0.02931494, -0.0294627 ,  0.00912614,\n",
       "        0.04256069, -0.0078347 , -0.01975232,  0.00852995,  0.01284253,\n",
       "       -0.00909581, -0.02728211, -0.03728725, -0.02742556, -0.00621618,\n",
       "        0.03099342,  0.03457431,  0.03430482,  0.00351389,  0.0278071 ,\n",
       "        0.00036978, -0.00294856,  0.03283571,  0.02467329,  0.02102164,\n",
       "       -0.0105974 ,  0.03344886, -0.01781635,  0.0456937 ,  0.04280451,\n",
       "       -0.01295062, -0.03528979, -0.0238891 ,  0.01655625, -0.03554971,\n",
       "        0.04133953, -0.01434727,  0.02707123,  0.09310813,  0.10542987,\n",
       "       -0.04514233, -0.05677646, -0.09330908,  0.08010872,  0.09729148,\n",
       "        0.07725318, -0.06265229,  0.05131223,  0.05403749,  0.09265389,\n",
       "        0.08783026, -0.05436022, -0.01692266,  0.08482353], dtype=float32)))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ec286d78-dd56-455d-90f8-4ffa88f3ac50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.140486 , 2.9922621], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.info.predicted_rewards_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c85a0087-7388-4f03-9278-aace31121233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0, dtype=int32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8266c7c-3688-46a5-a1c8-a63bb7146128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.info.bandit_policy_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9712bc00-7ea0-4003-a3c7-81a0be5bc995",
   "metadata": {},
   "source": [
    "# [9] Prepare training application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f62966-123f-410e-9610-832f20566635",
   "metadata": {},
   "source": [
    "To scale this training in Vertex AI's managed training service, we need to package all our code into a python application\n",
    "\n",
    "* This means, we need to modularize all steps executed up until this point. For example, the training loop we ran above is further parameterized in `train_perarm.py`, which will be called from our `task.py` when we submit the job to Vertex AI\n",
    "* This training package will eventually be used to create a custom training image (more on that in `04b-build-training-image` notebook)\n",
    "* For additional details on preparing the training application for Vertex AI, check out [the documentation](https://cloud.google.com/vertex-ai/docs/training/code-requirements) and [this code example](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_training.ipynb) (which also has good, related commentary)\n",
    "\n",
    "**The remaining cells will repeat steps executed above**\n",
    "> we are just illustrating / validating that they are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c3d30-bef4-4ec5-a978-116856a70e7e",
   "metadata": {},
   "source": [
    "### Distribution strategy\n",
    "\n",
    "Use `strategy_utils` to generate a strategy. Under the hood, passing the parameter:\n",
    "\n",
    "* `use_gpu = False` returns `tf.distribute.get_strategy()`, which uses CPU\n",
    "* `use_gpu = True` returns `tf.distribute.MirroredStrategy()`, which uses all GPUs that are visible to TensorFlow on one machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "68140c4d-12ff-4758-89ca-44fd710ca0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy at 0x7fbda05b82b0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.train.utils import strategy_utils\n",
    "\n",
    "use_gpu = True\n",
    "use_tpu = False\n",
    "\n",
    "distribution_strategy = strategy_utils.get_strategy(tpu=use_tpu, use_gpu=use_gpu)\n",
    "distribution_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e80724b1-6525-4986-b832-4af8b49d923c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_REPLICAS = distribution_strategy.num_replicas_in_sync\n",
    "NUM_REPLICAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129ee41-b43c-4d5a-b656-cdac6b79688c",
   "metadata": {},
   "source": [
    "### Vertex Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ef509680-4d19-4efa-9f01-21039e0d573b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : 02a-supervised-bandit-v3\n"
     ]
    }
   ],
   "source": [
    "# using same experiment name \n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ad98430-d836-40f1-8e4f-eb46f0c91fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : 02a-supervised-bandit-v3\n",
      "RUN_NAME          : run-20240314-025116\n",
      "\n",
      "CHECKPT_DIR       : gs://rec-bandits-v2-hybrid-vertex-bucket/02a-supervised-bandit-v3/chkpoint\n",
      "BASE_OUTPUT_DIR   : gs://rec-bandits-v2-hybrid-vertex-bucket/02a-supervised-bandit-v3/run-20240314-025116\n",
      "LOG_DIR           : gs://rec-bandits-v2-hybrid-vertex-bucket/02a-supervised-bandit-v3/run-20240314-025116/logs\n",
      "ROOT_DIR          : gs://rec-bandits-v2-hybrid-vertex-bucket/02a-supervised-bandit-v3/run-20240314-025116/root\n",
      "ARTIFACTS_DIR     : gs://rec-bandits-v2-hybrid-vertex-bucket/02a-supervised-bandit-v3/run-20240314-025116/artifacts\n"
     ]
    }
   ],
   "source": [
    "# new experiment\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "CHECKPT_DIR       = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/chkpoint\"\n",
    "BASE_OUTPUT_DIR   = f'{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "LOG_DIR           = f\"{BASE_OUTPUT_DIR}/logs\"\n",
    "ROOT_DIR          = f\"{BASE_OUTPUT_DIR}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BASE_OUTPUT_DIR}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "# aiplatform.init(\n",
    "#     project=PROJECT_ID,\n",
    "#     location=REGION,\n",
    "#     experiment=EXPERIMENT_NAME\n",
    "# )\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\\n\")\n",
    "print(f\"CHECKPT_DIR       : {CHECKPT_DIR}\")\n",
    "print(f\"BASE_OUTPUT_DIR   : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83de7c4-f7c7-4290-b44a-9e9194bac882",
   "metadata": {},
   "source": [
    "### Create TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17576ce0-727d-4297-a52d-f64fb75ca78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME : projects/934903580331/locations/us-central1/tensorboards/3778911513503334400\n",
      "TB display name  : 02a-supervised-bandit-v3-run-20240314-025116\n",
      "TB_ID            : 3778911513503334400\n"
     ]
    }
   ],
   "source": [
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}-{RUN_NAME}\"\n",
    "\n",
    "tensorboard = aiplatform.Tensorboard.create(\n",
    "    display_name=TENSORBOARD_DISPLAY_NAME\n",
    "    , project=PROJECT_ID\n",
    "    , location=REGION\n",
    ")\n",
    "\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "TB_ID = TB_RESOURCE_NAME.split('/')[-1]\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME : {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name  : {tensorboard.display_name}\")\n",
    "print(f\"TB_ID            : {TB_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "71d43cf9-db3f-437e-98ee-3791ac0c5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    experiment_tensorboard=TB_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2e082-c0f6-4792-a279-e827c48b5895",
   "metadata": {},
   "source": [
    "### trajectory function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "55e21068-a7a5-44c8-a16f-d8c41d976c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src import reward_factory as reward_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b9ce410e-ac03-48b2-8006-4591b38297a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with distribution_strategy.scope():\n",
    "\n",
    "    embs = emb_features.EmbeddingModel(\n",
    "        vocab_dict = vocab_dict,\n",
    "        num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "        global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "        mv_emb_size = MV_EMBEDDING_SIZE,\n",
    "        max_genre_length = data_config.MAX_GENRE_LENGTH\n",
    "    )\n",
    "\n",
    "    def _trajectory_fn(element): # hparams\n",
    "        \"\"\"\n",
    "        Converts a dataset element into a trajectory\n",
    "        \"\"\"\n",
    "        global_features = embs._get_global_context_features(element)\n",
    "        arm_features = embs._get_per_arm_features(element)\n",
    "\n",
    "        # Adds a time dimension.\n",
    "        arm_features = train_utils._add_outer_dimension(arm_features)\n",
    "\n",
    "        # obs spec\n",
    "        observation = {\n",
    "            bandit_spec_utils.GLOBAL_FEATURE_KEY:\n",
    "                train_utils._add_outer_dimension(global_features),\n",
    "        }\n",
    "\n",
    "        # reward = train_utils._add_outer_dimension(reward_factory._get_binary_rewards(element))\n",
    "        reward = train_utils._add_outer_dimension(reward_factory._get_rewards(element))\n",
    "\n",
    "        # To emit the predicted rewards in policy_info, we need to create dummy\n",
    "        # rewards to match the definition in TensorSpec for the ones specified in\n",
    "        # emit_policy_info set.\n",
    "        dummy_rewards = tf.zeros([HPARAMS['batch_size'], 1, HPARAMS['num_actions']])\n",
    "        policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "            chosen_arm_features=arm_features,\n",
    "            # Pass dummy mean rewards here to match the model_spec for emitting\n",
    "            # mean rewards in policy info\n",
    "            predicted_rewards_mean=dummy_rewards,\n",
    "            bandit_policy_type=tf.zeros([HPARAMS['batch_size'], 1, 1], dtype=tf.int32)\n",
    "        )\n",
    "\n",
    "        if HPARAMS['model_type'] == 'neural_ucb':\n",
    "            policy_info = policy_info._replace(\n",
    "                predicted_rewards_optimistic=dummy_rewards\n",
    "            )\n",
    "\n",
    "        return trajectory.single_step(\n",
    "            observation=observation,\n",
    "            action=tf.zeros_like(\n",
    "                reward, \n",
    "                dtype=tf.int32\n",
    "            ),\n",
    "            policy_info=policy_info,\n",
    "            reward=reward,\n",
    "            discount=tf.zeros_like(reward)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b759404-b282-4f55-add8-7d795867c99e",
   "metadata": {},
   "source": [
    "### get agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3be40320-a73f-45f7-9fc4-0bd64df0ae5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 128,\n",
       " 'eval_batch_size': 1,\n",
       " 'num_actions': 2,\n",
       " 'model_type': 'epsGreedy',\n",
       " 'network_type': 'commontower',\n",
       " 'global_layers': [72, 36, 18],\n",
       " 'per_arm_layers': [64, 32, 16],\n",
       " 'common_layers': [34, 8],\n",
       " 'learning_rate': 0.05,\n",
       " 'epsilon': 0.01,\n",
       " 'encoding_dim': 1}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HPARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fbdecf78-94c5-4f8c-a6c0-86f2eace4663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: NeuralEpsGreedyAgent\n",
      "Network: commontower\n"
     ]
    }
   ],
   "source": [
    "with distribution_strategy.scope():\n",
    "\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "    agent = agent_factory.PerArmAgentFactory._get_agent(\n",
    "        agent_type = HPARAMS['model_type'],\n",
    "        network_type = HPARAMS['network_type'],\n",
    "        time_step_spec = time_step_spec,\n",
    "        action_spec = action_spec,\n",
    "        observation_spec=observation_spec,\n",
    "        global_layers = HPARAMS['global_layers'],\n",
    "        arm_layers = HPARAMS['per_arm_layers'],\n",
    "        common_layers = HPARAMS['common_layers'],\n",
    "        agent_alpha = AGENT_ALPHA,\n",
    "        learning_rate = HPARAMS['learning_rate'],\n",
    "        epsilon = HPARAMS['epsilon'],\n",
    "        train_step_counter = global_step,\n",
    "        output_dim = HPARAMS['encoding_dim'],\n",
    "        eps_phase_steps = EPS_PHASE_STEPS,\n",
    "        summarize_grads_and_vars = True,\n",
    "        debug_summaries = True\n",
    "    )\n",
    "\n",
    "    agent.initialize()\n",
    "\n",
    "print(f\"Agent: {agent.name}\")\n",
    "if NETWORK_TYPE:\n",
    "    print(f\"Network: {NETWORK_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "108ed33c-d8ef-4524-97e9-ced9e01c154b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHKPT_INTERVAL  : 200\n",
      "eval_batch_size : 1\n",
      "NUM_EVAL_STEPS  : 100\n"
     ]
    }
   ],
   "source": [
    "IS_TESTING = True\n",
    "\n",
    "# train args\n",
    "NUM_EPOCHS          = 2\n",
    "TRAINING_LOOPS      = 100\n",
    "STEPS_PER_LOOP      = 1\n",
    "drop_arm_feature_fn = None\n",
    "\n",
    "LOG_INTERVAL        = 10\n",
    "CHKPT_INTERVAL      = 200\n",
    "NUM_EVAL_STEPS      = 100\n",
    "\n",
    "print(f\"CHKPT_INTERVAL  : {CHKPT_INTERVAL}\")\n",
    "print(f\"eval_batch_size : {EVAL_BATCH_SIZE}\")\n",
    "print(f\"NUM_EVAL_STEPS  : {NUM_EVAL_STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f66ff927-9a39-4abf-b247-845f0dc7721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval dataset\n",
    "eval_ds = val_dataset.batch(HPARAMS['eval_batch_size'])\n",
    "\n",
    "if NUM_EVAL_STEPS >= 0:\n",
    "    with distribution_strategy.scope():\n",
    "        eval_ds = eval_ds.take(NUM_EVAL_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "528441f5-64ec-4f09-bd50-b2ae85b553bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# TB summary writer\n",
    "# ====================================================\n",
    "with distribution_strategy.scope():\n",
    "    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        f\"{LOG_DIR}\", flush_millis=10 * 1000\n",
    "    )\n",
    "    train_summary_writer.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c98398-d545-4dcc-a71f-32249b5f0336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution_strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fbda05b82b0>\n",
      "number of train_files: 2\n",
      "Inpsecting agent policy from train_peram file...\n",
      "agent.policy: <tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x7fbda0338310>\n",
      "Inpsecting agent policy from train_peram file: Complete\n",
      "setting checkpoint_manager: gs://rec-bandits-v2-hybrid-vertex-bucket/02a-supervised-bandit-v3/chkpoint\n",
      "agent.train_step_counter: 50\n",
      "starting train loop...\n",
      "epoch: 1\n",
      "step = 60: loss = 1.340000033378601\n",
      "step = 70: loss = 1.4600000381469727\n",
      "step = 80: loss = 1.190000057220459\n"
     ]
    }
   ],
   "source": [
    "#start the timer and training\n",
    "start_time = time.time()\n",
    "\n",
    "metric_results, agent = train_perarm.train_perarm(\n",
    "    agent = agent,\n",
    "    reward_spec = reward_tensor_spec,\n",
    "    epsilon = HPARAMS['epsilon'],\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    num_iterations = TRAINING_LOOPS,\n",
    "    steps_per_loop = STEPS_PER_LOOP,\n",
    "    global_step = global_step,\n",
    "    train_summary_writer = train_summary_writer,\n",
    "    strategy = distribution_strategy,\n",
    "    # data\n",
    "    batch_size = HPARAMS['batch_size'],\n",
    "    # functions\n",
    "    _trajectory_fn = _trajectory_fn,\n",
    "    # train intervals\n",
    "    chkpt_interval = CHKPT_INTERVAL,\n",
    "    log_interval = LOG_INTERVAL,\n",
    "    # dirs\n",
    "    bucket_name = BUCKET_NAME,\n",
    "    data_dir_prefix_path = f\"{EXAMPLE_GEN_GCS_PATH}\",\n",
    "    log_dir = LOG_DIR,\n",
    "    model_dir = ARTIFACTS_DIR,\n",
    "    chkpoint_dir = CHECKPT_DIR,\n",
    "    use_gpu = True,\n",
    "    use_tpu = False,\n",
    "    profiler = False,\n",
    "    is_testing = IS_TESTING,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "runtime_mins = int((end_time - start_time) / 60)\n",
    "print(f\"complete train job in {runtime_mins} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "177aa3ab-05b4-48f9-b434-9335bd9eb54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2931468"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(metric_results)\n",
    "metric_results[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ba65417f-54e0-436e-b0d4-93cffb981ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(metric_results)\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61464938-a3e7-4ab0-9149-4a9124199dc1",
   "metadata": {},
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9108f8b6-7aea-48d6-a763-461b30671c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "083c2351-5ac8-4218-9bef-e249777aee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=$LOG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ba747-00d5-4908-8d0b-334ae9d2c791",
   "metadata": {},
   "source": [
    "#### eval trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c5cf9366-d9cd-4c9d-951a-99a50877a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82506e-95d7-4a4e-a821-e08512615db7",
   "metadata": {},
   "source": [
    "After training, our `agent.policy` is used as the deployment \"model\" that will generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6c82396a-7822-41fb-830f-1f6eb9c9b213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy at 0x7fae883d1ba0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment_agent = agent.policy\n",
    "deployment_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f6706224-ef5b-4185-bde4-bccb7c8e7ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating trained Agent...\n",
      "post-train val_loss     : 1.2780730724334717\n",
      "post-train eval runtime : 0\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Evaluate the agent's policy once after training\n",
    "# ====================================================\n",
    "print(f\"evaluating trained Agent...\")\n",
    "\n",
    "post_policy_tf = py_tf_eager_policy.PyTFEagerPolicy(deployment_agent, use_tf_function=True)\n",
    "start_time = time.time()\n",
    "\n",
    "val_loss, preds, tr_rewards = eval_perarm._run_bandit_eval(\n",
    "    policy = post_policy_tf,\n",
    "    data = eval_ds,\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"post-train val_loss     : {val_loss}\")\n",
    "print(f\"post-train eval runtime : {runtime_mins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f899120f-3706-4b72-8f2c-2169221dd66b",
   "metadata": {},
   "source": [
    "## Using Trained Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b0f691-860e-4853-928e-4c7ece714dd0",
   "metadata": {},
   "source": [
    "### load trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ea029f73-0638-462a-8f00-54b05c54d69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/artifacts\n"
     ]
    }
   ],
   "source": [
    "POLICY_URI = ARTIFACTS_DIR\n",
    "\n",
    "print(POLICY_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "66d1d819-0152-437c-aefc-45ffd34d808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/artifacts/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/artifacts/fingerprint.pb\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/artifacts/policy_specs.pbtxt\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/artifacts/saved_model.pb\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/artifacts/assets/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-supervised-bandits-v1/run-20240313-193640/artifacts/variables/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $POLICY_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6aad948e-e346-4da3-8cb2-747435c8dc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.py_tf_eager_policy.SavedModelPyTFEagerPolicy at 0x7fae806f7be0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy\n",
    "\n",
    "trained_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n",
    "    POLICY_URI, load_specs_from_pbtxt=True\n",
    ")\n",
    "\n",
    "trained_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "483f8fa5-cbd1-4e3c-8e62-47f91f42e137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action_spec', minimum=0, maximum=1)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_policy.action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8f658891-bc09-49b8-bc58-000a7926df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFER_SIZE = 1\n",
    "dummy_arm = tf.zeros([INFER_SIZE, PER_ARM_DIM], dtype=tf.float32)\n",
    "\n",
    "SKIP_NUM = 10\n",
    "\n",
    "for x in eval_ds.skip(SKIP_NUM).take(INFER_SIZE):\n",
    "    # get feature tensors\n",
    "    \n",
    "    # global_feat_infer = _get_global_context_features(x)\n",
    "    # arm_feat_infer = _get_per_arm_features(x)\n",
    "    \n",
    "    global_feat_infer = embs._get_global_context_features(x)\n",
    "    arm_feat_infer = embs._get_per_arm_features(x)\n",
    "    \n",
    "    # rewards = _get_rewards(x)\n",
    "    rewards = reward_factory._get_rewards(x)\n",
    "    \n",
    "    # reshape arm features\n",
    "    arm_feat_infer = tf.reshape(arm_feat_infer, [HPARAMS['eval_batch_size'], PER_ARM_DIM]) # perarm_dim\n",
    "    concat_arm = tf.concat([arm_feat_infer, dummy_arm], axis=0)\n",
    "    \n",
    "    # flatten global\n",
    "    flat_global_infer = tf.reshape(global_feat_infer, [GLOBAL_DIM])\n",
    "    feature = {'global': flat_global_infer, 'per_arm': concat_arm}\n",
    "    \n",
    "    # get actual reward\n",
    "    actual_reward = rewards.numpy()[0]\n",
    "    \n",
    "    # build trajectory step\n",
    "    trajectory_step = train_utils._get_eval_step(feature, actual_reward)\n",
    "    \n",
    "    prediction = trained_policy.action(trajectory_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "49ce41ed-41b7-404d-9796-1658e7955894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
       " 'observation': {'global': <tf.Tensor: shape=(72,), dtype=float32, numpy=\n",
       "array([ 0.02686488,  0.04024548,  0.02308411, -0.02001072, -0.04564461,\n",
       "       -0.01644728,  0.01685527,  0.03973028, -0.03262532,  0.02532138,\n",
       "       -0.00777791, -0.02473327, -0.01311972, -0.03418459, -0.00583006,\n",
       "       -0.01100175, -0.04789207, -0.01735733, -0.0450286 ,  0.03365994,\n",
       "       -0.02977235,  0.0157761 , -0.01726376,  0.04136987, -0.00549601,\n",
       "        0.04077182, -0.00583255, -0.01910355, -0.00927299,  0.03418962,\n",
       "        0.02432555, -0.03775217,  0.01009493,  0.02008145, -0.03067734,\n",
       "        0.0117029 ,  0.03952468,  0.04534792, -0.02857381, -0.02307111,\n",
       "       -0.00746765,  0.03818199,  0.04199522,  0.03592339,  0.00161424,\n",
       "       -0.03737838,  0.04256186,  0.00249221, -0.04535607,  0.02894286,\n",
       "       -0.01138566, -0.02463037, -0.03271401, -0.04997932,  0.01932846,\n",
       "        0.01135371,  0.01531801,  0.01694382, -0.04856749,  0.03487397,\n",
       "        0.03813579, -0.026586  ,  0.0091952 ,  0.0355577 ,  0.00053481,\n",
       "        0.0023469 ,  0.00912814,  0.02631464, -0.03674991,  0.04484452,\n",
       "        0.02695623, -0.01823224], dtype=float32)>,\n",
       "                 'per_arm': <tf.Tensor: shape=(2, 64), dtype=float32, numpy=\n",
       "array([[-0.03819902, -0.00963265,  0.02579476, -0.00280404, -0.00252074,\n",
       "         0.00483309, -0.03345426, -0.04098202, -0.03691676, -0.04223515,\n",
       "         0.00595496, -0.02698987,  0.02475912,  0.0415615 ,  0.03490516,\n",
       "         0.03919195, -0.03368784, -0.04664968, -0.04624866,  0.01436097,\n",
       "         0.02693458, -0.03615444, -0.02702515,  0.01146175,  0.03049691,\n",
       "         0.030593  , -0.01151949, -0.01228634,  0.04001122, -0.00657036,\n",
       "        -0.04044862,  0.00084016,  0.04357857,  0.03752908,  0.00150322,\n",
       "        -0.04452466, -0.03336992,  0.00519491,  0.01589949, -0.04609166,\n",
       "         0.03201555, -0.03351036, -0.02021856,  0.01854335, -0.00607276,\n",
       "         0.0152379 , -0.0168758 ,  0.03579767,  0.11524488,  0.04350803,\n",
       "         0.02395778,  0.05809326, -0.00364802, -0.07442126,  0.02335799,\n",
       "        -0.07629991,  0.13602136,  0.04747006,  0.02962836, -0.08189921,\n",
       "         0.07996084, -0.08975591,  0.08651204, -0.09866363],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "      dtype=float32)>},\n",
       " 'reward': <tf.Tensor: shape=(), dtype=float32, numpy=3.0>,\n",
       " 'step_type': <tf.Tensor: shape=(), dtype=int32, numpy=0>})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f515fcd0-2228-4f68-9ef9-665ce1fc3fb2",
   "metadata": {},
   "source": [
    "### view prediction response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6af4c297-d2e8-4f7b-bed5-e1da997dfe34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=array(1, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.425938, 3.507738], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd58d4b-6749-4177-8342-83ee969408c9",
   "metadata": {},
   "source": [
    "#### chosen action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d49adbbb-f747-4e4c-b82d-df88373c644f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1, dtype=int32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd12013-1d12-4197-8254-cef9138df42c",
   "metadata": {},
   "source": [
    "#### Per Arm Policy Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d6126290-546b-4702-8a1e-91f18adb05c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.425938, 3.507738], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f66961-ebb5-4ae5-8e6f-ad73114d8353",
   "metadata": {},
   "source": [
    "**Finished**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
