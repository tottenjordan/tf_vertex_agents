{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385307d6-2058-47ec-8da0-57f7ef5c43d6",
   "metadata": {},
   "source": [
    "# Train Bandits with per-arm features\n",
    "\n",
    "**Exploring linear and nonlinear** (e.g., those with neural network-based value functions) bandit methods for recommendations using TF-Agents\n",
    "\n",
    "> Neural linear bandits provide a nice way to leverage the representation power of deep learning and the bandit approach for uncertainty measure and efficient exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e9956-66cd-4bf4-9b4d-8c2c646f0313",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "In this notebook, we explore the following topics for training contextual bandits with per-arm features:\n",
    "\n",
    "1. Data preperation\n",
    "2. Sampling functions\n",
    "3. TensorSpecs\n",
    "4. Agent, Network, training policy\n",
    "5. Reward function\n",
    "6. Trajectory function\n",
    "7. Train & Eval loops\n",
    "8. Getting predictions -\n",
    "9. Preparing the training application - abstracting all steps above to be used in subsequent notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fd257-b98b-426a-a2cd-024429b014f1",
   "metadata": {},
   "source": [
    "## Load notebook config\n",
    "\n",
    "* use the prefix defined in `00-env-setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f39c9d08-d118-4013-a47f-88450f49f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX: rec-bandits-v2\n"
     ]
    }
   ],
   "source": [
    "# PREFIX = 'mabv1'\n",
    "VERSION        = \"v2\"                       # TODO\n",
    "PREFIX         = f'rec-bandits-{VERSION}'   # TODO\n",
    "\n",
    "print(f\"PREFIX: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908f6b95-b539-4a9f-a836-840d26ea3b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"rec-bandits-v2\"\n",
      "VERSION                  = \"v2\"\n",
      "\n",
      "BUCKET_NAME              = \"rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "BUCKET_URI               = \"gs://rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "DATA_PATH_KFP_DEMO       = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data/kfp_demo_data/u.data\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_NAME    = \"mvlens_rec_bandits_v2\"\n",
      "BIGQUERY_TABLE_NAME      = \"training_dataset\"\n",
      "\n",
      "REPOSITORY               = \"rl-movielens-rec-bandits-v2\"\n",
      "\n",
      "DOCKERNAME_01            = \"Dockerfile_train_my_perarm_env\"\n",
      "IMAGE_NAME_01            = \"train-my-perarm-env-v2\"\n",
      "IMAGE_URI_01             = \"gcr.io/hybrid-vertex/train-my-perarm-env-v2\"\n",
      "\n",
      "DOCKERNAME_02            = \"Dockerfile_perarm_feats\"\n",
      "IMAGE_NAME_02            = \"train-perarm-feats-v2\"\n",
      "IMAGE_URI_02             = \"gcr.io/hybrid-vertex/train-perarm-feats-v2\"\n",
      "\n",
      "DOCKERNAME_03            = \"Dockerfile_ranking_bandit\"\n",
      "IMAGE_NAME_03            = \"train-rank-bandit-v2\"\n",
      "IMAGE_URI_03             = \"gcr.io/hybrid-vertex/train-rank-bandit-v2\"\n",
      "\n",
      "DOCKERNAME_04            = \"Dockerfile_train_mab_e2e\"\n",
      "IMAGE_NAME_04            = \"train-mab-e2e-v2\"\n",
      "IMAGE_URI_04             = \"gcr.io/hybrid-vertex/train-mab-e2e-v2\"\n",
      "\n",
      "DOCKERNAME_04_pred       = \"Dockerfile_pred_mab_e2e\"\n",
      "IMAGE_NAME_04_pred       = \"pred-mab-e2e-v2\"\n",
      "IMAGE_URI_04_pred        = \"gcr.io/hybrid-vertex/pred-mab-e2e-v2\"\n",
      "\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-rec-bandits-v2/local_docker_tfa\"\n",
      "\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "RL_SUB_DIR               = \"per_arm_rl\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c690a9-e2bd-4759-ba41-4e2469098aee",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d0dfe4-695c-4dd4-9f24-67f7488ce1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c669f1a1-1af7-4efb-ab2d-6bf3b3847991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pprint import pprint\n",
    "import pickle as pkl\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "# from tf_agents.agents import TFAgent\n",
    "\n",
    "# from tf_agents.bandits.environments import stationary_stochastic_per_arm_py_environment as p_a_env\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "# from tf_agents.drivers import dynamic_step_driver\n",
    "# from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "# from tf_agents.bandits.agents import lin_ucb_agent\n",
    "# from tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\n",
    "from tf_agents.bandits.agents import neural_epsilon_greedy_agent\n",
    "from tf_agents.bandits.agents import neural_linucb_agent\n",
    "from tf_agents.bandits.networks import global_and_arm_feature_network\n",
    "from tf_agents.bandits.policies import policy_utilities\n",
    "\n",
    "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "nest = tf.nest\n",
    "\n",
    "# GPU\n",
    "from numba import cuda \n",
    "import gc\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# this repo\n",
    "from src import train_utils as train_utils\n",
    "from src import reward_factory as reward_factory\n",
    "from src.data import data_utils as data_utils\n",
    "from src.data import data_config as data_config\n",
    "from src.trainer import eval_perarm as eval_perarm\n",
    "from src.trainer import train_perarm as train_perarm\n",
    "from src.agents import agent_factory as agent_factory\n",
    "from src.networks import encoding_network as emb_features\n",
    "\n",
    "# tf exceptions and vars\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e58dd7-ab2b-419f-9771-bf1e98db758b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4969d3e-1fc0-45db-8a69-aa6b342019de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "274e7f4a-1802-4946-888e-876638f5c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a866b1-85b9-43e6-9546-edfbbf886bce",
   "metadata": {},
   "source": [
    "# [1] Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d4ed28-23d7-4785-b327-e5b543b0edb9",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "* Load train and eval datasets from TFRecords created in the `01-movielens-data-prep.ipynb` notebook\n",
    "* training examples represent historical (previously collected) interaction data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc3fcebe-818b-4767-afdc-cfb65b3b953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCS_DATA_PATH: gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/train/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/val/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/vocabs/\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_GEN_GCS_PATH = data_config.EXAMPLE_GEN_GCS_PATH\n",
    "GCS_DATA_PATH = f\"{BUCKET_URI}/{EXAMPLE_GEN_GCS_PATH}\"\n",
    "\n",
    "print(f\"GCS_DATA_PATH: {GCS_DATA_PATH}\")\n",
    "\n",
    "!gsutil ls $GCS_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5b953-14c0-42ed-a511-77147a1bc0ac",
   "metadata": {},
   "source": [
    "### Read TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d0157c8-a04c-4dbd-b6d9-a1ede97687a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da0cdac-76af-4608-b70e-be7dbe0584b5",
   "metadata": {},
   "source": [
    "**Train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c934c06-bf08-4c7f-b0cc-0de04ef3515c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/train/ml-1m-gen-001-of-008.tfrecord',\n",
       " 'gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/train/ml-1m-gen-002-of-008.tfrecord',\n",
       " 'gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/train/ml-1m-gen-003-of-008.tfrecord']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT = \"train\" # \"train\" | \"val\"\n",
    "\n",
    "train_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{EXAMPLE_GEN_GCS_PATH}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "train_files = train_files[:3]\n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7511e4d-bf81-4800-bde7-8b16dec9aeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_movie_genres': <tf.Tensor: shape=(1, 10), dtype=string, numpy=\n",
      "array([[b'Drama', b'UNK', b'UNK', b'UNK', b'UNK', b'UNK', b'UNK', b'UNK',\n",
      "        b'UNK', b'UNK']], dtype=object)>,\n",
      " 'target_movie_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'1775'], dtype=object)>,\n",
      " 'target_movie_rating': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([4.], dtype=float32)>,\n",
      " 'target_movie_title': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Live Flesh (1997)'], dtype=object)>,\n",
      " 'target_movie_year': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1997])>,\n",
      " 'target_rating_timestamp': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([974612615])>,\n",
      " 'user_age': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([50])>,\n",
      " 'user_gender': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'M'], dtype=object)>,\n",
      " 'user_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'2173'], dtype=object)>,\n",
      " 'user_occupation_text': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'programmer'], dtype=object)>,\n",
      " 'user_zip_code': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'87505'], dtype=object)>}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "train_dataset = train_dataset.map(data_utils._parse_function)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452750c1-46a0-4ee1-84d4-0c3b227de38f",
   "metadata": {},
   "source": [
    "**Val data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7b5376f-9438-4304-84aa-de09c30f5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = \"val\"\n",
    "\n",
    "val_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{EXAMPLE_GEN_GCS_PATH}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        val_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "val_dataset = tf.data.TFRecordDataset(val_files)\n",
    "val_dataset = val_dataset.map(data_utils._parse_function, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8959967-522e-41c8-9a1b-050ca8bc191f",
   "metadata": {},
   "source": [
    "### get vocab\n",
    "\n",
    "**TODO:** \n",
    "* streamline vocab calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3ccf137-7a72-42e7-aa89-3c81a99cf40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vocab...\n",
      "Downloaded vocab from: gs://rec-bandits-v2-hybrid-vertex-bucket/data/movielens/m1m/vocabs/vocab_dict.pkl\n",
      "\n",
      "'movie_id'\n",
      "'movie_year'\n",
      "'movie_genre'\n",
      "'movie_title'\n",
      "'user_id'\n",
      "'user_gender_vocab'\n",
      "'user_age_vocab'\n",
      "'user_occ_vocab'\n",
      "'user_zip_vocab'\n",
      "'min_timestamp'\n",
      "'max_timestamp'\n",
      "'timestamp_buckets'\n"
     ]
    }
   ],
   "source": [
    "EXISTING_VOCAB_FILE = f'gs://{BUCKET_NAME}/{EXAMPLE_GEN_GCS_PATH}/vocabs/{VOCAB_FILENAME}'\n",
    "print(f\"Downloading vocab...\")\n",
    "\n",
    "os.system(f'gsutil -q cp {EXISTING_VOCAB_FILE} .')\n",
    "print(f\"Downloaded vocab from: {EXISTING_VOCAB_FILE}\\n\")\n",
    "\n",
    "filehandler = open(VOCAB_FILENAME, 'rb')\n",
    "vocab_dict = pkl.load(filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "for key in vocab_dict.keys():\n",
    "    pprint(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfda012c-a2c3-4384-a5a4-54f5c6649006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocab_dict['user_occupation_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0138a295-2b0d-4359-8496-af8552ec8cff",
   "metadata": {},
   "source": [
    "# [2] Preprocessing layers for global and arm features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28c002-8d3a-4cf4-b69b-de56a6abab90",
   "metadata": {},
   "source": [
    "The preproccesing layers will ultimately feed the two sampling functions described below. These sampling functions will be used to create [trajectories](https://github.com/tensorflow/agents/blob/master/tf_agents/trajectories/trajectory.py#L36) (i.e., the training examples for our model)\n",
    "\n",
    "`global_context_sampling_fn`: \n",
    "* A function that outputs a random 1d array or list of ints or floats\n",
    "* This output is the global context. Its shape and type must be consistent across calls.\n",
    "\n",
    "`arm_context_sampling_fn`: \n",
    "* A function that outputs a random 1 array or list of ints or floats (same type as the output of `global_context_sampling_fn`). * This output is the per-arm context. Its shape must be consistent across calls.\n",
    "\n",
    "See `debug-and-learn-emb-models.ipynb` for more details and debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ea1f7-0621-44b8-b062-1f59b0964d72",
   "metadata": {},
   "source": [
    "## Global & Per-Arm feature embedding models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a1063-15d4-472e-b5a7-d92dcdea3c0f",
   "metadata": {},
   "source": [
    "**get expected dimensions**\n",
    "\n",
    "**common layers**\n",
    "* layer sizes for the final tower\n",
    "* The network that takes as input the concatenation of \n",
    "  the outputs of the global and the arm networks\n",
    "*  hidden layers ideally divisible by 8, e.g., [16, 32, 64, 128, 256, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8d77956-635c-438a-916a-185eec52f9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPECTED_GLOBAL_DIM: 72\n",
      "EXPECTED_PER_ARM_DIM: 64\n",
      "EXPECTED_GLOBAL_LAYERS      : [72, 36, 18]\n",
      "EXPECTED_ARM_LAYERS         : [64, 32, 16]\n",
      "EXPECTED_COMMON_LAYERS      : [34, 17, 8]\n"
     ]
    }
   ],
   "source": [
    "NUM_OOV_BUCKETS        = 1\n",
    "GLOBAL_EMBEDDING_SIZE  = 12\n",
    "MV_EMBEDDING_SIZE      = 16\n",
    "\n",
    "NUM_GLOBAL_FEATURES = len(data_utils.USER_FEATURE_NAMES)     # 6\n",
    "NUM_ARM_FEATURES    = len(data_utils.MOVIE_FEATURE_NAMES)    # 5\n",
    "EXPECTED_GLOBAL_DIM  = GLOBAL_EMBEDDING_SIZE * NUM_GLOBAL_FEATURES\n",
    "EXPECTED_PER_ARM_DIM = MV_EMBEDDING_SIZE * NUM_ARM_FEATURES\n",
    "print(f\"EXPECTED_GLOBAL_DIM: {EXPECTED_GLOBAL_DIM}\")\n",
    "print(f\"EXPECTED_PER_ARM_DIM: {EXPECTED_PER_ARM_DIM}\")\n",
    "\n",
    "EXPECTED_GLOBAL_LAYERS   = [\n",
    "    EXPECTED_GLOBAL_DIM, \n",
    "    int(EXPECTED_GLOBAL_DIM/2), \n",
    "    int(EXPECTED_GLOBAL_DIM/4)\n",
    "]\n",
    "EXPECTED_ARM_LAYERS      = [\n",
    "    EXPECTED_PER_ARM_DIM, \n",
    "    int(EXPECTED_PER_ARM_DIM/2), \n",
    "    int(EXPECTED_PER_ARM_DIM/4)\n",
    "]\n",
    "EXPECTED_FIRST_COMMON_LAYER = EXPECTED_GLOBAL_LAYERS[-1] + EXPECTED_ARM_LAYERS[-1]\n",
    "EXPECTED_COMMON_LAYERS = [\n",
    "    int(EXPECTED_FIRST_COMMON_LAYER), \n",
    "    int(EXPECTED_FIRST_COMMON_LAYER/2), \n",
    "    int(EXPECTED_FIRST_COMMON_LAYER/4)\n",
    "]\n",
    "print(f\"EXPECTED_GLOBAL_LAYERS      : {EXPECTED_GLOBAL_LAYERS}\")\n",
    "print(f\"EXPECTED_ARM_LAYERS         : {EXPECTED_ARM_LAYERS}\")\n",
    "print(f\"EXPECTED_COMMON_LAYERS      : {EXPECTED_COMMON_LAYERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c33673e-6069-477a-af80-0d2c436099bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.perarm_features import emb_feature_v2 as emb_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea335991-dcec-40b3-ba0d-a0fdfb9c2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test emb models\n",
    "for i in range(1):\n",
    "    \n",
    "    iterator = iter(train_dataset.batch(1))\n",
    "    data = next(iterator)\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48178fff-491f-4f03-b156-7765d1fa9707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.networks.encoding_network.EmbeddingModel at 0x7f4ae8c08b50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = emb_features.EmbeddingModel(\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "903c63c5-2468-4443-bcb8-5ff6d69810dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL_DIM: 72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 72), dtype=float32, numpy=\n",
       "array([[ 0.02488706,  0.01412397,  0.04554066,  0.01065205, -0.03262987,\n",
       "         0.03435917,  0.02582318, -0.02402839, -0.01531162,  0.03759971,\n",
       "         0.0169683 , -0.02016693, -0.04059341, -0.01467885, -0.0305671 ,\n",
       "        -0.01837245, -0.01828884,  0.04889296, -0.03605819,  0.00681466,\n",
       "         0.0050807 ,  0.04025793,  0.02148074,  0.00070983, -0.00647629,\n",
       "         0.0389542 ,  0.01500158, -0.00678085, -0.04015653,  0.0113899 ,\n",
       "        -0.00628312, -0.00262224,  0.03170909, -0.03163417, -0.00797208,\n",
       "         0.03994516, -0.02699273, -0.0435128 ,  0.01885439,  0.03103975,\n",
       "        -0.01578783, -0.0409723 ,  0.01237534,  0.01298692, -0.01935979,\n",
       "        -0.00373379,  0.01030274, -0.04423375, -0.01168343, -0.01009633,\n",
       "         0.04010144,  0.00215397, -0.00599377,  0.04436407, -0.03529128,\n",
       "        -0.01563434,  0.01158167,  0.0239563 ,  0.01573116, -0.01842538,\n",
       "        -0.04140295, -0.02108539, -0.01197152,  0.02811264,  0.01294304,\n",
       "         0.03103543,  0.00253682, -0.02895523,  0.00776571, -0.01971843,\n",
       "         0.01044537, -0.04143513]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_globals = embs._get_global_context_features(data)\n",
    "\n",
    "GLOBAL_DIM = test_globals.shape[1]            \n",
    "# shape checks out at batch_dim, nactions, arm feats\n",
    "print(f\"GLOBAL_DIM: {GLOBAL_DIM}\")\n",
    "\n",
    "test_globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebec4863-c614-4248-b680-8e29ec5b730d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER_ARM_DIM: 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 64), dtype=float32, numpy=\n",
       "array([[-0.01906179,  0.01534501, -0.0080623 , -0.04132438, -0.01456348,\n",
       "        -0.034427  ,  0.00314468,  0.03209409, -0.00505042,  0.03367421,\n",
       "         0.00469019,  0.00168335, -0.01414454, -0.0255244 ,  0.02317021,\n",
       "        -0.03609393, -0.00885345, -0.0272608 , -0.02871018,  0.03678447,\n",
       "         0.0407989 ,  0.01179961,  0.02757061,  0.01379795,  0.03976682,\n",
       "        -0.01703257, -0.00486839,  0.01115931, -0.0022273 , -0.01444559,\n",
       "         0.00243389, -0.00807588,  0.02580902,  0.02842895,  0.0045892 ,\n",
       "         0.0180091 , -0.02443273, -0.0133191 , -0.01889695, -0.04196911,\n",
       "        -0.04428039, -0.03632004, -0.01538169, -0.03424739,  0.03288931,\n",
       "         0.03261143,  0.03978003,  0.01413143,  0.14730477,  0.01480455,\n",
       "         0.04315525,  0.17966533, -0.19734389,  0.22788511, -0.09086376,\n",
       "         0.14531867,  0.15178806, -0.12877911,  0.06057346,  0.16690427,\n",
       "         0.20636664, -0.16504705,  0.04890133, -0.05306672]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arms = embs._get_per_arm_features(data)\n",
    "\n",
    "PER_ARM_DIM = test_arms.shape[1]            \n",
    "# shape checks out at batch_dim, nactions, arm feats\n",
    "print(f\"PER_ARM_DIM: {PER_ARM_DIM}\")\n",
    "\n",
    "test_arms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb4902-5bb9-4587-8d29-c01d78b006be",
   "metadata": {},
   "source": [
    "# [3] TensorSpecs\n",
    "\n",
    "* say you have a global observation spec of `[17]`. And you have two batch dimensions `[4, 5]`. Then your observation has to have the shape `[4, 5, 17]`\n",
    "* and then if you have arm_obs_spec with shape `[9, 13]`, then the arm obs shape has to be exactly `[4, 5, 9, 13]`\n",
    "* and this has to be true for every single tensor in your tensor nest\n",
    "* the first 2 dims are the outer dims that are the same for all tensors, the rest of the dimensions have to follow the spec for each tensor\n",
    "\n",
    "**TODO:**\n",
    "* explain relationship between Tensor Specs and their Tensor counterparts\n",
    "* highlight the errors, lessons learned, and utility functions to address these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ff9baaf-987d-448d-a981-742a79b581e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE      : 128\n",
      "EVAL_BATCH_SIZE : 1\n",
      "NUM_ACTIONS     : 2\n",
      "GLOBAL_DIM      : 72\n",
      "PER_ARM_DIM     : 64\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE      = 128\n",
    "EVAL_BATCH_SIZE = 1\n",
    "\n",
    "NUM_ACTIONS     = 2 #this is kinda deceptive - \n",
    "#our approach is to learn by \"flashing\" one movie rating at a time per user context. \n",
    "#The n_actions = show/don't show the movie with one degree of freedom (n-1)\n",
    "\n",
    "\n",
    "print(f\"BATCH_SIZE      : {BATCH_SIZE}\")\n",
    "print(f\"EVAL_BATCH_SIZE : {EVAL_BATCH_SIZE}\")\n",
    "print(f\"NUM_ACTIONS     : {NUM_ACTIONS}\")\n",
    "print(f\"GLOBAL_DIM      : {GLOBAL_DIM}\")\n",
    "print(f\"PER_ARM_DIM     : {PER_ARM_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20eca8d-8c73-4ec8-9d0f-f2b428055ac2",
   "metadata": {},
   "source": [
    "## Implementing MAB with TF-Agents\n",
    "\n",
    "In TF-Agents, the *per-arm features* implementation differs from the *global-only* feature examples in the following aspects:\n",
    "* Reward is modeled not per-arm, but globally.\n",
    "* The arms are permutation invariant: it doesn’t matter which arm is arm 1 or arm 2, only their features.\n",
    "* One can have a different number of arms to choose from in every step (note that unspecified/dynamically changing number of arms will have a problem with XLA compatibility).\n",
    "\n",
    "When implementing per-arm features in TF-Bandits, the following details have to be discussed:\n",
    "* Observation spec and observations,\n",
    "* Action spec and actions,\n",
    "* Implementation of specific policies and agents.\n",
    "\n",
    "\n",
    "**TODO:**\n",
    "* outline the components and highlight their interactions, dependencies on eachother, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68ebe8-116d-43b3-a6e1-4f5a5c7f4741",
   "metadata": {},
   "source": [
    "### Observation spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c4746-d01b-4ca4-aa53-ab68da54d37a",
   "metadata": {},
   "source": [
    "**This observation spec allows the user to have a global observation of fixed dimension**, and an unspecified number of *per-arm* features (also of fixed dimension)\n",
    "* The actions output by the policy are still integers as usual, and they indicate which row of the arm-features it has chosen \n",
    "* The action spec must be a single integer value without boundaries:\n",
    "\n",
    "```python\n",
    "global_spec = tensor_spec.TensorSpec([GLOBAL_DIM], tf.float32)\n",
    "per_arm_spec = tensor_spec.TensorSpec([None, PER_ARM_DIM], tf.float32)\n",
    "observation_spec = {'global': global_spec, 'per_arm': per_arm_spec}\n",
    "\n",
    "action_spec = tensor_spec.TensorSpec((), tf.int32)\n",
    "```\n",
    "> Here the only difference compared to the action spec with global features only is that the tensor spec is not bounded, as we don’t know how many arms there will be at any time step\n",
    "\n",
    "**XLA compatibility:**\n",
    "* Since dynamic tensor shapes are not compatible with XLA, the number of arm features (and consequently, number of arms for a step) cannot be dynamic. \n",
    "* One workaround is to fix the maximum number of arms for a problem, then pad the arm features in steps with fewer arms, and use action masking to indicate how many arms are actually active.\n",
    "\n",
    "```python\n",
    "per_arm_spec = tensor_spec.TensorSpec([NUM_ACTIONS, PER_ARM_DIM], tf.float32)\n",
    "\n",
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=(), dtype=tf.int32, minimum = 0, maximum = NUM_ACTIONS - 1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36bd3b33-635a-4274-8b9e-7172696ebb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_spec = {\n",
    "    'global': tf.TensorSpec([GLOBAL_DIM], tf.float32),\n",
    "    'per_arm': tf.TensorSpec([NUM_ACTIONS, PER_ARM_DIM], tf.float32) #excluding action dim here\n",
    "}\n",
    "observation_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2da92-7db2-4f42-94a7-b7bad1c8fc42",
   "metadata": {},
   "source": [
    "### Action spec\n",
    "\n",
    "> The time_step_spec and action_spec are specifications for the input time step and the output action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af42c7-18d7-480f-a7f3-d7da3f4840eb",
   "metadata": {},
   "source": [
    "```python\n",
    "    if (\n",
    "        not tensor_spec.is_bounded(action_spec)\n",
    "        or not tensor_spec.is_discrete(action_spec)\n",
    "        or action_spec.shape.rank > 1\n",
    "        or action_spec.shape.num_elements() != 1\n",
    "    ):\n",
    "      raise NotImplementedError(\n",
    "          'action_spec must be a BoundedTensorSpec of type int32 and shape (). '\n",
    "          'Found {}.'.format(action_spec)\n",
    "      )\n",
    "```\n",
    "\n",
    "* [src](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/policies/reward_prediction_base_policy.py#L97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "549a123c-349a-4103-b39a-4502f47d1e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=[], \n",
    "    dtype=tf.int32,\n",
    "    minimum=tf.constant(0),            \n",
    "    maximum=NUM_ACTIONS-1, # n degrees of freedom and will dictate the expected mean reward spec shape\n",
    "    name=\"action_spec\"\n",
    ")\n",
    "\n",
    "action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36ee2635-d4e3-4468-886c-ae9c62e3c80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected_num_actions: 2\n",
      "predicted_rewards_mean: TensorSpec(shape=(2,), dtype=tf.float32, name=None)\n"
     ]
    }
   ],
   "source": [
    "expected_num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "print(f\"expected_num_actions: {expected_num_actions}\")\n",
    "\n",
    "predicted_rewards_mean = tensor_spec.TensorSpec([expected_num_actions])\n",
    "print(f\"predicted_rewards_mean: {predicted_rewards_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a51efce-9f1b-42d1-bec4-7b788e3fd7e0",
   "metadata": {},
   "source": [
    "### TimeStep spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95f05860-0fbf-4a5a-8273-9c81761e0ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': {'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None),\n",
       "                 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)},\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec = ts.time_step_spec(\n",
    "    observation_spec = observation_spec, \n",
    "    # reward_spec = _reward_spec\n",
    ")\n",
    "time_step_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b129a-6d19-4b3d-a2e7-e27070f57ac0",
   "metadata": {},
   "source": [
    "### Reward Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b48e89aa-e010-4bd9-a7e0-ad62dd4c5949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': TensorSpec(shape=(128,), dtype=tf.float32, name='reward')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.specs import array_spec\n",
    "\n",
    "reward_spec = {\n",
    "    \"reward\": array_spec.ArraySpec(shape=[BATCH_SIZE], dtype=np.float32, name=\"reward\")\n",
    "}\n",
    "\n",
    "reward_tensor_spec = train_utils.from_spec(reward_spec)\n",
    "reward_tensor_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9181303-6565-45f5-a293-08d50420a805",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Inspect chosen arm features spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6b526a5-997c-4621-b21c-82c6ca1d6a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90d3e03e-619e-4fa4-b817-f02b4cefd6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(64,), dtype=tf.float32, name=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_arm_features_info = (\n",
    "  policy_utilities.create_chosen_arm_features_info_spec(\n",
    "      time_step_spec.observation,\n",
    "  )\n",
    ")\n",
    "chosen_arm_features_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc427092-beb7-4baf-a2e9-b8d10629182c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_policy_type = (\n",
    "    policy_utilities.BanditPolicyType.GREEDY\n",
    ")\n",
    "bandit_policy_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a4a2a33-c7d4-4d81-8082-80dfc3596741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_policy_type = (\n",
    "    policy_utilities.create_bandit_policy_type_tensor_spec(shape=[1])\n",
    ")\n",
    "bandit_policy_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e7ac02b-18bd-4519-92fe-5541b1a8ddc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorSpec(shape=(2,), dtype=tf.float32, name=None), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32)), chosen_arm_features=TensorSpec(shape=(64,), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_spec = policy_utilities.PerArmPolicyInfo(\n",
    "  predicted_rewards_mean=predicted_rewards_mean,\n",
    "  bandit_policy_type=bandit_policy_type,\n",
    "  chosen_arm_features=chosen_arm_features_info,\n",
    ")\n",
    "info_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21f28b9b-8183-495a-89b6-a01f30ea8637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PerArmPolicyInfo(\n",
    "#     log_probability=(), \n",
    "#     predicted_rewards_mean=TensorSpec(shape=(2,), \n",
    "#                                       dtype=tf.float32, name=None), \n",
    "#     multiobjective_scalarized_predicted_rewards_mean=(), \n",
    "#     predicted_rewards_optimistic=(), \n",
    "#     predicted_rewards_sampled=(), \n",
    "#     bandit_policy_type=BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32)), \n",
    "#     chosen_arm_features=TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9c197-ae9b-461d-8956-f078b929ac12",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [4] The Agent\n",
    "\n",
    "**Note** that contextual bandits form a special case of RL, where the actions taken by the agent do not alter the state of the environment \n",
    "\n",
    "> “Contextual” refers to the fact that the agent chooses among a set of actions while having knowledge of the context (environment observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d6e84-688e-48c5-aea1-77df031348c9",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Agent & Network defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aadb01-eb5c-4870-ae14-9e66624ba594",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Agent types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075e8a6-11a9-4346-8725-3653fba4bac4",
   "metadata": {},
   "source": [
    "> `AGENT_TYPE = ['LinUCB', 'LinTS', 'epsGreedy', 'NeuralLinUCB']`\n",
    "\n",
    "1. **LinearUCBAgent**: (`LinUCB`) - An agent implementing the Linear UCB bandit algorithm ([docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/lin_ucb_agent/LinearUCBAgent))\n",
    "\n",
    "2. **LinearThompsonSamplingAgent**: (`LinTS`) - Implements the Linear Thompson Sampling Agent from the [paper](https://arxiv.org/abs/1209.3352): *Thompson Sampling for Contextual Bandits with Linear Payoffs* ([docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/linear_thompson_sampling_agent/LinearThompsonSamplingAgent))\n",
    "\n",
    "3. **NeuralEpsilonGreedyAgent**: (`epsGreedy`) - A neural network based epsilon greedy agent ([docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_epsilon_greedy_agent/NeuralEpsilonGreedyAgent))\n",
    "\n",
    "4. **NeuralLinUCBAgent**: (`NeuralLinUCB`) - An agent implementing the LinUCB algorithm on top of a neural network ([docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_linucb_agent/NeuralLinUCBAgent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8e88e-c8ea-4193-a911-0d974ef3b1a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Network types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547087d-5fad-4521-a584-cb46ce52897d",
   "metadata": {},
   "source": [
    "Which network architecture to use for the `epsGreedy` or `NeuralLinUCB` agents\n",
    "\n",
    "```\n",
    "NETWORK_TYPE = ['commontower', 'dotproduct']\n",
    "```\n",
    "\n",
    "**GlobalAndArmCommonTowerNetwork:** (`commontower`)\n",
    "* This network takes the output of the global and per-arm networks, and leads them through a common network, that in turn outputs reward estimates\n",
    "> * `GLOBAL_LAYERS` - Iterable of ints. Specifies the layers of the global tower\n",
    "> * `ARM_LAYERS` - Iterable of ints. Specifies the layers of the arm tower\n",
    "> * `COMMON_LAYERS` - Iterable of ints. Specifies the layers of the common tower\n",
    "* The network produced by this function can be used either in `GreedyRewardPredictionPolicy`, or `NeuralLinUCBPolicy`\n",
    "> * In the former case, the network must have `output_dim=1`, it is going to be an instance of `QNetwork`, and used in the policy as a reward prediction network\n",
    "> * In the latter case, the network will be an encoding network with its output consumed by a reward layer or a `LinUCB` method. The specified `output_dim` will be the encoding dimension\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmCommonTowerNetwork)\n",
    "\n",
    "**GlobalAndArmDotProductNetwork:** (`dotproduct`)\n",
    "* This network calculates the **dot product** of the output of the global and per-arm networks and returns them as reward estimates\n",
    "> * `GLOBAL_LAYERS` - Iterable of ints. Specifies the layers of the global tower\n",
    "> * `ARM_LAYERS` - Iterable of ints. Specifies the layers of the arm tower\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmDotProductNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3dc270-deb5-4e96-8276-74759a06c318",
   "metadata": {},
   "source": [
    "### Agent config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbe29263-453d-405e-ac50-fee7f379f335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick check on the inputs of the agent - this can be used to diagnose spec shape inputs\n",
      "\n",
      "time_step_spec:  TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': {'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None),\n",
      "                 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)},\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n",
      "\n",
      "action_spec:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))\n",
      "\n",
      "observation_spec:  {'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Quick check on the inputs of the agent - this can be used to diagnose spec shape inputs\")\n",
    "print(\"\\ntime_step_spec: \", time_step_spec)\n",
    "print(\"\\naction_spec: \", action_spec)\n",
    "print(\"\\nobservation_spec: \", observation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d817bf2-1fa6-4bae-af90-745fad996b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128,\n",
      " 'common_layers': [34, 8],\n",
      " 'encoding_dim': 1,\n",
      " 'epsilon': 0.01,\n",
      " 'eval_batch_size': 1,\n",
      " 'global_layers': [72, 36, 18],\n",
      " 'learning_rate': 0.05,\n",
      " 'model_type': 'epsGreedy',\n",
      " 'network_type': 'commontower',\n",
      " 'num_actions': 2,\n",
      " 'per_arm_layers': [64, 32, 16]}\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Agents\n",
    "# ================================\n",
    "AGENT_TYPE      = 'epsGreedy' # 'LinUCB' | 'LinTS |, 'epsGreedy' | 'NeuralLinUCB'\n",
    "\n",
    "# Parameters for linear agents (LinUCB and LinTS).\n",
    "AGENT_ALPHA     = 0.1\n",
    "\n",
    "# Parameters for neural agents (NeuralEpsGreedy and NerualLinUCB).\n",
    "EPSILON         = 0.01\n",
    "LR              = 0.05\n",
    "\n",
    "# Parameters for NeuralLinUCB\n",
    "ENCODING_DIM    = 1\n",
    "EPS_PHASE_STEPS = 1000\n",
    "\n",
    "# ================================\n",
    "# Agent's Preprocess Network\n",
    "# ================================\n",
    " # beginning should be of size: GLOBAL_DIM\n",
    "GLOBAL_LAYERS   = [GLOBAL_DIM, int(GLOBAL_DIM/2), int(GLOBAL_DIM/4)]\n",
    "\n",
    "# beginning should be of size: PER_ARM_DIM\n",
    "ARM_LAYERS      = [PER_ARM_DIM, int(PER_ARM_DIM/2), int(PER_ARM_DIM/4)]\n",
    "\n",
    "# ================================\n",
    "# common layers\n",
    "# ================================\n",
    "\"\"\"\n",
    "> layer sizes for the final tower\n",
    "> The network that takes as input the concatenation of \n",
    "  the outputs of the global and the arm networks\n",
    "> hidden layers ideally divisible by 8, e.g., [16, 32, 64, 128, 256, 512]\n",
    "\"\"\"\n",
    "FIRST_COMMON_LAYER = GLOBAL_LAYERS[-1] + ARM_LAYERS[-1] # min(GLOBAL_LAYERS[-1], ARM_LAYERS[-1])\n",
    "\n",
    "COMMON_LAYERS = [\n",
    "    int(FIRST_COMMON_LAYER),\n",
    "    # int(FIRST_COMMON_LAYER/2),\n",
    "    int(FIRST_COMMON_LAYER/4)\n",
    "]\n",
    "\n",
    "NETWORK_TYPE    = \"commontower\" # 'commontower' | 'dotproduct'\n",
    "\n",
    "if AGENT_TYPE == 'NeuralLinUCB':\n",
    "    NETWORK_TYPE = 'commontower'\n",
    "    ENCODING_DIM = COMMON_LAYERS[-1]\n",
    "    \n",
    "if NETWORK_TYPE == 'dotproduct':\n",
    "    assert GLOBAL_LAYERS[0] == ARM_LAYERS[0]\n",
    "\n",
    "\n",
    "HPARAMS = {  # TODO - streamline and consolidate\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"eval_batch_size\" : EVAL_BATCH_SIZE,\n",
    "    \"num_actions\": NUM_ACTIONS,\n",
    "    \"model_type\": AGENT_TYPE,\n",
    "    \"network_type\": NETWORK_TYPE,\n",
    "    \"global_layers\": GLOBAL_LAYERS,\n",
    "    \"per_arm_layers\": ARM_LAYERS,\n",
    "    \"common_layers\": COMMON_LAYERS,\n",
    "    \"learning_rate\": LR,\n",
    "    \"epsilon\": EPSILON,\n",
    "    \"encoding_dim\": ENCODING_DIM\n",
    "}\n",
    "pprint(HPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bf060bb-9880-40b4-8583-dd7a3f915bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent: NeuralEpsGreedyAgent\n",
      "Network: commontower\n"
     ]
    }
   ],
   "source": [
    "# from src.perarm_features import agent_factory as agent_factory\n",
    "\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "agent = agent_factory.PerArmAgentFactory._get_agent(\n",
    "    agent_type = AGENT_TYPE,\n",
    "    network_type = NETWORK_TYPE,\n",
    "    time_step_spec = time_step_spec,\n",
    "    action_spec = action_spec,\n",
    "    observation_spec=observation_spec,\n",
    "    global_layers = GLOBAL_LAYERS,\n",
    "    arm_layers = ARM_LAYERS,\n",
    "    common_layers = COMMON_LAYERS,\n",
    "    agent_alpha = AGENT_ALPHA,\n",
    "    learning_rate = LR,\n",
    "    epsilon = EPSILON,\n",
    "    train_step_counter = global_step,\n",
    "    output_dim = ENCODING_DIM,\n",
    "    eps_phase_steps = EPS_PHASE_STEPS,\n",
    "    summarize_grads_and_vars = True,\n",
    "    debug_summaries = True\n",
    ")\n",
    "      \n",
    "agent.initialize()\n",
    "print(f'agent: {agent.name}')\n",
    "\n",
    "# if network:\n",
    "#     print(f\"Network: {network}\")\n",
    "\n",
    "if NETWORK_TYPE:\n",
    "    print(f\"Network: {NETWORK_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d77d1-a0b3-4898-adf0-0b32bfaf5bd7",
   "metadata": {},
   "source": [
    "#### Inspect the specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78ca56e7-a386-4bf4-8c2b-3d1665521f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41483f5a-bda6-48a4-9e1c-88d8b4afeea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TupleWrapper(TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': DictWrapper({'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.time_step_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f205716-f265-4ca8-81a7-4799de8f60a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TupleWrapper(Trajectory(\n",
       "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32)),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': DictWrapper({'global': TensorSpec(shape=(72,), dtype=tf.float32, name=None)}),\n",
       " 'policy_info': PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorSpec(shape=(2,), dtype=tf.float32, name=None), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32)), chosen_arm_features=TensorSpec(shape=(64,), dtype=tf.float32, name=None)),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.training_data_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d404dae-3cc3-4f81-962b-4641455ca4f2",
   "metadata": {},
   "source": [
    "# [5] Reward function\n",
    "\n",
    "> see `reward_factory.py`\n",
    "\n",
    "* Since we are training a policy with previously collected interaction data, we model the reward function from actual rewards\n",
    "* We will simply pass the `user_rating` (values 0-5) as rewards to the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "10a4f40a-b4ec-4fc2-9f73-3f830b21f351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'target_movie_rating'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.TARGET_FEATURE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df9aecd6-d20a-48a7-9ead-4da3bbcfdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_rewards(element):\n",
    "    \"\"\"Calculates reward for the actions.\"\"\"\n",
    "\n",
    "    def _calc_reward(x):\n",
    "        \"\"\"\n",
    "        Calculates reward for a single action.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### uncomment for linear rewards\n",
    "        r0 = lambda: tf.constant(0.0)\n",
    "        r1 = lambda: tf.constant(1.0)\n",
    "        r2 = lambda: tf.constant(2.0)\n",
    "        r3 = lambda: tf.constant(3.0)\n",
    "        r4 = lambda: tf.constant(4.0)\n",
    "        r5 = lambda: tf.constant(5.0)\n",
    "        \n",
    "        ### uncomment for binary rewards\n",
    "        # r0 = lambda: tf.constant(0.0) # 0.0\n",
    "        # r1 = lambda: tf.constant(0.0) # 1.0\n",
    "        # r2 = lambda: tf.constant(0.0) # 2.0\n",
    "        # r3 = lambda: tf.constant(0.0) # 3.0\n",
    "        # r4 = lambda: tf.constant(1.0) # 4.0\n",
    "        # r5 = lambda: tf.constant(1.0) # 5.0\n",
    "        \n",
    "        c1 = tf.equal(x, 1.0)\n",
    "        c2 = tf.equal(x, 2.0)\n",
    "        c3 = tf.equal(x, 3.0)\n",
    "        c4 = tf.equal(x, 4.0)\n",
    "        c5 = tf.equal(x, 5.0)\n",
    "        return tf.case(\n",
    "            [(c1, r1), (c2, r2), (c3, r3),(c4, r4),(c5, r5)], \n",
    "            default=r0, exclusive=True\n",
    "        )\n",
    "\n",
    "    return tf.map_fn(\n",
    "        fn=_calc_reward, \n",
    "        # elems=element['user_rating'],\n",
    "        elems=element[data_utils.TARGET_FEATURE_NAME],\n",
    "        dtype=tf.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59703a-9a00-4eba-a3b7-45b21b738dd4",
   "metadata": {},
   "source": [
    "# [6] Trajectory function\n",
    "\n",
    "> This function will convert training samples from the TF Records to `trajectories` which the Agent interprets as training records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3605d9f-d239-4871-9a61-6e9b21f9c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _trajectory_fn(element): # hparams\n",
    "    \n",
    "    \"\"\"Converts a dataset element into a trajectory.\"\"\"\n",
    "    global_features = embs._get_global_context_features(element)\n",
    "    arm_features = embs._get_per_arm_features(element)\n",
    "\n",
    "    # Adds a time dimension.\n",
    "    arm_features = train_utils._add_outer_dimension(arm_features)\n",
    "\n",
    "    # obs spec\n",
    "    observation = {\n",
    "        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n",
    "            train_utils._add_outer_dimension(global_features),\n",
    "    }\n",
    "\n",
    "    reward = train_utils._add_outer_dimension(_get_rewards(element))\n",
    "    # reward = train_utils._add_outer_dimension(reward_factory._get_rewards(element))\n",
    "\n",
    "    # To emit the predicted rewards in policy_info, we need to create dummy\n",
    "    # rewards to match the definition in TensorSpec for the ones specified in\n",
    "    # emit_policy_info set.\n",
    "    dummy_rewards = tf.zeros([HPARAMS['batch_size'], 1, HPARAMS['num_actions']])\n",
    "    policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "        chosen_arm_features=arm_features,\n",
    "        # Pass dummy mean rewards here to match the model_spec for emitting\n",
    "        # mean rewards in policy info\n",
    "        predicted_rewards_mean=dummy_rewards,\n",
    "        bandit_policy_type=tf.zeros([HPARAMS['batch_size'], 1, 1], dtype=tf.int32)\n",
    "        # policy_utilities.create_bandit_policy_type_tensor_spec(shape=[1]) # policy_utilities.BanditPolicyType.GREEDY\n",
    "        # tf.zeros([batch_size, 1, 1], dtype=tf.int32)\n",
    "    )\n",
    "    \n",
    "    if HPARAMS['model_type'] == 'neural_ucb':\n",
    "        policy_info = policy_info._replace(\n",
    "            predicted_rewards_optimistic=dummy_rewards\n",
    "        )\n",
    "        \n",
    "    return trajectory.single_step(\n",
    "        observation=observation,\n",
    "        action=tf.zeros_like(\n",
    "            reward, dtype=tf.int32\n",
    "        ),  # Arm features are copied from policy info, put dummy zeros here\n",
    "        policy_info=policy_info,\n",
    "        reward=reward,\n",
    "        discount=tf.zeros_like(reward)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a0989c-5a06-4fd5-a12d-d92dbbe71a76",
   "metadata": {},
   "source": [
    "Inspect the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "257791d1-c97c-48c0-bc92-f5a4737b6c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in train_dataset.batch(HPARAMS['batch_size']).take(1):\n",
    "    test_traj = _trajectory_fn(x)\n",
    "    \n",
    "print(f\"test_traj.action.shape      : {test_traj.action.shape}\") \n",
    "print(f\"test_traj.discount.shape    : {test_traj.discount.shape}\")\n",
    "print(f\"test_traj.reward.shape      : {test_traj.reward.shape}\")\n",
    "print(f\"test_traj.observation.shape : {test_traj.observation['global'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3dace3d1-ce61-48cf-82a4-f701d3fe337c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_traj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbfc9a-049e-49b9-b2a3-ecb9fd4b9964",
   "metadata": {},
   "source": [
    "# [7] Training the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13adc1ef-b50e-41a3-9b95-c128d0c55a2b",
   "metadata": {},
   "source": [
    "## set Vertex Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e861fb27-df24-4821-8448-cf1346b186f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : 02-big-context-bandits-v1-rec-bandits-v2\n",
      "RUN_NAME          : run-20240305-215040\n",
      "\n",
      "CHECKPT_DIR       : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/chkpoint\n",
      "BASE_OUTPUT_DIR   : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-215040\n",
      "LOG_DIR           : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-215040/logs\n",
      "ROOT_DIR          : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-215040/root\n",
      "ARTIFACTS_DIR     : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-215040/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'02-big-context-bandits-v1-{PREFIX}'\n",
    "\n",
    "# new experiment\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "CHECKPT_DIR       = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/chkpoint\"\n",
    "BASE_OUTPUT_DIR   = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}\"\n",
    "LOG_DIR           = f\"{BASE_OUTPUT_DIR}/logs\"\n",
    "ROOT_DIR          = f\"{BASE_OUTPUT_DIR}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BASE_OUTPUT_DIR}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "# vertex_ai.init(\n",
    "#     project=PROJECT_ID,\n",
    "#     location=REGION,\n",
    "#     experiment=EXPERIMENT_NAME\n",
    "# )\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\\n\")\n",
    "print(f\"CHECKPT_DIR       : {CHECKPT_DIR}\")\n",
    "print(f\"BASE_OUTPUT_DIR   : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de729bad-0bc9-429e-b4cb-7b24bf615aa1",
   "metadata": {},
   "source": [
    "### Create TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2570564-71f4-4dda-8d8a-59784db67632",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_TENSORBOARD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db63052a-7eea-4982-964d-1f7ecab0665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME: projects/934903580331/locations/us-central1/tensorboards/2981281798249512960\n",
      "TB display name: 02-big-context-bandits-v1-rec-bandits-v2\n"
     ]
    }
   ],
   "source": [
    "if NEW_TENSORBOARD:\n",
    "    # create new TB instance\n",
    "    TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}\"\n",
    "\n",
    "    tensorboard = aiplatform.Tensorboard.create(\n",
    "        display_name=TENSORBOARD_DISPLAY_NAME\n",
    "        , project=PROJECT_ID\n",
    "        , location=REGION\n",
    "    )\n",
    "\n",
    "    TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "else:\n",
    "    # use existing TB instance\n",
    "    TB_RESOURCE_NAME = 'projects/934903580331/locations/us-central1/tensorboards/10313419068538880' # bandit-rewards\n",
    "    tensorboard = aiplatform.Tensorboard(\n",
    "        tensorboard_name=TB_RESOURCE_NAME\n",
    "    )\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name: {tensorboard.display_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a0b708d-990e-468b-a1b1-a8ba8f71d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete Tensorboard\n",
    "# vertex_ai_tb.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c891d27-d9d1-4e64-8981-1a1ae343c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aiplatform.init(\n",
    "#     project=PROJECT_ID,\n",
    "#     location=REGION,\n",
    "#     experiment=EXPERIMENT_NAME,\n",
    "#     experiment_tensorboard=TB_ID\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7a5e4-efff-490b-b431-53037a2f4a16",
   "metadata": {},
   "source": [
    "## eval loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba0392e-90c8-4dc8-8df2-6d2af1ade490",
   "metadata": {},
   "source": [
    "> When evaluating an epsilon-greedy bandit, we need to remove those predictions that come from the `uniform_random` policy. We only want the predictions from the `GREEDY` policy, becasue we want to know how the model performs when it is actually used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b3d3c-75f7-46f4-9a1b-6329e419b7f5",
   "metadata": {},
   "source": [
    "## train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "97cfe7c3-4022-47a9-83c2-df0ad8a2abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.metrics import export_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.utils import common\n",
    "\n",
    "# from src.per_arm_rl import train_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb7ffc-00df-4d46-857b-86c87b78f597",
   "metadata": {},
   "source": [
    "`agent.train(experience=...)`\n",
    "\n",
    "where `experience` is a batch of trajectories data in the form of a Trajectory. \n",
    "* The structure of experience must match that of `self.training_data_spec`. \n",
    "* All tensors in experience must be shaped [batch, time, ...] where time must be equal to self.train_step_length if that property is not None."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d6d45-c71e-4fe4-9af8-af9a8bda4e4f",
   "metadata": {},
   "source": [
    "### logs & checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "049ee49b-3b1a-4b12-a360-15e759dff7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('metrics: [<tf_agents.metrics.tf_metrics.AverageReturnMetric object at '\n",
      " '0x7f4a6477beb0>]')\n",
      "setting checkpoint_manager: gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/chkpoint\n",
      "\n",
      "'saver: <tf_agents.policies.policy_saver.PolicySaver object at 0x7f4a644f37c0>'\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# TB summary writer\n",
    "# ====================================================\n",
    "train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    f\"{LOG_DIR}\", flush_millis=10 * 1000\n",
    ")\n",
    "# ====================================================\n",
    "# metrics\n",
    "# ====================================================\n",
    "step_metric = tf_metrics.EnvironmentSteps()\n",
    "metrics = [\n",
    "    # tf_metrics.NumberOfEpisodes(),\n",
    "    # tf_metrics.AverageEpisodeLengthMetric(batch_size=HPARAMS['batch_size']),\n",
    "    tf_metrics.AverageReturnMetric(batch_size=HPARAMS['batch_size'])\n",
    "]\n",
    "\n",
    "pprint(f\"metrics: {metrics}\")\n",
    "\n",
    "# ====================================================\n",
    "# get checkpoint manager\n",
    "# ====================================================\n",
    "print(f\"setting checkpoint_manager: {CHECKPT_DIR}\\n\")\n",
    "\n",
    "checkpoint_manager = train_utils.restore_and_get_checkpoint_manager(\n",
    "    root_dir=CHECKPT_DIR, \n",
    "    agent=agent, \n",
    "    metrics=metrics, \n",
    "    step_metric=step_metric\n",
    ")\n",
    "# ====================================================\n",
    "# policy saver\n",
    "# ====================================================\n",
    "saver = policy_saver.PolicySaver(\n",
    "    agent.policy, \n",
    "    train_step=global_step\n",
    ")\n",
    "pprint(f\"saver: {saver}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d155f1f4-0d95-40a8-a37c-c608a64af803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7f4a6333da20>,\n",
       " 'get_initial_state': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7f4a6333d3f0>,\n",
       " 'get_train_step': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7f4a6325b2b0>,\n",
       " 'get_metadata': <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7f4a632584f0>}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83683f4b-8a1a-44b9-8d5f-7ee3304c794c",
   "metadata": {},
   "source": [
    "### config\n",
    "\n",
    "* calculate train & val dataset sizes\n",
    "* define logging and chkpt intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9432613-6583-4828-8a2a-dd939adbc3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_DATA_SIZE : 300000\n",
      "NUM_TRAIN_STEPS : 50\n",
      "EVAL_DATA_SIZE  : 90000\n",
      "NUM_EVAL_STEPS  : 1000\n",
      "CHKPT_INTERVAL  : 50\n",
      "LOG_INTERVAL    : 10\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_SIZE = 300_000       # len(list(train_dataset))\n",
    "NUM_TRAIN_STEPS = 50            # TRAIN_DATA_SIZE // HPARAMS['batch_size']\n",
    "\n",
    "EVAL_DATA_SIZE  = 90_000        # len(list(val_dataset))\n",
    "NUM_EVAL_STEPS  = 1_000         # EVAL_DATA_SIZE // HPARAMS['eval_batch_size']\n",
    "\n",
    "CHKPT_INTERVAL  = NUM_TRAIN_STEPS # // 5\n",
    "LOG_INTERVAL    = 10\n",
    "# EVAL_INTERVAL = NUM_TRAIN_STEPS // 2\n",
    "\n",
    "print(f\"TRAIN_DATA_SIZE : {TRAIN_DATA_SIZE}\")\n",
    "print(f\"NUM_TRAIN_STEPS : {NUM_TRAIN_STEPS}\")\n",
    "print(f\"EVAL_DATA_SIZE  : {EVAL_DATA_SIZE}\")\n",
    "print(f\"NUM_EVAL_STEPS  : {NUM_EVAL_STEPS}\")\n",
    "print(f\"CHKPT_INTERVAL  : {CHKPT_INTERVAL}\")\n",
    "print(f\"LOG_INTERVAL    : {LOG_INTERVAL}\")\n",
    "# print(f\"EVAL_INTERVAL : {EVAL_INTERVAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "379ffc65-4eec-4af1-8fd4-912ef65a9808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TakeDataset element_spec={'target_movie_genres': TensorSpec(shape=(None, 10), dtype=tf.string, name=None), 'target_movie_id': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'target_movie_rating': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'target_movie_title': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'target_movie_year': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'target_rating_timestamp': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'user_age': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'user_gender': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_id': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_occupation_text': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_zip_code': TensorSpec(shape=(None,), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data\n",
    "train_ds_iterator = iter(train_dataset.batch(HPARAMS['batch_size']).repeat())\n",
    "\n",
    "# eval dataset\n",
    "eval_ds = val_dataset.batch(HPARAMS[\"eval_batch_size\"])\n",
    "\n",
    "if NUM_EVAL_STEPS > 0:\n",
    "    eval_ds = eval_ds.take(NUM_EVAL_STEPS)\n",
    "\n",
    "# eval_ds = eval_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "eval_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498d7e3-d166-4e1d-94a5-f721de58694a",
   "metadata": {},
   "source": [
    "### run train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5dd64d98-7d5b-4474-a567-b42426d630a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.perarm_features import eval_perarm as eval_perarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "da4680f3-693b-4530-8e4d-88bc43036c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating pre-trained Agent...\n",
      "pre-train val_loss     : 14.997200012207031\n",
      "pre-train eval runtime : 0\n",
      "starting train loop...\n",
      "step = 0: train loss = 15.930000305175781\n",
      "step = 10: train loss = 11.680000305175781\n",
      "step = 20: train loss = 3.359999895095825\n",
      "step = 30: train loss = 1.690000057220459\n",
      "step = 40: train loss = 0.8700000047683716\n",
      "train runtime_mins: 6\n",
      "saved trained policy to: gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-215040/artifacts\n",
      "evaluating trained Agent...\n",
      "post-train val_loss     : 1.6606827974319458\n",
      "post-train eval runtime : 0\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "# agent.train = common.function(agent.train)\n",
    "\n",
    "list_o_loss = []\n",
    "\n",
    "# ====================================================\n",
    "# Evaluate the agent's policy once before training\n",
    "# ====================================================\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "pre_policy_tf = py_tf_eager_policy.PyTFEagerPolicy(agent.policy, use_tf_function=True)\n",
    "\n",
    "print(f\"evaluating pre-trained Agent...\")\n",
    "start_time = time.time()\n",
    "\n",
    "pre_val_loss, pre_preds, pre_tr_rewards = eval_perarm._run_bandit_eval(\n",
    "    policy = pre_policy_tf,\n",
    "    data = eval_ds,\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"pre-train val_loss     : {pre_val_loss}\")\n",
    "print(f\"pre-train eval runtime : {runtime_mins}\")\n",
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "print(f\"starting train loop...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# for i in tqdm(range(NUM_TRAIN_STEPS)):\n",
    "for i in range(NUM_TRAIN_STEPS):\n",
    "    \n",
    "    with train_summary_writer.as_default():\n",
    "\n",
    "        data = next(train_ds_iterator)\n",
    "        trajectories = _trajectory_fn(data)\n",
    "\n",
    "        # All tensors in experience must be shaped [batch, time, ...] \n",
    "        step = agent.train_step_counter.numpy()\n",
    "        loss = agent.train(experience=trajectories)\n",
    "        list_o_loss.append(loss.loss.numpy())\n",
    "\n",
    "        train_utils._export_metrics_and_summaries(\n",
    "            step=i, \n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        # print step loss\n",
    "        if step % LOG_INTERVAL == 0:\n",
    "            print(\n",
    "                'step = {0}: train loss = {1}'.format(\n",
    "                    step, round(loss.loss.numpy(), 2)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if i > 0 and i % CHKPT_INTERVAL == 0:\n",
    "            saver.save(os.path.join(CHKPOINT_DIR, 'policy_%d' % step_metric.result()))\n",
    "            print(f\"saved policy to: {CHKPOINT_DIR}\")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"train runtime_mins: {runtime_mins}\")\n",
    "\n",
    "saver.save(ARTIFACTS_DIR)\n",
    "print(f\"saved trained policy to: {ARTIFACTS_DIR}\")\n",
    "\n",
    "# ====================================================\n",
    "# Evaluate the agent's policy once after training\n",
    "# ====================================================\n",
    "print(f\"evaluating trained Agent...\")\n",
    "\n",
    "post_policy_tf = py_tf_eager_policy.PyTFEagerPolicy(agent.policy, use_tf_function=True)\n",
    "start_time = time.time()\n",
    "\n",
    "val_loss, preds, tr_rewards = eval_perarm._run_bandit_eval(\n",
    "    policy = post_policy_tf,\n",
    "    data = eval_ds,\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"post-train val_loss     : {val_loss}\")\n",
    "print(f\"post-train eval runtime : {runtime_mins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17b2b0d2-9795-4367-b57b-c331e94d1000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6606828"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb1ed8-f67b-4e7b-9cbc-70edd50e49ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate train job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31fe28-2f05-499f-b1cc-505f3c9074e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### train loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41937f9a-6c28-48d2-bc7c-bfe4372c7cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_o_loss)\n",
    "# plt.ylim([0, 10])\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e947e1-67f4-4205-99af-6921c6e9c896",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "65627011-48eb-4e8e-981e-8b61b0f427c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6006: logdir gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-local-bandits-rec-bandits-v2/run-20240301-042649/logs (started 4 days, 17:23:44 ago; pid 3837675)\n",
      "  - port 6006: logdir gs://rec-bandits-v2-hybrid-vertex-bucket/02b-deep-bandits-rec-bandits-v2/run-20240214-180454/logs (started 20 days, 3:39:36 ago; pid 3029265)\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e0f77a33-0962-4af0-af97-cf05f895cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5d0509cb-0777-4d35-86c6-28c8acbcc793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-633569e57de01613\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-633569e57de01613\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=$LOG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc40143-30b1-479f-b9b1-fa4c07b1690e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [8] Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c94a0-122f-4ba6-acef-5653a3cc7850",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b1c382b5-e2e0-45a2-9c5f-ef2c468d0f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-215040/artifacts\n"
     ]
    }
   ],
   "source": [
    "POLICY_URI = ARTIFACTS_DIR\n",
    "\n",
    "print(POLICY_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68592d97-1272-4c6d-a4bd-43a6f7fe6c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.py_tf_eager_policy.SavedModelPyTFEagerPolicy at 0x7f45204b9990>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy\n",
    "\n",
    "trained_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n",
    "    POLICY_URI, load_specs_from_pbtxt=True\n",
    ")\n",
    "trained_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d910a06f-678f-4bfa-a132-10b885f871c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762c936-3c49-4ec5-a9eb-4df5816dc888",
   "metadata": {},
   "source": [
    "TODO - explain bandit_policy_type()\n",
    "\n",
    "**Dummy arm values?**\n",
    "* We set `chosen_arm_features` to dummy values of all zeros. We need to save dummy chosen arm features to make the returned policy step have the same structure as the policy state spec.\n",
    "* `emit_policy_info = ('predicted_rewards_mean', 'bandit_policy_type')` defines what side information we want to get as part of the policy info when we call policy network \n",
    "* This makes it so that the model always returns the expected rewards even if the model is exploring\n",
    "* This means that the largest predicted rewards may not match the selected action when the model is exploring (i.e. bandit_policy == UNIFORM == 2)\n",
    "\n",
    "**UNIFORM Random policy:**\n",
    "```\n",
    "PolicyStep(action=array(1, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.637535 , 3.5557823], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([2], dtype=int32), chosen_arm_features=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)))\n",
    "```\n",
    "\n",
    "**GREEDY policy:**\n",
    "```\n",
    "PolicyStep(action=array(0, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.8278818, 3.607565 ], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([ 0.04054676, -0.04735027,  0.02202327,  0.01276748, -0.01367382,\n",
    "       -0.04838754,  0.00104916, -0.01956806,  0.01028793, -0.00256665,\n",
    "       -0.04376553,  0.03900594,  0.02388967, -0.04016995, -0.04945569,\n",
    "       -0.04539652,  0.04423274, -0.04630332,  0.00390794,  0.03986299,\n",
    "       -0.0398974 ,  0.01655747, -0.0471294 , -0.00690235, -0.03643382,\n",
    "       -0.0437116 ,  0.04757959, -0.00991895,  0.0281055 , -0.04633433,\n",
    "        0.01897702, -0.00824345,  0.03232259, -0.04980658, -0.01144745,\n",
    "        0.01690939,  0.02536928,  0.03384003, -0.01558131,  0.04731432,\n",
    "        0.04579857, -0.03305101,  0.03521084, -0.03915765,  0.04927064,\n",
    "        0.04695194,  0.03792156,  0.040496  ,  0.02574866, -0.00665367,\n",
    "       -0.03839222, -0.01564004, -0.04710373,  0.03557512, -0.01392462,\n",
    "        0.01368621,  0.02792176, -0.01799218,  0.00246101,  0.02263175,\n",
    "        0.0170817 ,  0.01388383, -0.03048695,  0.01198485], dtype=float32)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "567f64e1-8149-4364-b561-1a9d12581b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aff30396-e0fd-4f43-acaa-6af8b20b12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFER_SIZE = 1\n",
    "dummy_arm = tf.zeros([INFER_SIZE, PER_ARM_DIM], dtype=tf.float32)\n",
    "\n",
    "SKIP_NUM = 10\n",
    "\n",
    "for x in eval_ds.skip(SKIP_NUM).take(INFER_SIZE):\n",
    "    # get feature tensors\n",
    "    \n",
    "    global_feat_infer = embs._get_global_context_features(x)\n",
    "    arm_feat_infer = embs._get_per_arm_features(x)\n",
    "    \n",
    "    rewards = _get_rewards(x)\n",
    "    # rewards = reward_factory._get_rewards(x)\n",
    "    \n",
    "    # reshape arm features\n",
    "    arm_feat_infer = tf.reshape(arm_feat_infer, [HPARAMS['eval_batch_size'], PER_ARM_DIM]) # perarm_dim\n",
    "    concat_arm = tf.concat([arm_feat_infer, dummy_arm], axis=0)\n",
    "    \n",
    "    # flatten global\n",
    "    flat_global_infer = tf.reshape(global_feat_infer, [GLOBAL_DIM])\n",
    "    feature = {'global': flat_global_infer, 'per_arm': concat_arm}\n",
    "    \n",
    "    # get actual reward\n",
    "    actual_reward = rewards.numpy()[0]\n",
    "    \n",
    "    # build trajectory step\n",
    "    trajectory_step = train_utils._get_eval_step(feature, actual_reward)\n",
    "    \n",
    "    prediction = trained_policy.action(trajectory_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2bcd1e82-168e-4df3-92bd-4cd34ecd3a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
       " 'observation': {'global': <tf.Tensor: shape=(72,), dtype=float32, numpy=\n",
       "array([-0.01516125, -0.03990994, -0.02092743,  0.03013213,  0.00099142,\n",
       "        0.00398227, -0.0313184 , -0.00903504,  0.04052765,  0.04434092,\n",
       "       -0.00630779, -0.02942972,  0.03291892, -0.04428911, -0.04632987,\n",
       "       -0.01934351, -0.0269029 , -0.0473554 , -0.0249314 , -0.03406465,\n",
       "        0.00801427,  0.03917781,  0.01341759,  0.02991948, -0.00105946,\n",
       "       -0.01418182, -0.01481956,  0.02664788, -0.01311366,  0.02148655,\n",
       "       -0.02059453,  0.00074029,  0.02855868, -0.02006453, -0.00631043,\n",
       "        0.00105479, -0.02699273, -0.0435128 ,  0.01885439,  0.03103975,\n",
       "       -0.01578783, -0.0409723 ,  0.01237534,  0.01298692, -0.01935979,\n",
       "       -0.00373379,  0.01030274, -0.04423375, -0.01168343, -0.01009633,\n",
       "        0.04010144,  0.00215397, -0.00599377,  0.04436407, -0.03529128,\n",
       "       -0.01563434,  0.01158167,  0.0239563 ,  0.01573116, -0.01842538,\n",
       "        0.01114056,  0.02476985,  0.03262847,  0.00181503,  0.01036154,\n",
       "        0.02358521, -0.02503004, -0.01965289, -0.01451147,  0.02037409,\n",
       "        0.04341059, -0.02993988], dtype=float32)>,\n",
       "                 'per_arm': <tf.Tensor: shape=(2, 64), dtype=float32, numpy=\n",
       "array([[ 0.0399575 , -0.035101  ,  0.00933775, -0.01295454,  0.04266239,\n",
       "         0.03834672, -0.03096471,  0.02987721, -0.00054013,  0.0026003 ,\n",
       "         0.04230278, -0.02825226,  0.03894294,  0.02496741,  0.00928389,\n",
       "        -0.00661758, -0.00885345, -0.0272608 , -0.02871018,  0.03678447,\n",
       "         0.0407989 ,  0.01179961,  0.02757061,  0.01379795,  0.03976682,\n",
       "        -0.01703257, -0.00486839,  0.01115931, -0.0022273 , -0.01444559,\n",
       "         0.00243389, -0.00807588,  0.04236089, -0.03327624, -0.04521332,\n",
       "         0.04770124, -0.03028374,  0.01047393, -0.01046742,  0.04756881,\n",
       "         0.02545985, -0.03799745, -0.01765518,  0.02925095,  0.02353   ,\n",
       "         0.02855964, -0.00913833, -0.00589989,  0.08838287,  0.00888273,\n",
       "         0.02589315,  0.1077992 , -0.11840633,  0.13673107, -0.05451826,\n",
       "         0.08719121,  0.09107283, -0.07726747,  0.03634408,  0.10014257,\n",
       "         0.12381999, -0.09902823,  0.0293408 , -0.03184003],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "      dtype=float32)>},\n",
       " 'reward': <tf.Tensor: shape=(), dtype=float32, numpy=3.0>,\n",
       " 'step_type': <tf.Tensor: shape=(), dtype=int32, numpy=0>})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e58af9ab-f4da-4857-afe5-6b8de26ed0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=array(0, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.511208 , 3.3765502], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([ 0.0399575 , -0.035101  ,  0.00933775, -0.01295454,  0.04266239,\n",
       "        0.03834672, -0.03096471,  0.02987721, -0.00054013,  0.0026003 ,\n",
       "        0.04230278, -0.02825226,  0.03894294,  0.02496741,  0.00928389,\n",
       "       -0.00661758, -0.00885345, -0.0272608 , -0.02871018,  0.03678447,\n",
       "        0.0407989 ,  0.01179961,  0.02757061,  0.01379795,  0.03976682,\n",
       "       -0.01703257, -0.00486839,  0.01115931, -0.0022273 , -0.01444559,\n",
       "        0.00243389, -0.00807588,  0.04236089, -0.03327624, -0.04521332,\n",
       "        0.04770124, -0.03028374,  0.01047393, -0.01046742,  0.04756881,\n",
       "        0.02545985, -0.03799745, -0.01765518,  0.02925095,  0.02353   ,\n",
       "        0.02855964, -0.00913833, -0.00589989,  0.08838287,  0.00888273,\n",
       "        0.02589315,  0.1077992 , -0.11840633,  0.13673107, -0.05451826,\n",
       "        0.08719121,  0.09107283, -0.07726747,  0.03634408,  0.10014257,\n",
       "        0.12381999, -0.09902823,  0.0293408 , -0.03184003], dtype=float32)))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ec286d78-dd56-455d-90f8-4ffa88f3ac50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.511208 , 3.3765502], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.info.predicted_rewards_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c85a0087-7388-4f03-9278-aace31121233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0, dtype=int32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8266c7c-3688-46a5-a1c8-a63bb7146128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.info.bandit_policy_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9712bc00-7ea0-4003-a3c7-81a0be5bc995",
   "metadata": {},
   "source": [
    "# [9] Prepare training application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f62966-123f-410e-9610-832f20566635",
   "metadata": {},
   "source": [
    "To scale this training in Vertex AI's managed training service, we need to package all our code into a python application\n",
    "\n",
    "* This means, we need to modularize all steps executed up until this point. For example, the training loop we ran above is further parameterized in `train_perarm.py`, which will be called from our `task.py` when we submit the job to Vertex AI\n",
    "* This training package will eventually be used to create a custom training image (more on that in `04b-build-training-image` notebook)\n",
    "* For additional details on preparing the training application for Vertex AI, check out [the documentation](https://cloud.google.com/vertex-ai/docs/training/code-requirements) and [this code example](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_training.ipynb) (which also has good, related commentary)\n",
    "\n",
    "**The remaining cells will repeat steps executed above**\n",
    "> we are just illustrating / validating that they are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c3d30-bef4-4ec5-a978-116856a70e7e",
   "metadata": {},
   "source": [
    "### Distribution strategy\n",
    "\n",
    "Use `strategy_utils` to generate a strategy. Under the hood, passing the parameter:\n",
    "\n",
    "* `use_gpu = False` returns `tf.distribute.get_strategy()`, which uses CPU\n",
    "* `use_gpu = True` returns `tf.distribute.MirroredStrategy()`, which uses all GPUs that are visible to TensorFlow on one machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "68140c4d-12ff-4758-89ca-44fd710ca0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy at 0x7f44d81c7e50>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.train.utils import strategy_utils\n",
    "\n",
    "use_gpu = True\n",
    "use_tpu = False\n",
    "\n",
    "distribution_strategy = strategy_utils.get_strategy(tpu=use_tpu, use_gpu=use_gpu)\n",
    "distribution_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e80724b1-6525-4986-b832-4af8b49d923c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_REPLICAS = distribution_strategy.num_replicas_in_sync\n",
    "NUM_REPLICAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129ee41-b43c-4d5a-b656-cdac6b79688c",
   "metadata": {},
   "source": [
    "### Vertex Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ef509680-4d19-4efa-9f01-21039e0d573b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : 02-big-context-bandits-v1-rec-bandits-v2\n"
     ]
    }
   ],
   "source": [
    "# using same experiment name \n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7ad98430-d836-40f1-8e4f-eb46f0c91fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : 02-big-context-bandits-v1-rec-bandits-v2\n",
      "RUN_NAME          : run-20240305-220049\n",
      "\n",
      "CHECKPT_DIR       : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/chkpoint\n",
      "BASE_OUTPUT_DIR   : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-220049\n",
      "LOG_DIR           : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-220049/logs\n",
      "ROOT_DIR          : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-220049/root\n",
      "ARTIFACTS_DIR     : gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-220049/artifacts\n"
     ]
    }
   ],
   "source": [
    "# new experiment\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "CHECKPT_DIR       = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/chkpoint\"\n",
    "BASE_OUTPUT_DIR   = f'{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "LOG_DIR           = f\"{BASE_OUTPUT_DIR}/logs\"\n",
    "ROOT_DIR          = f\"{BASE_OUTPUT_DIR}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BASE_OUTPUT_DIR}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "# aiplatform.init(\n",
    "#     project=PROJECT_ID,\n",
    "#     location=REGION,\n",
    "#     experiment=EXPERIMENT_NAME\n",
    "# )\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\\n\")\n",
    "print(f\"CHECKPT_DIR       : {CHECKPT_DIR}\")\n",
    "print(f\"BASE_OUTPUT_DIR   : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83de7c4-f7c7-4290-b44a-9e9194bac882",
   "metadata": {},
   "source": [
    "### Create TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "17576ce0-727d-4297-a52d-f64fb75ca78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME : projects/934903580331/locations/us-central1/tensorboards/3855191232191922176\n",
      "TB display name  : 02-big-context-bandits-v1-rec-bandits-v2-run-20240305-220049\n",
      "TB_ID            : 3855191232191922176\n"
     ]
    }
   ],
   "source": [
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}-{RUN_NAME}\"\n",
    "\n",
    "tensorboard = aiplatform.Tensorboard.create(\n",
    "    display_name=TENSORBOARD_DISPLAY_NAME\n",
    "    , project=PROJECT_ID\n",
    "    , location=REGION\n",
    ")\n",
    "\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "TB_ID = TB_RESOURCE_NAME.split('/')[-1]\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME : {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name  : {tensorboard.display_name}\")\n",
    "print(f\"TB_ID            : {TB_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "71d43cf9-db3f-437e-98ee-3791ac0c5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    experiment_tensorboard=TB_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2e082-c0f6-4792-a279-e827c48b5895",
   "metadata": {},
   "source": [
    "### trajectory function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "55e21068-a7a5-44c8-a16f-d8c41d976c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src import reward_factory as reward_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b9ce410e-ac03-48b2-8006-4591b38297a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with distribution_strategy.scope():\n",
    "    \n",
    "    embs = emb_features.EmbeddingModel(\n",
    "        vocab_dict = vocab_dict,\n",
    "        num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "        global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "        mv_emb_size = MV_EMBEDDING_SIZE,\n",
    "    )\n",
    "    \n",
    "    def _trajectory_fn(element): # hparams\n",
    "    \n",
    "        \"\"\"Converts a dataset element into a trajectory.\"\"\"\n",
    "        # global_features = _get_global_context_features(element)\n",
    "        # arm_features = _get_per_arm_features(element)\n",
    "\n",
    "        global_features = embs._get_global_context_features(element)\n",
    "        arm_features = embs._get_per_arm_features(element)\n",
    "\n",
    "        # Adds a time dimension.\n",
    "        arm_features = train_utils._add_outer_dimension(arm_features)\n",
    "\n",
    "        # obs spec\n",
    "        observation = {\n",
    "            bandit_spec_utils.GLOBAL_FEATURE_KEY:\n",
    "                train_utils._add_outer_dimension(global_features),\n",
    "        }\n",
    "\n",
    "        reward = train_utils._add_outer_dimension(reward_factory._get_binary_rewards(element))\n",
    "        # reward = train_utils._add_outer_dimension(reward_factory._get_rewards(element))\n",
    "\n",
    "        # To emit the predicted rewards in policy_info, we need to create dummy\n",
    "        # rewards to match the definition in TensorSpec for the ones specified in\n",
    "        # emit_policy_info set.\n",
    "        dummy_rewards = tf.zeros([HPARAMS['batch_size'], 1, HPARAMS['num_actions']])\n",
    "        policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "            chosen_arm_features=arm_features,\n",
    "            # Pass dummy mean rewards here to match the model_spec for emitting\n",
    "            # mean rewards in policy info\n",
    "            predicted_rewards_mean=dummy_rewards,\n",
    "            bandit_policy_type=tf.zeros([HPARAMS['batch_size'], 1, 1], dtype=tf.int32)\n",
    "            # policy_utilities.create_bandit_policy_type_tensor_spec(shape=[1]) \n",
    "            # policy_utilities.BanditPolicyType.GREEDY\n",
    "            # tf.zeros([batch_size, 1, 1], dtype=tf.int32)\n",
    "        )\n",
    "\n",
    "        if HPARAMS['model_type'] == 'neural_ucb':\n",
    "            policy_info = policy_info._replace(\n",
    "                predicted_rewards_optimistic=dummy_rewards\n",
    "            )\n",
    "\n",
    "        return trajectory.single_step(\n",
    "            observation=observation,\n",
    "            action=tf.zeros_like(\n",
    "                reward, \n",
    "                dtype=tf.int32\n",
    "            ),  # Arm features are copied from policy info, put dummy zeros here\n",
    "            policy_info=policy_info,\n",
    "            reward=reward,\n",
    "            discount=tf.zeros_like(reward)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b759404-b282-4f55-add8-7d795867c99e",
   "metadata": {},
   "source": [
    "### get agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3be40320-a73f-45f7-9fc4-0bd64df0ae5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 128,\n",
       " 'eval_batch_size': 1,\n",
       " 'num_actions': 2,\n",
       " 'model_type': 'epsGreedy',\n",
       " 'network_type': 'commontower',\n",
       " 'global_layers': [72, 36, 18],\n",
       " 'per_arm_layers': [64, 32, 16],\n",
       " 'common_layers': [34, 8],\n",
       " 'learning_rate': 0.05,\n",
       " 'epsilon': 0.01,\n",
       " 'encoding_dim': 1}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HPARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fbdecf78-94c5-4f8c-a6c0-86f2eace4663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: NeuralEpsGreedyAgent\n",
      "Network: commontower\n"
     ]
    }
   ],
   "source": [
    "with distribution_strategy.scope():\n",
    "\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "    agent = agent_factory.PerArmAgentFactory._get_agent(\n",
    "        agent_type = HPARAMS['model_type'],\n",
    "        network_type = HPARAMS['network_type'],\n",
    "        time_step_spec = time_step_spec,\n",
    "        action_spec = action_spec,\n",
    "        observation_spec=observation_spec,\n",
    "        global_layers = HPARAMS['global_layers'],\n",
    "        arm_layers = HPARAMS['per_arm_layers'],\n",
    "        common_layers = HPARAMS['common_layers'],\n",
    "        agent_alpha = AGENT_ALPHA,\n",
    "        learning_rate = HPARAMS['learning_rate'],\n",
    "        epsilon = HPARAMS['epsilon'],\n",
    "        train_step_counter = global_step,\n",
    "        output_dim = HPARAMS['encoding_dim'],\n",
    "        eps_phase_steps = EPS_PHASE_STEPS,\n",
    "        summarize_grads_and_vars = True,\n",
    "        debug_summaries = True\n",
    "    )\n",
    "    \n",
    "    agent.initialize()\n",
    "\n",
    "print(f\"Agent: {agent.name}\")\n",
    "\n",
    "# if network:\n",
    "#     print(f\"Network: {network}\")\n",
    "\n",
    "if NETWORK_TYPE:\n",
    "    print(f\"Network: {NETWORK_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "108ed33c-d8ef-4524-97e9-ced9e01c154b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHKPT_INTERVAL    : 200\n",
      "eval_batch_size   : 1\n",
      "EVAL_DATA_SIZE    : 90000\n",
      "NUM_EVAL_STEPS    : 100\n",
      "TOTAL_TRAIN_TAKE  : 10000\n"
     ]
    }
   ],
   "source": [
    "# from src.perarm_features import train_perarm as train_perarm\n",
    "\n",
    "# train args\n",
    "TRAINING_LOOPS        = 50\n",
    "STEPS_PER_LOOP        = 1\n",
    "\n",
    "drop_arm_feature_fn   = None\n",
    "ASYNC_STEPS_PER_LOOP  = 1\n",
    "\n",
    "LOG_INTERVAL          = 10\n",
    "CHKPT_INTERVAL        = 200\n",
    "\n",
    "# eval args\n",
    "NUM_EVAL_STEPS = 100\n",
    "\n",
    "TOTAL_TRAIN_TAKE = 10000 #TRAINING_LOOPS * HPARAMS['batch_size']\n",
    "\n",
    "print(f\"CHKPT_INTERVAL    : {CHKPT_INTERVAL}\")\n",
    "print(f\"eval_batch_size   : {EVAL_BATCH_SIZE}\")\n",
    "print(f\"EVAL_DATA_SIZE    : {EVAL_DATA_SIZE}\")\n",
    "print(f\"NUM_EVAL_STEPS    : {NUM_EVAL_STEPS}\")\n",
    "print(f\"TOTAL_TRAIN_TAKE  : {TOTAL_TRAIN_TAKE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f66ff927-9a39-4abf-b247-845f0dc7721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval dataset\n",
    "eval_ds = val_dataset.batch(HPARAMS['eval_batch_size'])\n",
    "\n",
    "if NUM_EVAL_STEPS >= 0:\n",
    "    with distribution_strategy.scope():\n",
    "        eval_ds = eval_ds.take(NUM_EVAL_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "528441f5-64ec-4f09-bd50-b2ae85b553bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# TB summary writer\n",
    "# ====================================================\n",
    "with distribution_strategy.scope():\n",
    "    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        f\"{LOG_DIR}\", flush_millis=10 * 1000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f6c98398-d545-4dcc-a71f-32249b5f0336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution_strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f44d81c7e50>\n",
      "number of train_files: 8\n",
      "Inpsecting agent policy from train_peram file...\n",
      "agent.policy: <tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x7f44a065c2e0>\n",
      "Inpsecting agent policy from train_peram file: Complete\n",
      "setting checkpoint_manager: gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/chkpoint\n",
      "agent.train_step_counter: 50\n",
      "starting train loop...\n",
      "step = 60: loss = 0.7099999785423279\n",
      "step = 70: loss = 0.5899999737739563\n",
      "step = 80: loss = 0.28999999165534973\n",
      "step = 90: loss = 0.25999999046325684\n",
      "step = 100: loss = 0.20000000298023224\n",
      "runtime_mins: 0\n",
      "saved trained policy to: gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-220049/artifacts\n",
      "saved trained policy to: gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/chkpoint\n",
      "complete train job in 0 minutes\n"
     ]
    }
   ],
   "source": [
    "#start the timer and training\n",
    "start_time = time.time()\n",
    "\n",
    "metric_results, agent = train_perarm.train_perarm(\n",
    "    agent = agent,\n",
    "    reward_spec = reward_tensor_spec,\n",
    "    epsilon = HPARAMS['epsilon'],\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    num_iterations = TRAINING_LOOPS,\n",
    "    steps_per_loop = STEPS_PER_LOOP,\n",
    "    num_eval_steps = NUM_EVAL_STEPS,\n",
    "    # data\n",
    "    batch_size = HPARAMS['batch_size'],\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    # functions\n",
    "    _trajectory_fn = _trajectory_fn,\n",
    "    # _run_bandit_eval_fn = _run_bandit_eval,\n",
    "    # train intervals\n",
    "    chkpt_interval = CHKPT_INTERVAL,\n",
    "    log_interval = LOG_INTERVAL,\n",
    "    # dirs\n",
    "    bucket_name = BUCKET_NAME,\n",
    "    data_dir_prefix_path = f\"{EXAMPLE_GEN_GCS_PATH}\",\n",
    "    log_dir = LOG_DIR,\n",
    "    model_dir = ARTIFACTS_DIR,\n",
    "    chkpoint_dir = CHECKPT_DIR,\n",
    "    async_steps_per_loop = ASYNC_STEPS_PER_LOOP,\n",
    "    resume_training_loops = False,\n",
    "    use_gpu = True,\n",
    "    use_tpu = False,\n",
    "    profiler = False,\n",
    "    global_step = global_step,\n",
    "    total_train_take = TOTAL_TRAIN_TAKE,\n",
    "    train_summary_writer = train_summary_writer,\n",
    "    strategy = distribution_strategy,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "runtime_mins = int((end_time - start_time) / 60)\n",
    "print(f\"complete train job in {runtime_mins} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "177aa3ab-05b4-48f9-b434-9335bd9eb54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20367417"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(metric_results)\n",
    "metric_results[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ba65417f-54e0-436e-b0d4-93cffb981ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(metric_results)\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61464938-a3e7-4ab0-9149-4a9124199dc1",
   "metadata": {},
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9108f8b6-7aea-48d6-a763-461b30671c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "083c2351-5ac8-4218-9bef-e249777aee97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-aa233e842c31a74\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-aa233e842c31a74\");\n",
       "          const url = new URL(\"/proxy/8008/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=$LOG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ba747-00d5-4908-8d0b-334ae9d2c791",
   "metadata": {},
   "source": [
    "#### eval trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c5cf9366-d9cd-4c9d-951a-99a50877a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82506e-95d7-4a4e-a821-e08512615db7",
   "metadata": {},
   "source": [
    "After training, our `agent.policy` is used as the deployment \"model\" that will generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6c82396a-7822-41fb-830f-1f6eb9c9b213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy at 0x7f44a065c2e0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment_agent = agent.policy\n",
    "deployment_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f6706224-ef5b-4185-bde4-bccb7c8e7ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating trained Agent...\n",
      "post-train val_loss     : 10.453433990478516\n",
      "post-train eval runtime : 0\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Evaluate the agent's policy once after training\n",
    "# ====================================================\n",
    "print(f\"evaluating trained Agent...\")\n",
    "\n",
    "post_policy_tf = py_tf_eager_policy.PyTFEagerPolicy(deployment_agent, use_tf_function=True)\n",
    "start_time = time.time()\n",
    "\n",
    "val_loss, preds, tr_rewards = eval_perarm._run_bandit_eval(\n",
    "    policy = post_policy_tf,\n",
    "    data = eval_ds,\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"post-train val_loss     : {val_loss}\")\n",
    "print(f\"post-train eval runtime : {runtime_mins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f899120f-3706-4b72-8f2c-2169221dd66b",
   "metadata": {},
   "source": [
    "## Using Trained Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b0f691-860e-4853-928e-4c7ece714dd0",
   "metadata": {},
   "source": [
    "### load trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ea029f73-0638-462a-8f00-54b05c54d69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-220049/artifacts\n"
     ]
    }
   ],
   "source": [
    "POLICY_URI = ARTIFACTS_DIR\n",
    "\n",
    "print(POLICY_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "66d1d819-0152-437c-aefc-45ffd34d808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-220049/artifacts/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-220049/artifacts/fingerprint.pb\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-220049/artifacts/policy_specs.pbtxt\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-220049/artifacts/saved_model.pb\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-220049/artifacts/assets/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-big-context-bandits-v1-rec-bandits-v2/run-20240305-220049/artifacts/variables/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $POLICY_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6aad948e-e346-4da3-8cb2-747435c8dc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.py_tf_eager_policy.SavedModelPyTFEagerPolicy at 0x7f449adeed10>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy\n",
    "\n",
    "trained_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n",
    "    POLICY_URI, load_specs_from_pbtxt=True\n",
    ")\n",
    "\n",
    "trained_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "483f8fa5-cbd1-4e3c-8e62-47f91f42e137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action_spec', minimum=0, maximum=1)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_policy.action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8f658891-bc09-49b8-bc58-000a7926df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFER_SIZE = 1\n",
    "dummy_arm = tf.zeros([INFER_SIZE, PER_ARM_DIM], dtype=tf.float32)\n",
    "\n",
    "SKIP_NUM = 10\n",
    "\n",
    "for x in eval_ds.skip(SKIP_NUM).take(INFER_SIZE):\n",
    "    # get feature tensors\n",
    "    \n",
    "    # global_feat_infer = _get_global_context_features(x)\n",
    "    # arm_feat_infer = _get_per_arm_features(x)\n",
    "    \n",
    "    global_feat_infer = embs._get_global_context_features(x)\n",
    "    arm_feat_infer = embs._get_per_arm_features(x)\n",
    "    \n",
    "    # rewards = _get_rewards(x)\n",
    "    rewards = reward_factory._get_rewards(x)\n",
    "    \n",
    "    # reshape arm features\n",
    "    arm_feat_infer = tf.reshape(arm_feat_infer, [HPARAMS['eval_batch_size'], PER_ARM_DIM]) # perarm_dim\n",
    "    concat_arm = tf.concat([arm_feat_infer, dummy_arm], axis=0)\n",
    "    \n",
    "    # flatten global\n",
    "    flat_global_infer = tf.reshape(global_feat_infer, [GLOBAL_DIM])\n",
    "    feature = {'global': flat_global_infer, 'per_arm': concat_arm}\n",
    "    \n",
    "    # get actual reward\n",
    "    actual_reward = rewards.numpy()[0]\n",
    "    \n",
    "    # build trajectory step\n",
    "    trajectory_step = train_utils._get_eval_step(feature, actual_reward)\n",
    "    \n",
    "    prediction = trained_policy.action(trajectory_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "49ce41ed-41b7-404d-9796-1658e7955894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
       " 'observation': {'global': <tf.Tensor: shape=(72,), dtype=float32, numpy=\n",
       "array([-0.04705841,  0.04683057, -0.03778448,  0.02914759, -0.0386763 ,\n",
       "        0.01173912, -0.01335969, -0.02889992,  0.01437483, -0.00318434,\n",
       "       -0.00407728,  0.03707756, -0.03203509, -0.02504793, -0.00947034,\n",
       "        0.03590522, -0.03323276, -0.03344803, -0.02343814,  0.0488182 ,\n",
       "        0.03663263,  0.01007676,  0.03270641, -0.0077613 ,  0.04630798,\n",
       "        0.01031951, -0.03692799, -0.03960656, -0.04806488,  0.00321865,\n",
       "       -0.01765796, -0.016837  , -0.02468973, -0.03095139,  0.02584647,\n",
       "        0.0438823 ,  0.01815024, -0.04988829, -0.01266839,  0.00592544,\n",
       "       -0.0414407 ,  0.04222909, -0.01060026,  0.03817162, -0.03449489,\n",
       "       -0.02985811, -0.03298762, -0.03830657, -0.04971961, -0.01005381,\n",
       "       -0.03635018,  0.04857948, -0.00173176, -0.01947852, -0.04865669,\n",
       "        0.01884368, -0.01955221,  0.03152113,  0.04142321, -0.01526149,\n",
       "        0.04816348,  0.03459445,  0.01596273,  0.01265543,  0.02926786,\n",
       "       -0.02958192,  0.00078953, -0.02041526,  0.01697687,  0.04289048,\n",
       "        0.00809411,  0.00053265], dtype=float32)>,\n",
       "                 'per_arm': <tf.Tensor: shape=(2, 64), dtype=float32, numpy=\n",
       "array([[ 0.04130803, -0.03971902, -0.03260282,  0.04028373, -0.03322162,\n",
       "         0.02483139, -0.03116958,  0.00462329, -0.00626897,  0.02900239,\n",
       "        -0.02184236,  0.01282761, -0.01079499,  0.01612555,  0.00587124,\n",
       "         0.03282321,  0.01103955,  0.03636224,  0.0021595 ,  0.03622077,\n",
       "        -0.03826945, -0.03191662, -0.02122372, -0.00220129, -0.01370477,\n",
       "         0.03536019, -0.01106892,  0.00686066,  0.00718308,  0.0269958 ,\n",
       "         0.02032862,  0.02766532, -0.04212163,  0.04705006, -0.04027025,\n",
       "        -0.04330723,  0.03678492, -0.00307963, -0.02789462, -0.00526527,\n",
       "         0.00558809, -0.00342355, -0.01108967, -0.04971028,  0.01215775,\n",
       "         0.01782143, -0.01952624,  0.00912448, -0.05165967,  0.10473576,\n",
       "         0.06113466,  0.10673214, -0.11267278, -0.06905437, -0.035862  ,\n",
       "        -0.02486008,  0.02465314, -0.13100037,  0.09405378,  0.12231089,\n",
       "        -0.08675605,  0.09879574,  0.04201348, -0.01796708],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "      dtype=float32)>},\n",
       " 'reward': <tf.Tensor: shape=(), dtype=float32, numpy=3.0>,\n",
       " 'step_type': <tf.Tensor: shape=(), dtype=int32, numpy=0>})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f515fcd0-2228-4f68-9ef9-665ce1fc3fb2",
   "metadata": {},
   "source": [
    "### view prediction response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6af4c297-d2e8-4f7b-bed5-e1da997dfe34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=array(0, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([0.6107741, 0.5908719], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([ 0.04130803, -0.03971902, -0.03260282,  0.04028373, -0.03322162,\n",
       "        0.02483139, -0.03116958,  0.00462329, -0.00626897,  0.02900239,\n",
       "       -0.02184236,  0.01282761, -0.01079499,  0.01612555,  0.00587124,\n",
       "        0.03282321,  0.01103955,  0.03636224,  0.0021595 ,  0.03622077,\n",
       "       -0.03826945, -0.03191662, -0.02122372, -0.00220129, -0.01370477,\n",
       "        0.03536019, -0.01106892,  0.00686066,  0.00718308,  0.0269958 ,\n",
       "        0.02032862,  0.02766532, -0.04212163,  0.04705006, -0.04027025,\n",
       "       -0.04330723,  0.03678492, -0.00307963, -0.02789462, -0.00526527,\n",
       "        0.00558809, -0.00342355, -0.01108967, -0.04971028,  0.01215775,\n",
       "        0.01782143, -0.01952624,  0.00912448, -0.05165967,  0.10473576,\n",
       "        0.06113466,  0.10673214, -0.11267278, -0.06905437, -0.035862  ,\n",
       "       -0.02486008,  0.02465314, -0.13100037,  0.09405378,  0.12231089,\n",
       "       -0.08675605,  0.09879574,  0.04201348, -0.01796708], dtype=float32)))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd58d4b-6749-4177-8342-83ee969408c9",
   "metadata": {},
   "source": [
    "#### chosen action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d49adbbb-f747-4e4c-b82d-df88373c644f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0, dtype=int32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd12013-1d12-4197-8254-cef9138df42c",
   "metadata": {},
   "source": [
    "#### Per Arm Policy Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d6126290-546b-4702-8a1e-91f18adb05c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([0.6107741, 0.5908719], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([ 0.04130803, -0.03971902, -0.03260282,  0.04028373, -0.03322162,\n",
       "        0.02483139, -0.03116958,  0.00462329, -0.00626897,  0.02900239,\n",
       "       -0.02184236,  0.01282761, -0.01079499,  0.01612555,  0.00587124,\n",
       "        0.03282321,  0.01103955,  0.03636224,  0.0021595 ,  0.03622077,\n",
       "       -0.03826945, -0.03191662, -0.02122372, -0.00220129, -0.01370477,\n",
       "        0.03536019, -0.01106892,  0.00686066,  0.00718308,  0.0269958 ,\n",
       "        0.02032862,  0.02766532, -0.04212163,  0.04705006, -0.04027025,\n",
       "       -0.04330723,  0.03678492, -0.00307963, -0.02789462, -0.00526527,\n",
       "        0.00558809, -0.00342355, -0.01108967, -0.04971028,  0.01215775,\n",
       "        0.01782143, -0.01952624,  0.00912448, -0.05165967,  0.10473576,\n",
       "        0.06113466,  0.10673214, -0.11267278, -0.06905437, -0.035862  ,\n",
       "       -0.02486008,  0.02465314, -0.13100037,  0.09405378,  0.12231089,\n",
       "       -0.08675605,  0.09879574,  0.04201348, -0.01796708], dtype=float32))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f66961-ebb5-4ae5-8e6f-ad73114d8353",
   "metadata": {},
   "source": [
    "**Finished**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
