{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd893de4-bd6e-4f53-b6bb-5418bb5c634b",
   "metadata": {},
   "source": [
    "# Create preprocessed dataset for optimized data loading\n",
    "\n",
    "> preparing dataset in this way will be similar to that needed during online training iterations\n",
    "\n",
    "In this notebook we will extend the `02b-example-optimized-dataset` example and preprocess the entire dataset. To avoid storing large intermediate files locally, and prepare for MLOps, we will submit this job as a Vertex Pipeline step\n",
    "\n",
    "**Things to consider**\n",
    "* the `batch_size` used will be the `batch_size` used for downstream training steps. This and other params may need to be shared between data preprocessing and training steps. Some include:\n",
    "  * embedding dimensions for global and per_arm features: `GLOBAL_DIM` and `PER_ARM_DIM`\n",
    "  * reward calculation\n",
    "  * feature config\n",
    "* if we are using an environment to generate simulated examples, these could easily be an input\n",
    "* trajectory data stored in BigQuery will not have the `BATCH` and `TIME` dimensions e.g., `[B, T, ...]`, we'll need to account for this when we parse the trajecotries for our TF record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31517da-5ca9-4e99-98a4-5f2ad2cbd7c7",
   "metadata": {},
   "source": [
    "**orchestrate with Vertex Pipelines**\n",
    "\n",
    "<img src=\"../imgs/data_pipeline.png\" \n",
    "     align=\"center\" \n",
    "     width=\"650\"\n",
    "     height=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9010a9df-dd73-4371-88c9-8d5c567872cf",
   "metadata": {},
   "source": [
    "## Notebook config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49665ba-f561-48cb-bc78-1304c85e56b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX: rec-bandits-v2\n"
     ]
    }
   ],
   "source": [
    "VERSION        = \"v2\"                       # TODO\n",
    "PREFIX         = f'rec-bandits-{VERSION}'   # TODO\n",
    "\n",
    "print(f\"PREFIX: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b6004f-61f4-4ce3-996a-8ede83a1d2f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"rec-bandits-v2\"\n",
      "VERSION                  = \"v2\"\n",
      "\n",
      "BUCKET_NAME              = \"rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "BUCKET_URI               = \"gs://rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_NAME    = \"mvlens_rec_bandits_v2\"\n",
      "BIGQUERY_TABLE_NAME      = \"training_dataset\"\n",
      "\n",
      "REPOSITORY               = \"rl-movielens-rec-bandits-v2\"\n",
      "\n",
      "DOCKERNAME_01            = \"Dockerfile_train_my_perarm_env\"\n",
      "IMAGE_NAME_01            = \"train-my-perarm-env-v2\"\n",
      "IMAGE_URI_01             = \"gcr.io/hybrid-vertex/train-my-perarm-env-v2\"\n",
      "\n",
      "DOCKERNAME_02            = \"Dockerfile_perarm_feats\"\n",
      "IMAGE_NAME_02            = \"train-perarm-feats-v2\"\n",
      "IMAGE_URI_02             = \"gcr.io/hybrid-vertex/train-perarm-feats-v2\"\n",
      "\n",
      "DOCKERNAME_03            = \"Dockerfile_ranking_bandit\"\n",
      "IMAGE_NAME_03            = \"train-rank-bandit-v2\"\n",
      "IMAGE_URI_03             = \"gcr.io/hybrid-vertex/train-rank-bandit-v2\"\n",
      "\n",
      "DOCKERNAME_04            = \"Dockerfile_train_bandit_e2e\"\n",
      "IMAGE_NAME_04            = \"train-mab-e2e-v2\"\n",
      "IMAGE_URI_04             = \"gcr.io/hybrid-vertex/train-mab-e2e-v2\"\n",
      "\n",
      "DOCKERNAME_04_pred       = \"Dockerfile_pred_bandit_e2e\"\n",
      "IMAGE_NAME_04_pred       = \"pred-mab-e2e-v2\"\n",
      "IMAGE_URI_04_pred        = \"gcr.io/hybrid-vertex/pred-mab-e2e-v2\"\n",
      "\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-rec-bandits-v2/local_docker_tfa\"\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e397fe-359b-4c55-bf4a-55a46bc34a44",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7e210e-c4de-4dd6-9d9b-f33c6358d810",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfp version           : 2.7.0\n",
      "storage SDK version   : 2.14.0\n",
      "bigquery SDK version  : 3.25.0\n",
      "vertex_ai SDK version : 1.71.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from pprint import pprint\n",
    "from typing import Callable, Dict, List, Optional, TypeVar, Any\n",
    "\n",
    "### pipelines\n",
    "import kfp\n",
    "from kfp import compiler, dsl, components\n",
    "from kfp.dsl import component, Metrics\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "#python warning \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage, bigquery\n",
    "\n",
    "KFP_SDK_VERSION = kfp.__version__\n",
    "GCS_SDK_VERSION = storage.__version__\n",
    "BQ_SDK_VERSION  = bigquery.__version__\n",
    "AIP_SDK_VERSION = aiplatform.__version__\n",
    "\n",
    "print(f'kfp version           : {KFP_SDK_VERSION}')\n",
    "print(f'storage SDK version   : {GCS_SDK_VERSION}')\n",
    "print(f'bigquery SDK version  : {BQ_SDK_VERSION}')\n",
    "print(f'vertex_ai SDK version : {AIP_SDK_VERSION}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b274f-dcab-44a9-ad0f-73e79360024a",
   "metadata": {},
   "source": [
    "## GCP clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22b4cd5-f648-4123-bfc5-4a2e28c1db16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# bigquery client\n",
    "bqclient = bigquery.Client(project=PROJECT_ID,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe729f5-06be-41ff-be95-a7f9dd6a682a",
   "metadata": {},
   "source": [
    "# Data preprocessing with Vertex Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b11153-425e-4c6b-8410-6cf8767b2b05",
   "metadata": {},
   "source": [
    "## create data preprocessing dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67160895-59a5-4f1a-bb78-8380b7a2ce8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/tf_vertex_agents/02-supervised-to-bandit-training/optimized_dataset\n"
     ]
    }
   ],
   "source": [
    "REPO_SRC  = \"src\"\n",
    "LOCAL_PREPROCESS_DIR = \"data_preprocessor\"\n",
    "PREPROCESS_SUBDIR = \"components\"\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c159dfa4-4e66-4102-8ab7-685897174862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf ../../$REPO_SRC/$LOCAL_PREPROCESS_DIR/$PREPROCESS_SUBDIR\n",
    "! mkdir ../../$REPO_SRC/$LOCAL_PREPROCESS_DIR/$PREPROCESS_SUBDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7677ead3-9662-4b12-b7e7-1a9622ec2b58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_config.py   write_tf_records.py\n",
      "train_validation.py  write_trajectories_to_bq.py\n"
     ]
    }
   ],
   "source": [
    "!ls ../../$REPO_SRC/$LOCAL_PREPROCESS_DIR/$PREPROCESS_SUBDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a660c-b1fb-487e-9fa1-327b37d71a4e",
   "metadata": {},
   "source": [
    "## custom components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d160a4a3-d682-466b-82b9-316be430c2b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/hybrid-vertex/mv-gpi-pipeline'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA_PIPELINE_IMAGE = f\"gcr.io/{PROJECT_ID}/rl-mv-preprocessing\"\n",
    "\n",
    "# gcr.io/hybrid-vertex/train-perarm-feats-v2\n",
    "# DATA_PIPELINE_IMAGE = IMAGE_URI_02\n",
    "\n",
    "# TODO: from section 05\n",
    "POLICY_TRAIN_IMAGE = f\"gcr.io/{PROJECT_ID}/mv-gpi-pipeline\" # mv-gpi-pipeline | mv-gpi-train\n",
    "DATA_PIPELINE_IMAGE = POLICY_TRAIN_IMAGE\n",
    "\n",
    "DATA_PIPELINE_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dec2326-5d23-4738-b77c-f68d93c12bce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID          = \"hybrid-vertex\"\n",
      "REGION              = \"us-central1\"\n",
      "PREFIX              = \"rec-bandits-v2\"\n",
      "BUCKET_NAME         = \"rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "DATA_PIPELINE_IMAGE = \"gcr.io/hybrid-vertex/mv-gpi-pipeline\"\n",
      "KFP_SDK_VERSION     = \"2.7.0\"\n",
      "GCS_SDK_VERSION     = \"2.14.0\"\n",
      "BQ_SDK_VERSION      = \"3.25.0\"\n",
      "AIP_SDK_VERSION     = \"1.71.0\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe_config = f'''PROJECT_ID          = \\\"{PROJECT_ID}\\\"\n",
    "REGION              = \\\"{REGION}\\\"\n",
    "PREFIX              = \\\"{PREFIX}\\\"\n",
    "BUCKET_NAME         = \\\"{BUCKET_NAME}\\\"\n",
    "DATA_PIPELINE_IMAGE = \\\"{DATA_PIPELINE_IMAGE}\"\n",
    "KFP_SDK_VERSION     = \\\"{KFP_SDK_VERSION}\\\"\n",
    "GCS_SDK_VERSION     = \\\"{GCS_SDK_VERSION}\\\"\n",
    "BQ_SDK_VERSION      = \\\"{BQ_SDK_VERSION}\\\"\n",
    "AIP_SDK_VERSION     = \\\"{AIP_SDK_VERSION}\\\"\n",
    "'''\n",
    "print(pipe_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18139112-291d-4055-8963-fdd452ef4dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'../../{REPO_SRC}/{LOCAL_PREPROCESS_DIR}/{PREPROCESS_SUBDIR}/pipeline_config.py', 'w') as f:\n",
    "    f.write(pipe_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f983cb5b-55d4-40cd-ba9c-c3fec23966d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_config.py\n"
     ]
    }
   ],
   "source": [
    "!ls ../../$REPO_SRC/$LOCAL_PREPROCESS_DIR/$PREPROCESS_SUBDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d997e7-5ad7-4c9f-b104-d31f06a56c60",
   "metadata": {},
   "source": [
    "### Write Trajectories to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cda82dcf-5561-4d82-b806-5171e42a0e83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../src/data_preprocessor/components/write_trajectories_to_bq.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../{REPO_SRC}/{LOCAL_PREPROCESS_DIR}/{PREPROCESS_SUBDIR}/write_trajectories_to_bq.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.dsl import (\n",
    "    component, \n",
    "    Metrics\n",
    ")\n",
    "from . import pipeline_config\n",
    "\n",
    "@component(\n",
    "    base_image=pipeline_config.DATA_PIPELINE_IMAGE,\n",
    "    install_kfp_package=False\n",
    ")\n",
    "def write_trajectories_to_bq(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    pipeline_version: str,\n",
    "    bq_dataset_name: str,\n",
    "    bucket_name: str,\n",
    "    example_gen_gcs_path: str,\n",
    "    global_emb_size: int,\n",
    "    mv_emb_size: int,\n",
    "    num_oov_buckets: int,\n",
    "    batch_size: int,\n",
    "    dataset_size: int = 0,\n",
    "    vocab_filename: str = \"vocab_dict.pkl\",\n",
    "    is_testing: bool = False,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('global_dim', int),\n",
    "    ('per_arm_dim', int),\n",
    "    ('tf_record_file', str),\n",
    "    ('bq_table_ref', str),\n",
    "    ('batch_size', int),\n",
    "]):\n",
    "    import os\n",
    "    import json\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pickle as pkl\n",
    "    from google.cloud import aiplatform, bigquery, storage\n",
    "    from typing import Callable, Dict, List, Optional, TypeVar, Any\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "    \n",
    "    # tf agents\n",
    "    import tensorflow as tf\n",
    "    from tf_agents import trajectories\n",
    "    from tf_agents.trajectories import trajectory\n",
    "    from tf_agents.bandits.policies import policy_utilities\n",
    "    from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "    \n",
    "    # this repo\n",
    "    from src.networks import encoding_network as emb_features\n",
    "    from src.data import data_utils as data_utils\n",
    "    from src.data import data_config as data_config\n",
    "    from src.utils import reward_factory as reward_factory\n",
    "    from src.data_preprocessor import preprocess_utils\n",
    "\n",
    "    # set client SDKs\n",
    "    aiplatform.init(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        # experiment=experiment_name,\n",
    "    )\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bqclient = bigquery.Client(project=project_id)\n",
    "    \n",
    "    # set variables\n",
    "    GCS_DATA_PATH        = f\"gs://{bucket_name}/{example_gen_gcs_path}\"\n",
    "    NUM_GLOBAL_FEATURES  = len(data_utils.USER_FEATURE_NAMES)     # 6\n",
    "    NUM_ARM_FEATURES     = len(data_utils.MOVIE_FEATURE_NAMES)    # 5\n",
    "    EXPECTED_GLOBAL_DIM  = global_emb_size * NUM_GLOBAL_FEATURES\n",
    "    EXPECTED_PER_ARM_DIM = mv_emb_size * NUM_ARM_FEATURES\n",
    "    \n",
    "    logging.info(f'GCS_DATA_PATH       : {GCS_DATA_PATH}')\n",
    "    logging.info(f'NUM_GLOBAL_FEATURES : {NUM_GLOBAL_FEATURES}')\n",
    "    logging.info(f'NUM_ARM_FEATURES    : {NUM_ARM_FEATURES}')\n",
    "    logging.info(f'EXPECTED_GLOBAL_DIM : {EXPECTED_GLOBAL_DIM}')\n",
    "    logging.info(f'EXPECTED_PER_ARM_DIM: {EXPECTED_PER_ARM_DIM}')\n",
    "\n",
    "    # =========================================================\n",
    "    # get data\n",
    "    # =========================================================\n",
    "    # download vocabs\n",
    "    LOCAL_VOCAB_FILENAME = 'vocab_dict.pkl'\n",
    "    print(f\"Downloading vocab...\")\n",
    "    data_utils.download_blob(\n",
    "        project_id = project_id,\n",
    "        bucket_name = bucket_name, \n",
    "        source_blob_name = f'{example_gen_gcs_path}/vocabs/{vocab_filename}', \n",
    "        destination_file_name= LOCAL_VOCAB_FILENAME\n",
    "    )\n",
    "    filehandler = open(f\"{LOCAL_VOCAB_FILENAME}\", 'rb')\n",
    "    vocab_dict = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "\n",
    "    # get train and val examples\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO\n",
    "    data_splits = [\"train\",\"val\"]\n",
    "    all_files = []\n",
    "    \n",
    "    for _split in data_splits:\n",
    "\n",
    "        for blob in storage_client.list_blobs(\n",
    "            f\"{bucket_name}\", \n",
    "            prefix=f'{example_gen_gcs_path}/{_split}'\n",
    "        ):\n",
    "            if '.tfrecord' in blob.name:\n",
    "                all_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "    print(\"Found these tfrecords:\")\n",
    "    print(all_files)\n",
    "\n",
    "    if is_testing:\n",
    "        all_files = all_files[:2]\n",
    "        print(f\"in testing mode; only using: {all_files}\")\n",
    "    dataset = tf.data.TFRecordDataset(all_files)\n",
    "    dataset = dataset.map(data_utils._parse_function)\n",
    "    \n",
    "    # =========================================================\n",
    "    # get emb dims\n",
    "    # =========================================================\n",
    "    print(f\"getting embedding dimensions...\")\n",
    "    for i in range(1):\n",
    "        iterator = iter(dataset.batch(1))\n",
    "        data = next(iterator)\n",
    "        \n",
    "    embs = emb_features.EmbeddingModel(\n",
    "        vocab_dict = vocab_dict,\n",
    "        num_oov_buckets = num_oov_buckets,\n",
    "        global_emb_size = global_emb_size,\n",
    "        mv_emb_size = mv_emb_size,\n",
    "        max_genre_length = data_config.MAX_GENRE_LENGTH,\n",
    "    )\n",
    "    test_globals = embs._get_global_context_features(data)\n",
    "    test_arms = embs._get_per_arm_features(data)\n",
    "    GLOBAL_DIM = test_globals.shape[1]            \n",
    "    PER_ARM_DIM = test_arms.shape[1]\n",
    "    print(f\"GLOBAL_DIM  : {GLOBAL_DIM}\")\n",
    "    print(f\"PER_ARM_DIM : {PER_ARM_DIM}\")\n",
    "    \n",
    "    # =========================================================\n",
    "    # trajectory function\n",
    "    # =========================================================\n",
    "    BQ_TMP_FILE   = \"tmp_bq.json\"\n",
    "    BQ_TABLE_NAME = f\"mv_b{batch_size}_g{global_emb_size}_a{mv_emb_size}_{pipeline_version}\"\n",
    "    BQ_TABLE_REF  = f\"{project_id}.{bq_dataset_name}.{BQ_TABLE_NAME}\"\n",
    "    DS_GCS_DIR_PATH = f\"gs://{bucket_name}/{example_gen_gcs_path}/{BQ_TABLE_NAME}\"\n",
    "    TFRECORD_FILE = f\"{DS_GCS_DIR_PATH}/{BQ_TABLE_NAME}.tfrecord\"\n",
    "    \n",
    "    print(f\"BQ_TMP_FILE   : {BQ_TMP_FILE}\")\n",
    "    print(f\"BQ_TABLE_NAME : {BQ_TABLE_NAME}\")\n",
    "    print(f\"BQ_TABLE_REF  : {BQ_TABLE_REF}\")\n",
    "    print(f\"DS_GCS_DIR_PATH : {DS_GCS_DIR_PATH}\")\n",
    "    print(f\"TFRECORD_FILE : {TFRECORD_FILE}\")\n",
    "    \n",
    "    # my trajectory functions\n",
    "    def my_trajectory_fn(element):\n",
    "        \"\"\"Converts a dataset element into a trajectory.\"\"\"\n",
    "        global_features = embs._get_global_context_features(element)\n",
    "        arm_features = embs._get_per_arm_features(element)\n",
    "\n",
    "        observation = {\n",
    "            bandit_spec_utils.GLOBAL_FEATURE_KEY: global_features\n",
    "        }\n",
    "        reward = reward_factory._get_rewards(element)\n",
    "\n",
    "        policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "            chosen_arm_features=arm_features,\n",
    "        )\n",
    "        return trajectory.single_step(\n",
    "            observation=observation,\n",
    "            action=tf.zeros_like(\n",
    "                reward, dtype=tf.int32\n",
    "            ),\n",
    "            policy_info=policy_info,\n",
    "            reward=reward,\n",
    "            discount=tf.zeros_like(reward)\n",
    "        )\n",
    "\n",
    "    # # calculate dataset_size\n",
    "    # if not dataset_size:\n",
    "    #     print(f\"getting size of dataset...\")\n",
    "    #     dataset_size = dataset.reduce(0, lambda x,_: x+1).numpy()\n",
    "    \n",
    "    # write to local file\n",
    "    print(f\"writting trajectories to tmp file...\")\n",
    "    with open(BQ_TMP_FILE, \"w\") as f:\n",
    "        for example in dataset.batch(batch_size, drop_remainder=True): #.take(count=dataset_size):\n",
    "            _trajectories = my_trajectory_fn(example)\n",
    "            _traj_dict = preprocess_utils.build_dict_from_trajectory(_trajectories)\n",
    "            f.write(json.dumps(_traj_dict) + \"\\n\")\n",
    "\n",
    "    print(f\"saving tmp file to: {example_gen_gcs_path}/{BQ_TABLE_NAME}/{BQ_TMP_FILE}\")\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(f\"{example_gen_gcs_path}/{BQ_TABLE_NAME}/{BQ_TMP_FILE}\")\n",
    "    blob.upload_from_filename(BQ_TMP_FILE)\n",
    "    \n",
    "    print(f\"loading tmp file to bigquery...\")\n",
    "    with open(BQ_TMP_FILE, \"rb\") as source_file:\n",
    "        load_job = bqclient.load_table_from_file(\n",
    "            source_file, \n",
    "            BQ_TABLE_REF, \n",
    "            job_config=preprocess_utils.job_config\n",
    "        )\n",
    "    load_job.result() \n",
    "    \n",
    "    # check table\n",
    "    bq_table = bqclient.get_table(BQ_TABLE_REF)\n",
    "    print(f\"Got table: `{bq_table.project}.{bq_table.dataset_id}.{bq_table.table_id}`\")\n",
    "    print(\"Table has {} rows\".format(bq_table.num_rows))\n",
    "\n",
    "    return (\n",
    "        GLOBAL_DIM,\n",
    "        PER_ARM_DIM,\n",
    "        TFRECORD_FILE,\n",
    "        BQ_TABLE_REF,\n",
    "        batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee1be7b-f41b-4cdf-a9c5-33840b4bccbe",
   "metadata": {},
   "source": [
    "### Write TF Records from BigQuery table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e49f32f0-c1a0-4ed1-b033-456a3fc38753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../src/data_preprocessor/components/write_tf_records.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../{REPO_SRC}/{LOCAL_PREPROCESS_DIR}/{PREPROCESS_SUBDIR}/write_tf_records.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.dsl import (\n",
    "    component, \n",
    "    Metrics\n",
    ")\n",
    "from . import pipeline_config\n",
    "\n",
    "@component(\n",
    "    base_image=pipeline_config.DATA_PIPELINE_IMAGE,\n",
    "    install_kfp_package=False\n",
    ")\n",
    "def write_tf_records(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    pipeline_version: str,\n",
    "    bq_table_ref: str,\n",
    "    tf_record_file: str,\n",
    "    global_dim: int,\n",
    "    per_arm_dim: int,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('tf_record_file', str),\n",
    "    ('global_dim', int),\n",
    "    ('per_arm_dim', int),\n",
    "    ('bq_table_ref', str),\n",
    "]):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    # this repo\n",
    "    from src.data import data_utils as data_utils\n",
    "    from src.data_preprocessor import preprocess_utils\n",
    "    \n",
    "    bqclient = bigquery.Client(project=project_id)\n",
    "    \n",
    "    # get bq table iterator\n",
    "    print(f\"getting bq table iterator...\")\n",
    "    \n",
    "    bq_table = bqclient.get_table(bq_table_ref)\n",
    "    print(f\"Got table: `{bq_table.project}.{bq_table.dataset_id}.{bq_table.table_id}`\")\n",
    "    print(\"Table has {} rows\".format(bq_table.num_rows))\n",
    "\n",
    "    table_row_iter = bqclient.list_rows(bq_table)\n",
    "    \n",
    "    print(f\"writting bq to tf records...\")\n",
    "    preprocess_utils.write_tfrecords(tf_record_file, table_row_iter)\n",
    "    \n",
    "    print(f\"tf record complete: {tf_record_file}\")\n",
    "    \n",
    "    return (\n",
    "        f'{tf_record_file}',\n",
    "        global_dim,\n",
    "        per_arm_dim,\n",
    "        bq_table_ref,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a395efc-7c60-4308-89ec-3a5b087e0a30",
   "metadata": {},
   "source": [
    "### Validate dataset with train job\n",
    "\n",
    "> here we'll create a simple train job to validate our dataset is formatted correctly and the trajectories can be consumed with `agent.train(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb9d40a2-74b9-4e8e-a9be-6b4ea6831a24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../src/data_preprocessor/components/train_validation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../{REPO_SRC}/{LOCAL_PREPROCESS_DIR}/{PREPROCESS_SUBDIR}/train_validation.py\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.dsl import (\n",
    "    component, \n",
    "    Metrics\n",
    ")\n",
    "from . import pipeline_config\n",
    "\n",
    "@component(\n",
    "    base_image=pipeline_config.DATA_PIPELINE_IMAGE,\n",
    "    install_kfp_package=False\n",
    ")\n",
    "def train_validation(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    pipeline_version: str,\n",
    "    bucket_name: str,\n",
    "    bq_table_ref: str,\n",
    "    tf_record_file: str,\n",
    "    batch_size: int,\n",
    "    num_actions: int,\n",
    "    global_dim: int,\n",
    "    per_arm_dim: int,\n",
    "    experiment_name: str,\n",
    "    num_epochs: int = 2,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('log_dir', str),\n",
    "    ('tf_record_file', str),\n",
    "]):\n",
    "    import os\n",
    "    import time\n",
    "    import numpy as np\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "    from google.cloud import aiplatform, storage\n",
    "    from typing import Dict, List, Any\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from tf_agents.specs import array_spec\n",
    "    from tf_agents.specs import tensor_spec\n",
    "    from tf_agents.policies import policy_saver\n",
    "    from tf_agents import trajectories\n",
    "    from tf_agents.trajectories import time_step as ts\n",
    "    from tf_agents.bandits.policies import policy_utilities\n",
    "    from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "    from tf_agents.metrics import tf_metrics\n",
    "    \n",
    "    # this repo\n",
    "    from src.utils import train_utils as train_utils\n",
    "    from src.data_preprocessor import preprocess_utils\n",
    "    from src.agents import agent_factory as agent_factory\n",
    "    \n",
    "    # set experiment config for tracking\n",
    "    invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    RUN_NAME          = f'run-{invoke_time}'\n",
    "    BASE_OUTPUT_DIR   = f\"gs://{bucket_name}/{experiment_name}/{RUN_NAME}\"\n",
    "    LOG_DIR           = f\"{BASE_OUTPUT_DIR}/logs\"\n",
    "    ARTIFACTS_DIR     = f\"{BASE_OUTPUT_DIR}/artifacts\"\n",
    "    \n",
    "    print(f\"BASE_OUTPUT_DIR : {BASE_OUTPUT_DIR}\")\n",
    "    print(f\"LOG_DIR         : {LOG_DIR}\")\n",
    "    print(f\"ARTIFACTS_DIR   : {ARTIFACTS_DIR}\")\n",
    "    \n",
    "    aiplatform.init(project=project_id, location=location)\n",
    "    \n",
    "    # tensorboard = aiplatform.Tensorboard.create(\n",
    "    #     display_name=experiment_name\n",
    "    #     , project=project_id\n",
    "    #     , location=location\n",
    "    # )\n",
    "    # TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "    # TB_ID = TB_RESOURCE_NAME.split('/')[-1]\n",
    "    \n",
    "    # set agent config\n",
    "    AGENT_TYPE      = 'epsGreedy' # 'LinUCB' | 'LinTS |, 'epsGreedy' | 'NeuralLinUCB'\n",
    "    AGENT_ALPHA     = 0.1\n",
    "    EPSILON         = 0.01\n",
    "    LR              = 0.05\n",
    "    ENCODING_DIM    = 1\n",
    "    EPS_PHASE_STEPS = 1000\n",
    "    GLOBAL_LAYERS   = [global_dim, int(global_dim/2), int(global_dim/4)]\n",
    "    ARM_LAYERS      = [per_arm_dim, int(per_arm_dim/2), int(per_arm_dim/4)]\n",
    "    FIRST_COMMON_LAYER = GLOBAL_LAYERS[-1] + ARM_LAYERS[-1]\n",
    "    COMMON_LAYERS = [\n",
    "        int(FIRST_COMMON_LAYER),\n",
    "        int(FIRST_COMMON_LAYER/4)\n",
    "    ]\n",
    "    NETWORK_TYPE = \"commontower\"\n",
    "    \n",
    "    # set tensor specs\n",
    "    observation_spec = {\n",
    "        'global': tf.TensorSpec([global_dim], tf.float32),\n",
    "        'per_arm': tf.TensorSpec([num_actions, per_arm_dim], tf.float32) #excluding action dim here\n",
    "    }\n",
    "    action_spec = tensor_spec.BoundedTensorSpec(\n",
    "        shape=[], \n",
    "        dtype=tf.int32,\n",
    "        minimum=tf.constant(0),            \n",
    "        maximum=num_actions-1, # n degrees of freedom and will dictate the expected mean reward spec shape\n",
    "        name=\"action_spec\"\n",
    "    )\n",
    "    time_step_spec = ts.time_step_spec(observation_spec = observation_spec)\n",
    "\n",
    "    reward_spec = {\n",
    "        \"reward\": array_spec.ArraySpec(\n",
    "            shape=[batch_size], \n",
    "            dtype=np.float32, name=\"reward\"\n",
    "        )\n",
    "    }\n",
    "    reward_tensor_spec = train_utils.from_spec(reward_spec)\n",
    "    \n",
    "    # create agent\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "    agent = agent_factory.PerArmAgentFactory._get_agent(\n",
    "        agent_type = AGENT_TYPE,\n",
    "        network_type = NETWORK_TYPE,\n",
    "        time_step_spec = time_step_spec,\n",
    "        action_spec = action_spec,\n",
    "        observation_spec=observation_spec,\n",
    "        global_layers = GLOBAL_LAYERS,\n",
    "        arm_layers = ARM_LAYERS,\n",
    "        common_layers = COMMON_LAYERS,\n",
    "        agent_alpha = AGENT_ALPHA,\n",
    "        learning_rate = LR,\n",
    "        epsilon = EPSILON,\n",
    "        train_step_counter = global_step,\n",
    "        output_dim = ENCODING_DIM,\n",
    "        eps_phase_steps = EPS_PHASE_STEPS,\n",
    "        summarize_grads_and_vars = False,\n",
    "        debug_summaries = True\n",
    "    )\n",
    "    agent.initialize()\n",
    "    print(f'agent: {agent.name}')\n",
    "    \n",
    "    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        f\"{LOG_DIR}\", flush_millis=10 * 1000\n",
    "    )\n",
    "    train_summary_writer.set_as_default()\n",
    "    saver = policy_saver.PolicySaver(\n",
    "        agent.policy, \n",
    "        train_step=global_step\n",
    "    )\n",
    "    metrics = [\n",
    "        # tf_metrics.NumberOfEpisodes(),\n",
    "        # tf_metrics.AverageEpisodeLengthMetric(batch_size=batch_size),\n",
    "        tf_metrics.AverageReturnMetric(batch_size=batch_size)\n",
    "    ]\n",
    "    # create dataset\n",
    "    raw_dataset = tf.data.TFRecordDataset([tf_record_file])\n",
    "    parsed_dataset = raw_dataset.map(\n",
    "        preprocess_utils._parse_record\n",
    "    ).prefetch(\n",
    "        tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    # trajectory function\n",
    "    def _build_trajectory_from_tfrecord(\n",
    "        parsed_record: Dict[str, tf.Tensor],\n",
    "        batch_size: int,\n",
    "        num_actions: int,\n",
    "        # policy_info: policies.utils.PolicyInfo\n",
    "    ) -> trajectories.Trajectory:\n",
    "        \"\"\"\n",
    "        Builds a `trajectories.Trajectory` object from `parsed_record`.\n",
    "\n",
    "        Args:\n",
    "          parsed_record: A dict mapping feature names to values as `tf.Tensor`\n",
    "            objects of type string containing serialized protos.\n",
    "          policy_info: Policy information specification.\n",
    "\n",
    "        Returns:\n",
    "          A `trajectories.Trajectory` object that contains values as de-serialized\n",
    "          `tf.Tensor` objects from `parsed_record`.\n",
    "        \"\"\"\n",
    "        dummy_rewards = tf.zeros([batch_size, 1, num_actions])\n",
    "\n",
    "        global_features = tf.expand_dims(\n",
    "            tf.io.parse_tensor(parsed_record[\"observation\"], out_type=tf.float32),\n",
    "            axis=1\n",
    "        )\n",
    "        observation = {\n",
    "            bandit_spec_utils.GLOBAL_FEATURE_KEY: global_features\n",
    "        }\n",
    "\n",
    "        arm_features = tf.expand_dims(\n",
    "            tf.io.parse_tensor(parsed_record[\"chosen_arm_features\"], out_type=tf.float32),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "            chosen_arm_features=arm_features,\n",
    "            predicted_rewards_mean=dummy_rewards,\n",
    "            bandit_policy_type=tf.zeros([batch_size, 1, 1], dtype=tf.int32)\n",
    "        )\n",
    "\n",
    "        return trajectories.Trajectory(\n",
    "            step_type=tf.expand_dims(\n",
    "                tf.io.parse_tensor(parsed_record[\"step_type\"], out_type=tf.int32),\n",
    "                axis=1\n",
    "            ),\n",
    "            observation = observation,\n",
    "            action=tf.expand_dims(\n",
    "                tf.io.parse_tensor(parsed_record[\"action\"], out_type=tf.int32),\n",
    "                axis=1\n",
    "            ),\n",
    "            policy_info=policy_info,\n",
    "            next_step_type=tf.expand_dims(\n",
    "                tf.io.parse_tensor(\n",
    "                    parsed_record[\"next_step_type\"], out_type=tf.int32),\n",
    "                axis=1\n",
    "            ),\n",
    "            reward=tf.expand_dims(\n",
    "                tf.io.parse_tensor(parsed_record[\"reward\"], out_type=tf.float32),\n",
    "                axis=1\n",
    "            ),\n",
    "            discount=tf.expand_dims(\n",
    "                tf.io.parse_tensor(parsed_record[\"discount\"], out_type=tf.float32),\n",
    "                axis=1\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # train job\n",
    "    list_o_loss = []\n",
    "    # Reset the train step\n",
    "    agent.train_step_counter.assign(0)\n",
    "\n",
    "    print(f\"starting train job...\")\n",
    "    start_time = time.time()\n",
    "    # tf.profiler.experimental.start(LOG_DIR)\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        print(f\"epoch: {i+1}\")\n",
    "\n",
    "        for parsed_record in parsed_dataset:\n",
    "\n",
    "            _trajectories = _build_trajectory_from_tfrecord(\n",
    "                parsed_record, batch_size, num_actions\n",
    "            )\n",
    "\n",
    "            step = agent.train_step_counter.numpy()\n",
    "            loss = agent.train(experience=_trajectories)\n",
    "            list_o_loss.append(loss.loss.numpy())\n",
    "\n",
    "            train_utils._export_metrics_and_summaries(\n",
    "                step=i, \n",
    "                metrics=metrics\n",
    "            )\n",
    "\n",
    "            # print step loss\n",
    "            if step % 100 == 0:\n",
    "                print(\n",
    "                    'step = {0}: train loss = {1}'.format(\n",
    "                        step, round(loss.loss.numpy(), 2)\n",
    "                    )\n",
    "                )\n",
    "    # tf.profiler.experimental.stop()\n",
    "    runtime_mins = int((time.time() - start_time) / 60)\n",
    "    print(f\"train runtime_mins: {runtime_mins}\")\n",
    "    \n",
    "    # # one time upload\n",
    "    # aiplatform.upload_tb_log(\n",
    "    #     tensorboard_id=TB_ID,\n",
    "    #     tensorboard_experiment_name=experiment_name,\n",
    "    #     logdir=LOG_DIR,\n",
    "    #     experiment_display_name=experiment_name,\n",
    "    #     run_name_prefix=RUN_NAME,\n",
    "    #     # description=description,\n",
    "    # )\n",
    "    \n",
    "    print(f\"LOG_DIR: {LOG_DIR}\")\n",
    "    print(f\"tf_record_file: {tf_record_file}\")\n",
    "    \n",
    "    return (\n",
    "        LOG_DIR,\n",
    "        tf_record_file\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7352db-623e-46d7-9c1f-ef6fe8e08b2f",
   "metadata": {},
   "source": [
    "# Create Vertex Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e44cfe85-9f29-41a5-b062-f18b0c0a0175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from src.data_preprocessor.components import (\n",
    "    write_trajectories_to_bq,\n",
    "    train_validation,\n",
    "    write_tf_records,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33efa3ab-70bb-4f63-a8eb-5d092a8d2a93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISPLAY_NAME: movielens-rl-data-pipeline-v8\n"
     ]
    }
   ],
   "source": [
    "PIPE_VERSION = \"v8\"\n",
    "EXPERIMENT_NAME = \"movielens-rl-data-pipeline\"\n",
    "DISPLAY_NAME = f\"{EXPERIMENT_NAME}-{PIPE_VERSION}\".replace(\"_\",\"-\")\n",
    "print(f\"DISPLAY_NAME: {DISPLAY_NAME}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b01bbb7-7aaa-43f6-9277-1922cd53f4de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=f\"{DISPLAY_NAME}\",\n",
    ")\n",
    "def data_preprocess_pipeline(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    pipeline_version: str,\n",
    "    experiment_name: str,\n",
    "    bq_dataset_name: str,\n",
    "    bucket_name: str,\n",
    "    example_gen_gcs_path: str,\n",
    "    batch_size: int,\n",
    "    num_actions: int,\n",
    "    global_emb_size: int,\n",
    "    mv_emb_size: int,\n",
    "    num_oov_buckets: int,\n",
    "    dataset_size: int = 0,\n",
    "    num_epochs: int = 2,\n",
    "    vocab_filename: str = \"vocab_dict.pkl\",\n",
    "    is_testing: bool = True\n",
    "):\n",
    "    import logging\n",
    "    \n",
    "    write_trajectories_op = (\n",
    "        write_trajectories_to_bq.write_trajectories_to_bq(\n",
    "            project_id=project_id,\n",
    "            location=location,\n",
    "            pipeline_version=pipeline_version,\n",
    "            bq_dataset_name=bq_dataset_name,\n",
    "            bucket_name=bucket_name,\n",
    "            example_gen_gcs_path=example_gen_gcs_path,\n",
    "            global_emb_size=global_emb_size,\n",
    "            mv_emb_size=mv_emb_size,\n",
    "            num_oov_buckets=num_oov_buckets,\n",
    "            batch_size=batch_size,\n",
    "            dataset_size=dataset_size,\n",
    "            vocab_filename=\"vocab_dict.pkl\",\n",
    "            is_testing=is_testing,\n",
    "        )\n",
    "        .set_display_name(\"Write to BQ\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    write_tf_records_op = (\n",
    "        write_tf_records.write_tf_records(\n",
    "            project_id=project_id,\n",
    "            location=location,\n",
    "            pipeline_version=pipeline_version,\n",
    "            bq_table_ref=write_trajectories_op.outputs['bq_table_ref'],\n",
    "            tf_record_file=write_trajectories_op.outputs['tf_record_file'],\n",
    "            global_dim=write_trajectories_op.outputs['global_dim'],\n",
    "            per_arm_dim=write_trajectories_op.outputs['per_arm_dim'],\n",
    "        )\n",
    "        .set_display_name(\"Write TF Records\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    train_validation_op = (\n",
    "        train_validation.train_validation(\n",
    "            project_id=project_id,\n",
    "            location=location,\n",
    "            pipeline_version=pipeline_version,\n",
    "            bucket_name=bucket_name,\n",
    "            bq_table_ref=write_tf_records_op.outputs['bq_table_ref'],\n",
    "            tf_record_file=write_tf_records_op.outputs['tf_record_file'],\n",
    "            batch_size=batch_size,\n",
    "            num_actions=num_actions,\n",
    "            global_dim=write_tf_records_op.outputs['global_dim'],\n",
    "            per_arm_dim=write_tf_records_op.outputs['per_arm_dim'],\n",
    "            experiment_name=experiment_name,\n",
    "            num_epochs=num_epochs,\n",
    "        )\n",
    "        .set_display_name(\"Test w/ agent\")\n",
    "        .set_caching_options(True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd3051bc-a5b6-49a0-9978-eaa0213f4bee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://preprocess_pipeline.yaml [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 31.2 KiB/ 31.2 KiB]                                                \n",
      "Operation completed over 1 objects/31.2 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "PIPELINE_YAML_FILENAME = \"preprocess_pipeline.yaml\"\n",
    "\n",
    "! rm -f $PIPELINE_YAML_FILENAME\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=data_preprocess_pipeline, \n",
    "    package_path=PIPELINE_YAML_FILENAME\n",
    ")\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/data-preprocess-pipelines/{EXPERIMENT_NAME}\"\n",
    "PIPELINES_FILEPATH = f\"{PIPELINE_ROOT}/{PIPELINE_YAML_FILENAME}\"\n",
    "\n",
    "!gsutil cp $PIPELINE_YAML_FILENAME $PIPELINES_FILEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ee54ea9-988e-42a8-8093-dd5147c84756",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE            : 256\n",
      "NUM_ACTIONS           : 2\n",
      "EXAMPLE_GEN_GCS_PATH  : data/movielens/m1m\n",
      "NUM_OOV_BUCKETS       : 1\n",
      "GLOBAL_EMBEDDING_SIZE : 12\n",
      "MV_EMBEDDING_SIZE     : 16\n",
      "EXPECTED_GLOBAL_DIM   : 72\n",
      "EXPECTED_PER_ARM_DIM  : 64\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"../..\")\n",
    "from src.data import data_config as data_config\n",
    "from src.data import data_utils as data_utils\n",
    "\n",
    "IS_TESTING = False\n",
    "\n",
    "BATCH_SIZE            = 256\n",
    "NUM_ACTIONS           = 2\n",
    "EXAMPLE_GEN_GCS_PATH  = data_config.EXAMPLE_GEN_GCS_PATH\n",
    "NUM_OOV_BUCKETS       = 1\n",
    "GLOBAL_EMBEDDING_SIZE = 12\n",
    "MV_EMBEDDING_SIZE     = 16\n",
    "\n",
    "NUM_GLOBAL_FEATURES   = len(data_utils.USER_FEATURE_NAMES)     # 6\n",
    "NUM_ARM_FEATURES      = len(data_utils.MOVIE_FEATURE_NAMES)    # 5\n",
    "EXPECTED_GLOBAL_DIM   = GLOBAL_EMBEDDING_SIZE * NUM_GLOBAL_FEATURES\n",
    "EXPECTED_PER_ARM_DIM  = MV_EMBEDDING_SIZE * NUM_ARM_FEATURES\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "print(f\"BATCH_SIZE            : {BATCH_SIZE}\")\n",
    "print(f\"NUM_ACTIONS           : {NUM_ACTIONS}\")\n",
    "print(f\"EXAMPLE_GEN_GCS_PATH  : {EXAMPLE_GEN_GCS_PATH}\")\n",
    "print(f\"NUM_OOV_BUCKETS       : {NUM_OOV_BUCKETS}\")\n",
    "print(f\"GLOBAL_EMBEDDING_SIZE : {GLOBAL_EMBEDDING_SIZE}\")\n",
    "print(f\"MV_EMBEDDING_SIZE     : {MV_EMBEDDING_SIZE}\")\n",
    "print(f\"EXPECTED_GLOBAL_DIM   : {EXPECTED_GLOBAL_DIM}\")\n",
    "print(f\"EXPECTED_PER_ARM_DIM  : {EXPECTED_PER_ARM_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f889689a-2fd9-4018-bad6-789aa2941dde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__pycache__\t    train_validation.py  write_trajectories_to_bq.py\n",
      "pipeline_config.py  write_tf_records.py\n"
     ]
    }
   ],
   "source": [
    "!ls ../../$REPO_SRC/$LOCAL_PREPROCESS_DIR/$PREPROCESS_SUBDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ecd5b5c-f915-4a4a-b889-00d4db7bc633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    location=LOCATION,\n",
    "    template_path=PIPELINE_YAML_FILENAME,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    failure_policy='fast',\n",
    "    parameter_values={\n",
    "        \"project_id\": PROJECT_ID, # str,\n",
    "        \"location\": LOCATION, # str,\n",
    "        \"pipeline_version\": PIPE_VERSION, # str,\n",
    "        \"experiment_name\": EXPERIMENT_NAME, # str,\n",
    "        \"bq_dataset_name\": BIGQUERY_DATASET_NAME, # str,\n",
    "        \"bucket_name\": BUCKET_NAME, # str,\n",
    "        \"example_gen_gcs_path\": EXAMPLE_GEN_GCS_PATH, # str,\n",
    "        \"batch_size\": BATCH_SIZE, # int,\n",
    "        \"num_actions\": NUM_ACTIONS, # int,\n",
    "        \"global_emb_size\": GLOBAL_EMBEDDING_SIZE, # int,\n",
    "        \"mv_emb_size\": MV_EMBEDDING_SIZE, # int,\n",
    "        \"num_oov_buckets\": NUM_OOV_BUCKETS, # int,\n",
    "        \"dataset_size\": 0, # int = 0,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"vocab_filename\": \"vocab_dict.pkl\", # str = \"vocab_dict.pkl\",\n",
    "        \"is_testing\": IS_TESTING, # bool = True\n",
    "        \n",
    "    },\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "job.submit(\n",
    "    # experiment=EXPERIMENT_NAME,\n",
    "    # sync=False,\n",
    "    service_account=VERTEX_SA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb201f45-9320-4119-b4e8-fc8faaa4cd55",
   "metadata": {},
   "source": [
    "**Finished**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
