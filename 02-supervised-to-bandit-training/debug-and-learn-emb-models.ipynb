{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34230a4a-7296-429e-b5b0-c8e0674ba2d2",
   "metadata": {},
   "source": [
    "# Building embedding models with preprocess global & per-arm features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9849b480-18f0-457b-bbc8-3d612eec7142",
   "metadata": {},
   "source": [
    "**Use this notebook to better understand how the emb preprocessing functions work:**\n",
    "* the dimensions produced at each step\n",
    "* working with tensors (e.g., concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7eaf96-d4ad-438b-bdb3-73cb0dfd4691",
   "metadata": {},
   "source": [
    "The preproccesing layers will ultimately feed the two sampling functions described below. These sampling functions will be used to create [trajectories](https://github.com/tensorflow/agents/blob/master/tf_agents/trajectories/trajectory.py#L36) (i.e., the training examples for our model)\n",
    "\n",
    "`global_context_sampling_fn`: \n",
    "* A function that outputs a random 1d array or list of ints or floats\n",
    "* This output is the global context. Its shape and type must be consistent across calls.\n",
    "\n",
    "`arm_context_sampling_fn`: \n",
    "* A function that outputs a random 1 array or list of ints or floats (same type as the output of `global_context_sampling_fn`). * This output is the per-arm context. Its shape must be consistent across calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd7426-859e-4e05-a7e9-bb587eb1663e",
   "metadata": {},
   "source": [
    "## Notebook config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad051d2a-a5d1-497c-809d-81a4c56e9c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'mabv1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6084faff-b86b-4fa7-9ff8-eb8bd5d70575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e3aa84-6c80-4f09-b5aa-be5e792056be",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15348de-9cd7-4e58-876c-60b2b1c006c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pprint import pprint\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# TF-agents\n",
    "# from tf_agents.bandits.agents import neural_epsilon_greedy_agent\n",
    "# from tf_agents.bandits.agents import neural_linucb_agent\n",
    "# from tf_agents.bandits.networks import global_and_arm_feature_network\n",
    "from tf_agents.bandits.policies import policy_utilities\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "# GPU\n",
    "from numba import cuda \n",
    "import gc\n",
    "\n",
    "# tf exceptions and vars\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a7472-2826-481d-866b-9ab38fdff807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# this repo\n",
    "# from src.per_arm_rl import data_utils\n",
    "# from src.per_arm_rl import data_config\n",
    "# from src.per_arm_rl import train_utils as train_utils\n",
    "\n",
    "# this repo\n",
    "# from src.utils import movielens_ds_utils\n",
    "from src.data import data_utils as data_utils\n",
    "from src.data import data_config as data_config\n",
    "\n",
    "# from src.per_arm_rl import train_utils as train_utils\n",
    "from src import train_utils as train_utils\n",
    "from src.trainer import eval_perarm as eval_perarm\n",
    "from src.trainer import train_perarm as train_perarm\n",
    "\n",
    "# from src.perarm_features import emb_feature as emb_features\n",
    "from src.networks import encoding_network as emb_features\n",
    "\n",
    "from src.perarm_features import agent_factory as agent_factory\n",
    "from src.perarm_features import reward_factory as reward_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9cb358-ef5c-4fcc-878c-8b8d591fd09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad85ca-1cdb-492b-815b-5c83012e0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8543b533-3b6d-4f8d-9cf2-726839264a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e38dd98-0598-45d8-b00f-3cee68cc862f",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e92fbe4-1486-4634-82ff-e9aa1c82002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET = \"movielens\"\n",
    "DATA_TAG = f\"{DATA_SET}/movielens-1m\" # movielens-100k | movielens-1m\n",
    "\n",
    "print(f\"DATA_TAG: {DATA_TAG}\")\n",
    "\n",
    "! gsutil ls $DATA_PATH/$DATA_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ea09c-3dc1-40c6-ad9e-7eae192e3053",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79a5cb-20f4-41d0-8ac9-18bdb288d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = \"train\" # \"train\" | \"val\"\n",
    "\n",
    "train_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{DATA_GCS_PREFIX}/{DATA_TAG}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "train_files = train_files[:3]\n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccdab28-a4bd-4913-bc34-401cfc912693",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "train_dataset = train_dataset.map(movielens_ds_utils.parse_tfrecord)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068d466d-0b05-411b-a148-8781d98b6e2a",
   "metadata": {},
   "source": [
    "### get vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c637490-3136-430c-b7d0-08abf1fbe725",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_VOCABS = False\n",
    "\n",
    "print(f\"GENERATE_VOCABS: {GENERATE_VOCABS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7fe132-730c-4f94-aed8-fea69aab9fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GENERATE_VOCABS:\n",
    "\n",
    "    EXISTING_VOCAB_FILE = f'gs://{BUCKET_NAME}/{DATA_GCS_PREFIX}/{DATA_TAG}/{VOCAB_FILENAME}'\n",
    "    print(f\"Downloading vocab...\")\n",
    "    \n",
    "    os.system(f'gsutil -q cp {EXISTING_VOCAB_FILE} .')\n",
    "    print(f\"Downloaded vocab from: {EXISTING_VOCAB_FILE}\\n\")\n",
    "\n",
    "    filehandler = open(VOCAB_FILENAME, 'rb')\n",
    "    vocab_dict = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "    for key in vocab_dict.keys():\n",
    "        pprint(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba727a-e3bb-44f3-a4d4-5ef51aacbbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    \n",
    "    iterator = iter(train_dataset.batch(1))\n",
    "    data = next(iterator)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e424e5c6-e030-405b-a0f0-bf2f1791c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OOV_BUCKETS        = 1\n",
    "GLOBAL_EMBEDDING_SIZE  = 16\n",
    "MV_EMBEDDING_SIZE      = 32 #32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf65f6c-b85b-4eba-ba9b-b593edc33569",
   "metadata": {},
   "source": [
    "## check your embedding / encoding network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18de0ee6-1146-46a0-a651-42ddf0ad7257",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = emb_features.EmbeddingModel(\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a455d8-5cd4-4334-92cc-4c055e9e901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb2a9f-2983-4158-a629-df7a8993b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_features = embs._get_global_context_features(data)\n",
    "global_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42675575-59f3-4374-8716-a04488919893",
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_features = embs._get_per_arm_features(data)\n",
    "arm_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f3e9d1-896f-4916-8715-0bff91a7556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_features = train_utils._add_outer_dimension(arm_features)\n",
    "arm_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d791a5-86a8-465d-9239-d9a1cd7dc081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # padded_batch(2, padded_shapes=5)\n",
    "\n",
    "# for x in train_dataset.padded_batch(HPARAMS['batch_size']).take(1):\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b600bf7-8616-4574-a856-e2d35fbf267f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3490f4f-b800-4cab-a21b-0c9797762167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac2b060-2997-4877-bd8f-9ef9b2eb8b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3192d12f-146e-4a74-8c08-1c4718d3c100",
   "metadata": {},
   "source": [
    "## Understanding tensor shapes, rank and how to manipulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd905b8c-a769-4435-a4ef-887efe3242c5",
   "metadata": {},
   "source": [
    "### Check the differences in these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ea1a8-2c2f-45d0-89db-9801cc1b5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_globals= [\n",
    "    0.05226095, -0.04546688, -0.05914654,  0.03443705, -0.04011744,\n",
    "   -0.05921736,  0.05578206, -0.02147666,  0.00166732,  0.04055796,\n",
    "    0.06458487, -0.05492309, -0.06472961, -0.00705546, -0.05592869,\n",
    "   -0.01938318,  0.03898788, -0.04043241, -0.0182637 , -0.0499408 ,\n",
    "   -0.05968586,  0.06301413,  0.00032848,  0.06395795,  0.01845439,\n",
    "    0.04108731, -0.05026846,  0.01969895, -0.02506991,  0.02361025,\n",
    "    0.00762446, -0.00464374,  0.01902852,  0.03852094, -0.04125774,\n",
    "   -0.04153034, -0.03931752,  0.05585755, -0.03481127, -0.04961544,\n",
    "   -0.04787084,  0.06189156, -0.04888101, -0.07491934, -0.07062666,\n",
    "   -0.02748476, -0.01719889, -0.06808205\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a94921f-9667-4819-a681-98c61ba7e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_globals\n",
    "test_list_seq = [test_globals[0].numpy(), test_globals[0].numpy()*2]\n",
    "test_list_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd4090-c9f2-4214-9331-80d2eccbd3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_test_v1 = tf.reduce_mean(test_list_seq, axis=[0,1])\n",
    "reduce_test_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a7a6a-f59a-4e35-abe7-c80431ca3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_test_v2 = tf.reduce_mean(test_list_seq, axis=[0])\n",
    "reduce_test_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03875aa3-0391-4bad-a30b-36ffc8c1aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_test_v3 = tf.reduce_mean(test_list_seq, axis=[1])\n",
    "reduce_test_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23063ad0-fb5d-4581-a835-7cae3aeb8163",
   "metadata": {},
   "source": [
    "### reshape tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9accc569-ede6-480c-a219-26b903779a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [[1, 2, 3,4,5,6,7,8,9]]\n",
    "a = [1, 2, 3,4,5,6,7,8,9]\n",
    "\n",
    "b = tf.reshape(a, [-1, 9, 1])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4be99-20b0-4ec3-840b-9590de6b50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.rank(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b32d9-672a-470f-9150-f1b9e72abf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_globals= [\n",
    "    0.05226095, -0.04546688, -0.05914654,  0.03443705, -0.04011744,\n",
    "   -0.05921736,  0.05578206, -0.02147666,  0.00166732,  0.04055796,\n",
    "    0.06458487, -0.05492309, -0.06472961, -0.00705546, -0.05592869,\n",
    "   -0.01938318,  0.03898788, -0.04043241, -0.0182637 , -0.0499408 ,\n",
    "   -0.05968586,  0.06301413,  0.00032848,  0.06395795,  0.01845439,\n",
    "    0.04108731, -0.05026846,  0.01969895, -0.02506991,  0.02361025,\n",
    "    0.00762446, -0.00464374,  0.01902852,  0.03852094, -0.04125774,\n",
    "   -0.04153034, -0.03931752,  0.05585755, -0.03481127, -0.04961544,\n",
    "   -0.04787084,  0.06189156, -0.04888101, -0.07491934, -0.07062666,\n",
    "   -0.02748476, -0.01719889, -0.06808205\n",
    "]\n",
    "\n",
    "# print(test_globals)\n",
    "print(tf.rank(test_globals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f3f62e-ff46-4992-988e-e224fae93e5b",
   "metadata": {},
   "source": [
    "### dont forget about the batch dimenion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a70983d-142f-4940-8e32-5f587ed5be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OOV_BUCKETS = 1\n",
    "MV_EMBEDDING_SIZE=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c5469-5da3-45a7-adee-92026a929d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_MAX_LENGTH=10\n",
    "MAX_VECT_LEN = 10\n",
    "\n",
    "# vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "#  max_tokens=max_features,\n",
    "#  output_mode='int',\n",
    "#  output_sequence_length=max_len)\n",
    "\n",
    "mv_tags_input_layer = tf.keras.Input(\n",
    "    name=\"movie_tags\",\n",
    "    shape=(TAG_MAX_LENGTH,1),\n",
    "    # shape=(1,),\n",
    "    dtype=tf.string,\n",
    "    # ragged=True\n",
    ")\n",
    "mv_tags_text = tf.keras.layers.TextVectorization(\n",
    "    # max_tokens=max_tokens, \n",
    "    ngrams=2, \n",
    "    vocabulary=vocab_dict['movie_tags'],\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_VECT_LEN,\n",
    ")(mv_tags_input_layer)\n",
    "mv_tags_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['movie_tags']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=MV_EMBEDDING_SIZE\n",
    ")(mv_tags_text)\n",
    "\n",
    "# mv_avg_pooling = tf.reduce_mean(mv_tags_embedding, axis=[-1])\n",
    "# mv_avg_pooling = tf.reduce_sum(mv_tags_embedding, axis=-2)\n",
    "\n",
    "mv_tags_pooling_v1 = tf.keras.layers.Reshape([-1, MV_EMBEDDING_SIZE])(mv_tags_embedding)\n",
    "# mv_avg_pooling = tf.keras.layers.GlobalAveragePooling2D()(mv_tags_pooling_v1)\n",
    "mv_avg_pooling = tf.keras.layers.GlobalAveragePooling1D()(mv_tags_pooling_v1)\n",
    "\n",
    "# mv_avg_pooling = tf.keras.layers.GlobalAveragePooling2D()(mv_tags_embedding)\n",
    "# mv_avg_pooling = tf.keras.layers.GlobalAveragePooling1D()(mv_tags_pooling_v1)\n",
    "\n",
    "test_mv_tags_model = tf.keras.Model(\n",
    "    inputs=mv_tags_input_layer, outputs=mv_avg_pooling\n",
    ")\n",
    "\n",
    "test_mv_tags_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf6438-4763-49ff-a90a-355e4908bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_tags_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6d892-c759-4810-8945-8338858bc4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE_T=1\n",
    "BATCH_SIZE_T=2\n",
    "\n",
    "for x in train_dataset.batch(BATCH_SIZE_T).take(1):\n",
    "    # print(x[\"movie_tags\"])\n",
    "    # print(test_user_id_model(data[\"movie_tags\"]))\n",
    "    # reshaped_tensor = tf.reshape(x['movie_tags'], [-1, 10])[0]\n",
    "    reshaped_tensor = tf.reshape(x['movie_tags'], [-1, 10, 1])\n",
    "    # reshaped_tensor = x['movie_tags'][0]\n",
    "    test_value = test_mv_tags_model(reshaped_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9451a995-dd52-4b54-a085-1122938bf5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x['movie_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c27021a-5433-48a3-97d5-70607331f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x['movie_tags'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e691ea9-fc78-4a15-a666-376fc6bad341",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_tt=1\n",
    "TAG_LENGTH_1=10\n",
    "# tf.reshape(x['movie_tags'], [BATCH_SIZE_tt, TAG_LENGTH_1, 1])\n",
    "reshaped_tensor = tf.reshape(x['movie_tags'], [-1, MV_EMBEDDING_SIZE])\n",
    "reshaped_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344329d4-6567-4b39-b6fc-63da637d42e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77df2e36-49f6-4953-925d-dde475f54c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSOR_TEST_T = x['movie_tags']\n",
    "TENSOR_TEST_T = x['movie_tags'][0]\n",
    "\n",
    "reshaped_tensor_v2 = tf.reshape(TENSOR_TEST_T, [-1, 10, 1])\n",
    "reshaped_tensor_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba78a2-f2cb-4e42-8e5f-7d9512b3fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a754e251-2587-4505-b8fd-a70fadd4bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ef201-47d5-4b16-a9c8-7e80cbe19646",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['movie_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb041c1-22fc-4cc1-9666-7a60d099659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaped_tensor = tf.reshape(data['movie_tags'], [-1])\n",
    "# reshaped_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edc0ec-a251-49f8-aaf6-f7d5e9846a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaped_tensor = tf.reshape(data['movie_tags'], [-1, 10])\n",
    "# reshaped_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccde538-d9a8-48e5-90ae-78ffd9a5cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaped_tensor = tf.reshape(data['movie_tags'], [-1, 10, 1])\n",
    "# reshaped_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25c966c-158d-4576-a5bd-c85caf1e3896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE_T=1\n",
    "BATCH_SIZE_T=2\n",
    "\n",
    "for x in train_dataset.batch(BATCH_SIZE_T).take(1):\n",
    "    # print(x[\"movie_tags\"])\n",
    "    # print(test_user_id_model(data[\"movie_tags\"]))\n",
    "    BATCH_SIZE_tt = x[\"movie_tags\"].shape[0]\n",
    "    TAG_LENGTH_1 = x[\"movie_tags\"].shape[1]\n",
    "    print(f\"BATCH_SIZE_tt : {BATCH_SIZE_tt}\")\n",
    "    print(f\"TAG_LENGTH_1  : {TAG_LENGTH_1}\")\n",
    "    # reshaped_tensor = x['movie_tags']\n",
    "    # reshaped_tensor = tf.reshape(x['movie_tags'], [-1])\n",
    "    # reshaped_tensor = tf.reshape(x['movie_tags'], [-1, 10])[0]\n",
    "    reshaped_tensor = tf.reshape(x['movie_tags'], [BATCH_SIZE_tt, TAG_LENGTH_1, 1])\n",
    "    # reshaped_tensor = tf.reshape(x['movie_tags'], [BATCH_SIZE_tt, 4, 1])\n",
    "    test_value = test_mv_tags_model(reshaped_tensor)\n",
    "    \n",
    "# test_mv_tags_model(data[\"movie_tags\"])\n",
    "# test_mv_tags_model(reshaped_tensor)\n",
    "\n",
    "test_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c0fca-50a9-4853-84cd-a96555d2770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c62d19-7a37-45b3-983f-508d301b9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSOR_TEST_T = x['movie_tags']\n",
    "TENSOR_TEST_T = x['movie_tags'][0]\n",
    "\n",
    "reshaped_tensor_v2 = tf.reshape(TENSOR_TEST_T, [-1, 10, 1])\n",
    "reshaped_tensor_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f815040d-4d64-4313-8e4e-d4dd238c45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list_seq = [test_globals[0].numpy(), test_globals[0].numpy()]\n",
    "\n",
    "test_list_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f40fde-6c4d-4d4e-ba68-249ae7c614fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_tensor_test = tf.reshape(test_list_seq, [-1, 48, 1])\n",
    "reshape_tensor_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4cf1b-5066-4f4e-9040-b6e113962cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_inputs = tf.keras.utils.pad_sequences(test_list_seq, maxlen=20,truncating='post')\n",
    "# padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec5d1ef-2f65-4b66-98ee-ce1bc2ee565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_dataset.padded_batch(HPARAMS['batch_size']).take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c841e758-b42e-4f78-a81b-b2b796729a08",
   "metadata": {},
   "source": [
    "#### tmp - delete START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84715e71-0aa5-4e93-b6f2-181b51dde2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    max_tokens = 10_000_00\n",
    "\n",
    "    self.title_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "          vocabulary=unique_movie_titles,mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, 32)\n",
    "    ])\n",
    "\n",
    "    self.title_vectorizer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=max_tokens,output_sequence_length = 4)\n",
    "\n",
    "    self.title_text_embedding = tf.keras.Sequential(\n",
    "        [\n",
    "            self.title_vectorizer,\n",
    "            tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "        ]\n",
    "    )\n",
    "    self.title_vectorizer.adapt(movies)\n",
    "\n",
    "  def call(self, titles, pool_size):\n",
    "    avg_layer = tf.keras.layers.AveragePooling2D(\n",
    "        pool_size=pool_size,strides=1,padding='valid',\n",
    "    )\n",
    "    len_titles=tf.shape(titles)[0]\n",
    "    \n",
    "    # return avg_layer(self.title_text_embedding(titles))\n",
    "    return tf.concat(\n",
    "        [\n",
    "            self.title_embedding(tf.reshape(titles,[len_titles,5,1])),\n",
    "            \n",
    "            avg_layer(\n",
    "                self.title_text_embedding(\n",
    "                    tf.reshape(\n",
    "                        titles,[len_titles,5,1]\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "        ], \n",
    "        axis=3\n",
    "    )\n",
    "\n",
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self, layer_sizes):\n",
    "    super().__init__()\n",
    "    self.query_model = QueryModel(layer_sizes)\n",
    "    self.candidate_model = CandidateModel(layer_sizes)\n",
    "    self.rating_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ])\n",
    "    # self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "    #     loss=tf.keras.losses.MeanSquaredError(),\n",
    "    #     metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "    # )\n",
    "    self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "        loss=tfr.keras.losses.ListMLELoss(),\n",
    "        metrics=[tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
    "                 tf.keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "\n",
    "  def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    # print(len(features[\"movie_title\"]))\n",
    "    # for x in features[\"movie_title\"]:\n",
    "    #   print(x)\n",
    "    self.query_embeddings = self.query_model({\n",
    "        \"user_id\": features[\"user_id\"],\n",
    "        # \"timestamp\": features[\"timestamp\"],\n",
    "    })\n",
    "    self.movie_embeddings = self.candidate_model(features[\"movie_title\"],pool_size=(1,4))\n",
    "\n",
    "    list_length = features[\"movie_title\"].shape[1]\n",
    "    \n",
    "    self.query_embeddings_repeated = tf.repeat(\n",
    "        tf.expand_dims(\n",
    "            tf.expand_dims(\n",
    "                self.query_embeddings, 1\n",
    "            ), 1\n",
    "        ), [list_length], axis=1\n",
    "    )\n",
    "    self.embd_concat=tf.concat(\n",
    "        [\n",
    "            self.query_embeddings_repeated, \n",
    "            self.movie_embeddings\n",
    "        ], 3\n",
    "    )\n",
    "    return (\n",
    "        self.query_embeddings,\n",
    "        self.movie_embeddings,\n",
    "        # We apply the multi-layered rating model to a concatentation of\n",
    "        # user and movie embeddings.\n",
    "        self.rating_model(\n",
    "            self.embd_concat\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c94063b-9e87-449e-b704-7e3e71d31acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    len_titles=tf.shape(titles)[0]\n",
    "    \n",
    "    # return avg_layer(self.title_text_embedding(titles))\n",
    "    return tf.concat(\n",
    "        [\n",
    "            self.title_embedding(tf.reshape(titles,[len_titles,5,1])),\n",
    "            \n",
    "            avg_layer(\n",
    "                self.title_text_embedding(\n",
    "                    tf.reshape(\n",
    "                        titles,[len_titles,5,1]\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "        ], \n",
    "        axis=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f56518f-299c-4074-a5e5-c94e967c0bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    list_length = features[\"movie_title\"].shape[1]\n",
    "    \n",
    "    self.query_embeddings_repeated = tf.repeat(\n",
    "        tf.expand_dims(\n",
    "            tf.expand_dims(\n",
    "                self.query_embeddings, 1\n",
    "            ), 1\n",
    "        ), [list_length], axis=1\n",
    "    )\n",
    "    self.embd_concat=tf.concat(\n",
    "        [\n",
    "            self.query_embeddings_repeated, \n",
    "            self.movie_embeddings\n",
    "        ], 3\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759641cb-d391-4f87-ad78-7db1d2a4f9c3",
   "metadata": {},
   "source": [
    "#### tmp - delete END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da71da5-2b50-4861-85f5-646165659098",
   "metadata": {},
   "source": [
    "## global context (user) features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058ed6c-69b5-4681-ad9b-d69632b12fbd",
   "metadata": {},
   "source": [
    "#### user ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8791d355-bac1-4450-8560-963e44a5e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_input_layer = tf.keras.Input(\n",
    "    name=\"user_id\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.string\n",
    ")\n",
    "\n",
    "user_id_lookup = tf.keras.layers.StringLookup(\n",
    "    max_tokens=len(vocab_dict['user_id']) + NUM_OOV_BUCKETS,\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    mask_token=None,\n",
    "    vocabulary=vocab_dict['user_id'],\n",
    ")(user_id_input_layer)\n",
    "\n",
    "user_id_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['user_id']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_id_lookup)\n",
    "\n",
    "user_id_embedding = tf.reduce_sum(user_id_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_id_input_layer)\n",
    "# global_features.append(user_id_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4f9a9-6b13-40e8-882c-a91de6147254",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_id_model = tf.keras.Model(inputs=user_id_input_layer, outputs=user_id_embedding)\n",
    "\n",
    "# for x in train_dataset.batch(1).take(1):\n",
    "#     print(x[\"user_id\"])\n",
    "#     print(test_user_id_model(x[\"user_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2692c5f-8e3c-4b19-b291-d6a27968df05",
   "metadata": {},
   "source": [
    "#### user AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dbe30e-792b-4046-a3d2-a3f0cfad50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_age_input_layer = tf.keras.Input(\n",
    "#     name=\"bucketized_user_age\",\n",
    "#     shape=(1,),\n",
    "#     dtype=tf.float32\n",
    "# )\n",
    "\n",
    "# user_age_lookup = tf.keras.layers.IntegerLookup(\n",
    "#     vocabulary=vocab_dict['bucketized_user_age'],\n",
    "#     num_oov_indices=NUM_OOV_BUCKETS,\n",
    "#     oov_value=0,\n",
    "# )(user_age_input_layer)\n",
    "\n",
    "# user_age_embedding = tf.keras.layers.Embedding(\n",
    "#     # Let's use the explicit vocabulary lookup.\n",
    "#     input_dim=len(vocab_dict['bucketized_user_age']) + NUM_OOV_BUCKETS,\n",
    "#     output_dim=GLOBAL_EMBEDDING_SIZE\n",
    "# )(user_age_lookup)\n",
    "\n",
    "# user_age_embedding = tf.reduce_sum(user_age_embedding, axis=-2)\n",
    "\n",
    "# # global_inputs.append(user_age_input_layer)\n",
    "# # global_features.append(user_age_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30558d19-e3ca-4bee-bab2-01a19a104351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_user_age_model = tf.keras.Model(inputs=user_age_input_layer, outputs=user_age_embedding)\n",
    "\n",
    "# # for x in train_dataset.batch(1).take(1):\n",
    "# #     print(x[\"bucketized_user_age\"])\n",
    "# #     print(test_user_age_model(x[\"bucketized_user_age\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b0610-9a1b-4ddb-abc6-4ebfa8902a77",
   "metadata": {},
   "source": [
    "#### user OCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef0ceba-793e-4ca1-862f-d46819d563ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_occ_input_layer = tf.keras.Input(\n",
    "#     name=\"user_occupation_text\",\n",
    "#     shape=(1,),\n",
    "#     dtype=tf.string\n",
    "# )\n",
    "\n",
    "# user_occ_lookup = tf.keras.layers.StringLookup(\n",
    "#     max_tokens=len(vocab_dict['user_occupation_text']) + NUM_OOV_BUCKETS,\n",
    "#     num_oov_indices=NUM_OOV_BUCKETS,\n",
    "#     mask_token=None,\n",
    "#     vocabulary=vocab_dict['user_occupation_text'],\n",
    "# )(user_occ_input_layer)\n",
    "\n",
    "# user_occ_embedding = tf.keras.layers.Embedding(\n",
    "#     # Let's use the explicit vocabulary lookup.\n",
    "#     input_dim=len(vocab_dict['user_occupation_text']) + NUM_OOV_BUCKETS,\n",
    "#     output_dim=GLOBAL_EMBEDDING_SIZE\n",
    "# )(user_occ_lookup)\n",
    "\n",
    "# user_occ_embedding = tf.reduce_sum(user_occ_embedding, axis=-2)\n",
    "\n",
    "# # global_inputs.append(user_occ_input_layer)\n",
    "# # global_features.append(user_occ_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d96bb-73e2-469c-a4e9-c084ecfbec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_user_occ_model = tf.keras.Model(inputs=user_occ_input_layer, outputs=user_occ_embedding)\n",
    "\n",
    "# # for x in train_dataset.batch(1).take(1):\n",
    "# #     print(x[\"user_occupation_text\"])\n",
    "# #     print(test_user_occ_model(x[\"user_occupation_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2fd9da-6845-4d2f-8a1f-2151186ecb4a",
   "metadata": {},
   "source": [
    "#### user Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9755aaa-8008-4ae2-b4c2-588c79998917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_ts_input_layer = tf.keras.Input(\n",
    "#     name=\"timestamp\",\n",
    "#     shape=(1,),\n",
    "#     dtype=tf.int64\n",
    "# )\n",
    "\n",
    "# user_ts_lookup = tf.keras.layers.Discretization(\n",
    "#     vocab_dict['timestamp_buckets'].tolist()\n",
    "# )(user_ts_input_layer)\n",
    "\n",
    "# user_ts_embedding = tf.keras.layers.Embedding(\n",
    "#     # Let's use the explicit vocabulary lookup.\n",
    "#     input_dim=len(vocab_dict['timestamp_buckets'].tolist()) + NUM_OOV_BUCKETS,\n",
    "#     output_dim=GLOBAL_EMBEDDING_SIZE\n",
    "# )(user_ts_lookup)\n",
    "\n",
    "# user_ts_embedding = tf.reduce_sum(user_ts_embedding, axis=-2)\n",
    "\n",
    "# # global_inputs.append(user_ts_input_layer)\n",
    "# # global_features.append(user_ts_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238783f-6f88-4400-96ac-924a3e831f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_user_ts_model = tf.keras.Model(inputs=user_ts_input_layer, outputs=user_ts_embedding)\n",
    "\n",
    "# # for x in train_dataset.batch(1).take(1):\n",
    "# #     print(x[\"timestamp\"])\n",
    "# #     print(test_user_ts_model(x[\"timestamp\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa494695-d3a1-4917-8e63-26f8c1d21cd8",
   "metadata": {},
   "source": [
    "### define global sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc66ef6f-4041-4163-8f81-4676495c8a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _get_global_context_features(x):\n",
    "#     \"\"\"\n",
    "#     This function generates a single global observation vector.\n",
    "#     \"\"\"\n",
    "#     user_id_value = x['user_id']\n",
    "#     user_age_value = x['bucketized_user_age']\n",
    "#     user_occ_value = x['user_occupation_text']\n",
    "#     user_ts_value = x['timestamp']\n",
    "\n",
    "#     _id = test_user_id_model(user_id_value) # input_tensor=tf.Tensor(shape=(4,), dtype=float32)\n",
    "#     _age = test_user_age_model(user_age_value)\n",
    "#     _occ = test_user_occ_model(user_occ_value)\n",
    "#     _ts = test_user_ts_model(user_ts_value)\n",
    "\n",
    "#     # # tmp - insepct numpy() values\n",
    "#     # print(_id.numpy()) #[0])\n",
    "#     # print(_age.numpy()) #[0])\n",
    "#     # print(_occ.numpy()) #[0])\n",
    "#     # print(_ts.numpy()) #[0])\n",
    "\n",
    "#     # to numpy array\n",
    "#     _id = np.array(_id.numpy())\n",
    "#     _age = np.array(_age.numpy())\n",
    "#     _occ = np.array(_occ.numpy())\n",
    "#     _ts = np.array(_ts.numpy())\n",
    "\n",
    "#     concat = np.concatenate(\n",
    "#         [_id, _age, _occ, _ts], axis=-1 # -1\n",
    "#     ).astype(np.float32)\n",
    "\n",
    "#     return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31569c4-db07-4371-91de-38e4b8351185",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_DIM = _get_global_context_features(data).shape[1]\n",
    "print(f\"GLOBAL_DIM: {GLOBAL_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce0d85b-25c3-4d81-a758-7c7aa1d3d699",
   "metadata": {},
   "source": [
    "## arm preprocessing layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ec63b-cad2-45e1-a143-5a233aad78b4",
   "metadata": {},
   "source": [
    "#### movie ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d06b8-3a73-4361-a549-0f565ddf46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mv_id_input_layer = tf.keras.Input(\n",
    "#     name=\"movie_id\",\n",
    "#     shape=(1,),\n",
    "#     dtype=tf.string\n",
    "# )\n",
    "\n",
    "# mv_id_lookup = tf.keras.layers.StringLookup(\n",
    "#     max_tokens=len(vocab_dict['movie_id']) + NUM_OOV_BUCKETS,\n",
    "#     num_oov_indices=NUM_OOV_BUCKETS,\n",
    "#     mask_token=None,\n",
    "#     vocabulary=vocab_dict['movie_id'],\n",
    "# )(mv_id_input_layer)\n",
    "\n",
    "# mv_id_embedding = tf.keras.layers.Embedding(\n",
    "#     # Let's use the explicit vocabulary lookup.\n",
    "#     input_dim=len(vocab_dict['movie_id']) + NUM_OOV_BUCKETS,\n",
    "#     output_dim=MV_EMBEDDING_SIZE\n",
    "# )(mv_id_lookup)\n",
    "\n",
    "# mv_id_embedding = tf.reduce_sum(mv_id_embedding, axis=-2)\n",
    "\n",
    "# # arm_inputs.append(mv_id_input_layer)\n",
    "# # arm_features.append(mv_id_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569185f7-e1ae-40eb-b319-088e2b8b6970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_mv_id_model = tf.keras.Model(inputs=mv_id_input_layer, outputs=mv_id_embedding)\n",
    "\n",
    "# # for x in train_dataset.batch(1).take(1):\n",
    "# #     print(x[\"movie_id\"])\n",
    "# #     print(test_mv_id_model(x[\"movie_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba4f2a-b741-4cca-935c-29735a8c9c65",
   "metadata": {},
   "source": [
    "#### movie genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86a848-7e68-4950-83c0-eae53deaf24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mv_genre_input_layer = tf.keras.Input(\n",
    "#     name=\"movie_genres\",\n",
    "#     shape=(1,),\n",
    "#     dtype=tf.float32\n",
    "# )\n",
    "\n",
    "# mv_genre_lookup = tf.keras.layers.IntegerLookup(\n",
    "#     vocabulary=vocab_dict['movie_genres'],\n",
    "#     num_oov_indices=NUM_OOV_BUCKETS,\n",
    "#     oov_value=0,\n",
    "# )(mv_genre_input_layer)\n",
    "\n",
    "# mv_genre_embedding = tf.keras.layers.Embedding(\n",
    "#     # Let's use the explicit vocabulary lookup.\n",
    "#     input_dim=len(vocab_dict['movie_genres']) + NUM_OOV_BUCKETS,\n",
    "#     output_dim=MV_EMBEDDING_SIZE\n",
    "# )(mv_genre_lookup)\n",
    "\n",
    "# mv_genre_embedding = tf.reduce_sum(mv_genre_embedding, axis=-2)\n",
    "\n",
    "# # arm_inputs.append(mv_genre_input_layer)\n",
    "# # arm_features.append(mv_genre_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7812a50-7188-4c09-b8b2-9f8a26e414bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_mv_gen_model = tf.keras.Model(inputs=mv_genre_input_layer, outputs=mv_genre_embedding)\n",
    "\n",
    "# for x in train_dataset.batch(1).take(1):\n",
    "#     print(x[\"movie_genres\"])\n",
    "#     print(test_mv_gen_model(x[\"movie_genres\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da0b72e-a971-423f-85b4-66319e1c441b",
   "metadata": {},
   "source": [
    "### define sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be4e2e-0862-4db0-963b-afc1e58ee3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _get_per_arm_features(x):\n",
    "#     \"\"\"\n",
    "#     This function generates a single per-arm observation vector\n",
    "#     \"\"\"\n",
    "#     mv_id_value = x['movie_id']\n",
    "#     mv_gen_value = x['movie_genres']\n",
    "\n",
    "#     _mid = test_mv_id_model(mv_id_value)\n",
    "#     _mgen = test_mv_gen_model(mv_gen_value)\n",
    "\n",
    "#     # to numpy array\n",
    "#     _mid = np.array(_mid.numpy())\n",
    "#     _mgen = np.array(_mgen.numpy())\n",
    "\n",
    "\n",
    "#     concat = np.concatenate(\n",
    "#         [_mid, _mgen], axis=-1 # -1\n",
    "#     ).astype(np.float32)\n",
    "#     # concat = tf.concat([_mid, _mgen], axis=-1).astype(np.float32)\n",
    "\n",
    "#     return concat #this is special to this example - there is only one action dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015db37c-1254-4586-9262-020299e91420",
   "metadata": {},
   "outputs": [],
   "source": [
    "PER_ARM_DIM = _get_per_arm_features(data).shape[1] #shape checks out at batchdim, nactions, arm feats\n",
    "print(f\"PER_ARM_DIM: {PER_ARM_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafbcafd-a840-4caf-a1e1-9d4c529161e5",
   "metadata": {},
   "source": [
    "## Global & Per-Arm feature embedding models \n",
    "\n",
    "> all these dimensions should match the class output below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9572d7f9-72b6-447c-ac0d-d9f2f6867ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.perarm_features import emb_features as emb_features\n",
    "\n",
    "embs = emb_features.EmbeddingModel(\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2998a87-0aec-4ca7-a989-ddc07e309065",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_globals = embs._get_global_context_features(data)\n",
    "\n",
    "GLOBAL_DIM = test_globals.shape[1]            \n",
    "# shape checks out at batch_dim, nactions, arm feats\n",
    "print(f\"GLOBAL_DIM: {GLOBAL_DIM}\")\n",
    "\n",
    "test_globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ff35d-7574-410c-ab3d-daf21e24883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arms = embs._get_per_arm_features(data)\n",
    "\n",
    "PER_ARM_DIM = test_arms.shape[1]            \n",
    "# shape checks out at batch_dim, nactions, arm feats\n",
    "print(f\"PER_ARM_DIM: {PER_ARM_DIM}\")\n",
    "\n",
    "test_arms"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
