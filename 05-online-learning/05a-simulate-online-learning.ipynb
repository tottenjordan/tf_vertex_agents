{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bbe739b-44f2-444f-989d-b1d289b1817e",
   "metadata": {},
   "source": [
    "# Simulate Online Learning with Contextual Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e150cd8-8e32-4c44-8d6b-0d0050e8a463",
   "metadata": {},
   "source": [
    "For \"online learning\" to take place, the agent's policy needs to be updated, where \"updated\" is conceptually similar to retraining a traditional supervised learning model (with latest training examples) and deploying the newly trained model\n",
    "* the bandit agent's policy is updated (i.e., learns) when it receives (e.g., `agent.train(...)`) a trajectory that includes the prediction/action AND the delayed feedback\n",
    "* once `agent.train(...)` takes place, the policy is updated (e.g., weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d04854-4b70-4529-9609-9de643461979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/tf_vertex_agents/05-online-learning\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0431346f-3097-4ca3-890e-9927d7b2d4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX: rec-bandits-v2\n"
     ]
    }
   ],
   "source": [
    "# PREFIX = 'mabv1'\n",
    "VERSION        = \"v2\"                       # TODO\n",
    "PREFIX         = f'rec-bandits-{VERSION}'   # TODO\n",
    "\n",
    "print(f\"PREFIX: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d76b584-5d0d-4b04-80b9-c97916ba4686",
   "metadata": {},
   "source": [
    "## Load notebook config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdc0e11b-d3fd-4140-86a7-459585be71c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"rec-bandits-v2\"\n",
      "VERSION                  = \"v2\"\n",
      "\n",
      "BUCKET_NAME              = \"rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "BUCKET_URI               = \"gs://rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "DATA_PATH_KFP_DEMO       = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data/kfp_demo_data/u.data\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_NAME    = \"mvlens_rec_bandits_v2\"\n",
      "BIGQUERY_TABLE_NAME      = \"training_dataset\"\n",
      "\n",
      "REPOSITORY               = \"rl-movielens-rec-bandits-v2\"\n",
      "\n",
      "DOCKERNAME_01            = \"Dockerfile_train_my_perarm_env\"\n",
      "IMAGE_NAME_01            = \"train-my-perarm-env-v2\"\n",
      "IMAGE_URI_01             = \"gcr.io/hybrid-vertex/train-my-perarm-env-v2\"\n",
      "\n",
      "DOCKERNAME_02            = \"Dockerfile_perarm_feats\"\n",
      "IMAGE_NAME_02            = \"train-perarm-feats-v2\"\n",
      "IMAGE_URI_02             = \"gcr.io/hybrid-vertex/train-perarm-feats-v2\"\n",
      "\n",
      "DOCKERNAME_03            = \"Dockerfile_ranking_bandit\"\n",
      "IMAGE_NAME_03            = \"train-rank-bandit-v2\"\n",
      "IMAGE_URI_03             = \"gcr.io/hybrid-vertex/train-rank-bandit-v2\"\n",
      "\n",
      "DOCKERNAME_04            = \"Dockerfile_train_mab_e2e\"\n",
      "IMAGE_NAME_04            = \"train-mab-e2e-v2\"\n",
      "IMAGE_URI_04             = \"gcr.io/hybrid-vertex/train-mab-e2e-v2\"\n",
      "\n",
      "DOCKERNAME_04_pred       = \"Dockerfile_pred_mab_e2e\"\n",
      "IMAGE_NAME_04_pred       = \"pred-mab-e2e-v2\"\n",
      "IMAGE_URI_04_pred        = \"gcr.io/hybrid-vertex/pred-mab-e2e-v2\"\n",
      "\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-rec-bandits-v2/local_docker_tfa\"\n",
      "\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "RL_SUB_DIR               = \"per_arm_rl\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd0378-3650-4cc5-bc05-4f407f200c9c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fab51a15-b189-4159-919c-04f2d81ce741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25dcc25d-030e-4d39-920b-2187e70fcdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pprint import pprint\n",
    "import pickle as pkl\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.metrics import tf_metrics\n",
    "\n",
    "from tf_agents.bandits.agents import neural_epsilon_greedy_agent\n",
    "from tf_agents.bandits.agents import neural_linucb_agent\n",
    "from tf_agents.bandits.networks import global_and_arm_feature_network\n",
    "\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.bandits.policies import policy_utilities\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "from tf_agents.train.utils import spec_utils\n",
    "from tf_agents.train.utils import strategy_utils\n",
    "from tf_agents.train.utils import train_utils as tfa_train_utils\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from tf_agents.metrics import export_utils\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.eval import metric_utils\n",
    "\n",
    "# GPU\n",
    "from numba import cuda \n",
    "import gc\n",
    "\n",
    "# tf exceptions and vars\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df137505-958f-48f3-9159-2e0b01df6146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_agents version: 0.17.0\n",
      "tensorflow version: 2.13.0\n"
     ]
    }
   ],
   "source": [
    "import tf_agents\n",
    "\n",
    "print(f\"tf_agents version: {tf_agents.__version__}\")\n",
    "print(f\"tensorflow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f664a98-df2e-4190-8c67-e8f4b970f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# this repo\n",
    "from src.per_arm_rl import data_utils\n",
    "from src.per_arm_rl import data_config\n",
    "from src.per_arm_rl import train_utils as train_utils\n",
    "\n",
    "from src.perarm_features import agent_factory as agent_factory\n",
    "from src.perarm_features import reward_factory as reward_factory\n",
    "from src.perarm_features import emb_features as emb_features\n",
    "from src.perarm_features import eval_perarm as eval_perarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15ccc299-9963-4014-a19f-394a8697d966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aae8bb01-8147-4586-913e-e02fbbbd4be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78c953a0-be4d-4859-8bd6-50773502e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc337cb0-aef5-4709-b341-6b6d186cffbf",
   "metadata": {},
   "source": [
    "### Generate Vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bd57537-79f2-449d-a7eb-97b2325a2569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATE_VOCABS: False\n"
     ]
    }
   ],
   "source": [
    "GENERATE_VOCABS = False\n",
    "\n",
    "print(f\"GENERATE_VOCABS: {GENERATE_VOCABS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "177c2b16-71ab-4d11-9259-b19c3b978550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vocab...\n",
      "Downloaded vocab from: gs://rec-bandits-v2-hybrid-vertex-bucket/vocabs/vocab_dict.pkl\n",
      "\n",
      "'movie_id'\n",
      "'user_id'\n",
      "'user_occupation_text'\n",
      "'movie_genres'\n",
      "'bucketized_user_age'\n",
      "'max_timestamp'\n",
      "'min_timestamp'\n",
      "'timestamp_buckets'\n"
     ]
    }
   ],
   "source": [
    "if not GENERATE_VOCABS:\n",
    "\n",
    "    EXISTING_VOCAB_FILE = f'gs://{BUCKET_NAME}/{VOCAB_SUBDIR}/{VOCAB_FILENAME}'\n",
    "    print(f\"Downloading vocab...\")\n",
    "    \n",
    "    os.system(f'gsutil -q cp {EXISTING_VOCAB_FILE} .')\n",
    "    print(f\"Downloaded vocab from: {EXISTING_VOCAB_FILE}\\n\")\n",
    "\n",
    "    filehandler = open(VOCAB_FILENAME, 'rb')\n",
    "    VOCAB_DICT = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "    for key in VOCAB_DICT.keys():\n",
    "        pprint(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f4837-9c6b-4b11-b950-ed4fa0fa2804",
   "metadata": {},
   "source": [
    "## Data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cf10444-f95a-4fbb-b8db-e12a38903639",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = \"train\" # \"train\" | \"val\"\n",
    "\n",
    "train_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{DATA_GCS_PREFIX}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "train_dataset = train_dataset.map(data_utils.parse_tfrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "615baf44-0b1e-4f79-aac2-f318d6709c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = \"val\"\n",
    "\n",
    "val_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{DATA_GCS_PREFIX}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        val_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "val_dataset = tf.data.TFRecordDataset(val_files)\n",
    "val_dataset = val_dataset.map(data_utils.parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89788e85-79a0-44a1-b48d-7259e99d241f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bucketized_user_age': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([35.], dtype=float32)>,\n",
       " 'movie_genres': <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[7]])>,\n",
       " 'movie_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'898'], dtype=object)>,\n",
       " 'timestamp': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([885409515])>,\n",
       " 'user_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'681'], dtype=object)>,\n",
       " 'user_occupation_text': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'marketing'], dtype=object)>,\n",
       " 'user_rating': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([4.], dtype=float32)>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    \n",
    "    iterator = iter(train_dataset.batch(1))\n",
    "    data = next(iterator)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a332a971-b73b-4814-a9d6-c03a72426608",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OOV_BUCKETS        = 1\n",
    "GLOBAL_EMBEDDING_SIZE  = 16\n",
    "MV_EMBEDDING_SIZE      = 32 #32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "645ccdc6-6ea9-4873-9e4c-12084fbe5c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.perarm_features.emb_features.EmbeddingModel at 0x7f5d3dfc3d30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = emb_features.EmbeddingModel(\n",
    "    vocab_dict = VOCAB_DICT,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    "    max_genre_length = data_confg.MAX_GENRE_LENGTH,\n",
    ")\n",
    "\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6782d836-f6bf-43a5-880c-56489bf229e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL_DIM: 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-7.0669502e-04,  3.2736287e-03,  4.3720875e-02,  1.3494503e-02,\n",
       "         4.6463493e-02,  1.8855002e-02, -4.1400801e-02,  4.7330860e-02,\n",
       "         3.0712258e-02, -4.9817674e-03,  3.4395944e-02,  1.0257103e-02,\n",
       "         3.0751910e-02,  3.7187699e-02,  3.1533372e-02, -1.1847794e-02,\n",
       "        -2.7862204e-02,  4.1931380e-02, -2.7042473e-02,  9.8444521e-05,\n",
       "         2.6006851e-02, -6.0479641e-03,  3.6260571e-02, -3.9354313e-02,\n",
       "         4.6048354e-02, -1.5799571e-02, -2.3982776e-02,  4.0826786e-02,\n",
       "        -2.0275176e-02,  4.8119191e-02,  2.4046276e-02,  2.8419044e-02,\n",
       "        -1.8150069e-02,  9.1669783e-03, -5.4595619e-04, -2.9378761e-02,\n",
       "        -1.2537647e-02,  1.6587265e-03,  9.1310255e-03,  3.4540858e-02,\n",
       "         2.8371040e-02, -2.8490648e-03, -3.5640337e-02,  1.0824405e-02,\n",
       "        -2.3828913e-02, -1.2451671e-02, -3.1139528e-02,  2.4883971e-03,\n",
       "        -1.2129795e-02, -3.1142581e-02,  4.1311290e-02,  1.9583549e-02,\n",
       "         1.9490693e-02,  3.6436323e-02, -4.0368438e-02, -3.2501616e-02,\n",
       "         3.0481819e-02, -3.2278098e-02,  1.7592225e-02, -2.3297800e-02,\n",
       "         1.0479890e-02, -3.6050439e-02,  4.9466405e-02,  5.9315786e-03]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_globals = embs._get_global_context_features(data)\n",
    "\n",
    "GLOBAL_DIM = test_globals.shape[1]            \n",
    "# shape checks out at batch_dim, nactions, arm feats\n",
    "print(f\"GLOBAL_DIM: {GLOBAL_DIM}\")\n",
    "\n",
    "test_globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5dfd0e8-6729-4235-a719-5258799ed0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER_ARM_DIM: 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.03168386, -0.02361635, -0.04177202, -0.01280922, -0.04570433,\n",
       "         0.00293136,  0.04662893,  0.03389878,  0.02759621,  0.02516707,\n",
       "        -0.01971465,  0.04677725, -0.03734259, -0.03346514, -0.0258178 ,\n",
       "         0.02205957, -0.04624597,  0.00736122, -0.01682913, -0.04941222,\n",
       "        -0.03502977,  0.02303982, -0.00829629, -0.01609255,  0.00634626,\n",
       "        -0.02750918, -0.02068441, -0.03389679, -0.04039789,  0.03764428,\n",
       "         0.01098546,  0.03406877,  0.04408174,  0.02645487,  0.0474542 ,\n",
       "         0.01894047,  0.04530459, -0.04246902, -0.03386635,  0.02150379,\n",
       "        -0.01577593, -0.00821938, -0.01326246,  0.00616535, -0.01901968,\n",
       "        -0.0228083 ,  0.02416537,  0.02131167,  0.00731204,  0.0413324 ,\n",
       "         0.01907099, -0.02565429, -0.01897984, -0.0051425 ,  0.01863119,\n",
       "        -0.00648588, -0.00153511,  0.03581622,  0.02928445, -0.01298493,\n",
       "        -0.00239474,  0.00100768, -0.0225958 ,  0.03053815]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arms = embs._get_per_arm_features(data)\n",
    "\n",
    "PER_ARM_DIM = test_arms.shape[1]            \n",
    "# shape checks out at batch_dim, nactions, arm feats\n",
    "print(f\"PER_ARM_DIM: {PER_ARM_DIM}\")\n",
    "\n",
    "test_arms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f9a8ec-829c-4032-aa6b-f845afb7cc89",
   "metadata": {},
   "source": [
    "### TensorSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38d560e6-56dc-454d-a409-fd307dcb7ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE      = 128\n",
    "EVAL_BATCH_SIZE = 1\n",
    "NUM_ACTIONS     = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5196055a-56e6-413f-b3c4-f8a3f8b18f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(64,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_spec = {\n",
    "    'global': tf.TensorSpec([GLOBAL_DIM], tf.float32),\n",
    "    'per_arm': tf.TensorSpec([NUM_ACTIONS, PER_ARM_DIM], tf.float32) #excluding action dim here\n",
    "}\n",
    "observation_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36dcb7a9-c589-44d0-a5e8-3960aca6a243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=[], \n",
    "    dtype=tf.int32,\n",
    "    minimum=tf.constant(0),            \n",
    "    maximum=NUM_ACTIONS-1, # n degrees of freedom and will dictate the expected mean reward spec shape\n",
    "    # name=\"action_spec\"\n",
    ")\n",
    "\n",
    "action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9207d05f-72b2-42f3-bc41-089fc2aaba38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': {'global': TensorSpec(shape=(64,), dtype=tf.float32, name=None),\n",
       "                 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)},\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec = ts.time_step_spec(\n",
    "    observation_spec = observation_spec, \n",
    "    # reward_spec = _reward_spec\n",
    ")\n",
    "time_step_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "169f3e07-0c0a-4e18-b620-756f9419ccb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(64,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5e78dfd-b96d-48d6-8b17-353536485d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': TensorSpec(shape=(128,), dtype=tf.float32, name='reward')}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.specs import array_spec\n",
    "\n",
    "reward_spec = {\n",
    "    \"reward\": array_spec.ArraySpec(shape=[BATCH_SIZE], dtype=np.float32, name=\"reward\")\n",
    "}\n",
    "\n",
    "reward_tensor_spec = train_utils.from_spec(reward_spec)\n",
    "reward_tensor_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9515c0f4-f01b-4ffa-968d-4d6e3ff599ef",
   "metadata": {},
   "source": [
    "### Distribution strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29f198a2-a945-49c2-a58d-6663b1cf0eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy at 0x7f5c802bf250>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_gpu = True\n",
    "use_tpu = False\n",
    "\n",
    "distribution_strategy = strategy_utils.get_strategy(tpu=use_tpu, use_gpu=use_gpu)\n",
    "distribution_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f85a3cb-0cbd-43fe-805d-a7c1047b0f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_REPLICAS = distribution_strategy.num_replicas_in_sync\n",
    "NUM_REPLICAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd87d58a-9273-4fa1-a9c3-bfd97554cf01",
   "metadata": {},
   "source": [
    "## Agent & Policy config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12d6a9d1-dcc6-4205-9996-c08751d032a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128,\n",
      " 'common_layers': [16, 8],\n",
      " 'encoding_dim': 1,\n",
      " 'epsilon': 0.01,\n",
      " 'eval_batch_size': 1,\n",
      " 'global_layers': [64, 32, 16],\n",
      " 'learning_rate': 0.05,\n",
      " 'model_type': 'epsGreedy',\n",
      " 'network_type': 'commontower',\n",
      " 'num_actions': 2,\n",
      " 'per_arm_layers': [64, 32, 16]}\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Agents\n",
    "# ================================\n",
    "AGENT_TYPE      = 'epsGreedy' # 'LinUCB' | 'LinTS |, 'epsGreedy' | 'NeuralLinUCB'\n",
    "\n",
    "# Parameters for linear agents (LinUCB and LinTS).\n",
    "AGENT_ALPHA     = 0.1\n",
    "\n",
    "# Parameters for neural agents (NeuralEpsGreedy and NerualLinUCB).\n",
    "EPSILON         = 0.01\n",
    "LR              = 0.05\n",
    "\n",
    "# Parameters for NeuralLinUCB\n",
    "ENCODING_DIM    = 1\n",
    "EPS_PHASE_STEPS = 1000\n",
    "\n",
    "# ================================\n",
    "# Agent's Preprocess Network\n",
    "# ================================\n",
    "GLOBAL_LAYERS   = [64, 32, 16] # beginning should be of size: GLOBAL_DIM\n",
    "ARM_LAYERS      = [64, 32, 16] # beginning should be of size: PER_ARM_DIM\n",
    "COMMON_LAYERS   = [16, 8]\n",
    "\n",
    "NETWORK_TYPE    = \"commontower\" # 'commontower' | 'dotproduct'\n",
    "\n",
    "if AGENT_TYPE == 'NeuralLinUCB':\n",
    "    NETWORK_TYPE = 'commontower'\n",
    "    ENCODING_DIM = COMMON_LAYERS[-1]\n",
    "\n",
    "\n",
    "HPARAMS = {  # TODO - streamline and consolidate\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"eval_batch_size\" : EVAL_BATCH_SIZE,\n",
    "    \"num_actions\": NUM_ACTIONS,\n",
    "    \"model_type\": AGENT_TYPE,\n",
    "    \"network_type\": NETWORK_TYPE,\n",
    "    \"global_layers\": GLOBAL_LAYERS,\n",
    "    \"per_arm_layers\": ARM_LAYERS,\n",
    "    \"common_layers\": COMMON_LAYERS,\n",
    "    \"learning_rate\": LR,\n",
    "    \"epsilon\": EPSILON,\n",
    "    \"encoding_dim\": ENCODING_DIM\n",
    "}\n",
    "pprint(HPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99d9d36c-fb72-4859-91a7-c0c52672702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent: NeuralEpsGreedyAgent\n",
      "Network: commontower\n"
     ]
    }
   ],
   "source": [
    "from src.perarm_features import agent_factory as agent_factory\n",
    "\n",
    "with distribution_strategy.scope():\n",
    "\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "    agent = agent_factory.PerArmAgentFactory._get_agent(\n",
    "        agent_type = AGENT_TYPE,\n",
    "        network_type = NETWORK_TYPE,\n",
    "        time_step_spec = time_step_spec,\n",
    "        action_spec = action_spec,\n",
    "        observation_spec=observation_spec,\n",
    "        global_layers = GLOBAL_LAYERS,\n",
    "        arm_layers = ARM_LAYERS,\n",
    "        common_layers = COMMON_LAYERS,\n",
    "        agent_alpha = AGENT_ALPHA,\n",
    "        learning_rate = LR,\n",
    "        epsilon = EPSILON,\n",
    "        train_step_counter = global_step,\n",
    "        output_dim = ENCODING_DIM,\n",
    "        eps_phase_steps = EPS_PHASE_STEPS,\n",
    "        summarize_grads_and_vars = True,\n",
    "        debug_summaries = True\n",
    "    )\n",
    "      \n",
    "agent.initialize()\n",
    "print(f'agent: {agent.name}')\n",
    "\n",
    "# if network:\n",
    "#     print(f\"Network: {network}\")\n",
    "\n",
    "if NETWORK_TYPE:\n",
    "    print(f\"Network: {NETWORK_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ff479-af1d-4437-93c5-1635fed4ee66",
   "metadata": {},
   "source": [
    "### Inspect the specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a755a4e1-84c6-40d9-972c-706188a95c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50c9e9c3-3965-4aba-8141-c6b114c089e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TupleWrapper(TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': DictWrapper({'global': TensorSpec(shape=(64,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.time_step_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b277541-4429-4e11-993b-759b665a9188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TupleWrapper(Trajectory(\n",
       "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(1, dtype=int32)),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': DictWrapper({'global': TensorSpec(shape=(64,), dtype=tf.float32, name=None)}),\n",
       " 'policy_info': PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorSpec(shape=(2,), dtype=tf.float32, name=None), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32)), chosen_arm_features=TensorSpec(shape=(64,), dtype=tf.float32, name=None)),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.training_data_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e3977-eca9-427c-a7cd-2762f36c623d",
   "metadata": {},
   "source": [
    "## Trajectory function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed9ba099-7fc9-46d7-8558-610adf56b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _trajectory_fn(element): # hparams\n",
    "    \n",
    "    \"\"\"Converts a dataset element into a trajectory.\"\"\"\n",
    "    # global_features = _get_global_context_features(element)\n",
    "    # arm_features = _get_per_arm_features(element)\n",
    "    \n",
    "    global_features = embs._get_global_context_features(element)\n",
    "    arm_features = embs._get_per_arm_features(element)\n",
    "    \n",
    "    # Adds a time dimension.\n",
    "    arm_features = train_utils._add_outer_dimension(arm_features)\n",
    "\n",
    "    # obs spec\n",
    "    observation = {\n",
    "        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n",
    "            train_utils._add_outer_dimension(global_features),\n",
    "    }\n",
    "\n",
    "    # reward = train_utils._add_outer_dimension(_get_rewards(element))\n",
    "    reward = train_utils._add_outer_dimension(reward_factory._get_rewards(element))\n",
    "\n",
    "    # To emit the predicted rewards in policy_info, we need to create dummy\n",
    "    # rewards to match the definition in TensorSpec for the ones specified in\n",
    "    # emit_policy_info set.\n",
    "    dummy_rewards = tf.zeros([HPARAMS['batch_size'], 1, HPARAMS['num_actions']])\n",
    "    policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "        chosen_arm_features=arm_features,\n",
    "        # Pass dummy mean rewards here to match the model_spec for emitting\n",
    "        # mean rewards in policy info\n",
    "        predicted_rewards_mean=dummy_rewards,\n",
    "        bandit_policy_type=tf.zeros([HPARAMS['batch_size'], 1, 1], dtype=tf.int32)\n",
    "        # policy_utilities.create_bandit_policy_type_tensor_spec(shape=[1]) # policy_utilities.BanditPolicyType.GREEDY\n",
    "        # tf.zeros([batch_size, 1, 1], dtype=tf.int32)\n",
    "    )\n",
    "    \n",
    "    if HPARAMS['model_type'] == 'neural_ucb':\n",
    "        policy_info = policy_info._replace(\n",
    "            predicted_rewards_optimistic=dummy_rewards\n",
    "        )\n",
    "        \n",
    "    return trajectory.single_step(\n",
    "        observation=observation,\n",
    "        action=tf.zeros_like(\n",
    "            reward, dtype=tf.int32\n",
    "        ),  # Arm features are copied from policy info, put dummy zeros here\n",
    "        policy_info=policy_info,\n",
    "        reward=reward,\n",
    "        discount=tf.zeros_like(reward)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb7e89-794d-4547-80eb-ea40aefc7a30",
   "metadata": {},
   "source": [
    "# Create train & eval loop for demonstration\n",
    "\n",
    "> TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ebbca5-754b-4ffd-91d9-6c0480a78c06",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e507e412-31e5-4fa1-9fe6-e6dbe26b4bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(\n",
    "    iterations: int,\n",
    "):\n",
    "    # train agent for X iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136d213-77bb-47b0-b96d-5237b9562e66",
   "metadata": {},
   "source": [
    "# (1) Offline training \n",
    "\n",
    "> TODO: add environment simulation for first iteration?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c456249-8299-4df6-8c25-fa8477c5963c",
   "metadata": {},
   "source": [
    "## Set Vertex Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dfbb5004-396d-4b1c-8557-4e8c95597dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_VERSION = \"v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5207887c-bf58-421e-9b27-6a41dbf23c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : 05a-onlinev1-rec-bandits-v2\n",
      "RUN_NAME          : run-20240207-153934\n",
      "\n",
      "BASE_OUTPUT_DIR   : gs://rec-bandits-v2-hybrid-vertex-bucket/05a-onlinev1-rec-bandits-v2/run-20240207-153934\n",
      "LOG_DIR           : gs://rec-bandits-v2-hybrid-vertex-bucket/05a-onlinev1-rec-bandits-v2/run-20240207-153934/logs\n",
      "ROOT_DIR          : gs://rec-bandits-v2-hybrid-vertex-bucket/05a-onlinev1-rec-bandits-v2/run-20240207-153934/root\n",
      "ARTIFACTS_DIR     : gs://rec-bandits-v2-hybrid-vertex-bucket/05a-onlinev1-rec-bandits-v2/run-20240207-153934/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'05a-online-{EXPERIMENT_VERSION}-{PREFIX}'\n",
    "\n",
    "# new experiment\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "BASE_OUTPUT_DIR   = f'{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "LOG_DIR           = f\"{BASE_OUTPUT_DIR}/logs\"\n",
    "ROOT_DIR          = f\"{BASE_OUTPUT_DIR}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BASE_OUTPUT_DIR}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "# aiplatform.init(\n",
    "#     project=PROJECT_ID,\n",
    "#     location=REGION,\n",
    "#     experiment=EXPERIMENT_NAME\n",
    "# )\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\\n\")\n",
    "print(f\"BASE_OUTPUT_DIR   : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e89ef-bd3f-4fb8-bbe5-8a1a3f5c0275",
   "metadata": {},
   "source": [
    "### Create TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "513f5eb4-3b16-47cc-b2ad-6ba7fcb4a320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME : projects/934903580331/locations/us-central1/tensorboards/2146567754814062592\n",
      "TB display name  : 05a-onlinev1-rec-bandits-v2-run-20240207-153934\n",
      "TB_ID            : 2146567754814062592\n"
     ]
    }
   ],
   "source": [
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}-{RUN_NAME}\"\n",
    "\n",
    "tensorboard = aiplatform.Tensorboard.create(\n",
    "    display_name=TENSORBOARD_DISPLAY_NAME\n",
    "    , project=PROJECT_ID\n",
    "    , location=REGION\n",
    ")\n",
    "\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "TB_ID = TB_RESOURCE_NAME.split('/')[-1]\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME : {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name  : {tensorboard.display_name}\")\n",
    "print(f\"TB_ID            : {TB_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "286ceb96-cdf6-4a99-8092-e4988829a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    # experiment_tensorboard=TB_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505865a-006a-4159-9a47-65ad3e09dd5b",
   "metadata": {},
   "source": [
    "### Saver & Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6d27959-3d5a-43a2-a9ec-38301535107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04b07870-2c24-4c8a-bb6d-79980323174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_DIR = f\"tmp_dir_{EXPERIMENT_VERSION}\"\n",
    "\n",
    "# !rm -rf {TMP_DIR}\n",
    "# !mkdir -p {TMP_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a5fdc1d-3404-43aa-99ea-5d5741d3a0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMP_DIR     : tmp_dir_v1\n",
      "global_step : MirroredVariable:{\n",
      "  0: <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=0>\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# tempdir = os.getenv(f\"tmp_dir_{EXPERIMENT_VERSION}\", tempfile.gettempdir())\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "print(f\"TMP_DIR     : {TMP_DIR}\")\n",
    "print(f\"global_step : {global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7fd3daf7-b1ba-44a0-851c-3ecbd2dae325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('metrics: [<tf_agents.metrics.tf_metrics.AverageReturnMetric object at '\n",
      " '0x7f5c802bd2d0>]')\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# TB summary writer\n",
    "# ====================================================\n",
    "with distribution_strategy.scope():\n",
    "    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        f\"{LOG_DIR}\", flush_millis=10 * 1000\n",
    "    )\n",
    "\n",
    "    train_summary_writer.set_as_default()\n",
    "\n",
    "# ====================================================\n",
    "# metrics\n",
    "# ====================================================\n",
    "step_metric = tf_metrics.EnvironmentSteps()\n",
    "metrics = [\n",
    "    # tf_metrics.NumberOfEpisodes(),\n",
    "    # tf_metrics.AverageEpisodeLengthMetric(batch_size=HPARAMS['batch_size']),\n",
    "    tf_metrics.AverageReturnMetric(batch_size=HPARAMS['batch_size'])\n",
    "]\n",
    "\n",
    "pprint(f\"metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48ffc4e7-ae79-409b-9c71-8edba8bbc6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting checkpoint_manager: tmp_dir_v1/checkpoint\n",
      "\n",
      "setting POLICY_DIR: tmp_dir_v1/policy\n",
      "\n",
      "saver: <tf_agents.policies.policy_saver.PolicySaver object at 0x7f5c027f5660>\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# get checkpoint manager\n",
    "# ====================================================\n",
    "# CHKPOINT_DIR = f\"{ROOT_DIR}/chkpoint\"\n",
    "CHKPOINT_DIR = os.path.join(TMP_DIR, 'checkpoint')\n",
    "print(f\"setting checkpoint_manager: {CHKPOINT_DIR}\\n\")\n",
    "\n",
    "checkpoint_manager = train_utils.restore_and_get_checkpoint_manager(\n",
    "    root_dir=CHKPOINT_DIR, \n",
    "    agent=agent, \n",
    "    metrics=metrics, \n",
    "    step_metric=step_metric\n",
    ")\n",
    "# ====================================================\n",
    "# policy saver\n",
    "# ====================================================\n",
    "POLICY_DIR = os.path.join(TMP_DIR, 'policy')\n",
    "print(f\"setting POLICY_DIR: {POLICY_DIR}\\n\")\n",
    "saver = policy_saver.PolicySaver(\n",
    "    agent.policy, \n",
    "    train_step=global_step\n",
    ")\n",
    "print(f\"saver: {saver}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9957cc5-0e9f-4f68-bcf5-4ffb1dc32f2a",
   "metadata": {},
   "source": [
    "## Train & Eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cfc930f9-f430-45e5-8b5e-42cd4cf6b6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_DATA_SIZE : 80000\n",
      "NUM_TRAIN_STEPS : 50\n",
      "EVAL_DATA_SIZE : 1000\n",
      "NUM_EVAL_STEPS : 1000\n",
      "CHKPT_INTERVAL: 50\n",
      "LOG_INTERVAL : 10\n"
     ]
    }
   ],
   "source": [
    "# TRAIN_DATA_SIZE = 80000          # len(list(train_dataset))\n",
    "NUM_TRAIN_STEPS = 50            # TRAIN_DATA_SIZE // HPARAMS['batch_size']\n",
    "\n",
    "EVAL_DATA_SIZE  = 1000          # len(list(val_dataset))\n",
    "NUM_EVAL_STEPS  = 1000           # EVAL_DATA_SIZE // HPARAMS['eval_batch_size']\n",
    "\n",
    "CHKPT_INTERVAL  = NUM_TRAIN_STEPS # // 5\n",
    "LOG_INTERVAL    = 10\n",
    "# EVAL_INTERVAL = NUM_TRAIN_STEPS // 2\n",
    "\n",
    "# print(f\"TRAIN_DATA_SIZE : {TRAIN_DATA_SIZE}\")\n",
    "print(f\"NUM_TRAIN_STEPS : {NUM_TRAIN_STEPS}\")\n",
    "print(f\"EVAL_DATA_SIZE : {EVAL_DATA_SIZE}\")\n",
    "print(f\"NUM_EVAL_STEPS : {NUM_EVAL_STEPS}\")\n",
    "print(f\"CHKPT_INTERVAL: {CHKPT_INTERVAL}\")\n",
    "print(f\"LOG_INTERVAL : {LOG_INTERVAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3dde5dfb-e6cf-4b39-8cc8-64b667885d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TakeDataset element_spec={'bucketized_user_age': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'movie_genres': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None), 'movie_id': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'timestamp': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'user_id': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_occupation_text': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_rating': TensorSpec(shape=(None,), dtype=tf.float32, name=None)}>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data\n",
    "train_ds_iterator = iter(train_dataset.batch(HPARAMS['batch_size']).repeat())\n",
    "\n",
    "# eval dataset\n",
    "eval_ds = val_dataset.batch(HPARAMS[\"eval_batch_size\"])\n",
    "\n",
    "if NUM_EVAL_STEPS > 0:\n",
    "    eval_ds = eval_ds.take(NUM_EVAL_STEPS)\n",
    "\n",
    "# eval_ds = eval_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "eval_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b2c67a-5822-48f0-ab6b-51f8fe6e38fa",
   "metadata": {},
   "source": [
    "### evaluate pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01a87a44-cd04-47c7-8e8a-e4fd34e4c877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating pre-trained Agent...\n",
      "pre-train val_loss     : 15.335515975952148\n",
      "pre-train eval runtime : 0\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Evaluate the agent's policy once before training\n",
    "# ====================================================\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "pre_policy_tf = py_tf_eager_policy.PyTFEagerPolicy(agent.policy, use_tf_function=True)\n",
    "\n",
    "print(f\"evaluating pre-trained Agent...\")\n",
    "start_time = time.time()\n",
    "\n",
    "pre_val_loss, pre_preds, pre_tr_rewards = eval_perarm._run_bandit_eval(\n",
    "    policy = pre_policy_tf,\n",
    "    data = eval_ds,\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    vocab_dict = VOCAB_DICT,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"pre-train val_loss     : {pre_val_loss}\")\n",
    "print(f\"pre-train eval runtime : {runtime_mins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677fb309-e72f-44ce-b601-e5a70e6f6c16",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e33216ae-76eb-4cba-a658-f3649f1b070c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting train loop...\n",
      "View your Tensorboard at https://us-central1.tensorboard.googleusercontent.com/experiment/projects+934903580331+locations+us-central1+tensorboards+5890523594647142400+experiments+05a-onlinev1-rec-bandits-v2\n",
      "step = 0: train loss = 15.890000343322754\n",
      "step = 10: train loss = 1.690000057220459\n",
      "step = 20: train loss = 1.5499999523162842\n",
      "step = 30: train loss = 1.159999966621399\n",
      "step = 40: train loss = 1.0099999904632568\n",
      "train runtime_mins: 5\n"
     ]
    }
   ],
   "source": [
    "list_o_loss = []\n",
    "\n",
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "print(f\"starting train loop...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Continuous monitoring\n",
    "aiplatform.start_upload_tb_log(\n",
    "    # tensorboard_id=TB_RESOURCE_NAME,\n",
    "    tensorboard_experiment_name=EXPERIMENT_NAME,\n",
    "    logdir=LOG_DIR,\n",
    "    experiment_display_name=EXPERIMENT_NAME,\n",
    "    run_name_prefix=RUN_NAME,\n",
    "    # description=description,\n",
    ")\n",
    "\n",
    "# for i in tqdm(range(NUM_TRAIN_STEPS)):\n",
    "for i in range(NUM_TRAIN_STEPS):\n",
    "    \n",
    "    with train_summary_writer.as_default():\n",
    "\n",
    "        data = next(train_ds_iterator)\n",
    "        trajectories = _trajectory_fn(data)\n",
    "\n",
    "        # All tensors in experience must be shaped [batch, time, ...] \n",
    "        step = agent.train_step_counter.numpy()\n",
    "        loss = agent.train(experience=trajectories)\n",
    "        list_o_loss.append(loss.loss.numpy())\n",
    "\n",
    "        train_utils._export_metrics_and_summaries(\n",
    "            step=i, \n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        # print step loss\n",
    "        if step % LOG_INTERVAL == 0:\n",
    "            print(\n",
    "                'step = {0}: train loss = {1}'.format(\n",
    "                    step, round(loss.loss.numpy(), 2)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if i > 0 and i % CHKPT_INTERVAL == 0:\n",
    "            # saver.save(os.path.join(CHKPOINT_DIR, 'policy_%d' % step_metric.result()))\n",
    "            checkpoint_manager.save(global_step)\n",
    "            print(f\"saved policy to: {CHKPOINT_DIR}\")\n",
    "            \n",
    "aiplatform.end_upload_tb_log()\n",
    "   \n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"train runtime_mins: {runtime_mins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9d164ebc-ca0a-46b6-beca-695836dae067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp_dir_v1/checkpoint/ckpt-50'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_manager.save(global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2d96210-282b-4f7a-8d27-f265f7c5e45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MirroredVariable:{\n",
       "  0: <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=50>\n",
       "}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint_manager.initialize_or_restore()\n",
    "global_step = tf.compat.v1.train.get_global_step()\n",
    "global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b9a721-d9f4-4ba6-a385-3001d58a9ed5",
   "metadata": {},
   "source": [
    "Alternatively, you can save the policy (model) and restore it. Unlike checkpointer, you cannot continue with the training, but you can still deploy the model. Note that the downloaded file is much smaller than that of the checkpointer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69ab6d4a-8323-46ba-bf3c-6abd716c7f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "177a6e01-1fe5-4db8-be4c-8ed4510aa068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp_dir_v1/policy/policy_0'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_POLICY_DIR = os.path.join(POLICY_DIR, 'policy_%d' % step_metric.result())\n",
    "SAVE_POLICY_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b2db70c-760b-40f7-8b86-26454846adb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved trained policy to: tmp_dir_v1/policy/policy_0\n"
     ]
    }
   ],
   "source": [
    "saver.save(SAVE_POLICY_DIR)\n",
    "print(f\"saved trained policy to: {SAVE_POLICY_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc16bdc3-a401-4a36-95e0-d7ccef1680cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list_o_loss)\n",
    "# plt.ylim([0, 10])\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5cc1d463-f5ed-4478-b9c1-2ae87e1096b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e9224b2b-44d5-4792-b5d4-c0688c86743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir=$LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "42a851f1-e362-4286-838b-1f16f7dbe324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6006: logdir gs://rec-bandits-v2-hybrid-vertex-bucket/05a-onlinev1-rec-bandits-v2/run-20240207-153934/logs (started 1 day, 22:33:58 ago; pid 709913)\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb393f-b785-459c-97fa-5e8c221d3e93",
   "metadata": {},
   "source": [
    "### evaluate post-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a5c6af1-24ac-4a68-9d5e-e3a2156db18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating trained Agent...\n",
      "post-train val_loss     : 1.3562124967575073\n",
      "post-train eval runtime : 0\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Evaluate the agent's policy once after training\n",
    "# ====================================================\n",
    "print(f\"evaluating trained Agent...\")\n",
    "\n",
    "post_policy_tf = py_tf_eager_policy.PyTFEagerPolicy(agent.policy, use_tf_function=True)\n",
    "start_time = time.time()\n",
    "\n",
    "val_loss, preds, tr_rewards = eval_perarm._run_bandit_eval(\n",
    "    policy = post_policy_tf,\n",
    "    data = eval_ds,\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    vocab_dict = VOCAB_DICT,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"post-train val_loss     : {val_loss}\")\n",
    "print(f\"post-train eval runtime : {runtime_mins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5627ba05-392d-44a3-8a74-49cb71e683bb",
   "metadata": {},
   "source": [
    "# (2) Serving Trained Policy v1\n",
    "\n",
    "* Can't load with `tf.saved_model.load(SAVE_POLICY_DIR)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "246f2693-a34d-444a-a18f-5c83f6484c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp_dir_v1/policy/policy_0'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_POLICY_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "14d14189-50c7-4ca6-8ea9-33a1f93bcaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.py_tf_eager_policy.SavedModelPyTFEagerPolicy at 0x7f57782d3f70>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n",
    "    SAVE_POLICY_DIR, load_specs_from_pbtxt=True\n",
    ")\n",
    "\n",
    "saved_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7070c76b-38e8-40e9-9d31-d6713cba6fb4",
   "metadata": {},
   "source": [
    "### Generate predictions (actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1ab7322e-5dfa-44b7-aa94-5161078a586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFER_SIZE = 1\n",
    "dummy_arm = tf.zeros([INFER_SIZE, PER_ARM_DIM], dtype=tf.float32)\n",
    "\n",
    "SKIP_NUM = 10\n",
    "\n",
    "for x in eval_ds.skip(SKIP_NUM).take(INFER_SIZE):\n",
    "    # get feature tensors\n",
    "    \n",
    "    global_feat_infer = embs._get_global_context_features(x)\n",
    "    arm_feat_infer = embs._get_per_arm_features(x)\n",
    "    \n",
    "    # rewards = _get_rewards(x)\n",
    "    rewards = reward_factory._get_rewards(x)\n",
    "    \n",
    "    # reshape arm features\n",
    "    arm_feat_infer = tf.reshape(arm_feat_infer, [HPARAMS['eval_batch_size'], PER_ARM_DIM]) # perarm_dim\n",
    "    concat_arm = tf.concat([arm_feat_infer, dummy_arm], axis=0)\n",
    "    \n",
    "    # flatten global\n",
    "    flat_global_infer = tf.reshape(global_feat_infer, [GLOBAL_DIM])\n",
    "    feature = {'global': flat_global_infer, 'per_arm': concat_arm}\n",
    "    \n",
    "    # get actual reward\n",
    "    actual_reward = rewards.numpy()[0]\n",
    "    \n",
    "    # build trajectory step\n",
    "    trajectory_step = train_utils._get_eval_step(feature, actual_reward)\n",
    "    \n",
    "    prediction = saved_policy.action(trajectory_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d112c061-598b-460e-9693-0c42f79574e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=array(0, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.4420016, 3.2875795], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([ 0.01512526,  0.01463176,  0.04306493,  0.02634757,  0.02050364,\n",
       "       -0.02691734, -0.04732387, -0.01014247,  0.00909616, -0.02351855,\n",
       "        0.03621415, -0.03008725,  0.01309245, -0.01096519,  0.00378894,\n",
       "       -0.02403705,  0.02907662,  0.01840464, -0.0266034 , -0.00288539,\n",
       "        0.0293371 ,  0.02887173, -0.02802934,  0.02223355, -0.00519679,\n",
       "       -0.04653555,  0.01358154,  0.01241325, -0.02529163,  0.03460256,\n",
       "        0.02198031, -0.00950336,  0.0447169 , -0.04901514, -0.03849282,\n",
       "       -0.01054412, -0.0187006 ,  0.03128841,  0.01501706,  0.01084423,\n",
       "        0.01198772, -0.03545277, -0.0271072 , -0.04548055, -0.0316849 ,\n",
       "        0.02547684, -0.0468393 ,  0.02831631,  0.03042262, -0.03449301,\n",
       "        0.04182558,  0.04888989, -0.04323658, -0.04478512,  0.02827256,\n",
       "        0.01930076, -0.01705332, -0.04987627, -0.00813253, -0.02174031,\n",
       "        0.00963182,  0.01701209, -0.00239141, -0.02452952], dtype=float32)))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e8d3a-e68b-43a8-bedd-19c30dcd1d1e",
   "metadata": {},
   "source": [
    "# (3) Online learning\n",
    "\n",
    "> Here we'll simulate all the actions that need to happen on an online endpoint\n",
    "\n",
    "For \"online learning\" to take place, the agent's policy needs to be updated, where \"updated\" is conceptually similar to retraining a traditional supervised learning model (with latest training examples) and deploying the newly trained model\n",
    "* the bandit agent's policy is updated (i.e., learns) when it receives (e.g., `agent.train(...)`) a trajectory that includes the prediction/action AND the delayed feedback\n",
    "* once `agent.train(...)` takes place, the policy is updated (e.g., weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78edfb6a-05cf-4445-babe-e5b50ae45c84",
   "metadata": {},
   "source": [
    "### Load agent checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "eae177d0-1e62-414d-99e8-98e6e5799c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('deployed_metrics: [<tf_agents.metrics.tf_metrics.AverageReturnMetric object '\n",
      " 'at 0x7f5779324e80>]')\n"
     ]
    }
   ],
   "source": [
    "deployed_step_metric = tf_metrics.EnvironmentSteps()\n",
    "deployed_metrics = [\n",
    "    # tf_metrics.NumberOfEpisodes(),\n",
    "    # tf_metrics.AverageEpisodeLengthMetric(batch_size=HPARAMS['batch_size']),\n",
    "    tf_metrics.AverageReturnMetric(batch_size=HPARAMS['batch_size'])\n",
    "]\n",
    "\n",
    "pprint(f\"deployed_metrics: {deployed_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "077b3059-538a-4c7c-a7cd-0d9d8d800001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent: NeuralEpsGreedyAgent\n"
     ]
    }
   ],
   "source": [
    "deployed_agent = agent_factory.PerArmAgentFactory._get_agent(\n",
    "    agent_type = AGENT_TYPE,\n",
    "    network_type = NETWORK_TYPE,\n",
    "    time_step_spec = time_step_spec,\n",
    "    action_spec = action_spec,\n",
    "    observation_spec=observation_spec,\n",
    "    global_layers = GLOBAL_LAYERS,\n",
    "    arm_layers = ARM_LAYERS,\n",
    "    common_layers = COMMON_LAYERS,\n",
    "    agent_alpha = AGENT_ALPHA,\n",
    "    learning_rate = LR,\n",
    "    epsilon = EPSILON,\n",
    "    # train_step_counter = global_step,\n",
    "    output_dim = ENCODING_DIM,\n",
    "    eps_phase_steps = EPS_PHASE_STEPS,\n",
    "    # summarize_grads_and_vars = True,\n",
    "    debug_summaries = True\n",
    ")\n",
    "      \n",
    "deployed_agent.initialize()\n",
    "print(f'agent: {deployed_agent.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ab5f7094-fabb-478e-9ba4-fbb1e9e4638f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MirroredVariable:{\n",
       "  0: <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=150>\n",
       "}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6ecc624c-3496-43cf-bab5-52911ecaf634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp_dir_v1/checkpoint'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHKPOINT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8123c4cc-05ed-4828-a5ca-111ae32bd516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MirroredVariable:{\n",
       "  0: <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=150>\n",
       "}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployed_agent.train_step_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8649dc9-c485-4044-8e42-d1cbe5d212d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_checkpoint_manager = train_utils.restore_and_get_checkpoint_manager(\n",
    "    root_dir=CHKPOINT_DIR, \n",
    "    agent=deployed_agent, \n",
    "    metrics=deployed_metrics, \n",
    "    step_metric=deployed_step_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "531764a5-6c54-466d-b364-2d1cb3214b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MirroredVariable:{\n",
       "  0: <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=50>\n",
       "}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm train step counter\n",
    "deployed_agent.train_step_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66690d05-879e-4cc2-b364-208852cb9e8a",
   "metadata": {},
   "source": [
    "#### eval deployed agent\n",
    "\n",
    "> validate metrics on eval set reflect trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f248d047-fb3e-4a5f-a387-53e168977a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating pre-trained Agent...\n",
      "pre-train val_loss     : 1.3393412828445435\n",
      "pre-train eval runtime : 0\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Evaluate the agent's policy once before training\n",
    "# ====================================================\n",
    "deployed_policy_tf = py_tf_eager_policy.PyTFEagerPolicy(deployed_agent.policy, use_tf_function=True)\n",
    "\n",
    "print(f\"evaluating pre-trained Agent...\")\n",
    "start_time = time.time()\n",
    "\n",
    "pre_val_loss, pre_preds, pre_tr_rewards = eval_perarm._run_bandit_eval(\n",
    "    policy = deployed_policy_tf,\n",
    "    data = eval_ds,\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    vocab_dict = VOCAB_DICT,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"pre-train val_loss     : {pre_val_loss}\")\n",
    "print(f\"pre-train eval runtime : {runtime_mins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1839e39-b1a8-45d2-be78-5f694e78820b",
   "metadata": {},
   "source": [
    "#### Train loop on endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "79a1c276-4773-4e93-91fc-1a5eca957f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6439c628-a905-43ca-8e4f-bd2febdfe781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting train loop...\n",
      "step = 50: train loss = 1.2000000476837158\n",
      "step = 60: train loss = 1.4600000381469727\n",
      "step = 70: train loss = 1.409999966621399\n",
      "step = 80: train loss = 1.4199999570846558\n",
      "step = 90: train loss = 1.1799999475479126\n",
      "step = 100: train loss = 1.649999976158142\n",
      "step = 110: train loss = 1.3200000524520874\n",
      "step = 120: train loss = 1.1200000047683716\n",
      "step = 130: train loss = 0.9100000262260437\n",
      "step = 140: train loss = 1.0800000429153442\n",
      "train runtime_mins: 1\n"
     ]
    }
   ],
   "source": [
    "list_o_loss = []\n",
    "\n",
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "print(f\"starting train loop...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# for i in tqdm(range(NUM_TRAIN_STEPS)):\n",
    "for i in range(NUM_TRAIN_STEPS):\n",
    "\n",
    "    with train_summary_writer.as_default():\n",
    "\n",
    "        data = next(train_ds_iterator)\n",
    "        trajectories = _trajectory_fn(data)\n",
    "\n",
    "        # All tensors in experience must be shaped [batch, time, ...] \n",
    "        step = deployed_agent.train_step_counter.numpy()\n",
    "        loss = deployed_agent.train(experience=trajectories)\n",
    "        list_o_loss.append(loss.loss.numpy())\n",
    "\n",
    "        train_utils._export_metrics_and_summaries(\n",
    "            step=i, \n",
    "            metrics=deployed_metrics\n",
    "        )\n",
    "\n",
    "        # print step loss\n",
    "        if step % LOG_INTERVAL == 0:\n",
    "            print(\n",
    "                'step = {0}: train loss = {1}'.format(\n",
    "                    step, round(loss.loss.numpy(), 2)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # if i > 0 and i % CHKPT_INTERVAL == 0:\n",
    "            # saver.save(os.path.join(CHKPOINT_DIR, 'policy_%d' % step_metric.result()))\n",
    "            # deployed_checkpoint_manager.save(global_step)\n",
    "            # print(f\"saved policy to: {CHKPOINT_DIR}\")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"train runtime_mins: {runtime_mins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2f826ce9-a6c3-49aa-951a-6c2b5eb58794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint  ckpt-50.data-00000-of-00001  ckpt-50.index\n"
     ]
    }
   ],
   "source": [
    "!ls $CHKPOINT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7e498034-5104-4ab6-9805-394f82e8ff59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp_dir_v1/checkpoint/ckpt-150'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployed_checkpoint_manager.save(global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f7e73061-aadb-42d3-a9f5-27067d150c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t      ckpt-150.index\t\t   ckpt-50.index\n",
      "ckpt-150.data-00000-of-00001  ckpt-50.data-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "!ls $CHKPOINT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17978e3-12f1-4421-916d-6d880592ec53",
   "metadata": {},
   "source": [
    "### save newly trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "490fbb88-dc29-4992-ae41-b164a0e081ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deployed_step_metric.result()\n",
    "deployed_agent.train_step_counter.value().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "71917a67-8429-4329-958f-e7fdd187b0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp_dir_v1/policy/policy_150'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEW_SAVE_POLICY_DIR = os.path.join(POLICY_DIR, 'policy_%d' % deployed_agent.train_step_counter.value().numpy())\n",
    "NEW_SAVE_POLICY_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5116acf0-446d-4e3c-a54c-7111e94e3151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved trained policy to: tmp_dir_v1/policy/policy_150\n"
     ]
    }
   ],
   "source": [
    "# saver.save(POLICY_DIR)\n",
    "saver.save(NEW_SAVE_POLICY_DIR)\n",
    "print(f\"saved trained policy to: {NEW_SAVE_POLICY_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345a1d3-7beb-43c4-b8c5-b20a63352b0f",
   "metadata": {},
   "source": [
    "### Create zip file and upload zip file\n",
    "\n",
    "> export / import checkpointer and policy directories such that you can continue training at a later point and deploy the model without having to train again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c213d61-f4cd-49b8-b014-9e6663ca56f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_zip_file(dirname, base_filename):\n",
    "#     return shutil.make_archive(base_filename, 'zip', dirname)\n",
    "\n",
    "# def upload_and_unzip_file_to(dirname):\n",
    "#     if files is None:\n",
    "#         return\n",
    "#     uploaded = files.upload()\n",
    "#     for fn in uploaded.keys():\n",
    "#         print(\n",
    "#             'User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "#                 name=fn, length=len(uploaded[fn])\n",
    "#             )\n",
    "#         )\n",
    "#         shutil.rmtree(dirname)\n",
    "#         zip_files = zipfile.ZipFile(io.BytesIO(uploaded[fn]), 'r')\n",
    "#         zip_files.extractall(dirname)\n",
    "#         zip_files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e495e1-4a48-46d4-9444-4244a8906ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_zip_filename = create_zip_file(CHKPOINT_DIR, os.path.join(tempdir, 'exported_cp'))\n",
    "# checkpoint_zip_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d64b3-4d31-4a83-b813-6c18ef235361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download zip file\n",
    "# if files is not None:\n",
    "#     files.download(checkpoint_zip_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d59d2-1e56-479f-b294-a0966bfee1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload_and_unzip_file_to(CHKPOINT_DIR)\n",
    "# checkpoint_manager.initialize_or_restore()\n",
    "# global_step = tf.compat.v1.train.get_global_step()\n",
    "# print(f\"global_step : {global_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c265ea-7951-4825-bdc3-e448633a4b6e",
   "metadata": {},
   "source": [
    "# (4) Serving Trained Policy v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "55151d77-0f3e-4996-a50e-8e621f3b3e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp_dir_v1/policy/policy_150'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEW_SAVE_POLICY_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c699b4f3-c01d-44e4-87b6-a0833ef057cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.py_tf_eager_policy.SavedModelPyTFEagerPolicy at 0x7f577887ea10>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_policy_v2 = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n",
    "    NEW_SAVE_POLICY_DIR, load_specs_from_pbtxt=True\n",
    ")\n",
    "\n",
    "saved_policy_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ead0516b-aeac-4cd5-aa79-0081fe12505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFER_SIZE = 1\n",
    "dummy_arm = tf.zeros([INFER_SIZE, PER_ARM_DIM], dtype=tf.float32)\n",
    "\n",
    "SKIP_NUM = 10\n",
    "\n",
    "for x in eval_ds.skip(SKIP_NUM).take(INFER_SIZE):\n",
    "    # get feature tensors\n",
    "    \n",
    "    global_feat_infer = embs._get_global_context_features(x)\n",
    "    arm_feat_infer = embs._get_per_arm_features(x)\n",
    "    \n",
    "    # rewards = _get_rewards(x)\n",
    "    rewards = reward_factory._get_rewards(x)\n",
    "    \n",
    "    # reshape arm features\n",
    "    arm_feat_infer = tf.reshape(arm_feat_infer, [HPARAMS['eval_batch_size'], PER_ARM_DIM]) # perarm_dim\n",
    "    concat_arm = tf.concat([arm_feat_infer, dummy_arm], axis=0)\n",
    "    \n",
    "    # flatten global\n",
    "    flat_global_infer = tf.reshape(global_feat_infer, [GLOBAL_DIM])\n",
    "    feature = {'global': flat_global_infer, 'per_arm': concat_arm}\n",
    "    \n",
    "    # get actual reward\n",
    "    actual_reward = rewards.numpy()[0]\n",
    "    \n",
    "    # build trajectory step\n",
    "    trajectory_step = train_utils._get_eval_step(feature, actual_reward)\n",
    "    \n",
    "    prediction = saved_policy_v2.action(trajectory_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "954e834e-d5d6-4b60-89fa-badbd8c13bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=array(0, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.4420016, 3.2875795], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=array([1], dtype=int32), chosen_arm_features=array([ 0.01512526,  0.01463176,  0.04306493,  0.02634757,  0.02050364,\n",
       "       -0.02691734, -0.04732387, -0.01014247,  0.00909616, -0.02351855,\n",
       "        0.03621415, -0.03008725,  0.01309245, -0.01096519,  0.00378894,\n",
       "       -0.02403705,  0.02907662,  0.01840464, -0.0266034 , -0.00288539,\n",
       "        0.0293371 ,  0.02887173, -0.02802934,  0.02223355, -0.00519679,\n",
       "       -0.04653555,  0.01358154,  0.01241325, -0.02529163,  0.03460256,\n",
       "        0.02198031, -0.00950336,  0.0447169 , -0.04901514, -0.03849282,\n",
       "       -0.01054412, -0.0187006 ,  0.03128841,  0.01501706,  0.01084423,\n",
       "        0.01198772, -0.03545277, -0.0271072 , -0.04548055, -0.0316849 ,\n",
       "        0.02547684, -0.0468393 ,  0.02831631,  0.03042262, -0.03449301,\n",
       "        0.04182558,  0.04888989, -0.04323658, -0.04478512,  0.02827256,\n",
       "        0.01930076, -0.01705332, -0.04987627, -0.00813253, -0.02174031,\n",
       "        0.00963182,  0.01701209, -0.00239141, -0.02452952], dtype=float32)))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5f2c1332-74c3-4753-b28b-c025f1918aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bandit_policy_type': 1,\n",
       " 'chosen_arm_features': [0.01512526348233223,\n",
       "  0.014631759375333786,\n",
       "  0.04306492581963539,\n",
       "  0.026347566395998,\n",
       "  0.02050364390015602,\n",
       "  -0.026917338371276855,\n",
       "  -0.04732387140393257,\n",
       "  -0.01014246791601181,\n",
       "  0.009096156805753708,\n",
       "  -0.023518551141023636,\n",
       "  0.03621414676308632,\n",
       "  -0.030087245628237724,\n",
       "  0.013092447072267532,\n",
       "  -0.010965190827846527,\n",
       "  0.003788936883211136,\n",
       "  -0.02403705194592476,\n",
       "  0.029076624661684036,\n",
       "  0.018404636532068253,\n",
       "  -0.026603400707244873,\n",
       "  -0.0028853900730609894,\n",
       "  0.029337096959352493,\n",
       "  0.028871726244688034,\n",
       "  -0.028029335662722588,\n",
       "  0.022233549505472183,\n",
       "  -0.005196787416934967,\n",
       "  -0.04653555154800415,\n",
       "  0.013581540435552597,\n",
       "  0.012413252145051956,\n",
       "  -0.02529163472354412,\n",
       "  0.03460255637764931,\n",
       "  0.02198031172156334,\n",
       "  -0.009503364562988281,\n",
       "  0.04471689835190773,\n",
       "  -0.049015142023563385,\n",
       "  -0.03849282115697861,\n",
       "  -0.010544121265411377,\n",
       "  -0.018700599670410156,\n",
       "  0.03128841146826744,\n",
       "  0.015017058700323105,\n",
       "  0.010844230651855469,\n",
       "  0.011987723410129547,\n",
       "  -0.03545277193188667,\n",
       "  -0.027107203379273415,\n",
       "  -0.045480549335479736,\n",
       "  -0.03168489784002304,\n",
       "  0.025476839393377304,\n",
       "  -0.04683929681777954,\n",
       "  0.028316307812929153,\n",
       "  0.030422616750001907,\n",
       "  -0.03449300676584244,\n",
       "  0.04182558134198189,\n",
       "  0.0488898865878582,\n",
       "  -0.04323657974600792,\n",
       "  -0.04478511959314346,\n",
       "  0.028272558003664017,\n",
       "  0.019300762563943863,\n",
       "  -0.01705331727862358,\n",
       "  -0.049876272678375244,\n",
       "  -0.008132528513669968,\n",
       "  -0.021740306168794632,\n",
       "  0.009631823748350143,\n",
       "  0.0170120932161808,\n",
       "  -0.0023914091289043427,\n",
       "  -0.02452951669692993],\n",
       " 'predicted_rewards_mean': [3.4420015811920166, 3.2875795364379883],\n",
       " 'action': 0}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_pred_dict = {\n",
    "    \"bandit_policy_type\" : int(prediction.info.bandit_policy_type[0]),\n",
    "    \"chosen_arm_features\" : prediction.info.chosen_arm_features.tolist(),\n",
    "    \"predicted_rewards_mean\" : prediction.info.predicted_rewards_mean.tolist(),\n",
    "    \"action\" : int(prediction.action.tolist()),\n",
    "}\n",
    "\n",
    "processed_pred_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347eb5ee-669b-4942-9428-9fb979fad3ce",
   "metadata": {},
   "source": [
    "**Finished**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
