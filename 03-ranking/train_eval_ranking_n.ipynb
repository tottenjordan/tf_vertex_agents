{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba718a1-4ab9-46ed-baa8-ade095f395d1",
   "metadata": {},
   "source": [
    "# Baseline ranking example\n",
    "\n",
    "> see how the pieces fit together with synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc77f69b-b376-4015-9ac3-4bc8de0ba4ee",
   "metadata": {},
   "source": [
    "## trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da5a6622-dfa2-44ee-93ce-ac4a5d71acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generic TF-Agents training function for bandits.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "\n",
    "from absl import logging\n",
    "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
    "from tf_agents.bandits.replay_buffers import bandit_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.eval import metric_utils\n",
    "# from tf_agents.google.metrics import export_utils\n",
    "from tf_agents.metrics import export_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "tf = tf.compat.v2\n",
    "\n",
    "AGENT_CHECKPOINT_NAME = 'agent'\n",
    "STEP_CHECKPOINT_NAME = 'step'\n",
    "CHECKPOINT_FILE_PREFIX = 'ckpt'\n",
    "\n",
    "# GPU\n",
    "from numba import cuda \n",
    "import gc\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# tf exceptions and vars\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a2ea3d-c02d-43f8-9cad-a1a96286da45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f02f924-9395-4aae-a63a-9b9d29f62044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d1c26ae-4eb3-4a1c-b6e4-e68bf36ddf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_replay_buffer(\n",
    "    data_spec, batch_size, steps_per_loop, async_steps_per_loop\n",
    "):\n",
    "    \"\"\"Return a `TFUniformReplayBuffer` for the given `agent`.\"\"\"\n",
    "    return bandit_replay_buffer.BanditReplayBuffer(\n",
    "        data_spec=data_spec,\n",
    "        batch_size=batch_size,\n",
    "        max_length=steps_per_loop * async_steps_per_loop,\n",
    "    )\n",
    "\n",
    "\n",
    "def set_expected_shape(experience, num_steps):\n",
    "    \"\"\"Sets expected shape.\"\"\"\n",
    "\n",
    "    def set_time_dim(input_tensor, steps):\n",
    "        tensor_shape = input_tensor.shape.as_list()\n",
    "        if len(tensor_shape) < 2:\n",
    "            raise ValueError(\n",
    "                'input_tensor is expected to be of rank-2, but found otherwise: '\n",
    "                f'input_tensor={input_tensor}, tensor_shape={tensor_shape}'\n",
    "            )\n",
    "        tensor_shape[1] = steps\n",
    "        input_tensor.set_shape(tensor_shape)\n",
    "\n",
    "    tf.nest.map_structure(lambda t: set_time_dim(t, num_steps), experience)\n",
    "\n",
    "\n",
    "def _get_training_loop(\n",
    "    driver, replay_buffer, agent, steps, async_steps_per_loop\n",
    "):\n",
    "    \"\"\"Returns a `tf.function` that runs the driver and training loops.\n",
    "\n",
    "    Args:\n",
    "    driver: an instance of `Driver`.\n",
    "    replay_buffer: an instance of `ReplayBuffer`.\n",
    "    agent: an instance of `TFAgent`.\n",
    "    steps: an integer indicating how many driver steps should be executed and\n",
    "      presented to the trainer during each training loop.\n",
    "    async_steps_per_loop: an integer. In each training loop, the driver runs\n",
    "      this many times, and then the agent gets asynchronously trained over this\n",
    "      many batches sampled from the replay buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def _export_metrics_and_summaries(step, metrics):\n",
    "        \"\"\"Exports metrics and tf summaries.\"\"\"\n",
    "        metric_utils.log_metrics(metrics)\n",
    "        export_utils.export_metrics(step=step, metrics=metrics)\n",
    "        for metric in metrics:\n",
    "            metric.tf_summaries(train_step=step)\n",
    "\n",
    "    def training_loop(train_step, metrics):\n",
    "        \"\"\"Returns a function that runs a single training loop and logs metrics.\"\"\"\n",
    "        for batch_id in range(async_steps_per_loop):\n",
    "            driver.run()\n",
    "            _export_metrics_and_summaries(\n",
    "                step=train_step * async_steps_per_loop + batch_id, metrics=metrics\n",
    "            )\n",
    "        batch_size = driver.env.batch_size\n",
    "        dataset_it = iter(\n",
    "            replay_buffer.as_dataset(\n",
    "                sample_batch_size=batch_size,\n",
    "                num_steps=steps,\n",
    "                single_deterministic_pass=True,\n",
    "            )\n",
    "        )\n",
    "        for batch_id in range(async_steps_per_loop):\n",
    "            experience, unused_buffer_info = dataset_it.get_next()\n",
    "            set_expected_shape(experience, steps)\n",
    "            loss_info = agent.train(experience)\n",
    "            export_utils.export_metrics(\n",
    "                step=train_step * async_steps_per_loop + batch_id,\n",
    "                metrics=[],\n",
    "                loss_info=loss_info,\n",
    "            )\n",
    "            if train_step % 100 == 0:\n",
    "                print(\n",
    "                    f'step = {train_step}: train loss = {round(loss_info.loss.numpy(), 2)}'\n",
    "                )\n",
    "\n",
    "        replay_buffer.clear()\n",
    "\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d7749c-cf5b-4234-a342-198ba38c229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_and_get_checkpoint_manager(root_dir, agent, metrics, step_metric):\n",
    "    \"\"\"Restores from `root_dir` and returns a function that writes checkpoints.\"\"\"\n",
    "    trackable_objects = {metric.name: metric for metric in metrics}\n",
    "    trackable_objects[AGENT_CHECKPOINT_NAME] = agent\n",
    "    trackable_objects[STEP_CHECKPOINT_NAME] = step_metric\n",
    "    checkpoint = tf.train.Checkpoint(**trackable_objects)\n",
    "    checkpoint_manager = tf.train.CheckpointManager(\n",
    "        checkpoint=checkpoint, directory=root_dir, max_to_keep=5\n",
    "    )\n",
    "    latest = checkpoint_manager.latest_checkpoint\n",
    "    if latest is not None:\n",
    "        logging.info('Restoring checkpoint from %s.', latest)\n",
    "        checkpoint.restore(latest)\n",
    "        logging.info('Successfully restored to step %s.', step_metric.result())\n",
    "    else:\n",
    "        logging.info(\n",
    "            'Did not find a pre-existing checkpoint. Starting from scratch.'\n",
    "        )\n",
    "    return checkpoint_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1711946-6112-4ae9-97b2-23cf011a3eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    root_dir,\n",
    "    agent,\n",
    "    environment,\n",
    "    training_loops,\n",
    "    steps_per_loop,\n",
    "    async_steps_per_loop=None,\n",
    "    additional_metrics=(),\n",
    "    get_replay_buffer_fn=None,\n",
    "    get_training_loop_fn=None,\n",
    "    training_data_spec_transformation_fn=None,\n",
    "    save_policy=True,\n",
    "    resume_training_loops=False,\n",
    "):\n",
    "    \"\"\"Perform `training_loops` iterations of training.\n",
    "\n",
    "    Checkpoint results.\n",
    "\n",
    "    If one or more baseline_reward_fns are provided, the regret is computed\n",
    "    against each one of them. Here is example baseline_reward_fn:\n",
    "\n",
    "    def baseline_reward_fn(observation, per_action_reward_fns):\n",
    "    rewards = ... # compute reward for each arm\n",
    "    optimal_action_reward = ... # take the maximum reward\n",
    "    return optimal_action_reward\n",
    "\n",
    "    Args:\n",
    "    root_dir: path to the directory where checkpoints and metrics will be\n",
    "      written.\n",
    "    agent: an instance of `TFAgent`.\n",
    "    environment: an instance of `TFEnvironment`.\n",
    "    training_loops: an integer indicating how many training loops should be run.\n",
    "    steps_per_loop: an integer indicating how many driver steps should be\n",
    "      executed in a single driver run.\n",
    "    async_steps_per_loop: an optional integer for simulating offline or\n",
    "      asynchronous training: In each training loop iteration, the driver runs\n",
    "      this many times, each executing `steps_per_loop` driver steps, and then\n",
    "      the agent gets asynchronously trained over this many batches sampled from\n",
    "      the replay buffer. When unset or set to 1, the function performs\n",
    "      synchronous training, where the agent gets trained on a single batch\n",
    "      immediately after the driver runs.\n",
    "    additional_metrics: Tuple of metric objects to log, in addition to default\n",
    "      metrics `NumberOfEpisodes`, `AverageReturnMetric`, and\n",
    "      `AverageEpisodeLengthMetric`.\n",
    "    get_replay_buffer_fn: An optional function that creates a replay buffer by\n",
    "      taking a data_spec, batch size, the number of driver steps per loop, and\n",
    "      the number of asynchronous training steps per loop. Note that the returned\n",
    "      replay buffer will be passed to `get_training_loop_fn` below to generate a\n",
    "      traininig loop function. If `None`, the `get_replay_buffer` function\n",
    "      defined in this module will be used.\n",
    "    get_training_loop_fn: An optional function that constructs the traininig\n",
    "      loop function executing a single train step. This function takes a driver,\n",
    "      a replay buffer, an agent, the number of driver steps per loop, and the\n",
    "      number of asynchronous training steps per loop. If `None`, the\n",
    "      `get_training_loop` function defined in this module will be used.\n",
    "    training_data_spec_transformation_fn: Optional function that transforms the\n",
    "      data items before they get to the replay buffer.\n",
    "    save_policy: (bool) whether to save the policy or not.\n",
    "    resume_training_loops: A boolean flag indicating whether `training_loops`\n",
    "      should be enforced relatively to the initial (True) or the last (False)\n",
    "      checkpoint.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO(b/127641485): create evaluation loop with configurable metrics.\n",
    "    if training_data_spec_transformation_fn is None:\n",
    "        data_spec = agent.policy.trajectory_spec\n",
    "    else:\n",
    "        data_spec = training_data_spec_transformation_fn(\n",
    "            agent.policy.trajectory_spec\n",
    "        )\n",
    "    if async_steps_per_loop is None:\n",
    "        async_steps_per_loop = 1\n",
    "    if get_replay_buffer_fn is None:\n",
    "        get_replay_buffer_fn = _get_replay_buffer\n",
    "    replay_buffer = get_replay_buffer_fn(\n",
    "        data_spec, environment.batch_size, steps_per_loop, async_steps_per_loop\n",
    "    )\n",
    "\n",
    "    # `step_metric` records the number of individual rounds of bandit interaction;\n",
    "    # that is, (number of trajectories) * batch_size.\n",
    "    step_metric = tf_metrics.EnvironmentSteps()\n",
    "    metrics = [\n",
    "        tf_metrics.NumberOfEpisodes(),\n",
    "        tf_metrics.AverageEpisodeLengthMetric(batch_size=environment.batch_size),\n",
    "    ] + list(additional_metrics)\n",
    "\n",
    "    # If the reward anything else than a single scalar, we're adding multimetric\n",
    "    # average reward.\n",
    "    if isinstance(\n",
    "        environment.reward_spec(), dict\n",
    "    ) or environment.reward_spec().shape != tf.TensorShape(()):\n",
    "        metrics += [\n",
    "            tf_metrics.AverageReturnMultiMetric(\n",
    "                reward_spec=environment.reward_spec(),\n",
    "                batch_size=environment.batch_size,\n",
    "            )\n",
    "        ]\n",
    "    if not isinstance(environment.reward_spec(), dict):\n",
    "        metrics += [\n",
    "            tf_metrics.AverageReturnMetric(batch_size=environment.batch_size)\n",
    "        ]\n",
    "\n",
    "    if training_data_spec_transformation_fn is not None:\n",
    "        add_batch_fn = lambda data: replay_buffer.add_batch(  # pylint: disable=g-long-lambda\n",
    "        training_data_spec_transformation_fn(data)\n",
    "    )\n",
    "    else:\n",
    "        add_batch_fn = replay_buffer.add_batch\n",
    "\n",
    "    observers = [add_batch_fn, step_metric] + metrics\n",
    "\n",
    "    driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        env=environment,\n",
    "        policy=agent.collect_policy,\n",
    "        num_steps=steps_per_loop * environment.batch_size,\n",
    "        observers=observers,\n",
    "    )\n",
    "\n",
    "    if get_training_loop_fn is None:\n",
    "        get_training_loop_fn = _get_training_loop\n",
    "    training_loop = get_training_loop_fn(\n",
    "        driver, replay_buffer, agent, steps_per_loop, async_steps_per_loop\n",
    "    )\n",
    "    checkpoint_manager = restore_and_get_checkpoint_manager(\n",
    "        root_dir, agent, metrics, step_metric\n",
    "    )\n",
    "    train_step_counter = tf.compat.v1.train.get_or_create_global_step()\n",
    "    if save_policy:\n",
    "        saver = policy_saver.PolicySaver(\n",
    "            agent.policy, train_step=train_step_counter\n",
    "        )\n",
    "\n",
    "    summary_writer = tf.summary.create_file_writer(root_dir)\n",
    "    summary_writer.set_as_default()\n",
    "\n",
    "    if resume_training_loops:\n",
    "        train_step_count_per_loop = (\n",
    "            steps_per_loop * environment.batch_size * async_steps_per_loop\n",
    "        )\n",
    "        last_checkpointed_step = step_metric.result().numpy()\n",
    "        if last_checkpointed_step % train_step_count_per_loop != 0:\n",
    "            raise ValueError(\n",
    "                'Last checkpointed step is expected to be a multiple of '\n",
    "                'steps_per_loop * batch_size * async_steps_per_loop, but found '\n",
    "                f'otherwise: last checkpointed step: {last_checkpointed_step}, '\n",
    "                f'steps_per_loop: {steps_per_loop}, batch_size: '\n",
    "                f'{environment.batch_size}, async_steps_per_loop: '\n",
    "                f'{async_steps_per_loop}'\n",
    "            )\n",
    "        starting_loop = last_checkpointed_step // train_step_count_per_loop\n",
    "    else:\n",
    "        starting_loop = 0\n",
    "\n",
    "    for i in range(starting_loop, training_loops):\n",
    "        training_loop(train_step=i, metrics=metrics)\n",
    "        checkpoint_manager.save()\n",
    "        if save_policy & (i % 100 == 0):\n",
    "            saver.save(os.path.join(root_dir, 'policy_%d' % step_metric.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645179e0-3c6b-4dfa-9e8a-8b859c7cbdf6",
   "metadata": {},
   "source": [
    "## train_eval_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5a5e2db-c7c7-4d58-9d32-0de5ef794f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"End-to-end test for ranking.\"\"\"\n",
    "\n",
    "import os\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
    "from tf_agents.bandits.agents import ranking_agent\n",
    "from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "from tf_agents.bandits.environments import ranking_environment\n",
    "from tf_agents.bandits.networks import global_and_arm_feature_network\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.specs import bandit_spec_utils\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67f6728f-050d-4f52-a143-5dbf67f8f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### CHANGE ME\n",
    "\n",
    "root_dir           = \"gs://rec-bandits-v1-hybrid-vertex-bucket/tmp-example-rootdir-v3\"\n",
    "\n",
    "\n",
    "policy_type        = \"cosine_distance\"\n",
    "ff_feedback_model  = \"cascading\"                  # \"score_vector\" | \"cascading\"\n",
    "click_model        = \"ghost_actions\"\n",
    "distance_threshold = 5.0\n",
    "env_type           = \"base\"\n",
    "bias_type          = \"\"\n",
    "bias_severity      = 1.0\n",
    "bias_positive_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f219940-0e94-4a74-88e0-3766c3e63638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and driver parameters.\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_ITEMS = 20\n",
    "NUM_SLOTS = 3\n",
    "GLOBAL_DIM = 32\n",
    "ITEM_DIM = 64\n",
    "\n",
    "TRAINING_LOOPS = 300\n",
    "STEPS_PER_LOOP = 2\n",
    "\n",
    "LR = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b099e6e-caad-4fb7-9dc2-5a4c29885bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_dim = min(GLOBAL_DIM, ITEM_DIM)\n",
    "min_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8245e9e7-fada-4a21-a0ad-bc1b74bf2155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _global_sampling_fn():\n",
    "    return np.random.randint(-1, 1, [GLOBAL_DIM]).astype(np.float32)\n",
    "\n",
    "def _item_sampling_fn():\n",
    "    unnormalized = np.random.randint(-2, 3, [ITEM_DIM]).astype(np.float32)\n",
    "    return unnormalized / np.linalg.norm(unnormalized)\n",
    "\n",
    "def _relevance_fn(global_obs, item_obs):\n",
    "    min_dim = min(GLOBAL_DIM, ITEM_DIM)\n",
    "    dot_prod = np.dot(global_obs[:min_dim], item_obs[:min_dim]).astype(\n",
    "        np.float32\n",
    "    )\n",
    "    return 1 / (1 + np.exp(-dot_prod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6994c144-fb7c-4fd9-bdb5-a3ade9d716c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_obs_test.shape: (32,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1., -1.,  0.,  0., -1., -1.,  0.,  0., -1., -1.,  0.,  0., -1.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,\n",
       "       -1.,  0.,  0., -1.,  0., -1.], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_obs_test = _global_sampling_fn()\n",
    "\n",
    "# GLOBAL_DIM = global_obs_test[0]\n",
    "print(f\"global_obs_test.shape: {global_obs_test.shape}\")\n",
    "\n",
    "global_obs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e9f59ee-c91e-4282-ae97-f4ca4d9204f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_obs_test shape: (64,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.18898225, -0.09449112, -0.09449112,  0.        ,  0.09449112,\n",
       "        0.        ,  0.09449112,  0.09449112, -0.09449112, -0.18898225,\n",
       "       -0.18898225,  0.09449112,  0.        , -0.09449112,  0.        ,\n",
       "        0.        ,  0.18898225,  0.18898225,  0.18898225,  0.18898225,\n",
       "        0.18898225, -0.18898225,  0.18898225, -0.09449112,  0.09449112,\n",
       "       -0.09449112,  0.09449112,  0.        , -0.18898225,  0.        ,\n",
       "        0.09449112,  0.09449112, -0.18898225,  0.        ,  0.18898225,\n",
       "       -0.09449112,  0.18898225,  0.        ,  0.        , -0.18898225,\n",
       "        0.        ,  0.09449112,  0.09449112,  0.        ,  0.        ,\n",
       "        0.09449112, -0.09449112, -0.09449112, -0.09449112, -0.18898225,\n",
       "       -0.09449112, -0.09449112,  0.18898225, -0.18898225,  0.09449112,\n",
       "        0.09449112, -0.18898225,  0.18898225,  0.        , -0.18898225,\n",
       "        0.        ,  0.09449112,  0.09449112,  0.09449112], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_obs_test = _item_sampling_fn()\n",
    "\n",
    "# PER_ARM_DIM = item_obs_test.shape\n",
    "print(f\"item_obs_test shape: {item_obs_test.shape}\")\n",
    "\n",
    "item_obs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85512b66-066c-49f7-b2b3-eb07c50864f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4763947549717827"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_test = _relevance_fn(global_obs_test, item_obs_test)\n",
    "rel_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ef1cb27-03ef-49e7-82d8-a451c3de99cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ghost_actions'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "click_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7a37615-db8e-47a6-9bd3-6e9fa6581de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FeedbackModel.SCORE_VECTOR: 2>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_agent.FeedbackModel.SCORE_VECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f3c7df5-9122-45ab-97ed-415787106544",
   "metadata": {},
   "outputs": [],
   "source": [
    "if env_type == 'exp_pos_bias':\n",
    "    positional_biases = list(1.0 / np.arange(1, NUM_SLOTS + 1) ** 1.3)\n",
    "    \n",
    "    env = ranking_environment.ExplicitPositionalBiasRankingEnvironment(\n",
    "        _global_sampling_fn,\n",
    "        _item_sampling_fn,\n",
    "        _relevance_fn,\n",
    "        NUM_ITEMS,\n",
    "        positional_biases,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    \n",
    "    feedback_model = ranking_agent.FeedbackModel.SCORE_VECTOR\n",
    "\n",
    "elif env_type == 'base':\n",
    "    # Inner product with the excess dimensions ignored.\n",
    "    scores_weight_matrix = np.eye(ITEM_DIM, GLOBAL_DIM, dtype=np.float32)\n",
    "\n",
    "    feedback_model = ranking_agent.FeedbackModel.SCORE_VECTOR\n",
    "    \n",
    "    if ff_feedback_model == 'cascading':\n",
    "        feedback_model = ranking_agent.FeedbackModel.CASCADING\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            'Feedback model {} not implemented'.format(feedback_model)\n",
    "        )\n",
    "        \n",
    "    if click_model == 'ghost_actions':\n",
    "        click_model = ranking_environment.ClickModel.GHOST_ACTIONS\n",
    "    elif click_model == 'distance_based':\n",
    "        click_model = ranking_environment.ClickModel.DISTANCE_BASED\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            'Diversity mode {} not implemented'.format(click_model)\n",
    "        )\n",
    "        \n",
    "    tf_env = ranking_environment.RankingPyEnvironment(\n",
    "        _global_sampling_fn,\n",
    "        _item_sampling_fn,\n",
    "        num_items=NUM_ITEMS,\n",
    "        num_slots=NUM_SLOTS,\n",
    "        scores_weight_matrix=scores_weight_matrix,\n",
    "        # TODO(b/247995883): Merge the two feedback model enums from the agent\n",
    "        # and the enviroment.\n",
    "        feedback_model=feedback_model.value,\n",
    "        click_model=click_model,\n",
    "        distance_threshold=distance_threshold,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06e93a17-351b-49d2-9140-27a8630d15b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ranking_environment'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_env.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "239124af-e713-4b91-bed1-f5cfa132478d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_weight_matrix.shape: (64, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"scores_weight_matrix.shape: {scores_weight_matrix.shape}\")\n",
    "scores_weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "207ed76f-60d2-4abf-abda-67ee57516c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.environments.tf_py_environment.TFPyEnvironment at 0x7fa73d3d0730>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment = tf_py_environment.TFPyEnvironment(tf_env)\n",
    "environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ac25196-539a-46c0-bef2-cd0a988dcbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(32,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(20, 64), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_spec = environment.observation_spec()\n",
    "obs_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c3ff90b-125e-49fe-9d10-61ce1da3135e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(3,), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(19, dtype=int32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.action_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bdce837-6cd3-42f5-9ba8-659505e5acb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen_index': BoundedTensorSpec(shape=(), dtype=tf.int32, name='chosen_index', minimum=array(0, dtype=int32), maximum=array(3, dtype=int32)),\n",
       " 'chosen_value': TensorSpec(shape=(), dtype=tf.float32, name='chosen_value')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.reward_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "447cb91e-f903-46c8-8174-5a4308649a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': {'global': TensorSpec(shape=(32,), dtype=tf.float32, name=None),\n",
       "                 'per_arm': TensorSpec(shape=(20, 64), dtype=tf.float32, name=None)},\n",
       " 'reward': {'chosen_index': BoundedTensorSpec(shape=(), dtype=tf.int32, name='chosen_index', minimum=array(0, dtype=int32), maximum=array(3, dtype=int32)),\n",
       "            'chosen_value': TensorSpec(shape=(), dtype=tf.float32, name='chosen_value')},\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.time_step_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "170df2c2-9695-42df-b94a-bfe282ca3a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.bandits.networks.global_and_arm_feature_network.GlobalAndArmCommonTowerNetwork at 0x7fa7e6225c30>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GLOBAL_LAYERS   = [GLOBAL_DIM, int(GLOBAL_DIM / 2)]\n",
    "# ARM_LAYERS      = [PER_ARM_DIM, int(PER_ARM_DIM / 2), int(PER_ARM_DIM / 4)]\n",
    "# COMMON_LAYERS   = [16, 8]\n",
    "\n",
    "\n",
    "network = (\n",
    "  global_and_arm_feature_network.create_feed_forward_common_tower_network(\n",
    "      observation_spec = obs_spec, \n",
    "      global_layers = (20, 10), \n",
    "      arm_layers = (20, 10),\n",
    "      common_layers = (10, 5),\n",
    "  )\n",
    ")\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1830d68a-abed-4fda-bf9c-cc9da40c6b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cosine_distance'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8402c4a-7e15-46ab-a21b-9d0bbd6479ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RankingPolicyType.COSINE_DISTANCE: 1>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if policy_type == 'cosine_distance':\n",
    "    policy_type = ranking_agent.RankingPolicyType.COSINE_DISTANCE\n",
    "elif policy_type == 'no_penalty':\n",
    "    policy_type = ranking_agent.RankingPolicyType.NO_PENALTY\n",
    "elif policy_type == 'descending_scores':\n",
    "    policy_type = ranking_agent.RankingPolicyType.DESCENDING_SCORES\n",
    "else:\n",
    "    raise NotImplementedError(\n",
    "        'Policy type {} is not implemented'.format(policy_type)\n",
    "    )\n",
    "    \n",
    "policy_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8037f97-0e06-44f3-8291-39240e1243d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57eb85b4-a524-4780-a888-38e03bf9b7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ranking_agent'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_bias_type = None # bias_type or None\n",
    "agent = ranking_agent.RankingAgent(\n",
    "    time_step_spec=environment.time_step_spec(),\n",
    "    action_spec=environment.action_spec(),\n",
    "    scoring_network=network,\n",
    "    optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n",
    "    policy_type=policy_type,\n",
    "    feedback_model=feedback_model,\n",
    "    positional_bias_type=positional_bias_type,\n",
    "    positional_bias_severity=bias_severity,\n",
    "    positional_bias_positive_only=bias_positive_only,\n",
    "    summarize_grads_and_vars=True,\n",
    ")\n",
    "agent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "503d23b1-6a69-4175-a3c7-4a55519ed8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(3,), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(19, dtype=int32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "029f048e-6324-4e18-9113-8d043d351e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TupleWrapper(TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': DictWrapper({'global': TensorSpec(shape=(32,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(20, 64), dtype=tf.float32, name=None)}),\n",
       " 'reward': DictWrapper({'chosen_index': BoundedTensorSpec(shape=(), dtype=tf.int32, name='chosen_index', minimum=array(0, dtype=int32), maximum=array(3, dtype=int32)), 'chosen_value': TensorSpec(shape=(), dtype=tf.float32, name='chosen_value')}),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.time_step_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6f0dd37-d445-4aed-b4b7-7f0f057ef488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TupleWrapper(Trajectory(\n",
       "{'action': (),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': DictWrapper({'global': TensorSpec(shape=(32,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(3, 64), dtype=tf.float32, name=None)}),\n",
       " 'policy_info': (),\n",
       " 'reward': DictWrapper({'chosen_index': BoundedTensorSpec(shape=(), dtype=tf.int32, name='chosen_index', minimum=array(0, dtype=int32), maximum=array(3, dtype=int32)), 'chosen_value': TensorSpec(shape=(), dtype=tf.float32, name='chosen_value')}),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.training_data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "324fd7d6-3e84-49d5-baaf-f51d22643dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_items_from_action_fn(orig_trajectory):\n",
    "    \"\"\"Puts the features of the selected items in the recommendation order.\n",
    "\n",
    "    This function is used to make sure that at training the item observation is\n",
    "    filled with features of items selected by the policy, in the order of the\n",
    "    selection. Features of unselected items are discarded.\n",
    "\n",
    "    Args:\n",
    "      orig_trajectory: The trajectory as output by the policy\n",
    "\n",
    "    Returns:\n",
    "      The modified trajectory that contains slotted item features.\n",
    "    \"\"\"\n",
    "    item_obs = orig_trajectory.observation[\n",
    "        bandit_spec_utils.PER_ARM_FEATURE_KEY\n",
    "    ]\n",
    "    action = orig_trajectory.action\n",
    "    if isinstance(\n",
    "        orig_trajectory.observation[bandit_spec_utils.PER_ARM_FEATURE_KEY],\n",
    "        tensor_spec.TensorSpec,\n",
    "    ):\n",
    "        dtype = orig_trajectory.observation[\n",
    "            bandit_spec_utils.PER_ARM_FEATURE_KEY\n",
    "        ].dtype\n",
    "        shape = [\n",
    "            NUM_SLOTS,\n",
    "            orig_trajectory.observation[\n",
    "                bandit_spec_utils.PER_ARM_FEATURE_KEY\n",
    "            ].shape[-1],\n",
    "        ]\n",
    "        new_observation = {\n",
    "            bandit_spec_utils.GLOBAL_FEATURE_KEY: orig_trajectory.observation[\n",
    "                bandit_spec_utils.GLOBAL_FEATURE_KEY\n",
    "            ],\n",
    "            bandit_spec_utils.PER_ARM_FEATURE_KEY: tensor_spec.TensorSpec(\n",
    "                dtype=dtype, shape=shape\n",
    "            ),\n",
    "        }\n",
    "    else:\n",
    "        slotted_items = tf.gather(item_obs, action, batch_dims=1)\n",
    "        new_observation = {\n",
    "            bandit_spec_utils.GLOBAL_FEATURE_KEY: orig_trajectory.observation[\n",
    "                bandit_spec_utils.GLOBAL_FEATURE_KEY\n",
    "            ],\n",
    "            bandit_spec_utils.PER_ARM_FEATURE_KEY: slotted_items,\n",
    "        }\n",
    "    \n",
    "    return trajectory.Trajectory(\n",
    "        step_type=orig_trajectory.step_type,\n",
    "        observation=new_observation,\n",
    "        action=(),\n",
    "        policy_info=(),\n",
    "        next_step_type=orig_trajectory.next_step_type,\n",
    "        reward=orig_trajectory.reward,\n",
    "        discount=orig_trajectory.discount,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b08fc5be-c18e-4bce-974a-38ba47e405ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TupleWrapper(Trajectory(\n",
       "{'action': BoundedTensorSpec(shape=(3,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(19, dtype=int32)),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': DictWrapper({'global': TensorSpec(shape=(32,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(20, 64), dtype=tf.float32, name=None)}),\n",
       " 'policy_info': PolicyInfo(log_probability=(), predicted_rewards_mean=TensorSpec(shape=(3,), dtype=tf.float32, name=None), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=()),\n",
       " 'reward': DictWrapper({'chosen_index': BoundedTensorSpec(shape=(), dtype=tf.int32, name='chosen_index', minimum=array(0, dtype=int32), maximum=array(3, dtype=int32)), 'chosen_value': TensorSpec(shape=(), dtype=tf.float32, name='chosen_value')}),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy.trajectory_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91b2a997-0a51-4ca1-876a-45cdd525bf0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(\n",
       "{'action': (),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': {'global': TensorSpec(shape=(32,), dtype=tf.float32, name=None),\n",
       "                 'per_arm': TensorSpec(shape=(3, 64), dtype=tf.float32, name=None)},\n",
       " 'policy_info': (),\n",
       " 'reward': DictWrapper({'chosen_index': BoundedTensorSpec(shape=(), dtype=tf.int32, name='chosen_index', minimum=array(0, dtype=int32), maximum=array(3, dtype=int32)), 'chosen_value': TensorSpec(shape=(), dtype=tf.float32, name='chosen_value')}),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_spec = order_items_from_action_fn(\n",
    "    agent.policy.trajectory_spec\n",
    ")\n",
    "data_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04307c4a-c15b-4980-a3e3-a358bee6b080",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f04eab1e-f4a2-429b-b35f-bc789956bb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE     : 64\n",
      "NUM_ITEMS      : 20\n",
      "NUM_SLOTS      : 3\n",
      "GLOBAL_DIM     : 32\n",
      "ITEM_DIM       : 64\n",
      "TRAINING_LOOPS : 300\n",
      "STEPS_PER_LOOP : 2\n",
      "LR             : 0.05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(f\"BATCH_SIZE     : {BATCH_SIZE}\")\n",
    "print(f\"NUM_ITEMS      : {NUM_ITEMS}\")\n",
    "print(f\"NUM_SLOTS      : {NUM_SLOTS}\")\n",
    "print(f\"GLOBAL_DIM     : {GLOBAL_DIM}\")\n",
    "print(f\"ITEM_DIM       : {ITEM_DIM}\")\n",
    "print(f\"TRAINING_LOOPS : {TRAINING_LOOPS}\")\n",
    "print(f\"STEPS_PER_LOOP : {STEPS_PER_LOOP}\")\n",
    "print(f\"LR             : {LR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be90a0d5-05a5-4188-a82b-6aa8c1892fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0: train loss = 0.6299999952316284\n",
      "step = 100: train loss = 0.03999999910593033\n",
      "step = 200: train loss = 0.05000000074505806\n",
      "train runtime_mins: 23\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train(\n",
    "    root_dir=root_dir,\n",
    "    agent=agent,\n",
    "    environment=environment,\n",
    "    training_loops=TRAINING_LOOPS,\n",
    "    steps_per_loop=STEPS_PER_LOOP,\n",
    "    training_data_spec_transformation_fn=order_items_from_action_fn,\n",
    "    save_policy=False,\n",
    ")\n",
    "\n",
    "runtime_mins = int((time.time() - start_time) / 60)\n",
    "print(f\"train runtime_mins: {runtime_mins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a39f6a72-4c4e-42f8-bb6d-e3cbe382e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c49ade44-8d47-48da-99b9-dd994aa1452a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v1-hybrid-vertex-bucket/tmp-example-rootdir-v3/\n",
      "gs://rec-bandits-v1-hybrid-vertex-bucket/tmp-example-rootdir-v3/checkpoint\n",
      "gs://rec-bandits-v1-hybrid-vertex-bucket/tmp-example-rootdir-v3/ckpt-296.data-00000-of-00001\n",
      "gs://rec-bandits-v1-hybrid-vertex-bucket/tmp-example-rootdir-v3/ckpt-296.index\n",
      "gs://rec-bandits-v1-hybrid-vertex-bucket/tmp-example-rootdir-v3/ckpt-297.data-00000-of-00001\n",
      "gs://rec-bandits-v1-hybrid-vertex-bucket/tmp-example-rootdir-v3/ckpt-297.index\n",
      "gs://rec-bandits-v1-hybrid-vertex-bucket/tmp-example-rootdir-v3/ckpt-298.data-00000-of-00001\n",
      "gs://rec-bandits-v1-hybrid-vertex-bucket/tmp-example-rootdir-v3/ckpt-298.index\n",
      "gs://rec-bandits-v1-hybrid-vertex-bucket/tmp-example-rootdir-v3/ckpt-299.data-00000-of-00001\n",
      "gs://rec-bandits-v1-hybrid-vertex-bucket/tmp-example-rootdir-v3/ckpt-299.index\n",
      "gs://rec-bandits-v1-hybrid-vertex-bucket/tmp-example-rootdir-v3/ckpt-300.data-00000-of-00001\n",
      "gs://rec-bandits-v1-hybrid-vertex-bucket/tmp-example-rootdir-v3/ckpt-300.index\n",
      "gs://rec-bandits-v1-hybrid-vertex-bucket/tmp-example-rootdir-v3/events.out.tfevents.1695931260.jt-tfa-bandit-rankers-2023.3277072.0.v2\n"
     ]
    }
   ],
   "source": [
    "LOG_DIR = \"gs://rec-bandits-v1-hybrid-vertex-bucket/tmp-example-rootdir-v3\"\n",
    "\n",
    "! gsutil ls $LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba5194d9-a9ef-4536-9bb7-a21a847fd6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3037443-4c20-4e70-88cf-a73417a010c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3c785170ca5ecde1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3c785170ca5ecde1\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=$LOG_DIR "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1af07ca-babd-4dd7-82ae-55a4f9f90ef3",
   "metadata": {},
   "source": [
    "# Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3161bfc-d4fa-4d3f-9880-9bbe676afd01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
