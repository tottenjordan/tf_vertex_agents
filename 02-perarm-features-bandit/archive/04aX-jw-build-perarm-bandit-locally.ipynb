{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385307d6-2058-47ec-8da0-57f7ef5c43d6",
   "metadata": {},
   "source": [
    "# Train Bandits with per-arm features\n",
    "\n",
    "**Exploring linear and nonlinear** (e.g., those with neural network-based value functions) bandit methods for recommendations using TF-Agents\n",
    "\n",
    "> Neural linear bandits provide a nice way to leverage the representation power of deep learning and the bandit approach for uncertainty measure and efficient exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fd257-b98b-426a-a2cd-024429b014f1",
   "metadata": {},
   "source": [
    "## Load notebook config\n",
    "\n",
    "* use the prefix defined in `00-env-setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39c9d08-d118-4013-a47f-88450f49f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'mabv1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908f6b95-b539-4a9f-a836-840d26ea3b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"wortz-project-352116\"\n",
      "PROJECT_NUM              = \"679926387543\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"679926387543-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"mabv1\"\n",
      "VERSION                  = \"v1\"\n",
      "\n",
      "BUCKET_NAME              = \"mabv1-wortz-project-352116-bucket\"\n",
      "BUCKET_URI               = \"gs://mabv1-wortz-project-352116-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://mabv1-wortz-project-352116-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/679926387543/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_ID      = \"wortz-project-352116.movielens_dataset_mabv1\"\n",
      "BIGQUERY_TABLE_ID        = \"wortz-project-352116.movielens_dataset_mabv1.training_dataset\"\n",
      "\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "RL_SUB_DIR               = \"per_arm_rl\"\n",
      "REPOSITORY               = \"rl-movielens-mabv1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c690a9-e2bd-4759-ba41-4e2469098aee",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d0dfe4-695c-4dd4-9f24-67f7488ce1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c669f1a1-1af7-4efb-ab2d-6bf3b3847991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pprint import pprint\n",
    "import pickle as pkl\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "# from tf_agents.agents import TFAgent\n",
    "\n",
    "# from tf_agents.bandits.environments import stationary_stochastic_per_arm_py_environment as p_a_env\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "# from tf_agents.drivers import dynamic_step_driver\n",
    "# from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "# from tf_agents.bandits.agents import lin_ucb_agent\n",
    "# from tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\n",
    "from tf_agents.bandits.agents import neural_epsilon_greedy_agent\n",
    "from tf_agents.bandits.agents import neural_linucb_agent\n",
    "from tf_agents.bandits.networks import global_and_arm_feature_network\n",
    "from tf_agents.bandits.policies import policy_utilities\n",
    "\n",
    "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "# GPU\n",
    "from numba import cuda \n",
    "import gc\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# this repo\n",
    "from src.per_arm_rl import data_utils\n",
    "from src.per_arm_rl import data_config\n",
    "\n",
    "# tf exceptions and vars\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e58dd7-ab2b-419f-9771-bf1e98db758b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "# gpus\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4969d3e-1fc0-45db-8a69-aa6b342019de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = cuda.get_current_device()\n",
    "# device.reset()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "274e7f4a-1802-4946-888e-876638f5c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a866b1-85b9-43e6-9546-edfbbf886bce",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5b953-14c0-42ed-a511-77147a1bc0ac",
   "metadata": {},
   "source": [
    "### Read TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d0157c8-a04c-4dbd-b6d9-a1ede97687a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c934c06-bf08-4c7f-b0cc-0de04ef3515c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://mabv1-wortz-project-352116-bucket/data/val/ml-ratings-100k-val.tfrecord']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT = \"val\" # \"train\" | \"val\"\n",
    "\n",
    "train_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{DATA_GCS_PREFIX}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7511e4d-bf81-4800-bde7-8b16dec9aeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([25.], dtype=float32)>,\n",
      " 'movie_genres': <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[4]])>,\n",
      " 'movie_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'211'], dtype=object)>,\n",
      " 'timestamp': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([874948475])>,\n",
      " 'user_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'346'], dtype=object)>,\n",
      " 'user_occupation_text': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'other'], dtype=object)>,\n",
      " 'user_rating': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([4.], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "train_dataset = train_dataset.map(data_utils.parse_tfrecord)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8959967-522e-41c8-9a1b-050ca8bc191f",
   "metadata": {},
   "source": [
    "### get vocab\n",
    "\n",
    "**TODO:** \n",
    "* streamline vocab calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b9169bc-d6dc-497e-9dff-6ebb175282ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATE_VOCABS: False\n"
     ]
    }
   ],
   "source": [
    "GENERATE_VOCABS = False\n",
    "print(f\"GENERATE_VOCABS: {GENERATE_VOCABS}\")\n",
    "\n",
    "VOCAB_SUBDIR   = \"vocabs\"\n",
    "VOCAB_FILENAME = \"vocab_dict.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3ccf137-7a72-42e7-aa89-3c81a99cf40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vocab...\n",
      "Downloaded vocab from: gs://mabv1-wortz-project-352116-bucket/vocabs/vocab_dict.pkl\n",
      "\n",
      "'movie_id'\n",
      "'user_id'\n",
      "'user_occupation_text'\n",
      "'movie_genres'\n",
      "'bucketized_user_age'\n",
      "'max_timestamp'\n",
      "'min_timestamp'\n",
      "'timestamp_buckets'\n"
     ]
    }
   ],
   "source": [
    "if not GENERATE_VOCABS:\n",
    "\n",
    "    EXISTING_VOCAB_FILE = f'gs://{BUCKET_NAME}/{VOCAB_SUBDIR}/{VOCAB_FILENAME}'\n",
    "    print(f\"Downloading vocab...\")\n",
    "    \n",
    "    os.system(f'gsutil -q cp {EXISTING_VOCAB_FILE} .')\n",
    "    print(f\"Downloaded vocab from: {EXISTING_VOCAB_FILE}\\n\")\n",
    "\n",
    "    filehandler = open(VOCAB_FILENAME, 'rb')\n",
    "    vocab_dict = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "    for key in vocab_dict.keys():\n",
    "        pprint(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccaef62-882a-46ff-a1b1-3837e69fdf74",
   "metadata": {},
   "source": [
    "## helper functions\n",
    "\n",
    "**TODO:**\n",
    "* modularize in a train_utils or similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cba2bb14-bf94-466b-b60f-8c7d96c7aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_outer_dimension(x):\n",
    "    \"\"\"Adds an extra outer dimension.\"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        for key, value in x.items():\n",
    "            x[key] = tf.expand_dims(value, 1)\n",
    "        return x\n",
    "    return tf.expand_dims(x, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b941063b-ad48-4817-aef0-9afa8a444632",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits with Per-Arm Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28770b8d-836b-448d-8dd1-203d76fc6d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "nest = tf.nest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0138a295-2b0d-4359-8496-af8552ec8cff",
   "metadata": {},
   "source": [
    "## Preprocessing layers for global and arm features\n",
    "\n",
    "The preproccesing layers will ultimately feed the two functions described below, both of which will ultimately feed the `Environment`\n",
    "\n",
    "`global_context_sampling_fn`: \n",
    "* A function that outputs a random 1d array or list of ints or floats\n",
    "* This output is the global context. Its shape and type must be consistent across calls.\n",
    "\n",
    "`arm_context_sampling_fn`: \n",
    "* A function that outputs a random 1 array or list of ints or floats (same type as the output of `global_context_sampling_fn`). * This output is the per-arm context. Its shape must be consistent across calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8d77956-635c-438a-916a-185eec52f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OOV_BUCKETS        = 1\n",
    "GLOBAL_EMBEDDING_SIZE  = 4\n",
    "MV_EMBEDDING_SIZE      = 8 #32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142e63e-0a20-4d51-997c-7a4733517f7e",
   "metadata": {},
   "source": [
    "### global context (user) features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195acd92-06b6-42e4-bef7-798fd09da856",
   "metadata": {},
   "source": [
    "#### user ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c28e887b-421a-4603-8899-87071056783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_input_layer = tf.keras.Input(\n",
    "    name=\"user_id\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.string\n",
    ")\n",
    "\n",
    "user_id_lookup = tf.keras.layers.StringLookup(\n",
    "    max_tokens=len(vocab_dict['user_id']) + NUM_OOV_BUCKETS,\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    mask_token=None,\n",
    "    vocabulary=vocab_dict['user_id'],\n",
    ")(user_id_input_layer)\n",
    "\n",
    "user_id_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['user_id']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_id_lookup)\n",
    "\n",
    "user_id_embedding = tf.reduce_sum(user_id_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_id_input_layer)\n",
    "# global_features.append(user_id_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d6a0fe7-26cb-4c62-a3ef-17f98e6ccddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'346'], shape=(1,), dtype=string)\n",
      "tf.Tensor([[ 0.02754349  0.02460494 -0.00171218 -0.04865749]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_user_id_model = tf.keras.Model(inputs=user_id_input_layer, outputs=user_id_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"user_id\"])\n",
    "    print(test_user_id_model(x[\"user_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352d2227-92ec-4386-926f-df2fdb9434ec",
   "metadata": {},
   "source": [
    "#### user AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70785bf0-5ece-4875-ab72-06d9c45ea9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_age_input_layer = tf.keras.Input(\n",
    "    name=\"bucketized_user_age\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "user_age_lookup = tf.keras.layers.IntegerLookup(\n",
    "    vocabulary=vocab_dict['bucketized_user_age'],\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    oov_value=0,\n",
    ")(user_age_input_layer)\n",
    "\n",
    "user_age_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['bucketized_user_age']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_age_lookup)\n",
    "\n",
    "user_age_embedding = tf.reduce_sum(user_age_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_age_input_layer)\n",
    "# global_features.append(user_age_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e01622a-9418-4ca7-8925-9b0ebef8940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([25.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([[0.01305303 0.01258517 0.00351674 0.0104368 ]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_user_age_model = tf.keras.Model(inputs=user_age_input_layer, outputs=user_age_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"bucketized_user_age\"])\n",
    "    print(test_user_age_model(x[\"bucketized_user_age\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997ffaa8-ca92-4851-b7e3-bb06fba8958b",
   "metadata": {},
   "source": [
    "#### user OCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03e7344d-71fb-423a-89dd-f1abeb270e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_occ_input_layer = tf.keras.Input(\n",
    "    name=\"user_occupation_text\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.string\n",
    ")\n",
    "\n",
    "user_occ_lookup = tf.keras.layers.StringLookup(\n",
    "    max_tokens=len(vocab_dict['user_occupation_text']) + NUM_OOV_BUCKETS,\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    mask_token=None,\n",
    "    vocabulary=vocab_dict['user_occupation_text'],\n",
    ")(user_occ_input_layer)\n",
    "\n",
    "user_occ_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['user_occupation_text']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_occ_lookup)\n",
    "\n",
    "user_occ_embedding = tf.reduce_sum(user_occ_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_occ_input_layer)\n",
    "# global_features.append(user_occ_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39cbbc31-ca43-4f8f-a804-a4b830e99d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'other'], shape=(1,), dtype=string)\n",
      "tf.Tensor([[-0.04216068 -0.04978548  0.00867306 -0.03317123]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_user_occ_model = tf.keras.Model(inputs=user_occ_input_layer, outputs=user_occ_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"user_occupation_text\"])\n",
    "    print(test_user_occ_model(x[\"user_occupation_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee0098-a48a-4de6-88bf-6219ce8c0533",
   "metadata": {},
   "source": [
    "#### user Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61a4e01a-e742-4c68-93a9-aa66eb9a5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ts_input_layer = tf.keras.Input(\n",
    "    name=\"timestamp\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.int64\n",
    ")\n",
    "\n",
    "user_ts_lookup = tf.keras.layers.Discretization(\n",
    "    vocab_dict['timestamp_buckets'].tolist()\n",
    ")(user_ts_input_layer)\n",
    "\n",
    "user_ts_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['timestamp_buckets'].tolist()) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_ts_lookup)\n",
    "\n",
    "user_ts_embedding = tf.reduce_sum(user_ts_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_ts_input_layer)\n",
    "# global_features.append(user_ts_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db99f90b-57f8-45e6-9f28-871658e17358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([874948475], shape=(1,), dtype=int64)\n",
      "tf.Tensor([[-0.0385089  -0.01885829  0.04150064 -0.0250862 ]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_user_ts_model = tf.keras.Model(inputs=user_ts_input_layer, outputs=user_ts_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"timestamp\"])\n",
    "    print(test_user_ts_model(x[\"timestamp\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc734ea-cb5e-4c6b-8b94-2a8853220178",
   "metadata": {},
   "source": [
    "#### define global sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff58c380-8b53-4dfa-b5b4-d36853638ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_global_context_features(x):\n",
    "    \"\"\"\n",
    "    This function generates a single global observation vector.\n",
    "    \"\"\"\n",
    "    user_id_value = x['user_id']\n",
    "    user_age_value = x['bucketized_user_age']\n",
    "    user_occ_value = x['user_occupation_text']\n",
    "    user_ts_value = x['timestamp']\n",
    "\n",
    "    _id = test_user_id_model(user_id_value) # input_tensor=tf.Tensor(shape=(4,), dtype=float32)\n",
    "    _age = test_user_age_model(user_age_value)\n",
    "    _occ = test_user_occ_model(user_occ_value)\n",
    "    _ts = test_user_ts_model(user_ts_value)\n",
    "\n",
    "    # # tmp - insepct numpy() values\n",
    "    # print(_id.numpy()) #[0])\n",
    "    # print(_age.numpy()) #[0])\n",
    "    # print(_occ.numpy()) #[0])\n",
    "    # print(_ts.numpy()) #[0])\n",
    "\n",
    "    # to numpy array\n",
    "    _id = np.array(_id.numpy())\n",
    "    _age = np.array(_age.numpy())\n",
    "    _occ = np.array(_occ.numpy())\n",
    "    _ts = np.array(_ts.numpy())\n",
    "\n",
    "    concat = np.concatenate(\n",
    "        [_id, _age, _occ, _ts], axis=-1 # -1\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d583b8f8-df19-4bfc-a963-81874d41523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    \n",
    "    iterator = iter(train_dataset.batch(5))\n",
    "    data = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d693e5b-ab19-438c-b8eb-3be677e4c621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bucketized_user_age': <tf.Tensor: shape=(5,), dtype=float32, numpy=array([25., 45., 18., 25., 35.], dtype=float32)>,\n",
       " 'movie_genres': <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
       " array([[4],\n",
       "        [7],\n",
       "        [7],\n",
       "        [1],\n",
       "        [0]])>,\n",
       " 'movie_id': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'211', b'678', b'135', b'97', b'568'], dtype=object)>,\n",
       " 'timestamp': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([874948475, 888638193, 887747108, 882475618, 875350485])>,\n",
       " 'user_id': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'346', b'602', b'393', b'152', b'738'], dtype=object)>,\n",
       " 'user_occupation_text': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
       " array([b'other', b'other', b'student', b'educator', b'technician'],\n",
       "       dtype=object)>,\n",
       " 'user_rating': <tf.Tensor: shape=(5,), dtype=float32, numpy=array([4., 4., 1., 5., 3.], dtype=float32)>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c9f64a0-a713-49bc-9043-d6e9598ecc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #check how this works with batches - new JW\n",
    "\n",
    "# batch_elem = train_dataset.batch(4)\n",
    "# _get_global_context_features(batch_elem)\n",
    "_get_global_context_features(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bba133ab-bf12-4b3b-926d-6d1dba940837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02754349,  0.02460494, -0.00171218, -0.04865749,  0.01305303,\n",
       "         0.01258517,  0.00351674,  0.0104368 , -0.04216068, -0.04978548,\n",
       "         0.00867306, -0.03317123, -0.0385089 , -0.01885829,  0.04150064,\n",
       "        -0.0250862 ]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in train_dataset.batch(1).take(1):\n",
    "    test_globals = _get_global_context_features(x)\n",
    "\n",
    "\n",
    "test_globals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249fa771-35d7-4d04-ab68-2b70911bac17",
   "metadata": {},
   "source": [
    "### arm preprocessing layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534b3bf1-a2ea-4bfb-8c77-efa057f4e391",
   "metadata": {},
   "source": [
    "#### movie ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa53cbe9-2616-4da4-90dc-dc5616258af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_id_input_layer = tf.keras.Input(\n",
    "    name=\"movie_id\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.string\n",
    ")\n",
    "\n",
    "mv_id_lookup = tf.keras.layers.StringLookup(\n",
    "    max_tokens=len(vocab_dict['movie_id']) + NUM_OOV_BUCKETS,\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    mask_token=None,\n",
    "    vocabulary=vocab_dict['movie_id'],\n",
    ")(mv_id_input_layer)\n",
    "\n",
    "mv_id_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['movie_id']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=MV_EMBEDDING_SIZE\n",
    ")(mv_id_lookup)\n",
    "\n",
    "mv_id_embedding = tf.reduce_sum(mv_id_embedding, axis=-2)\n",
    "\n",
    "# arm_inputs.append(mv_id_input_layer)\n",
    "# arm_features.append(mv_id_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9bd19f09-a12e-4a21-a1a1-5ec5bc116559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'211'], shape=(1,), dtype=string)\n",
      "tf.Tensor(\n",
      "[[-0.03246672 -0.00266901  0.00036205  0.04155249 -0.00806403  0.03575673\n",
      "   0.01046394  0.0409542 ]], shape=(1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_mv_id_model = tf.keras.Model(inputs=mv_id_input_layer, outputs=mv_id_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"movie_id\"])\n",
    "    print(test_mv_id_model(x[\"movie_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a0e97-c477-4042-b9c0-fcb0f428de0d",
   "metadata": {},
   "source": [
    "#### movie genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f04a0091-d7b0-4f90-ba7c-3eb41dd0b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_genre_input_layer = tf.keras.Input(\n",
    "    name=\"movie_genres\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "mv_genre_lookup = tf.keras.layers.IntegerLookup(\n",
    "    vocabulary=vocab_dict['movie_genres'],\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    oov_value=0,\n",
    ")(mv_genre_input_layer)\n",
    "\n",
    "mv_genre_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['movie_genres']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=MV_EMBEDDING_SIZE\n",
    ")(mv_genre_lookup)\n",
    "\n",
    "mv_genre_embedding = tf.reduce_sum(mv_genre_embedding, axis=-2)\n",
    "\n",
    "# arm_inputs.append(mv_genre_input_layer)\n",
    "# arm_features.append(mv_genre_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51701f0a-9b3e-461c-a9d9-a0c146e310ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([b'211'], shape=(1,), dtype=string)\n",
      "tf.Tensor(\n",
      "[[-0.03759988  0.0384873   0.0219068  -0.03094243 -0.00252521 -0.0283044\n",
      "  -0.01522502  0.0386533 ]], shape=(1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_mv_gen_model = tf.keras.Model(inputs=mv_genre_input_layer, outputs=mv_genre_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"movie_genres\"])\n",
    "    print(x[\"movie_id\"])\n",
    "    print(test_mv_gen_model(x[\"movie_genres\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b41cc9-63f5-4559-a943-1288be9c0892",
   "metadata": {},
   "source": [
    "#### define sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8727904e-e9b6-4005-8cf3-9da461ca88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_per_arm_features(x):\n",
    "    \"\"\"\n",
    "    This function generates a single per-arm observation vector\n",
    "    \"\"\"\n",
    "    mv_id_value = x['movie_id']\n",
    "    mv_gen_value = x['movie_genres']\n",
    "\n",
    "    _mid = test_mv_id_model(mv_id_value)\n",
    "    _mgen = test_mv_gen_model(mv_gen_value)\n",
    "\n",
    "    # to numpy array\n",
    "    _mid = np.array(_mid.numpy())\n",
    "    _mgen = np.array(_mgen.numpy())\n",
    "\n",
    "    # print(_mid)\n",
    "    # print(_mgen)\n",
    "\n",
    "    concat = np.concatenate(\n",
    "        [_mid, _mgen], axis=-1 # -1\n",
    "    ).astype(np.float32)\n",
    "    # concat = tf.concat([_mid, _mgen], axis=-1).astype(np.float32)\n",
    "\n",
    "    return concat #this is special to this example - there is only one action dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78d3c6bb-e525-4408-b321-34ac9684f881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 16)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_per_arm_features(data).shape #shape checks out at batchdim, nactions, arm feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6836c-67b7-4fd4-917a-24ddad708edd",
   "metadata": {},
   "source": [
    "## TF-Agents implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877c79c-b6c8-4048-b1ce-05f011e8d69e",
   "metadata": {},
   "source": [
    "In TF-Agents, the *per-arm features* implementation differs from the *global-only* feature examples in the following aspects:\n",
    "* Reward is modeled not per-arm, but globally.\n",
    "* The arms are permutation invariant: it doesn’t matter which arm is arm 1 or arm 2, only their features.\n",
    "* One can have a different number of arms to choose from in every step (note that unspecified/dynamically changing number of arms will have a problem with XLA compatibility).\n",
    "\n",
    "When implementing per-arm features in TF-Bandits, the following details have to be discussed:\n",
    "* Observation spec and observations,\n",
    "* Action spec and actions,\n",
    "* Implementation of specific policies and agents.\n",
    "\n",
    "\n",
    "**TODO:**\n",
    "* outline the components and highlight their interactions, dependencies on eachother, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ff9baaf-987d-448d-a981-742a79b581e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE  : 8\n",
      "NUM_ACTIONS : 2\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE  = 8\n",
    "NUM_ACTIONS = 2 #this is kinda deceptive - \n",
    "#our approach is to learn by \"flashing\" one movie rating at a time per user context. \n",
    "#The n_actions = show/don't show the movie with one degree of freedom (n-1)\n",
    "GLOBAL_DIM = 16\n",
    "PER_ARM_DIM = 16\n",
    "\n",
    "print(f\"BATCH_SIZE  : {BATCH_SIZE}\")\n",
    "print(f\"NUM_ACTIONS : {NUM_ACTIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb4902-5bb9-4587-8d29-c01d78b006be",
   "metadata": {},
   "source": [
    "## Tensor Specs\n",
    "\n",
    "**TODO:**\n",
    "* explain relationship between Tensor Specs and their Tensor counterparts\n",
    "* highlight the errors, lessons learned, and utility functions to address these"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68ebe8-116d-43b3-a6e1-4f5a5c7f4741",
   "metadata": {},
   "source": [
    "### Observation spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c4746-d01b-4ca4-aa53-ab68da54d37a",
   "metadata": {},
   "source": [
    "**This observation spec allows the user to have a global observation of fixed dimension**, and an unspecified number of *per-arm* features (also of fixed dimension)\n",
    "* The actions output by the policy are still integers as usual, and they indicate which row of the arm-features it has chosen \n",
    "* The action spec must be a single integer value without boundaries:\n",
    "\n",
    "```python\n",
    "global_spec = tensor_spec.TensorSpec([GLOBAL_DIM], tf.float32)\n",
    "per_arm_spec = tensor_spec.TensorSpec([None, PER_ARM_DIM], tf.float32)\n",
    "observation_spec = {'global': global_spec, 'per_arm': per_arm_spec}\n",
    "\n",
    "action_spec = tensor_spec.TensorSpec((), tf.int32)\n",
    "```\n",
    "> Here the only difference compared to the action spec with global features only is that the tensor spec is not bounded, as we don’t know how many arms there will be at any time step\n",
    "\n",
    "**XLA compatibility:**\n",
    "* Since dynamic tensor shapes are not compatible with XLA, the number of arm features (and consequently, number of arms for a step) cannot be dynamic. \n",
    "* One workaround is to fix the maximum number of arms for a problem, then pad the arm features in steps with fewer arms, and use action masking to indicate how many arms are actually active.\n",
    "\n",
    "```python\n",
    "per_arm_spec = tensor_spec.TensorSpec([NUM_ACTIONS, PER_ARM_DIM], tf.float32)\n",
    "\n",
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=(), dtype=tf.int32, minimum = 0, maximum = NUM_ACTIONS - 1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36bd3b33-635a-4274-8b9e-7172696ebb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(16,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(2, 16), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_spec = {\n",
    "    'global': tf.TensorSpec([GLOBAL_DIM], tf.float32),\n",
    "    'per_arm': tf.TensorSpec([NUM_ACTIONS, PER_ARM_DIM], tf.float32) #excluding action dim here\n",
    "}\n",
    "observation_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2da92-7db2-4f42-94a7-b7bad1c8fc42",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Action spec\n",
    "\n",
    "> The time_step_spec and action_spec are specifications for the input time step and the output action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af42c7-18d7-480f-a7f3-d7da3f4840eb",
   "metadata": {},
   "source": [
    "```python\n",
    "    if (\n",
    "        not tensor_spec.is_bounded(action_spec)\n",
    "        or not tensor_spec.is_discrete(action_spec)\n",
    "        or action_spec.shape.rank > 1\n",
    "        or action_spec.shape.num_elements() != 1\n",
    "    ):\n",
    "      raise NotImplementedError(\n",
    "          'action_spec must be a BoundedTensorSpec of type int32 and shape (). '\n",
    "          'Found {}.'.format(action_spec)\n",
    "      )\n",
    "```\n",
    "\n",
    "* [src](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/policies/reward_prediction_base_policy.py#L97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "549a123c-349a-4103-b39a-4502f47d1e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=[], \n",
    "    dtype=tf.int32,\n",
    "    minimum=tf.constant(0),            \n",
    "    maximum=NUM_ACTIONS-1, #n degrees of freedom and will dictate the expected mean reward spec shape\n",
    "    name=\"action_spec\"\n",
    ")\n",
    "\n",
    "action_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a51efce-9f1b-42d1-bec4-7b788e3fd7e0",
   "metadata": {},
   "source": [
    "### TimeStep spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95f05860-0fbf-4a5a-8273-9c81761e0ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': {'global': TensorSpec(shape=(16,), dtype=tf.float32, name=None),\n",
       "                 'per_arm': TensorSpec(shape=(2, 16), dtype=tf.float32, name=None)},\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec = ts.time_step_spec(observation_spec)#, reward_spec=tf.TensorSpec([1, NUM_ACTIONS]))\n",
    "time_step_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9c197-ae9b-461d-8956-f078b929ac12",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Agent\n",
    "\n",
    "**Note** that contextual bandits form a special case of RL, where the actions taken by the agent do not alter the state of the environment \n",
    "\n",
    "> “Contextual” refers to the fact that the agent chooses among a set of actions while having knowledge of the context (environment observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aadb01-eb5c-4870-ae14-9e66624ba594",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Agent types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075e8a6-11a9-4346-8725-3653fba4bac4",
   "metadata": {},
   "source": [
    "**Possible Agent Types:**\n",
    "\n",
    "```\n",
    "AGENT_TYPE = ['LinUCB', 'LinTS', 'epsGreedy', 'NeuralLinUCB']\n",
    "```\n",
    "\n",
    "**LinearUCBAgent:** (`LinUCB`)\n",
    "* An agent implementing the Linear UCB bandit algorithm\n",
    "* (whitepaper) [A contextual bandit approach to personalized news recommendation](https://arxiv.org/abs/1003.0146)\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/lin_ucb_agent/LinearUCBAgent)\n",
    "\n",
    "**LinearThompsonSamplingAgent:** (`LinTS`)\n",
    "* Implements the Linear Thompson Sampling Agent from the paper: [Thompson Sampling for Contextual Bandits with Linear Payoffs](https://arxiv.org/abs/1209.3352)\n",
    "* the agent maintains two parameters `weight_covariances` and `parameter_estimators`, and updates them based on experience.\n",
    "* The inverse of the weight covariance parameters are updated with the outer product of the observations using the Woodbury inverse matrix update, while the parameter estimators are updated by the reward-weighted observation vectors for every action\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/linear_thompson_sampling_agent/LinearThompsonSamplingAgent)\n",
    "\n",
    "**NeuralEpsilonGreedyAgent:** (`epsGreedy`) \n",
    "* A neural network based epsilon greedy agent\n",
    "* This agent receives a neural network that it trains to predict rewards\n",
    "* The action is chosen greedily with respect to the prediction with probability `1 - epsilon`, and uniformly randomly with probability epsilon\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_epsilon_greedy_agent/NeuralEpsilonGreedyAgent)\n",
    "\n",
    "**NeuralLinUCBAgent:** (`NeuralLinUCB`)\n",
    "* An agent implementing the LinUCB algorithm on top of a neural network\n",
    "* `ENCODING_DIM` is the output dimension of the encoding network \n",
    "> * This output will be used by either a linear reward layer and epsilon greedy exploration, or by a LinUCB logic, depending on the number of training steps executed so far\n",
    "* `EPS_PHASE_STEPS` is the number training steps to run for training the encoding network before switching to `LinUCB`\n",
    "> * If negative, the encoding network is assumed to be already trained\n",
    "> * If the number of steps is less than or equal to `EPS_PHASE_STEPS`, `epsilon greedy` is used, otherwise `LinUCB`\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_linucb_agent/NeuralLinUCBAgent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8e88e-c8ea-4193-a911-0d974ef3b1a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### network types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547087d-5fad-4521-a584-cb46ce52897d",
   "metadata": {},
   "source": [
    "Which network architecture to use for the `epsGreedy` or `NeuralLinUCB` agents\n",
    "\n",
    "```\n",
    "NETWORK_TYPE = ['commontower', 'dotproduct']\n",
    "```\n",
    "\n",
    "**GlobalAndArmCommonTowerNetwork:** (`commontower`)\n",
    "* This network takes the output of the global and per-arm networks, and leads them through a common network, that in turn outputs reward estimates\n",
    "> * `GLOBAL_LAYERS` - Iterable of ints. Specifies the layers of the global tower\n",
    "> * `ARM_LAYERS` - Iterable of ints. Specifies the layers of the arm tower\n",
    "> * `COMMON_LAYERS` - Iterable of ints. Specifies the layers of the common tower\n",
    "* The network produced by this function can be used either in `GreedyRewardPredictionPolicy`, or `NeuralLinUCBPolicy`\n",
    "> * In the former case, the network must have `output_dim=1`, it is going to be an instance of `QNetwork`, and used in the policy as a reward prediction network\n",
    "> * In the latter case, the network will be an encoding network with its output consumed by a reward layer or a `LinUCB` method. The specified `output_dim` will be the encoding dimension\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmCommonTowerNetwork)\n",
    "\n",
    "**GlobalAndArmDotProductNetwork:** (`dotproduct`)\n",
    "* This network calculates the **dot product** of the output of the global and per-arm networks and returns them as reward estimates\n",
    "> * `GLOBAL_LAYERS` - Iterable of ints. Specifies the layers of the global tower\n",
    "> * `ARM_LAYERS` - Iterable of ints. Specifies the layers of the arm tower\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmDotProductNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3dc270-deb5-4e96-8276-74759a06c318",
   "metadata": {},
   "source": [
    "### define agent and network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d817bf2-1fa6-4bae-af90-745fad996b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 8,\n",
      " 'common_layers': [4],\n",
      " 'epsilon': 0.01,\n",
      " 'global_layers': [16, 4],\n",
      " 'learning_rate': 0.05,\n",
      " 'model_type': 'epsGreedy',\n",
      " 'network_type': 'dotproduct',\n",
      " 'num_actions': 2,\n",
      " 'per_arm_layers': [16, 4]}\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Agents\n",
    "# ================================\n",
    "AGENT_TYPE      = 'epsGreedy' # 'LinUCB' | 'LinTS |, 'epsGreedy' | 'NeuralLinUCB'\n",
    "\n",
    "# Parameters for linear agents (LinUCB and LinTS).\n",
    "AGENT_ALPHA     = 0.1\n",
    "\n",
    "# Parameters for neural agents (NeuralEpsGreedy and NerualLinUCB).\n",
    "EPSILON         = 0.01\n",
    "LR              = 0.05\n",
    "\n",
    "# Parameters for NeuralLinUCB\n",
    "ENCODING_DIM    = 1\n",
    "EPS_PHASE_STEPS = 1000\n",
    "\n",
    "# ================================\n",
    "# Agent's Preprocess Network\n",
    "# ================================\n",
    "NETWORK_TYPE    = \"dotproduct\" # 'commontower' | 'dotproduct'\n",
    "\n",
    "if AGENT_TYPE == 'NeuralLinUCB':\n",
    "    NETWORK_TYPE = 'commontower'\n",
    "    \n",
    "\n",
    "GLOBAL_LAYERS   = [16, 4]\n",
    "ARM_LAYERS      = [16, 4]\n",
    "COMMON_LAYERS   = [4]\n",
    "\n",
    "observation_and_action_constraint_splitter = None\n",
    "\n",
    "HPARAMS = {  # TODO - streamline and consolidate\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"num_actions\": NUM_ACTIONS,\n",
    "    \"model_type\": AGENT_TYPE,\n",
    "    \"network_type\": NETWORK_TYPE,\n",
    "    \"global_layers\": GLOBAL_LAYERS,\n",
    "    \"per_arm_layers\": ARM_LAYERS,\n",
    "    \"common_layers\": COMMON_LAYERS,\n",
    "    \"learning_rate\": LR,\n",
    "    \"epsilon\": EPSILON,\n",
    "}\n",
    "pprint(HPARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db165067-6e9d-4b79-b675-bae69ec98c10",
   "metadata": {},
   "source": [
    "### Agent Factory\n",
    "\n",
    "**TODO:**\n",
    "* consolidate agent, network, and hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7c79df4-f975-49fc-b598-d6fd2f6be125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick check on the inputs of the agent - this can be used to diagnose spec shape inputs\n",
      "\n",
      "time_step_spec:  TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': {'global': TensorSpec(shape=(16,), dtype=tf.float32, name=None),\n",
      "                 'per_arm': TensorSpec(shape=(2, 16), dtype=tf.float32, name=None)},\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n",
      "\n",
      "action_spec:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))\n",
      "\n",
      "observation_spec:  {'global': TensorSpec(shape=(16,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(2, 16), dtype=tf.float32, name=None)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Quick check on the inputs of the agent - this can be used to diagnose spec shape inputs\")\n",
    "print(\"\\ntime_step_spec: \", time_step_spec)\n",
    "print(\"\\naction_spec: \", action_spec)\n",
    "print(\"\\nobservation_spec: \", observation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6cb60f0b-90b7-49ab-9c41-046ea00ab750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: OffpolicyNeuralEpsGreedyAgent\n",
      "\n",
      "Network: GlobalAndArmDotProductNetwork\n"
     ]
    }
   ],
   "source": [
    "# from tf_agents.bandits.policies import policy_utilities\n",
    "# from tf_agents.bandits.agents import greedy_reward_prediction_agent\n",
    "\n",
    "network = None\n",
    "observation_and_action_constraint_splitter = None\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "if AGENT_TYPE == 'LinUCB':\n",
    "    agent = lin_ucb_agent.LinearUCBAgent(\n",
    "        time_step_spec=time_step_spec,\n",
    "        action_spec=action_spec,\n",
    "        alpha=AGENT_ALPHA,\n",
    "        accepts_per_arm_features=True,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "elif AGENT_TYPE == 'LinTS':\n",
    "    agent = lin_ts_agent.LinearThompsonSamplingAgent(\n",
    "        time_step_spec=time_step_spec,\n",
    "        action_spec=action_spec,\n",
    "        alpha=AGENT_ALPHA,\n",
    "        observation_and_action_constraint_splitter=(\n",
    "            observation_and_action_constraint_splitter\n",
    "        ),\n",
    "        accepts_per_arm_features=True,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "elif AGENT_TYPE == 'epsGreedy':\n",
    "    # obs_spec = per_arm_tf_env.observation_spec()\n",
    "    if NETWORK_TYPE == 'commontower':\n",
    "        network = global_and_arm_feature_network.create_feed_forward_common_tower_network(\n",
    "            observation_spec = observation_spec, \n",
    "            global_layers = GLOBAL_LAYERS, \n",
    "            arm_layers = ARM_LAYERS, \n",
    "            common_layers = COMMON_LAYERS,\n",
    "            # output_dim = 1\n",
    "        )\n",
    "    elif NETWORK_TYPE == 'dotproduct':\n",
    "        network = global_and_arm_feature_network.create_feed_forward_dot_product_network(\n",
    "            observation_spec = observation_spec, \n",
    "            global_layers = GLOBAL_LAYERS, \n",
    "            arm_layers = ARM_LAYERS\n",
    "        )\n",
    "    agent = neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(\n",
    "        time_step_spec=time_step_spec,\n",
    "        action_spec=action_spec,\n",
    "        reward_network=network,\n",
    "        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=HPARAMS['learning_rate']),\n",
    "        epsilon=HPARAMS['epsilon'],\n",
    "        observation_and_action_constraint_splitter=(\n",
    "            observation_and_action_constraint_splitter\n",
    "        ),\n",
    "        accepts_per_arm_features=True,\n",
    "        emit_policy_info=policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN,\n",
    "        train_step_counter=global_step,\n",
    "        # info_fields_to_inherit_from_greedy=[\n",
    "        #     policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN\n",
    "        # ],\n",
    "        name='OffpolicyNeuralEpsGreedyAgent'\n",
    "    )\n",
    "\n",
    "elif AGENT_TYPE == 'NeuralLinUCB':\n",
    "    # obs_spec = per_arm_tf_env.observation_spec()\n",
    "    network = (\n",
    "        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n",
    "            observation_spec = observation_spec, \n",
    "            global_layers = GLOBAL_LAYERS, \n",
    "            arm_layers = ARM_LAYERS, \n",
    "            common_layers = COMMON_LAYERS,\n",
    "            output_dim = ENCODING_DIM\n",
    "        )\n",
    "    )\n",
    "    agent = neural_linucb_agent.NeuralLinUCBAgent(\n",
    "        time_step_spec=per_arm_tf_env.time_step_spec(),\n",
    "        action_spec=per_arm_tf_env.action_spec(),\n",
    "        encoding_network=network,\n",
    "        encoding_network_num_train_steps=EPS_PHASE_STEPS,\n",
    "        encoding_dim=ENCODING_DIM,\n",
    "        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n",
    "        alpha=1.0,\n",
    "        gamma=1.0,\n",
    "        epsilon_greedy=EPSILON,\n",
    "        accepts_per_arm_features=True,\n",
    "        debug_summaries=True,\n",
    "        summarize_grads_and_vars=True,\n",
    "        emit_policy_info=policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN,\n",
    "    )\n",
    "    \n",
    "agent.initialize() # TODO - does this go here?\n",
    "    \n",
    "print(f\"Agent: {agent.name}\\n\")\n",
    "if network:\n",
    "    print(f\"Network: {network.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d404dae-3cc3-4f81-962b-4641455ca4f2",
   "metadata": {},
   "source": [
    "## Reward function\n",
    "\n",
    "**TODO:**\n",
    "* explain how to translate reward to this common recommendation objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df9aecd6-d20a-48a7-9ead-4da3bbcfdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_rewards(element):\n",
    "    \"\"\"Calculates reward for the actions.\"\"\"\n",
    "\n",
    "    def _calc_reward(x):\n",
    "        \"\"\"Calculates reward for a single action.\"\"\"\n",
    "        r0 = lambda: tf.constant(0.0)\n",
    "        r1 = lambda: tf.constant(-10.0)\n",
    "        r2 = lambda: tf.constant(2.0)\n",
    "        r3 = lambda: tf.constant(3.0)\n",
    "        r4 = lambda: tf.constant(4.0)\n",
    "        r5 = lambda: tf.constant(10.0)\n",
    "        c1 = tf.equal(x, 1.0)\n",
    "        c2 = tf.equal(x, 2.0)\n",
    "        c3 = tf.equal(x, 3.0)\n",
    "        c4 = tf.equal(x, 4.0)\n",
    "        c5 = tf.equal(x, 5.0)\n",
    "        return tf.case(\n",
    "            [(c1, r1), (c2, r2), (c3, r3),(c4, r4),(c5, r5)], \n",
    "            default=r0, exclusive=True\n",
    "        )\n",
    "\n",
    "    return tf.map_fn(\n",
    "        fn=_calc_reward, \n",
    "        elems=element['user_rating'], \n",
    "        dtype=tf.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59703a-9a00-4eba-a3b7-45b21b738dd4",
   "metadata": {},
   "source": [
    "## Trajectory function\n",
    "\n",
    "**parking lot**\n",
    "* does trajectory fn need concept of `dummy_chosen_arm_features`, similar to [this](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/policies/reward_prediction_base_policy.py#L297)\n",
    "\n",
    "```python\n",
    "      dummy_chosen_arm_features = tf.nest.map_structure(\n",
    "          lambda obs: tf.zeros_like(obs[:, 0, ...]),\n",
    "          time_step.observation[bandit_spec_utils.PER_ARM_FEATURE_KEY],\n",
    "      )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b3d3c-75f7-46f4-9a1b-6329e419b7f5",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb7ffc-00df-4d46-857b-86c87b78f597",
   "metadata": {},
   "source": [
    "`agent.train(experience=...)`\n",
    "\n",
    "where `experience` is a batch of trajectories data in the form of a Trajectory. \n",
    "* The structure of experience must match that of `self.training_data_spec`. \n",
    "* All tensors in experience must be shaped [batch, time, ...] where time must be equal to self.train_step_length if that property is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bcb31c7b-f03e-4d9f-a0f8-2f8ad8318d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "def _trajectory_fn(element): # hparams\n",
    "    \n",
    "    \"\"\"Converts a dataset element into a trajectory.\"\"\"\n",
    "    global_features = _get_global_context_features(element)\n",
    "    arm_features = _get_per_arm_features(element)\n",
    "    \n",
    "    # Adds a time dimension.\n",
    "    arm_features = _add_outer_dimension(arm_features)\n",
    "\n",
    "    # obs spec\n",
    "    observation = {\n",
    "        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n",
    "            _add_outer_dimension(global_features), #timedim bloat\n",
    "        # bandit_spec_utils.PER_ARM_FEATURE_KEY:\n",
    "        #     arm_features\n",
    "    }\n",
    "\n",
    "    reward = _add_outer_dimension(_get_rewards(element))\n",
    "\n",
    "\n",
    "    # To emit the predicted rewards in policy_info, we need to create dummy\n",
    "    # rewards to match the definition in TensorSpec for the ones specified in\n",
    "    # emit_policy_info set.\n",
    "    dummy_rewards = tf.zeros([HPARAMS['batch_size'], 1, HPARAMS['num_actions']])\n",
    "    # dummy_rewards = tf.zeros([HPARAMS['batch_size'], HPARAMS['num_actions']])\n",
    "    # dummy_rewards = tf.zeros([HPARAMS['num_actions']])\n",
    "    policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "        chosen_arm_features=arm_features,\n",
    "        # Pass dummy mean rewards here to match the model_spec for emitting\n",
    "        # mean rewards in policy info\n",
    "        predicted_rewards_mean=dummy_rewards\n",
    "    )\n",
    "    \n",
    "    if HPARAMS['model_type'] == 'neural_ucb':\n",
    "        policy_info = policy_info._replace(\n",
    "            predicted_rewards_optimistic=dummy_rewards\n",
    "        )\n",
    "        \n",
    "    return trajectory.single_step(\n",
    "        observation=observation,\n",
    "        action=tf.zeros_like(\n",
    "            reward, dtype=tf.int32\n",
    "        ),  # Arm features are copied from policy info, put dummy zeros here\n",
    "        policy_info=policy_info,\n",
    "        reward=reward,\n",
    "        discount=tf.zeros_like(reward)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cf53ca1-f827-424f-adf4-0edaf863da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "##todo - create a function that selects the best movie features along with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da4680f3-693b-4530-8e4d-88bc43036c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from tf_agents.utils import common\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# global_step = tf.compat.v1.train.get_global_step()\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "train_loss = collections.defaultdict(list)\n",
    "list_o_loss = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    iterator = iter(train_dataset.batch(HPARAMS['batch_size']))\n",
    "    data = next(iterator)\n",
    "    # print(f\"print data: {data}\")\n",
    "    \n",
    "    trajectories = _trajectory_fn(data)\n",
    "    # print(f\"print trajectories: {trajectories}\")\n",
    "    \n",
    "    # All tensors in experience must be shaped [batch, time, ...] \n",
    "    step = agent.train_step_counter.numpy()\n",
    "    loss = agent.train(experience=trajectories)\n",
    "    list_o_loss.append(loss.loss.numpy())\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f23f1-6870-48fb-97fd-1bda065edffc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize the agent's loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a4c4dc23-83fe-465b-aff6-c786a0866408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_o_loss)\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb6393-e016-48c9-a1eb-0e39e35ee9e6",
   "metadata": {},
   "source": [
    "## debugging notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d010b6-1e97-4cbb-a6fc-30695852843f",
   "metadata": {},
   "source": [
    "* say you have a global observation spec of [17]. And you have two batch dimensions [4, 5]. Then your observation has to have the shape [4, 5, 17]\n",
    "* and then if you have arm_obs_spec with shape [9, 13], then the arm obs shape has to be exactly [4, 5, 9, 13]\n",
    "* and this has to be true for every single tensor in your tensor nest\n",
    "* the first 2 dims are the outer dims that are the same for all tensors, the rest of the dimensions have to follow the spec for each tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5376fca9-b283-4f90-94cc-ff9f691ff0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "local-conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
