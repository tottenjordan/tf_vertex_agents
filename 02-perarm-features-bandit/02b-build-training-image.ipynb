{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64583cdb-2502-4f40-a254-8fa68abe7a32",
   "metadata": {},
   "source": [
    "# Build custom container for Vertex training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e1beb37-0c48-4110-8ca0-24ab78c72b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/tf_vertex_agents/02-perarm-features-bandit\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66298299-d4ad-460e-8ab3-ea9fbc012086",
   "metadata": {},
   "source": [
    "## Load env config\n",
    "\n",
    "* use the prefix from `00-env-setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1d2542f-d87b-436d-b4bb-08edcbb532ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX: rec-bandits-v2\n"
     ]
    }
   ],
   "source": [
    "# PREFIX = 'mabv1'\n",
    "VERSION        = \"v2\"                       # TODO\n",
    "PREFIX         = f'rec-bandits-{VERSION}'   # TODO\n",
    "\n",
    "print(f\"PREFIX: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35449112-3c26-4ea9-8c3a-21c283951350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"rec-bandits-v2\"\n",
      "VERSION                  = \"v2\"\n",
      "\n",
      "BUCKET_NAME              = \"rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "BUCKET_URI               = \"gs://rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "DATA_PATH_KFP_DEMO       = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data/kfp_demo_data/u.data\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_NAME    = \"mvlens_rec_bandits_v2\"\n",
      "BIGQUERY_TABLE_NAME      = \"training_dataset\"\n",
      "\n",
      "REPOSITORY               = \"rl-movielens-rec-bandits-v2\"\n",
      "\n",
      "DOCKERNAME_01            = \"Dockerfile_train_my_perarm_env\"\n",
      "IMAGE_NAME_01            = \"train-my-perarm-env-v2\"\n",
      "IMAGE_URI_01             = \"gcr.io/hybrid-vertex/train-my-perarm-env-v2\"\n",
      "\n",
      "DOCKERNAME_02            = \"Dockerfile_perarm_feats\"\n",
      "IMAGE_NAME_02            = \"train-perarm-feats-v2\"\n",
      "IMAGE_URI_02             = \"gcr.io/hybrid-vertex/train-perarm-feats-v2\"\n",
      "\n",
      "DOCKERNAME_03            = \"Dockerfile_ranking_bandit\"\n",
      "IMAGE_NAME_03            = \"train-rank-bandit-v2\"\n",
      "IMAGE_URI_03             = \"gcr.io/hybrid-vertex/train-rank-bandit-v2\"\n",
      "\n",
      "DOCKERNAME_04            = \"Dockerfile_train_mab_e2e\"\n",
      "IMAGE_NAME_04            = \"train-mab-e2e-v2\"\n",
      "IMAGE_URI_04             = \"gcr.io/hybrid-vertex/train-mab-e2e-v2\"\n",
      "\n",
      "DOCKERNAME_04_pred       = \"Dockerfile_pred_mab_e2e\"\n",
      "IMAGE_NAME_04_pred       = \"pred-mab-e2e-v2\"\n",
      "IMAGE_URI_04_pred        = \"gcr.io/hybrid-vertex/pred-mab-e2e-v2\"\n",
      "\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-rec-bandits-v2/local_docker_tfa\"\n",
      "\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "RL_SUB_DIR               = \"per_arm_rl\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b16af06-e188-4f4e-a04b-4e0eb8873ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil ls $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be79c652-45c8-4b5f-b410-4908829375a3",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92272ed8-bfb5-4b40-b7f4-cdbe029f3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261a452d-8ce7-427b-8091-d7d41c57e556",
   "metadata": {},
   "source": [
    "# Build Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d192a4dd-b3e1-4c48-b69f-fd022078e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tree src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9bdef1-ac23-49be-96fd-25795f483643",
   "metadata": {},
   "source": [
    "## Container Image Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eedbe408-e2fe-4944-912f-f2b1f6587440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCKERNAME_02     = Dockerfile_perarm_feats\n",
      "REPOSITORY        = rl-movielens-rec-bandits-v2\n",
      "IMAGE_NAME_02     = train-perarm-feats-v2\n",
      "REMOTE_IMAGE_NAME = us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-rec-bandits-v2/local_docker_tfa\n",
      "IMAGE_URI_02      = gcr.io/hybrid-vertex/train-perarm-feats-v2\n"
     ]
    }
   ],
   "source": [
    "# DOCKERNAME = DOCKERNAME_02\n",
    "# IMAGE_NAME = IMAGE_NAME_02\n",
    "# IMAGE_URI = IMAGE_URI_02\n",
    "\n",
    "print(f\"DOCKERNAME_02     = {DOCKERNAME_02}\")\n",
    "print(f\"REPOSITORY        = {REPOSITORY}\")\n",
    "print(f\"IMAGE_NAME_02     = {IMAGE_NAME_02}\")\n",
    "print(f\"REMOTE_IMAGE_NAME = {REMOTE_IMAGE_NAME}\")\n",
    "print(f\"IMAGE_URI_02      = {IMAGE_URI_02}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf953a8e-d282-4d7b-9a7d-fd0bee28c70a",
   "metadata": {},
   "source": [
    "## Create Artifact Repository\n",
    "\n",
    "If you don't have an existing artifact repository, create one using the gcloud command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8288932-054a-46e6-90e8-326e015f2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gcloud artifacts repositories create $REPOSITORY --repository-format=docker --location=$LOCATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1374bc-7b85-4e65-86ab-2721ce4908b3",
   "metadata": {},
   "source": [
    "## Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88a8b25d-e55c-4a00-ab0e-1151eae8eaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/tf_vertex_agents'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_path = '..'\n",
    "os.chdir(root_path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bffd9bb-6d22-4d4e-9eb8-893472702837",
   "metadata": {},
   "source": [
    "### Create train image\n",
    "\n",
    "* see [example Dockerfile for GPU](https://github.com/GoogleCloudPlatform/cloudml-samples/blob/main/pytorch/containers/quickstart/mnist/Dockerfile-gpu) jobs in Vertex AI\n",
    "* see deep learning container [example here](https://cloud.google.com/deep-learning-containers/docs/derivative-container), and here for [available DL containers](https://cloud.google.com/deep-learning-containers/docs/choosing-container#versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7022f8d1-43d0-4291-91b8-4cb27491c2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_profiling : True\n"
     ]
    }
   ],
   "source": [
    "gpu_profiling = True # True | False\n",
    "\n",
    "print(f\"gpu_profiling : {gpu_profiling}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4596864b-f059-438d-a04f-7416c436b644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_BASE_IMAGE : tensorflow/tensorflow:2.14.0-gpu\n",
      "NVTOP_RUN        : RUN apt update && apt -y install nvtop\n",
      "RUN_EXPORT       : RUN export PYTHONPATH=${PYTHONPATH}:${APP_HOME}/\n"
     ]
    }
   ],
   "source": [
    "# TRAIN_BASE_IMAGE = 'us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-11:latest'\n",
    "# docker pull tensorflow/tensorflow:2.13.0-gpu\n",
    "\n",
    "if gpu_profiling:\n",
    "    TRAIN_BASE_IMAGE = 'tensorflow/tensorflow:2.14.0-gpu'\n",
    "    # TRAIN_BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-gpu.2-13.py310'\n",
    "    NVTOP_RUN = 'RUN apt update && apt -y install nvtop'\n",
    "    # NVTOP_RUN = 'RUN apt-get update && apt-get -y install nvtop'\n",
    "else:\n",
    "    TRAIN_BASE_IMAGE = 'python:3.10'\n",
    "    NVTOP_RUN = None\n",
    "    \n",
    "RUN_EXPORT = \"RUN export PYTHONPATH=${PYTHONPATH}:${APP_HOME}/\"\n",
    "    \n",
    "print(f\"TRAIN_BASE_IMAGE : {TRAIN_BASE_IMAGE}\")\n",
    "print(f\"NVTOP_RUN        : {NVTOP_RUN}\")\n",
    "print(f\"RUN_EXPORT       : {RUN_EXPORT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b1b1fc6-022b-4e56-b62f-673179bab7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FROM tensorflow/tensorflow:2.14.0-gpu\n",
      "\n",
      "ENV PYTHONUNBUFFERED True\n",
      "\n",
      "ENV APP_HOME /workspace\n",
      "\n",
      "WORKDIR $APP_HOME\n",
      "\n",
      "COPY /requirements.txt $APP_HOME/requirements.txt\n",
      "\n",
      "RUN pip install --upgrade pip\n",
      "RUN pip install --no-cache-dir -r $APP_HOME/requirements.txt\n",
      "\n",
      "RUN ls $APP_HOME\n",
      "\n",
      "COPY src/perarm_features $APP_HOME/src/perarm_features\n",
      "COPY src/per_arm_rl $APP_HOME/src/per_arm_rl\n",
      "\n",
      "RUN apt update && apt -y install nvtop\n",
      "\n",
      "RUN ls $APP_HOME\n",
      "\n",
      "RUN export PYTHONPATH=${PYTHONPATH}:${APP_HOME}/\n",
      "\n",
      "# RUN pip freeze\n",
      "\n",
      "# Sets up the entry point to invoke the task.\n",
      "ENTRYPOINT [\"python3\", \"-m\", \"src.perarm_features.task\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dockerfile = f'''\n",
    "FROM {TRAIN_BASE_IMAGE}\n",
    "\n",
    "ENV PYTHONUNBUFFERED True\n",
    "\n",
    "ENV APP_HOME /workspace\n",
    "\n",
    "WORKDIR $APP_HOME\n",
    "\n",
    "COPY /requirements.txt $APP_HOME/requirements.txt\n",
    "\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r $APP_HOME/requirements.txt\n",
    "\n",
    "RUN ls $APP_HOME\n",
    "\n",
    "COPY src/perarm_features $APP_HOME/src/perarm_features\n",
    "COPY src/per_arm_rl $APP_HOME/src/per_arm_rl\n",
    "\n",
    "{NVTOP_RUN}\n",
    "\n",
    "RUN ls $APP_HOME\n",
    "\n",
    "{RUN_EXPORT}\n",
    "\n",
    "# RUN pip freeze\n",
    "\n",
    "# Sets up the entry point to invoke the task.\n",
    "ENTRYPOINT [\"python3\", \"-m\", \"src.perarm_features.task\"]\n",
    "'''\n",
    "print(dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cc6cb35-fcb6-4b4d-a665-b6c038ca6f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DOCKERNAME_02}', 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04bd12e-c61b-4a5c-b2cb-b7843642ce0e",
   "metadata": {},
   "source": [
    "## Build image with Cloud Build\n",
    "\n",
    "Building images with Cloud Build is best practices\n",
    "* images are centrally stored and better managed for robust CI/CD\n",
    "* building images on local workbench instance can alter notebook image config (base image for notebooks vs train images are different)\n",
    "* if building locally, consider using virutal environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb3ef36-2cfe-4199-963b-6c65bd0ec621",
   "metadata": {},
   "source": [
    "#### set `.gcloudignore`\n",
    "* to adjust this see the `gcloudignore` section at the end of `00-env-setup.ipynb` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "857c7a59-92ec-49c0-bb6f-cfd77ecbb378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .gcloudignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .gcloudignore\n",
    ".gcloudignore\n",
    "WIP/*\n",
    "imgs/*\n",
    "learning/*\n",
    "*.pkl\n",
    "*.png\n",
    "*.ipynb\n",
    ".git\n",
    ".github\n",
    ".gitignore\n",
    ".DS_Store\n",
    "*.md\n",
    "*.tfrecord\n",
    ".ipynb_checkpoints/*\n",
    "*cpython-37.pyc\n",
    "**.cpython-310.pyc\n",
    "*/__pycache__/*\n",
    "src/ranking/*\n",
    "src/archive/*\n",
    "04-pipelines/*\n",
    "03-ranking/*\n",
    "01-baseline-perarm-bandit/*\n",
    "src/pred/*\n",
    "src/serve/*\n",
    "Dockerfile_perarm_feats_tpu\n",
    "Dockerfile_predict_mab_02\n",
    "vertex_env/*\n",
    "credentials.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f9a8891-faa3-4659-a36e-521bf9f186e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile_perarm_feats\n",
      "requirements.txt\n",
      "Dockerfile_train_my_perarm_env\n",
      "pred_instances.json\n",
      "Dockerfile_predict_mab_02e\n",
      "cloudbuild.yaml\n",
      "src/per_arm_rl/utils_config.py\n",
      "src/per_arm_rl/perarm_task.py\n",
      "src/per_arm_rl/__init__.py\n",
      "src/per_arm_rl/my_per_arm_py_env.py\n",
      "src/per_arm_rl/policy_util.py\n",
      "src/per_arm_rl/train_utils.py\n",
      "src/per_arm_rl/trainer_baseline.py\n",
      "src/per_arm_rl/data_utils.py\n",
      "src/per_arm_rl/data_config.py\n",
      "src/perarm_features/train_perarm.py\n",
      "src/perarm_features/reward_factory.py\n",
      "src/perarm_features/emb_features.py\n",
      "src/perarm_features/agent_factory.py\n",
      "src/perarm_features/__init__.py\n",
      "src/perarm_features/trainer_common.py\n",
      "src/perarm_features/task.py\n",
      "src/perarm_features/eval_perarm.py\n",
      "src/perarm_features/ranking_bandit_policy.py\n"
     ]
    }
   ],
   "source": [
    "# check eligble files\n",
    "!gcloud meta list-files-for-upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84e545-d287-431b-89c0-ab48ab1b59d9",
   "metadata": {},
   "source": [
    "### Submit container to Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e3b8e87-3e4c-410c-a554-3be3b8ed3e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCKERNAME_02 : Dockerfile_perarm_feats\n",
      "IMAGE_URI_02  : gcr.io/hybrid-vertex/train-perarm-feats-v2\n",
      "FILE_LOCATION : .\n",
      "MACHINE_TYPE  : e2-highcpu-32\n"
     ]
    }
   ],
   "source": [
    "# image definitions for training\n",
    "MACHINE_TYPE            ='e2-highcpu-32'\n",
    "FILE_LOCATION           = \".\" # './src'\n",
    "\n",
    "print(f\"DOCKERNAME_02 : {DOCKERNAME_02}\")\n",
    "print(f\"IMAGE_URI_02  : {IMAGE_URI_02}\")\n",
    "print(f\"FILE_LOCATION : {FILE_LOCATION}\")\n",
    "print(f\"MACHINE_TYPE  : {MACHINE_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d11ac85-16f7-4050-8632-cb6e4337a65a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 24 file(s) totalling 219.3 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://hybrid-vertex_cloudbuild/source/1700047979.118267-a83271d6c66941369d0f68a8bd681dcb.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/global/builds/452c2c98-85a8-4f03-a3e6-be0be20a5269].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/452c2c98-85a8-4f03-a3e6-be0be20a5269?project=934903580331 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"452c2c98-85a8-4f03-a3e6-be0be20a5269\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1700047979.118267-a83271d6c66941369d0f68a8bd681dcb.tgz#1700047979616674\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1700047979.118267-a83271d6c66941369d0f68a8bd681dcb.tgz#1700047979616674...\n",
      "/ [1 files][ 50.5 KiB/ 50.5 KiB]                                                \n",
      "Operation completed over 1 objects/50.5 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon    257kB\n",
      "Step 1/14 : FROM tensorflow/tensorflow:2.14.0-gpu\n",
      "2.14.0-gpu: Pulling from tensorflow/tensorflow\n",
      "6b851dcae6ca: Pulling fs layer\n",
      "4586c00479c6: Pulling fs layer\n",
      "4304fa233a80: Pulling fs layer\n",
      "afa3f70b397f: Pulling fs layer\n",
      "d963a42bc712: Pulling fs layer\n",
      "01d85bc616ac: Pulling fs layer\n",
      "7d32cc707b9d: Pulling fs layer\n",
      "4cea12640dc8: Pulling fs layer\n",
      "076ff2aa313a: Pulling fs layer\n",
      "5a520d59fe5e: Pulling fs layer\n",
      "22d5103f16e8: Pulling fs layer\n",
      "74669e8faa15: Pulling fs layer\n",
      "d963a42bc712: Waiting\n",
      "afa3f70b397f: Waiting\n",
      "01d85bc616ac: Waiting\n",
      "076ff2aa313a: Waiting\n",
      "7d32cc707b9d: Waiting\n",
      "4cea12640dc8: Waiting\n",
      "5a520d59fe5e: Waiting\n",
      "22d5103f16e8: Waiting\n",
      "400aa4b0a962: Pulling fs layer\n",
      "81960391dc73: Pulling fs layer\n",
      "45d81655ae23: Pulling fs layer\n",
      "4db7c9771a19: Pulling fs layer\n",
      "91c6784a9bf6: Pulling fs layer\n",
      "2ef3e831f865: Pulling fs layer\n",
      "91c6784a9bf6: Waiting\n",
      "400aa4b0a962: Waiting\n",
      "81960391dc73: Waiting\n",
      "2ef3e831f865: Waiting\n",
      "74669e8faa15: Waiting\n",
      "45d81655ae23: Waiting\n",
      "4586c00479c6: Verifying Checksum\n",
      "4586c00479c6: Download complete\n",
      "afa3f70b397f: Verifying Checksum\n",
      "afa3f70b397f: Download complete\n",
      "d963a42bc712: Download complete\n",
      "01d85bc616ac: Verifying Checksum\n",
      "01d85bc616ac: Download complete\n",
      "7d32cc707b9d: Download complete\n",
      "4cea12640dc8: Verifying Checksum\n",
      "4cea12640dc8: Download complete\n",
      "6b851dcae6ca: Verifying Checksum\n",
      "6b851dcae6ca: Download complete\n",
      "076ff2aa313a: Verifying Checksum\n",
      "076ff2aa313a: Download complete\n",
      "4304fa233a80: Verifying Checksum\n",
      "4304fa233a80: Download complete\n",
      "22d5103f16e8: Verifying Checksum\n",
      "22d5103f16e8: Download complete\n",
      "74669e8faa15: Verifying Checksum\n",
      "74669e8faa15: Download complete\n",
      "400aa4b0a962: Verifying Checksum\n",
      "400aa4b0a962: Download complete\n",
      "45d81655ae23: Verifying Checksum\n",
      "45d81655ae23: Download complete\n",
      "4db7c9771a19: Download complete\n",
      "91c6784a9bf6: Verifying Checksum\n",
      "91c6784a9bf6: Download complete\n",
      "2ef3e831f865: Verifying Checksum\n",
      "2ef3e831f865: Download complete\n",
      "6b851dcae6ca: Pull complete\n",
      "4586c00479c6: Pull complete\n",
      "4304fa233a80: Pull complete\n",
      "afa3f70b397f: Pull complete\n",
      "d963a42bc712: Pull complete\n",
      "01d85bc616ac: Pull complete\n",
      "7d32cc707b9d: Pull complete\n",
      "4cea12640dc8: Pull complete\n",
      "076ff2aa313a: Pull complete\n",
      "81960391dc73: Verifying Checksum\n",
      "81960391dc73: Download complete\n",
      "5a520d59fe5e: Verifying Checksum\n",
      "5a520d59fe5e: Download complete\n",
      "5a520d59fe5e: Pull complete\n",
      "22d5103f16e8: Pull complete\n",
      "74669e8faa15: Pull complete\n",
      "400aa4b0a962: Pull complete\n",
      "81960391dc73: Pull complete\n",
      "45d81655ae23: Pull complete\n",
      "4db7c9771a19: Pull complete\n",
      "91c6784a9bf6: Pull complete\n",
      "2ef3e831f865: Pull complete\n",
      "Digest: sha256:64602abcd8cc4f4bdd6268ca0abc39e6d37113d700886afd15f6dd151210b206\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:2.14.0-gpu\n",
      " ---> 68d96d40670e\n",
      "Step 2/14 : ENV PYTHONUNBUFFERED True\n",
      " ---> Running in 1a017ee55f9e\n",
      "Removing intermediate container 1a017ee55f9e\n",
      " ---> 495d9190bbd1\n",
      "Step 3/14 : ENV APP_HOME /workspace\n",
      " ---> Running in a86aac9e3de6\n",
      "Removing intermediate container a86aac9e3de6\n",
      " ---> 90d7fefd75c6\n",
      "Step 4/14 : WORKDIR $APP_HOME\n",
      " ---> Running in 6d2f24a43ed9\n",
      "Removing intermediate container 6d2f24a43ed9\n",
      " ---> f215e88cf134\n",
      "Step 5/14 : COPY /requirements.txt $APP_HOME/requirements.txt\n",
      " ---> b5aef52ff7dd\n",
      "Step 6/14 : RUN pip install --upgrade pip\n",
      " ---> Running in e8bf2dc68f05\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (23.2.1)\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/47/6a/453160888fab7c6a432a6e25f8afe6256d0d9f2cbd25971021da6491d899/pip-23.3.1-py3-none-any.whl.metadata\n",
      "  Downloading pip-23.3.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Downloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 23.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.2.1\n",
      "    Uninstalling pip-23.2.1:\n",
      "      Successfully uninstalled pip-23.2.1\n",
      "Successfully installed pip-23.3.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container e8bf2dc68f05\n",
      " ---> f2d714adb7bf\n",
      "Step 7/14 : RUN pip install --no-cache-dir -r $APP_HOME/requirements.txt\n",
      " ---> Running in 286a282741b0\n",
      "Collecting google-cloud-aiplatform==1.33.1 (from -r /workspace/requirements.txt (line 1))\n",
      "  Downloading google_cloud_aiplatform-1.33.1-py2.py3-none-any.whl.metadata (27 kB)\n",
      "Collecting google-cloud-storage (from -r /workspace/requirements.txt (line 2))\n",
      "  Downloading google_cloud_storage-2.13.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r /workspace/requirements.txt (line 3)) (1.26.0)\n",
      "Collecting tf-agents==0.17.0 (from -r /workspace/requirements.txt (line 5))\n",
      "  Downloading tf_agents-0.17.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tensorflow-datasets (from -r /workspace/requirements.txt (line 6))\n",
      "  Downloading tensorflow_datasets-4.9.3-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting tensorflow-probability (from -r /workspace/requirements.txt (line 7))\n",
      "  Downloading tensorflow_probability-0.22.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from -r /workspace/requirements.txt (line 8)) (2.14.0)\n",
      "Collecting tensorboard-plugin-profile (from -r /workspace/requirements.txt (line 9))\n",
      "  Downloading tensorboard_plugin_profile-2.14.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting tensorboard-plugin-wit (from -r /workspace/requirements.txt (line 10))\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 30.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorboard-data-server in /usr/local/lib/python3.11/dist-packages (from -r /workspace/requirements.txt (line 11)) (0.7.1)\n",
      "Collecting tensorflow-io (from -r /workspace/requirements.txt (line 12))\n",
      "  Downloading tensorflow_io-0.34.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: tensorflow[and-cuda] in /usr/local/lib/python3.11/dist-packages (from -r /workspace/requirements.txt (line 4)) (2.14.0)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading google_api_core-2.14.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0 (from google-cloud-aiplatform==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading proto_plus-1.22.3-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform==1.33.1->-r /workspace/requirements.txt (line 1)) (4.24.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform==1.33.1->-r /workspace/requirements.txt (line 1)) (23.1)\n",
      "Collecting google-cloud-bigquery<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading google_cloud_bigquery-3.13.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading google_cloud_resource_manager-1.10.4-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting shapely<2.0.0 (from google-cloud-aiplatform==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading Shapely-1.8.5.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 65.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from tf-agents==0.17.0->-r /workspace/requirements.txt (line 5)) (2.0.0)\n",
      "Collecting cloudpickle>=1.3 (from tf-agents==0.17.0->-r /workspace/requirements.txt (line 5))\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gin-config>=0.4.0 (from tf-agents==0.17.0->-r /workspace/requirements.txt (line 5))\n",
      "  Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.3/61.3 kB 214.9 MB/s eta 0:00:00\n",
      "Collecting gym<=0.23.0,>=0.17.0 (from tf-agents==0.17.0->-r /workspace/requirements.txt (line 5))\n",
      "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 624.4/624.4 kB 131.4 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pillow (from tf-agents==0.17.0->-r /workspace/requirements.txt (line 5))\n",
      "  Downloading Pillow-10.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tf-agents==0.17.0->-r /workspace/requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from tf-agents==0.17.0->-r /workspace/requirements.txt (line 5)) (1.14.1)\n",
      "Collecting typing-extensions<4.6.0,>=3.7.4.3 (from tf-agents==0.17.0->-r /workspace/requirements.txt (line 5))\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting pygame==2.1.3 (from tf-agents==0.17.0->-r /workspace/requirements.txt (line 5))\n",
      "  Downloading pygame-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.7/13.7 MB 181.6 MB/s eta 0:00:00\n",
      "Collecting tensorflow-probability (from -r /workspace/requirements.txt (line 7))\n",
      "  Downloading tensorflow_probability-0.20.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting google-auth<3.0dev,>=2.23.3 (from google-cloud-storage->-r /workspace/requirements.txt (line 2))\n",
      "  Downloading google_auth-2.23.4-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0 (from google-cloud-storage->-r /workspace/requirements.txt (line 2))\n",
      "  Downloading google_cloud_core-2.3.3-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting google-resumable-media>=2.6.0 (from google-cloud-storage->-r /workspace/requirements.txt (line 2))\n",
      "  Downloading google_resumable_media-2.6.0-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->-r /workspace/requirements.txt (line 2)) (2.31.0)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage->-r /workspace/requirements.txt (line 2))\n",
      "  Downloading google_crc32c-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4)) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4)) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4)) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4)) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4)) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4)) (68.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4)) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4)) (1.58.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4)) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4)) (2.14.0)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.8.89 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 875.6/875.6 kB 249.1 MB/s eta 0:00:00\n",
      "Collecting nvidia-cublas-cu11==11.11.3.6 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 417.9/417.9 MB 223.3 MB/s eta 0:00:00\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.4/168.4 MB 221.0 MB/s eta 0:00:00\n",
      "Collecting nvidia-cudnn-cu11==8.7.0.84 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 728.5/728.5 MB 254.8 MB/s eta 0:00:00\n",
      "Collecting nvidia-curand-cu11==10.3.0.86 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.1/58.1 MB 223.6 MB/s eta 0:00:00\n",
      "Collecting nvidia-cusolver-cu11==11.4.1.48 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.2/128.2 MB 227.6 MB/s eta 0:00:00\n",
      "Collecting nvidia-cusparse-cu11==11.7.5.86 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 204.1/204.1 MB 203.6 MB/s eta 0:00:00\n",
      "Collecting nvidia-nccl-cu11==2.16.5 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_nccl_cu11-2.16.5-py3-none-manylinux1_x86_64.whl (210.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210.3/210.3 MB 227.9 MB/s eta 0:00:00\n",
      "Collecting nvidia-cuda-cupti-cu11==11.8.87 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 223.1 MB/s eta 0:00:00\n",
      "Collecting nvidia-cuda-nvcc-cu11==11.8.89 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cuda_nvcc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (19.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.5/19.5 MB 44.4 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of tensorflow[and-cuda] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow[and-cuda] (from -r /workspace/requirements.txt (line 4))\n",
      "  Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting tensorboard (from -r /workspace/requirements.txt (line 8))\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting nvidia-cublas-cu12==12.2.5.6 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cublas_cu12-12.2.5.6-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.2.142 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.2.142-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-nvcc-cu12==12.2.140 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cuda_nvcc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.2.140 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.2.140 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.4.25 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cudnn_cu12-8.9.4.25-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.8.103 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cufft_cu12-11.0.8.103-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.3.141 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_curand_cu12-10.3.3.141-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.5.2.141 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cusolver_cu12-11.5.2.141-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.2.141 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.2.141-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.16.5 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_nccl_cu12-2.16.5-py3-none-manylinux1_x86_64.whl (188.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 188.7/188.7 MB 139.4 MB/s eta 0:00:00\n",
      "Collecting nvidia-nvjitlink-cu12==12.2.140 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting tensorrt==8.6.1.post1 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading tensorrt-8.6.1.post1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tensorrt-bindings==8.6.1 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading tensorrt_bindings-8.6.1-cp311-none-manylinux_2_17_x86_64.whl (980 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 980.8/980.8 kB 255.5 MB/s eta 0:00:00\n",
      "Collecting tensorflow[and-cuda] (from -r /workspace/requirements.txt (line 4))\n",
      "  Downloading tensorflow-2.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "  Downloading tensorflow-2.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "\u001b[91mWARNING: tensorflow 2.13.1 does not provide the extra 'and-cuda'\n",
      "\u001b[0mCollecting gast<=0.4.0,>=0.2.1 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting numpy (from -r /workspace/requirements.txt (line 3))\n",
      "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 239.1 MB/s eta 0:00:00\n",
      "Collecting tensorboard (from -r /workspace/requirements.txt (line 8))\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 118.4 MB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting array-record (from tensorflow-datasets->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading array_record-0.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (503 bytes)\n",
      "Collecting click (from tensorflow-datasets->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting dm-tree (from tensorflow-datasets->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading dm_tree-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.8/152.8 kB 248.0 MB/s eta 0:00:00\n",
      "Collecting etils>=0.9.0 (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading etils-1.5.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting promise (from tensorflow-datasets->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting psutil (from tensorflow-datasets->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading psutil-5.9.6-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting toml (from tensorflow-datasets->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tqdm (from tensorflow-datasets->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 207.9 MB/s eta 0:00:00\n",
      "Collecting decorator (from tensorflow-probability->-r /workspace/requirements.txt (line 7))\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r /workspace/requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r /workspace/requirements.txt (line 8)) (3.4.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r /workspace/requirements.txt (line 8)) (2.3.7)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r /workspace/requirements.txt (line 8)) (0.41.2)\n",
      "Collecting gviz-api>=1.9.0 (from tensorboard-plugin-profile->-r /workspace/requirements.txt (line 9))\n",
      "  Downloading gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting fsspec (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting importlib_resources (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: zipp in /usr/lib/python3/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->-r /workspace/requirements.txt (line 6)) (1.0.0)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading grpcio_status-1.59.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage->-r /workspace/requirements.txt (line 2)) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage->-r /workspace/requirements.txt (line 2)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage->-r /workspace/requirements.txt (line 2)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r /workspace/requirements.txt (line 8)) (1.3.1)\n",
      "Collecting python-dateutil<3.0dev,>=2.7.2 (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 237.9 MB/s eta 0:00:00\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading grpc_google_iam_v1-0.12.7-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting gym-notices>=0.0.4 (from gym<=0.23.0,>=0.17.0->tf-agents==0.17.0->-r /workspace/requirements.txt (line 5))\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->-r /workspace/requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->-r /workspace/requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->-r /workspace/requirements.txt (line 2)) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->-r /workspace/requirements.txt (line 2)) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->-r /workspace/requirements.txt (line 8)) (2.1.3)\n",
      "Collecting absl-py>=0.6.1 (from tf-agents==0.17.0->-r /workspace/requirements.txt (line 5))\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 221.5 MB/s eta 0:00:00\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 (from google-cloud-aiplatform==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 162.1/162.1 kB 228.9 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading grpcio_status-1.59.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.58.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.57.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Downloading grpcio_status-1.56.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.56.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.55.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.54.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.54.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.54.0-py3-none-any.whl (5.1 kB)\n",
      "  Downloading grpcio_status-1.53.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.53.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.53.0-py3-none-any.whl (5.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading grpcio_status-1.51.3-py3-none-any.whl (5.1 kB)\n",
      "  Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
      "  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
      "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.33.1->-r /workspace/requirements.txt (line 1))\n",
      "  Downloading googleapis_common_protos-1.60.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading googleapis_common_protos-1.59.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 223.6/223.6 kB 247.2 MB/s eta 0:00:00\n",
      "  Downloading googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 223.0/223.0 kB 222.4 MB/s eta 0:00:00\n",
      "  Downloading googleapis_common_protos-1.57.1-py2.py3-none-any.whl (218 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 218.0/218.0 kB 232.6 MB/s eta 0:00:00\n",
      "  Downloading googleapis_common_protos-1.57.0-py2.py3-none-any.whl (217 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 218.0/218.0 kB 237.9 MB/s eta 0:00:00\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.7/211.7 kB 246.3 MB/s eta 0:00:00\n",
      "  Downloading googleapis_common_protos-1.56.3-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.8/211.8 kB 260.0 MB/s eta 0:00:00\n",
      "  Downloading googleapis_common_protos-1.56.2-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.8/211.8 kB 254.9 MB/s eta 0:00:00\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets->-r /workspace/requirements.txt (line 6))\n",
      "  Downloading tensorflow_metadata-1.13.1-py3-none-any.whl (28 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow[and-cuda]->-r /workspace/requirements.txt (line 4))\n",
      "  Downloading grpcio-1.59.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage->-r /workspace/requirements.txt (line 2)) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r /workspace/requirements.txt (line 8)) (3.2.0)\n",
      "Downloading google_cloud_aiplatform-1.33.1-py2.py3-none-any.whl (2.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 230.0 MB/s eta 0:00:00\n",
      "Downloading tf_agents-0.17.0-py3-none-any.whl (1.4 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 231.4 MB/s eta 0:00:00\n",
      "Downloading google_cloud_storage-2.13.0-py2.py3-none-any.whl (121 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.1/121.1 kB 218.6 MB/s eta 0:00:00\n",
      "Downloading tensorflow_datasets-4.9.3-py3-none-any.whl (5.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/5.0 MB 210.4 MB/s eta 0:00:00\n",
      "Downloading tensorflow_probability-0.20.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.9/6.9 MB 214.3 MB/s eta 0:00:00\n",
      "Downloading tensorboard_plugin_profile-2.14.0-py3-none-any.whl (5.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 210.9 MB/s eta 0:00:00\n",
      "Downloading tensorflow_io-0.34.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (28.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 28.8/28.8 MB 221.3 MB/s eta 0:00:00\n",
      "Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading etils-1.5.2-py3-none-any.whl (140 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 216.6 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.14.0-py3-none-any.whl (122 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.2/122.2 kB 227.3 MB/s eta 0:00:00\n",
      "Downloading google_auth-2.23.4-py2.py3-none-any.whl (183 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 183.3/183.3 kB 221.9 MB/s eta 0:00:00\n",
      "Downloading google_cloud_bigquery-3.13.0-py2.py3-none-any.whl (222 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 222.8/222.8 kB 241.4 MB/s eta 0:00:00\n",
      "Downloading google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_cloud_resource_manager-1.10.4-py2.py3-none-any.whl (320 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 321.0/321.0 kB 245.7 MB/s eta 0:00:00\n",
      "Downloading google_resumable_media-2.6.0-py2.py3-none-any.whl (80 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.3/80.3 kB 199.8 MB/s eta 0:00:00\n",
      "Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 245.3 MB/s eta 0:00:00\n",
      "Downloading proto_plus-1.22.3-py3-none-any.whl (48 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.1/48.1 kB 179.6 MB/s eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 440.8/440.8 kB 189.4 MB/s eta 0:00:00\n",
      "Downloading array_record-0.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 162.3 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 202.8 MB/s eta 0:00:00\n",
      "Downloading Pillow-10.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 216.5 MB/s eta 0:00:00\n",
      "Downloading psutil-5.9.6-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 283.6/283.6 kB 230.7 MB/s eta 0:00:00\n",
      "Downloading tensorflow-2.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.7 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 479.7/479.7 MB 141.2 MB/s eta 0:00:00\n",
      "Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 230.9/230.9 kB 238.3 MB/s eta 0:00:00\n",
      "Downloading grpcio_status-1.59.2-py3-none-any.whl (14 kB)\n",
      "Downloading grpcio-1.59.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 214.0 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 202.1 MB/s eta 0:00:00\n",
      "Downloading grpc_google_iam_v1-0.12.7-py2.py3-none-any.whl (26 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.4/166.4 kB 229.6 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Building wheels for collected packages: gym, promise\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697629 sha256=0ed8cc3a7f5628e8c7d7ae33ed7c5853a076627d1ebe4ceab72ab9a6c80acfad\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d9l9km5a/wheels/bf/19/ce/d2b762b6d61115bf0b4260ca59650ba2d55d49f34f61e095f6\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21484 sha256=46e3c4da87d17cd2fabf6762c50080b43b51ef7d2fe02db6be6b0c793e8a062a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d9l9km5a/wheels/90/74/b1/9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
      "Successfully built gym promise\n",
      "Installing collected packages: tensorboard-plugin-wit, gym-notices, gin-config, dm-tree, typing-extensions, tqdm, toml, tensorflow-io, tensorflow-estimator, shapely, python-dateutil, pygame, psutil, proto-plus, promise, pillow, numpy, keras, importlib_resources, gviz-api, grpcio, googleapis-common-protos, google-crc32c, gast, fsspec, etils, decorator, cloudpickle, click, absl-py, tensorflow-probability, tensorflow-metadata, tensorboard-plugin-profile, gym, grpcio-status, google-resumable-media, google-auth, tf-agents, grpc-google-iam-v1, google-api-core, tensorboard, google-cloud-core, array-record, tensorflow, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, tensorflow-datasets, google-cloud-aiplatform\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.14.0\n",
      "    Uninstalling tensorflow-estimator-2.14.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.0\n",
      "    Uninstalling numpy-1.26.0:\n",
      "      Successfully uninstalled numpy-1.26.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.14.0\n",
      "    Uninstalling keras-2.14.0:\n",
      "      Successfully uninstalled keras-2.14.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.58.0\n",
      "    Uninstalling grpcio-1.58.0:\n",
      "      Successfully uninstalled grpcio-1.58.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.5.4\n",
      "    Uninstalling gast-0.5.4:\n",
      "      Successfully uninstalled gast-0.5.4\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 2.0.0\n",
      "    Uninstalling absl-py-2.0.0:\n",
      "      Successfully uninstalled absl-py-2.0.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.23.1\n",
      "    Uninstalling google-auth-2.23.1:\n",
      "      Successfully uninstalled google-auth-2.23.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.14.0\n",
      "    Uninstalling tensorboard-2.14.0:\n",
      "      Successfully uninstalled tensorboard-2.14.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.14.0\n",
      "    Uninstalling tensorflow-2.14.0:\n",
      "      Successfully uninstalled tensorflow-2.14.0\n",
      "Successfully installed absl-py-1.4.0 array-record-0.5.0 click-8.1.7 cloudpickle-3.0.0 decorator-5.1.1 dm-tree-0.1.8 etils-1.5.2 fsspec-2023.10.0 gast-0.4.0 gin-config-0.5.0 google-api-core-2.14.0 google-auth-2.23.4 google-cloud-aiplatform-1.33.1 google-cloud-bigquery-3.13.0 google-cloud-core-2.3.3 google-cloud-resource-manager-1.10.4 google-cloud-storage-2.13.0 google-crc32c-1.5.0 google-resumable-media-2.6.0 googleapis-common-protos-1.61.0 grpc-google-iam-v1-0.12.7 grpcio-1.59.2 grpcio-status-1.59.2 gviz-api-1.10.0 gym-0.23.0 gym-notices-0.0.8 importlib_resources-6.1.1 keras-2.13.1 numpy-1.24.3 pillow-10.1.0 promise-2.3 proto-plus-1.22.3 psutil-5.9.6 pygame-2.1.3 python-dateutil-2.8.2 shapely-1.8.5.post1 tensorboard-2.13.0 tensorboard-plugin-profile-2.14.0 tensorboard-plugin-wit-1.8.1 tensorflow-2.13.1 tensorflow-datasets-4.9.3 tensorflow-estimator-2.13.0 tensorflow-io-0.34.0 tensorflow-metadata-1.13.1 tensorflow-probability-0.20.1 tf-agents-0.17.0 toml-0.10.2 tqdm-4.66.1 typing-extensions-4.5.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 286a282741b0\n",
      " ---> 0725924279df\n",
      "Step 8/14 : RUN ls $APP_HOME\n",
      " ---> Running in f1c57f5d7cd9\n",
      "requirements.txt\n",
      "Removing intermediate container f1c57f5d7cd9\n",
      " ---> 66144e40c814\n",
      "Step 9/14 : COPY src/perarm_features $APP_HOME/src/perarm_features\n",
      " ---> 4a5e3f1461e3\n",
      "Step 10/14 : COPY src/per_arm_rl $APP_HOME/src/per_arm_rl\n",
      " ---> 54ce57468556\n",
      "Step 11/14 : RUN apt update && apt -y install nvtop\n",
      " ---> Running in c908e2cd550d\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "\u001b[0mGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\n",
      "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [591 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.8 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1472 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1275 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1430 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [32.6 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [78.3 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1202 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1404 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.0 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1009 kB]\n",
      "Fetched 28.9 MB in 2s (15.3 MB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "31 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\u001b[0mReading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following NEW packages will be installed:\n",
      "  nvtop\n",
      "0 upgraded, 1 newly installed, 0 to remove and 31 not upgraded.\n",
      "Need to get 43.9 kB of archives.\n",
      "After this operation, 106 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 nvtop amd64 1.2.2-1 [43.9 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 43.9 kB in 0s (732 kB/s)\n",
      "Selecting previously unselected package nvtop.\n",
      "(Reading database ... 18629 files and directories currently installed.)\n",
      "Preparing to unpack .../nvtop_1.2.2-1_amd64.deb ...\n",
      "Unpacking nvtop (1.2.2-1) ...\n",
      "Setting up nvtop (1.2.2-1) ...\n",
      "Removing intermediate container c908e2cd550d\n",
      " ---> 40a7f1e788f2\n",
      "Step 12/14 : RUN ls $APP_HOME\n",
      " ---> Running in 0782d90e5d28\n",
      "requirements.txt\n",
      "src\n",
      "Removing intermediate container 0782d90e5d28\n",
      " ---> a04cedcd1093\n",
      "Step 13/14 : RUN export PYTHONPATH=${PYTHONPATH}:${APP_HOME}/\n",
      " ---> Running in d54f76d39d75\n",
      "Removing intermediate container d54f76d39d75\n",
      " ---> 59d7185824f8\n",
      "Step 14/14 : ENTRYPOINT [\"python3\", \"-m\", \"src.perarm_features.task\"]\n",
      " ---> Running in 81de63fefc84\n",
      "Removing intermediate container 81de63fefc84\n",
      " ---> 9787934c4e69\n",
      "Successfully built 9787934c4e69\n",
      "Successfully tagged gcr.io/hybrid-vertex/train-perarm-feats-v2:latest\n",
      "PUSH\n",
      "Pushing gcr.io/hybrid-vertex/train-perarm-feats-v2\n",
      "The push refers to repository [gcr.io/hybrid-vertex/train-perarm-feats-v2]\n",
      "60c1fb5aada0: Preparing\n",
      "485b22d50dc8: Preparing\n",
      "6cbc23eb3e74: Preparing\n",
      "364c5dcc51fa: Preparing\n",
      "f271d905b178: Preparing\n",
      "8e91eddcc6fd: Preparing\n",
      "f3eac7c5f3f1: Preparing\n",
      "9e8876090a5f: Preparing\n",
      "9968fc1a7ff0: Preparing\n",
      "a63802a5a170: Preparing\n",
      "8e043671cc8c: Preparing\n",
      "c1c72ea2a665: Preparing\n",
      "f3eac7c5f3f1: Waiting\n",
      "9e8876090a5f: Waiting\n",
      "9968fc1a7ff0: Waiting\n",
      "5ab24eb92ad6: Preparing\n",
      "a63802a5a170: Waiting\n",
      "8e043671cc8c: Waiting\n",
      "22a930fcca2c: Preparing\n",
      "c1c72ea2a665: Waiting\n",
      "63e7e3e18fca: Preparing\n",
      "26c9a2e38755: Preparing\n",
      "1702f50e3098: Preparing\n",
      "5ab24eb92ad6: Waiting\n",
      "22a930fcca2c: Waiting\n",
      "39473a3327fc: Preparing\n",
      "26c9a2e38755: Waiting\n",
      "89fe2705b695: Preparing\n",
      "1702f50e3098: Waiting\n",
      "8e91eddcc6fd: Waiting\n",
      "63e7e3e18fca: Waiting\n",
      "39473a3327fc: Waiting\n",
      "b6a3a29f541f: Preparing\n",
      "cda9215846ee: Preparing\n",
      "c5eafb4bee8f: Preparing\n",
      "81182eb0608d: Preparing\n",
      "b6a3a29f541f: Waiting\n",
      "f2baf76d88ee: Preparing\n",
      "cdd7c7392317: Preparing\n",
      "c5eafb4bee8f: Waiting\n",
      "f2baf76d88ee: Waiting\n",
      "81182eb0608d: Waiting\n",
      "cda9215846ee: Waiting\n",
      "6cbc23eb3e74: Pushed\n",
      "485b22d50dc8: Pushed\n",
      "f3eac7c5f3f1: Pushed\n",
      "9e8876090a5f: Layer already exists\n",
      "f271d905b178: Pushed\n",
      "8e91eddcc6fd: Pushed\n",
      "9968fc1a7ff0: Layer already exists\n",
      "a63802a5a170: Layer already exists\n",
      "c1c72ea2a665: Layer already exists\n",
      "8e043671cc8c: Layer already exists\n",
      "5ab24eb92ad6: Layer already exists\n",
      "22a930fcca2c: Layer already exists\n",
      "26c9a2e38755: Layer already exists\n",
      "1702f50e3098: Layer already exists\n",
      "39473a3327fc: Layer already exists\n",
      "63e7e3e18fca: Layer already exists\n",
      "89fe2705b695: Layer already exists\n",
      "b6a3a29f541f: Layer already exists\n",
      "81182eb0608d: Layer already exists\n",
      "cda9215846ee: Layer already exists\n",
      "c5eafb4bee8f: Layer already exists\n",
      "f2baf76d88ee: Layer already exists\n",
      "cdd7c7392317: Layer already exists\n",
      "60c1fb5aada0: Pushed\n",
      "364c5dcc51fa: Pushed\n",
      "latest: digest: sha256:bfdfca261feb56d6935985f25c99c40c1a88b13c3746a46ae6f38e155963f8ff size: 5553\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                                STATUS\n",
      "452c2c98-85a8-4f03-a3e6-be0be20a5269  2023-11-15T11:32:59+00:00  4M40S     gs://hybrid-vertex_cloudbuild/source/1700047979.118267-a83271d6c66941369d0f68a8bd681dcb.tgz  gcr.io/hybrid-vertex/train-perarm-feats-v2 (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --config ./cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME_02,_IMAGE_URI=$IMAGE_URI_02,_FILE_LOCATION=$FILE_LOCATION \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE \\\n",
    "    --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629eb246-f2fc-48a0-8478-4f9c99dddd0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## (Optional) Build Image Locally\n",
    "\n",
    "Building images with Cloud Build is best practices\n",
    "* images are centrally stored and better managed for robust CI/CD\n",
    "* building images on local workbench instance can alter notebook image config (base image for notebooks vs train images are different)\n",
    "* if building locally, consider using virutal environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5278eb39-1dbb-48fd-b872-9cd55643a1ed",
   "metadata": {},
   "source": [
    "Provide a name for your dockerfile and make sure you are authenticated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53874306-777f-459f-9250-4a59dbc283a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gcloud auth configure-docker $REGION-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25faef9c-cb5e-425d-9ee6-7ae8003a95af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy these commands into terminal:\n",
      "\n",
      "virtualenv vertex_env\n",
      "source vertex_env/bin/activate\n"
     ]
    }
   ],
   "source": [
    "print(\"copy these commands into terminal:\\n\")\n",
    "print(f\"virtualenv vertex_env\")\n",
    "print(f\"source vertex_env/bin/activate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "027de7fe-dd2a-4be5-80c4-877f7abe4753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy these commands into terminal:\n",
      "\n",
      "export REMOTE_IMAGE_NAME=us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-rec-bandits-v2/train-perarm-feats-v2\n",
      "export DOCKERNAME=Dockerfile_perarm_feats\n",
      "docker build -t $REMOTE_IMAGE_NAME -f ./$DOCKERNAME .\n"
     ]
    }
   ],
   "source": [
    "# # set variables if running in terminal\n",
    "print(\"copy these commands into terminal:\\n\")\n",
    "print(f\"export REMOTE_IMAGE_NAME={REMOTE_IMAGE_NAME}\")\n",
    "print(f\"export DOCKERNAME={DOCKERNAME}\")\n",
    "print(f\"docker build -t $REMOTE_IMAGE_NAME -f ./$DOCKERNAME .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a82cbe6-0ed5-478f-a38f-e6dedc114e2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !docker build -t $REMOTE_IMAGE_NAME -f $DOCKERNAME ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6110439-f66a-4d6b-8105-17a08ce9b8da",
   "metadata": {},
   "source": [
    "### Push container to Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54a714cf-053c-4f14-b375-2a42ae2092f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy this command into terminal:\n",
      "\n",
      "docker push $REMOTE_IMAGE_NAME\n"
     ]
    }
   ],
   "source": [
    "# ### push the container to registry\n",
    "\n",
    "print(\"copy this command into terminal:\\n\")\n",
    "print(f\"docker push $REMOTE_IMAGE_NAME\")\n",
    "\n",
    "# !docker push $REMOTE_IMAGE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bc85f6-a86e-48f2-a040-89f35376d57c",
   "metadata": {},
   "source": [
    "### GPU profiling\n",
    "\n",
    "> enter these commands in the Vertex interactive terminal:\n",
    "\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt -y install nvtop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18df8169-e4ed-45fb-9279-85e966bc6f3e",
   "metadata": {},
   "source": [
    "**Finished**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
