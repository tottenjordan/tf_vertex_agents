{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e406936-2fb6-4a23-a6dd-e51977dc5ee2",
   "metadata": {},
   "source": [
    "# Using GPUs & TPUs with TF-Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965670ad-987d-401e-aeca-f0492a52dbea",
   "metadata": {},
   "source": [
    "## Load notebook config\n",
    "\n",
    "* use the prefix defined in `00-env-setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b8d292-a86a-4c1c-9b22-244b0d28dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'mabv1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c137c648-e01c-4288-900c-608bd1f7bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"mabv1\"\n",
      "VERSION                  = \"v1\"\n",
      "\n",
      "BUCKET_NAME              = \"mabv1-hybrid-vertex-bucket\"\n",
      "BUCKET_URI               = \"gs://mabv1-hybrid-vertex-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://mabv1-hybrid-vertex-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_ID      = \"hybrid-vertex.movielens_dataset_mabv1\"\n",
      "BIGQUERY_TABLE_ID        = \"hybrid-vertex.movielens_dataset_mabv1.training_dataset\"\n",
      "\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "RL_SUB_DIR               = \"per_arm_rl\"\n",
      "\n",
      "REPOSITORY               = \"rl-movielens-mabv1\"\n",
      "IMAGE_NAME               = \"train-perarm-feats-v1\"\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-mabv1/train-perarm-feats-v1\"\n",
      "DOCKERNAME               = \"Dockerfile_perarm_feats\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20dda8-6c27-4690-9981-59c251dc602b",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc94ebe3-7952-4ed2-8b3d-e1764cb60c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37bc712f-c7d9-442a-ad92-9d865fa4faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pprint import pprint\n",
    "import pickle as pkl\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.metrics import tf_metrics\n",
    "\n",
    "from tf_agents.bandits.agents import neural_epsilon_greedy_agent\n",
    "from tf_agents.bandits.agents import neural_linucb_agent\n",
    "from tf_agents.bandits.networks import global_and_arm_feature_network\n",
    "\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.bandits.policies import policy_utilities\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "from tf_agents.train.utils import spec_utils\n",
    "from tf_agents.train.utils import strategy_utils\n",
    "from tf_agents.train.utils import train_utils\n",
    "\n",
    "# GPU\n",
    "from numba import cuda \n",
    "import gc\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# this repo\n",
    "from src.per_arm_rl import data_utils\n",
    "from src.per_arm_rl import data_config\n",
    "from src.per_arm_rl import train_utils as train_utils\n",
    "\n",
    "# tf exceptions and vars\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f884f232-2c14-43a9-b90e-ec80b25736d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.perarm_features import agent_factory as agent_factory\n",
    "from src.perarm_features import reward_factory as reward_factory\n",
    "from src.perarm_features import emb_features as emb_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03eb0137-aff7-43f3-929b-03568d4cf287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2522f6a-3826-4509-bd9b-f878c74ca8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "865d4b19-a078-44d3-b0bd-f407a16f6a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "vertex_ai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e75a40f-00ca-4bf9-93f8-063a1056ba48",
   "metadata": {},
   "source": [
    "### Generate Vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "960f5c54-cee3-4563-ba77-4062265a6447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATE_VOCABS: False\n"
     ]
    }
   ],
   "source": [
    "GENERATE_VOCABS = False\n",
    "\n",
    "print(f\"GENERATE_VOCABS: {GENERATE_VOCABS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa227f5c-364b-4488-b65f-617a3a80cf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vocab...\n",
      "Downloaded vocab from: gs://mabv1-hybrid-vertex-bucket/vocabs/vocab_dict.pkl\n",
      "\n",
      "'movie_id'\n",
      "'user_id'\n",
      "'user_occupation_text'\n",
      "'movie_genres'\n",
      "'bucketized_user_age'\n",
      "'max_timestamp'\n",
      "'min_timestamp'\n",
      "'timestamp_buckets'\n"
     ]
    }
   ],
   "source": [
    "if not GENERATE_VOCABS:\n",
    "\n",
    "    EXISTING_VOCAB_FILE = f'gs://{BUCKET_NAME}/{VOCAB_SUBDIR}/{VOCAB_FILENAME}'\n",
    "    print(f\"Downloading vocab...\")\n",
    "    \n",
    "    os.system(f'gsutil -q cp {EXISTING_VOCAB_FILE} .')\n",
    "    print(f\"Downloaded vocab from: {EXISTING_VOCAB_FILE}\\n\")\n",
    "\n",
    "    filehandler = open(VOCAB_FILENAME, 'rb')\n",
    "    VOCAB_DICT = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "    for key in VOCAB_DICT.keys():\n",
    "        pprint(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26da66-7284-4b5e-82cd-f261400726aa",
   "metadata": {},
   "source": [
    "### train config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e71482f-b262-46f7-9bd2-c492b7df5189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_OOV_BUCKETS        : 1\n",
      "GLOBAL_EMBEDDING_SIZE  : 16\n",
      "MV_EMBEDDING_SIZE      : 32\n",
      "GLOBAL_DIM             : 64\n",
      "PER_ARM_DIM            : 64\n",
      "BATCH_SIZE             : 128\n",
      "EVAL_BATCH_SIZE        : 1\n",
      "NUM_ACTIONS            : 2\n"
     ]
    }
   ],
   "source": [
    "NUM_OOV_BUCKETS        = 1\n",
    "GLOBAL_EMBEDDING_SIZE  = 16\n",
    "MV_EMBEDDING_SIZE      = 32 #32\n",
    "\n",
    "GLOBAL_DIM             = 64\n",
    "PER_ARM_DIM            = 64\n",
    "\n",
    "BATCH_SIZE             = 128\n",
    "EVAL_BATCH_SIZE        = 1\n",
    "NUM_ACTIONS            = 2 \n",
    "#this is kinda deceptive - \n",
    "#our approach is to learn by \"flashing\" one movie rating at a time per user context. \n",
    "#The n_actions = show/don't show the movie with one degree of freedom (n-1)\n",
    "\n",
    "print(f\"NUM_OOV_BUCKETS        : {NUM_OOV_BUCKETS}\")\n",
    "print(f\"GLOBAL_EMBEDDING_SIZE  : {GLOBAL_EMBEDDING_SIZE}\")\n",
    "print(f\"MV_EMBEDDING_SIZE      : {MV_EMBEDDING_SIZE}\")\n",
    "print(f\"GLOBAL_DIM             : {GLOBAL_DIM}\")\n",
    "print(f\"PER_ARM_DIM            : {PER_ARM_DIM}\")\n",
    "print(f\"BATCH_SIZE             : {BATCH_SIZE}\")\n",
    "print(f\"EVAL_BATCH_SIZE        : {EVAL_BATCH_SIZE}\")\n",
    "print(f\"NUM_ACTIONS            : {NUM_ACTIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e98fb-9f15-4220-a00a-9b501929d672",
   "metadata": {},
   "source": [
    "#### tmp - debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e947733-3785-48e0-ab99-690090df58e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options = tf.data.Options()\n",
    "# options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO\n",
    "\n",
    "SPLIT = \"train\" # \"train\" | \"val\"\n",
    "\n",
    "train_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{DATA_GCS_PREFIX}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "train_dataset = train_dataset.map(data_utils.parse_tfrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09f19f54-30e0-4e28-b49a-8e716e210bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bucketized_user_age': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([35.], dtype=float32)>,\n",
       " 'movie_genres': <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[7]])>,\n",
       " 'movie_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'898'], dtype=object)>,\n",
       " 'timestamp': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([885409515])>,\n",
       " 'user_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'681'], dtype=object)>,\n",
       " 'user_occupation_text': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'marketing'], dtype=object)>,\n",
       " 'user_rating': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([4.], dtype=float32)>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    \n",
    "    iterator = iter(train_dataset.batch(1))\n",
    "    data = next(iterator)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "165119f6-8f93-4188-a027-64dd0323f2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.perarm_features.emb_features.EmbeddingModel at 0x7f87504f5840>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = emb_features.EmbeddingModel(\n",
    "    vocab_dict = VOCAB_DICT,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3911012-decb-43e8-b8f9-1c3d83f8a106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL_DIM: 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-5.8112741e-03, -2.7347589e-02,  4.3792393e-02,  1.6522121e-02,\n",
       "         7.0177317e-03,  1.2044527e-02,  2.9648174e-02,  4.8587535e-02,\n",
       "        -3.7824489e-02, -4.6437230e-02,  2.8784622e-02,  4.9463283e-02,\n",
       "        -2.0462085e-02, -1.3792682e-02,  2.5525723e-02, -2.0910427e-04,\n",
       "         1.5089873e-02, -1.1984646e-02,  2.0747852e-02,  1.3852607e-02,\n",
       "         3.8168337e-02, -1.8995032e-03, -2.2122730e-02, -7.0257783e-03,\n",
       "        -2.9075587e-02, -3.5006009e-02, -2.0313764e-02,  1.2704741e-02,\n",
       "        -3.1890251e-02, -3.3981062e-02,  4.4725649e-03,  4.5412529e-02,\n",
       "         4.1759815e-02, -3.5454381e-02,  1.4163878e-02,  2.1573972e-02,\n",
       "        -9.1575086e-05,  3.8136732e-02, -4.3634884e-03, -7.1371421e-03,\n",
       "        -8.2083941e-03,  3.8899425e-02,  2.0087805e-02,  2.1784905e-02,\n",
       "        -4.7824811e-02,  3.1043414e-02,  2.6928950e-02, -1.3213765e-02,\n",
       "        -2.6299417e-02,  1.1380233e-02,  4.6162233e-03,  2.5053646e-02,\n",
       "         4.4929121e-02,  3.6789332e-02, -4.3007877e-02, -2.0222520e-02,\n",
       "         4.6729255e-02,  3.6006000e-02,  2.2680555e-02,  4.3875542e-02,\n",
       "         1.4299665e-02,  4.9477305e-02,  4.0914360e-02,  4.3258857e-02]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_globals = embs._get_global_context_features(data)\n",
    "\n",
    "GLOBAL_DIM = test_globals.shape[1]            \n",
    "# shape checks out at batch_dim, nactions, arm feats\n",
    "print(f\"GLOBAL_DIM: {GLOBAL_DIM}\")\n",
    "\n",
    "test_globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a47b5c29-ecdf-458f-8068-5279e0fee08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER_ARM_DIM: 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.02741807,  0.00556612, -0.01999872, -0.02548009,  0.00022346,\n",
       "        -0.03077864, -0.02162802, -0.01771561,  0.0005706 ,  0.02783409,\n",
       "         0.01844798, -0.01719248,  0.04455631, -0.01407462, -0.0392247 ,\n",
       "         0.04200652,  0.022199  ,  0.00384817, -0.0049236 , -0.04400627,\n",
       "         0.04262852, -0.0420866 , -0.01662021, -0.04701102, -0.03161716,\n",
       "         0.00522899,  0.04285392, -0.04825796,  0.02212382,  0.04059419,\n",
       "         0.02113916, -0.04131491, -0.00903697, -0.0389972 ,  0.04078387,\n",
       "        -0.03191327, -0.00114865,  0.02111926, -0.00052362,  0.04829225,\n",
       "        -0.04404092,  0.04542711, -0.02823715,  0.04736667, -0.03265704,\n",
       "        -0.04854608, -0.01960797, -0.03714675, -0.02089479,  0.04189124,\n",
       "        -0.04564349, -0.02927328, -0.00450555, -0.0470851 , -0.03183492,\n",
       "         0.01130641, -0.03140491, -0.00212942,  0.04605046,  0.04059507,\n",
       "         0.01767178, -0.03736533, -0.00224836, -0.01994274]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arms = embs._get_per_arm_features(data)\n",
    "\n",
    "PER_ARM_DIM = test_arms.shape[1]            \n",
    "# shape checks out at batch_dim, nactions, arm feats\n",
    "print(f\"PER_ARM_DIM: {PER_ARM_DIM}\")\n",
    "\n",
    "test_arms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d59ad1-e850-4bb3-ae4a-94efcbb89c9b",
   "metadata": {},
   "source": [
    "### TensorSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f03d7f8d-a168-4e47-92f1-5242fc723599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(64,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_spec = {\n",
    "    'global': tf.TensorSpec([GLOBAL_DIM], tf.float32),\n",
    "    'per_arm': tf.TensorSpec([NUM_ACTIONS, PER_ARM_DIM], tf.float32) #excluding action dim here\n",
    "}\n",
    "observation_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b927e6a2-d01c-4e9b-866f-9afaba5bb795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=[], \n",
    "    dtype=tf.int32,\n",
    "    minimum=tf.constant(0),            \n",
    "    maximum=NUM_ACTIONS-1, # n degrees of freedom and will dictate the expected mean reward spec shape\n",
    "    name=\"action_spec\"\n",
    ")\n",
    "\n",
    "action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd146ddd-0bf6-47c8-baf9-a81633909211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': {'global': TensorSpec(shape=(64,), dtype=tf.float32, name=None),\n",
       "                 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)},\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec = ts.time_step_spec(\n",
    "    observation_spec = observation_spec, \n",
    "    # reward_spec = _reward_spec\n",
    ")\n",
    "time_step_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f73646-b69e-4f1a-960b-055602fd36fa",
   "metadata": {},
   "source": [
    "## Distribution strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a2cd7-5484-4db2-b7ac-a57a61a452f5",
   "metadata": {},
   "source": [
    "Use `strategy_utils` to generate a strategy. Under the hood, passing the parameter:\n",
    "\n",
    "* `use_gpu = False` returns `tf.distribute.get_strategy()`, which uses CPU\n",
    "* `use_gpu = True` returns `tf.distribute.MirroredStrategy()`, which uses all GPUs that are visible to TensorFlow on one machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aedc5c8-788e-443b-994b-3c4021a273a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy at 0x7f880780d660>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_gpu = True\n",
    "use_tpu = False\n",
    "\n",
    "distribution_strategy = strategy_utils.get_strategy(tpu=use_tpu, use_gpu=use_gpu)\n",
    "distribution_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afc5e2da-e5f0-4056-9e07-4797834ddfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution_strategy = tf.distribute.MirroredStrategy()\n",
    "# distribution_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ccdd846-9373-402d-a9d3-7c9fe90b5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution_strategy.cluster_resolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ff56f57-7dfa-45fd-9c65-dcfe62b0c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86791ffd-ae3c-4558-8a08-335abb242a9c",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d8ff74e-fcdd-4523-838d-68e399bbb39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128,\n",
      " 'common_layers': [16, 8],\n",
      " 'epsilon': 0.01,\n",
      " 'eval_batch_size': 1,\n",
      " 'global_layers': [64, 32, 16],\n",
      " 'learning_rate': 0.05,\n",
      " 'model_type': 'epsGreedy',\n",
      " 'network_type': 'commontower',\n",
      " 'num_actions': 2,\n",
      " 'num_eval_steps': 10000,\n",
      " 'per_arm_layers': [64, 32, 16]}\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Agents\n",
    "# ================================\n",
    "AGENT_TYPE      = 'epsGreedy' # 'LinUCB' | 'LinTS |, 'epsGreedy' | 'NeuralLinUCB'\n",
    "\n",
    "# Parameters for linear agents (LinUCB and LinTS).\n",
    "AGENT_ALPHA     = 0.1\n",
    "\n",
    "# Parameters for neural agents (NeuralEpsGreedy and NerualLinUCB).\n",
    "EPSILON         = 0.01\n",
    "LR              = 0.05\n",
    "\n",
    "# Parameters for NeuralLinUCB\n",
    "ENCODING_DIM    = 1\n",
    "EPS_PHASE_STEPS = 1000\n",
    "\n",
    "NUM_EVAL_STEPS = 10000\n",
    "\n",
    "# ================================\n",
    "# Agent's Preprocess Network\n",
    "# ================================\n",
    "NETWORK_TYPE    = \"commontower\" # 'commontower' | 'dotproduct'\n",
    "\n",
    "if AGENT_TYPE == 'NeuralLinUCB':\n",
    "    NETWORK_TYPE = 'commontower'\n",
    "    \n",
    "GLOBAL_LAYERS   = [64, 32, 16]\n",
    "ARM_LAYERS      = [64, 32, 16]\n",
    "COMMON_LAYERS   = [16, 8]\n",
    "\n",
    "HPARAMS = {  # TODO - streamline and consolidate\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"eval_batch_size\" : EVAL_BATCH_SIZE,\n",
    "    \"num_actions\": NUM_ACTIONS,\n",
    "    \"model_type\": AGENT_TYPE,\n",
    "    \"network_type\": NETWORK_TYPE,\n",
    "    \"global_layers\": GLOBAL_LAYERS,\n",
    "    \"per_arm_layers\": ARM_LAYERS,\n",
    "    \"common_layers\": COMMON_LAYERS,\n",
    "    \"learning_rate\": LR,\n",
    "    \"epsilon\": EPSILON,\n",
    "    \"num_eval_steps\": NUM_EVAL_STEPS,\n",
    "}\n",
    "pprint(HPARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c27bf4-7d1d-485c-9644-4c0d60d77a83",
   "metadata": {},
   "source": [
    "### trajectory function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5338c598-1c22-409b-8f67-b4a906d579ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _trajectory_fn(element): # hparams\n",
    "    \n",
    "    \"\"\"Converts a dataset element into a trajectory.\"\"\"\n",
    "    # global_features = _get_global_context_features(element)\n",
    "    # arm_features = _get_per_arm_features(element)\n",
    "    \n",
    "    global_features = embs._get_global_context_features(element)\n",
    "    arm_features = embs._get_per_arm_features(element)\n",
    "    \n",
    "    # Adds a time dimension.\n",
    "    arm_features = train_utils._add_outer_dimension(arm_features)\n",
    "\n",
    "    # obs spec\n",
    "    observation = {\n",
    "        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n",
    "            train_utils._add_outer_dimension(global_features),\n",
    "    }\n",
    "\n",
    "    reward = train_utils._add_outer_dimension(reward_factory._get_rewards(element))\n",
    "\n",
    "    # To emit the predicted rewards in policy_info, we need to create dummy\n",
    "    # rewards to match the definition in TensorSpec for the ones specified in\n",
    "    # emit_policy_info set.\n",
    "    dummy_rewards = tf.zeros([HPARAMS['batch_size'], 1, HPARAMS['num_actions']])\n",
    "    policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "        chosen_arm_features=arm_features,\n",
    "        # Pass dummy mean rewards here to match the model_spec for emitting\n",
    "        # mean rewards in policy info\n",
    "        predicted_rewards_mean=dummy_rewards,\n",
    "        bandit_policy_type=policy_utilities.BanditPolicyType.GREEDY\n",
    "    )\n",
    "    \n",
    "    if HPARAMS['model_type'] == 'neural_ucb':\n",
    "        policy_info = policy_info._replace(\n",
    "            predicted_rewards_optimistic=dummy_rewards\n",
    "        )\n",
    "        \n",
    "    return trajectory.single_step(\n",
    "        observation=observation,\n",
    "        action=tf.zeros_like(\n",
    "            reward, dtype=tf.int32\n",
    "        ),  # Arm features are copied from policy info, put dummy zeros here\n",
    "        policy_info=policy_info,\n",
    "        reward=reward,\n",
    "        discount=tf.zeros_like(reward)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f8e95-eede-4b0e-8dd2-90bfc0ba18c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### tmp - debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d589417-c0f0-4e1b-96d9-f572d3eea5b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for x in train_dataset.batch(HPARAMS['batch_size']).take(1):\n",
    "#     test_traj_v1 = _trajectory_fn(x)\n",
    "    \n",
    "# test_traj_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df6c2218-ebef-4bb6-822c-9a67ee9b8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"test_traj_v1.action.shape: {test_traj_v1.action.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c19523ab-42ac-485b-aa39-167515a4d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"test_traj_v1.discount.shape: {test_traj_v1.discount.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86f184d9-49c4-419f-8b48-f8cbbca30172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"test_traj_v1.observation.shape: {test_traj_v1.observation['global'].shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd89853c-935c-4e7e-8d03-9c6479be3155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"test_traj_v1.reward.shape: {test_traj_v1.reward.shape}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a634d31-306f-445c-8d34-e431ee5839a2",
   "metadata": {},
   "source": [
    "### Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb73c9d5-1b50-40ef-8a3c-eb724fe81f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.profiler.experimental.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cd7b0e8-93aa-4846-b532-7b62dee81f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: NeuralEpsGreedyAgent\n",
      "NETWORK_TYPE: commontower\n"
     ]
    }
   ],
   "source": [
    "with distribution_strategy.scope():\n",
    "    \n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "    \n",
    "    agent = agent_factory.PerArmAgentFactory._get_agent(\n",
    "        agent_type = AGENT_TYPE,\n",
    "        network_type = NETWORK_TYPE,\n",
    "        time_step_spec = time_step_spec,\n",
    "        action_spec = action_spec,\n",
    "        observation_spec=observation_spec,\n",
    "        global_layers = GLOBAL_LAYERS,\n",
    "        arm_layers = ARM_LAYERS,\n",
    "        common_layers = COMMON_LAYERS,\n",
    "        agent_alpha = AGENT_ALPHA,\n",
    "        learning_rate = LR,\n",
    "        epsilon = EPSILON,\n",
    "        train_step_counter = global_step,\n",
    "        output_dim = ENCODING_DIM,\n",
    "        eps_phase_steps = EPS_PHASE_STEPS,\n",
    "    )\n",
    "    \n",
    "agent.initialize()\n",
    "\n",
    "print(f\"Agent: {agent.name}\")\n",
    "\n",
    "if NETWORK_TYPE:\n",
    "    print(f\"NETWORK_TYPE: {NETWORK_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55bf86b5-945e-46e7-b6e5-30e64e44870b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MirroredVariable:{\n",
       "  0: <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=0>\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0fa43-5336-4904-ac3d-d04c35139512",
   "metadata": {},
   "source": [
    "#### tmp - debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92319db1-692f-403c-ac51-88f0bab440d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b953988-2fbf-4913-96dd-d10f3379ae35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TupleWrapper(TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': DictWrapper({'global': TensorSpec(shape=(64,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(2, 64), dtype=tf.float32, name=None)}),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.time_step_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72297b66-27c0-45ad-b2e3-06a13e2653d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TupleWrapper(Trajectory(\n",
       "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32)),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': DictWrapper({'global': TensorSpec(shape=(64,), dtype=tf.float32, name=None)}),\n",
       " 'policy_info': PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorSpec(shape=(2,), dtype=tf.float32, name=None), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=(), chosen_arm_features=TensorSpec(shape=(64,), dtype=tf.float32, name=None)),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.training_data_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293aef61-3336-4041-befa-27cadbd756fa",
   "metadata": {},
   "source": [
    "## Vertex Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6037dde7-e446-40b3-8c9d-5913b6139147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : acc-paf-v2\n",
      "RUN_NAME          : run-20230825-104310\n",
      "\n",
      "BASE_OUTPUT_DIR   : gs://mabv1-hybrid-vertex-bucket/acc-paf-v2/run-20230825-104310\n",
      "LOG_DIR           : gs://mabv1-hybrid-vertex-bucket/acc-paf-v2/run-20230825-104310/logs\n",
      "ROOT_DIR          : gs://mabv1-hybrid-vertex-bucket/acc-paf-v2/run-20230825-104310/root\n",
      "ARTIFACTS_DIR     : gs://mabv1-hybrid-vertex-bucket/acc-paf-v2/run-20230825-104310/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'acc-paf-v2'\n",
    "\n",
    "# new experiment\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "BASE_OUTPUT_DIR   = f'{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "LOG_DIR           = f\"{BASE_OUTPUT_DIR}/logs\"\n",
    "ROOT_DIR          = f\"{BASE_OUTPUT_DIR}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BASE_OUTPUT_DIR}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    experiment=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\\n\")\n",
    "print(f\"BASE_OUTPUT_DIR   : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "033135a5-0fd2-421a-a172-5a15e1e28dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# TB summary writer\n",
    "# ====================================================\n",
    "with distribution_strategy.scope():\n",
    "    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        f\"{LOG_DIR}/train\", flush_millis=10 * 1000\n",
    "    )\n",
    "\n",
    "    train_summary_writer.set_as_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2b102e-35c9-4ff3-8ae0-6b15103c27de",
   "metadata": {},
   "source": [
    "## Train loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e68792ed-9b90-47aa-80d8-d190c4580079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_ITER_STEPS       : 100\n",
      "STEPS_PER_LOOP       : 1\n",
      "LOG_INTERVAL         : 10\n",
      "CHKPT_INTERVAL       : 200\n",
      "NUM_EVAL_STEPS       : 100\n",
      "ASYNC_STEPS_PER_LOOP : 1\n"
     ]
    }
   ],
   "source": [
    "from src.perarm_features import train_perarm as train_perarm\n",
    "\n",
    "NUM_ITER_STEPS       = 100\n",
    "STEPS_PER_LOOP       = 1\n",
    "LOG_INTERVAL         = 10\n",
    "CHKPT_INTERVAL       = 200\n",
    "NUM_EVAL_STEPS       = 100\n",
    "ASYNC_STEPS_PER_LOOP = 1\n",
    "\n",
    "print(f\"NUM_ITER_STEPS       : {NUM_ITER_STEPS}\")       #  = 50\n",
    "print(f\"STEPS_PER_LOOP       : {STEPS_PER_LOOP}\")       #  = 1\n",
    "print(f\"LOG_INTERVAL         : {LOG_INTERVAL}\")         #  = 10\n",
    "print(f\"CHKPT_INTERVAL       : {CHKPT_INTERVAL}\")       #  = 200\n",
    "print(f\"NUM_EVAL_STEPS       : {NUM_EVAL_STEPS}\")       #  = 100\n",
    "print(f\"ASYNC_STEPS_PER_LOOP : {ASYNC_STEPS_PER_LOOP}\") # = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef52a68d-d601-4d64-8365-9df5998fae86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_TRAIN_TAKE = NUM_ITER_STEPS * HPARAMS['batch_size']\n",
    "TOTAL_TRAIN_TAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c56c7092-b4cb-420c-9b7c-6f1410a740de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution_strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f87105b0d60>\n",
      "train_files: ['gs://mabv1-hybrid-vertex-bucket/data/train/ml-ratings-100k-train.tfrecord']\n",
      "train_ds_iterator: <tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f871051d960>\n",
      "setting checkpoint_manager: gs://mabv1-hybrid-vertex-bucket/acc-paf-v2/run-20230825-104310/root/chkpoint\n",
      "wrapping agent.train in tf-function\n",
      "starting_loop: 0\n",
      "starting train loop...\n",
      "step = 0: loss = 15.920000076293945\n",
      "step = 10: loss = 11.210000038146973\n",
      "step = 20: loss = 8.430000305175781\n",
      "step = 30: loss = 2.4100000858306885\n",
      "step = 40: loss = 1.0800000429153442\n",
      "step = 50: loss = 1.4900000095367432\n",
      "step = 60: loss = 1.6399999856948853\n",
      "step = 70: loss = 1.4199999570846558\n",
      "step = 80: loss = 1.350000023841858\n",
      "step = 90: loss = 1.2699999809265137\n",
      "runtime_mins: 0\n",
      "saved trained policy to: gs://mabv1-hybrid-vertex-bucket/acc-paf-v2/run-20230825-104310/artifacts\n",
      "complete train job in 1 minutes\n"
     ]
    }
   ],
   "source": [
    "#start the timer and training\n",
    "start_time = time.time()\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "metric_results, agent = train_perarm.train_perarm(\n",
    "    agent = agent,\n",
    "    global_dim = GLOBAL_DIM,\n",
    "    per_arm_dim = PER_ARM_DIM,\n",
    "    num_iterations = NUM_ITER_STEPS,\n",
    "    steps_per_loop = STEPS_PER_LOOP,\n",
    "    num_eval_steps = NUM_EVAL_STEPS,\n",
    "    # data\n",
    "    batch_size = HPARAMS['batch_size'],\n",
    "    eval_batch_size = HPARAMS['eval_batch_size'],\n",
    "    # functions\n",
    "    _trajectory_fn = _trajectory_fn,\n",
    "    # _run_bandit_eval_fn = _run_bandit_eval,\n",
    "    # train intervals\n",
    "    chkpt_interval = CHKPT_INTERVAL,\n",
    "    log_interval = LOG_INTERVAL,\n",
    "    # dirs\n",
    "    bucket_name = BUCKET_NAME,\n",
    "    data_dir_prefix_path = DATA_GCS_PREFIX,\n",
    "    log_dir = LOG_DIR,\n",
    "    model_dir = ARTIFACTS_DIR,\n",
    "    root_dir = ROOT_DIR,\n",
    "    async_steps_per_loop = ASYNC_STEPS_PER_LOOP,\n",
    "    resume_training_loops = False,\n",
    "    use_gpu = True,\n",
    "    use_tpu = False,\n",
    "    profiler = True,\n",
    "    global_step = global_step,\n",
    "    total_train_take = TOTAL_TRAIN_TAKE, # TODO - remove?\n",
    "    train_summary_writer = train_summary_writer,\n",
    "    # additional_metrics = metrics,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "runtime_mins = int((end_time - start_time) / 60)\n",
    "print(f\"complete train job in {runtime_mins} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f923259-8cca-4a7b-9e4e-8ec9b543f34a",
   "metadata": {},
   "source": [
    "### GPU profiling\n",
    "\n",
    "> once training job begins, enter these commands in the Vertex interactive terminal:\n",
    "\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt -y install nvtop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba9a3a-bb07-4619-88f0-34cb08fbdad1",
   "metadata": {},
   "source": [
    "#### Load TensorBoard\n",
    "\n",
    "> on the right-hand side, find `PROFILE` in the drop down:\n",
    "\n",
    "<img src=\"imgs/getting_profiler.png\" \n",
    "     align=\"center\" \n",
    "     width=\"850\"\n",
    "     height=\"850\"/>\n",
    "     \n",
    "<!-- tf_vertex_agents/imgs/getting_profiler.png -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1b61b02-8ca0-4d1c-9bf8-4914715611d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://mabv1-hybrid-vertex-bucket/acc-paf-v2/run-20230825-104310/logs'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae8de567-c152-4deb-99dc-54ed840c9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9ba7b2d-3ee7-478a-b634-c809a895103d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e8c595f3d5bec9ca\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e8c595f3d5bec9ca\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=$LOG_DIR "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d8a19f-c56e-4176-b29f-b62c1a3979e8",
   "metadata": {},
   "source": [
    "**Finished**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
