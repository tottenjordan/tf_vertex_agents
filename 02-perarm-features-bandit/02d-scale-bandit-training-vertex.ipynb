{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4699e32-3938-4c72-9eb7-4fe36bea46a0",
   "metadata": {},
   "source": [
    "# Scaling bandit training with Vertex AI \n",
    "\n",
    "**prerequisites:**\n",
    "* build training image in `04b-build-training-image` noteook\n",
    "\n",
    "**Recommendation**\n",
    "\n",
    "When profiling a train job, we don't need to do a full train. \n",
    "\n",
    "> We just need to get multiple iterations of going through the entire Agent graph (i.e., from data iterator --> agent.train a few times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b47b011f-cc14-44d7-9ccd-4d6fc2aadd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# protobuf==4.24.4\n",
    "# protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "725d9fa8-ad43-49b4-8bf5-75fda5e337fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiplatform SDK version: 1.33.1\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28447656-866e-4403-92ae-e2b3700a71bb",
   "metadata": {},
   "source": [
    "## setup notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38e75f3e-73b9-4f14-8aff-08d3e4ea849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/tf_vertex_agents/02-perarm-features-bandit\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6546a-ec55-46eb-a2fe-c4e8b6a5ad9d",
   "metadata": {},
   "source": [
    "### Load env config\n",
    "* use the prefix from `00-env-setup` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c928177-d72f-4715-a149-91c9a0a9f7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX: rec-bandits-v2\n"
     ]
    }
   ],
   "source": [
    "# PREFIX = 'mabv1'\n",
    "VERSION        = \"v2\"                       # TODO\n",
    "PREFIX         = f'rec-bandits-{VERSION}'   # TODO\n",
    "\n",
    "print(f\"PREFIX: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314139a3-e896-4f55-acd7-9daac475e98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"rec-bandits-v2\"\n",
      "VERSION                  = \"v2\"\n",
      "\n",
      "BUCKET_NAME              = \"rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "BUCKET_URI               = \"gs://rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "DATA_PATH_KFP_DEMO       = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data/kfp_demo_data/u.data\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_NAME    = \"mvlens_rec_bandits_v2\"\n",
      "BIGQUERY_TABLE_NAME      = \"training_dataset\"\n",
      "\n",
      "REPOSITORY               = \"rl-movielens-rec-bandits-v2\"\n",
      "\n",
      "DOCKERNAME_01            = \"Dockerfile_train_my_perarm_env\"\n",
      "IMAGE_NAME_01            = \"train-my-perarm-env-v2\"\n",
      "IMAGE_URI_01             = \"gcr.io/hybrid-vertex/train-my-perarm-env-v2\"\n",
      "\n",
      "DOCKERNAME_02            = \"Dockerfile_perarm_feats\"\n",
      "IMAGE_NAME_02            = \"train-perarm-feats-v2\"\n",
      "IMAGE_URI_02             = \"gcr.io/hybrid-vertex/train-perarm-feats-v2\"\n",
      "\n",
      "DOCKERNAME_03            = \"Dockerfile_ranking_bandit\"\n",
      "IMAGE_NAME_03            = \"train-rank-bandit-v2\"\n",
      "IMAGE_URI_03             = \"gcr.io/hybrid-vertex/train-rank-bandit-v2\"\n",
      "\n",
      "DOCKERNAME_04            = \"Dockerfile_train_mab_e2e\"\n",
      "IMAGE_NAME_04            = \"train-mab-e2e-v2\"\n",
      "IMAGE_URI_04             = \"gcr.io/hybrid-vertex/train-mab-e2e-v2\"\n",
      "\n",
      "DOCKERNAME_04_pred       = \"Dockerfile_pred_mab_e2e\"\n",
      "IMAGE_NAME_04_pred       = \"pred-mab-e2e-v2\"\n",
      "IMAGE_URI_04_pred        = \"gcr.io/hybrid-vertex/pred-mab-e2e-v2\"\n",
      "\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-rec-bandits-v2/local_docker_tfa\"\n",
      "\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "RL_SUB_DIR               = \"per_arm_rl\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4a5c39-b56c-4422-9124-c58cbb9951dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil iam ch serviceAccount:{VERTEX_SA}:roles/storage.objects.get $BUCKET_URI\n",
    "# ! gsutil iam ch serviceAccount:{VERTEX_SA}:roles/storage.objects.get $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce57c0ef-7e44-4c0b-afb2-adb1021c6e0e",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9f1333-417a-4d12-b28d-c5183b5b4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2490c9ee-9a23-4903-a659-228268a31081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "import pickle as pkl\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "786af747-66b9-46ab-aef5-2f3dfe8cc564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.per_arm_rl import data_utils\n",
    "from src.per_arm_rl import train_utils\n",
    "from src.per_arm_rl import data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a53e33e-c682-43db-9de6-240721914a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "vertex_ai.init(project=PROJECT_ID,location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ec42ad-e318-4e9c-b1fe-4f875c9e3574",
   "metadata": {},
   "source": [
    "# Vertex Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d534c68-7ab2-4e15-bdf4-220aab5a6531",
   "metadata": {},
   "source": [
    "## job compute\n",
    "\n",
    "Set the variable `TRAIN_COMPUTE` to configure the compute resources for the VMs you will use for for training.\n",
    "\n",
    "**Machine Type:**\n",
    "* `n1-standard`: 3.75GB of memory per vCPU.\n",
    "* `n1-highmem`: 6.5GB of memory per vCPU\n",
    "* `n1-highcpu`: 0.9 GB of memory per vCPU\n",
    "* `vCPUs`: number of `[2, 4, 8, 16, 32, 64, 96 ]`\n",
    "\n",
    "**Note:** The following is not supported for training:\n",
    "\n",
    "* `standard`: 2 vCPUs\n",
    "* `highcpu`: 2, 4 and 8 vCPUs\n",
    "\n",
    "> Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs.\n",
    "\n",
    "relevant docs: \n",
    "* [Configure compute resources for training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types) for more details\n",
    "* [Machine series comparison](https://cloud.google.com/compute/docs/machine-resource#machine_type_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0700c7d-4c38-491d-90a5-7cf052753d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCELERATOR: t4\n"
     ]
    }
   ],
   "source": [
    "ACCELERATOR = \"t4\" # str: \"a100\" | \"t4\" | None | l4\n",
    "ACCELERATOR = str(ACCELERATOR)\n",
    "print(f\"ACCELERATOR: {ACCELERATOR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd78debb-2363-4ed6-a8cf-7da7ae44a050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKER_MACHINE_TYPE            : n1-highcpu-16\n",
      "REPLICA_COUNT                  : 1\n",
      "ACCELERATOR_TYPE               : NVIDIA_TESLA_T4\n",
      "PER_MACHINE_ACCELERATOR_COUNT  : 1\n",
      "DISTRIBUTE_STRATEGY            : single\n",
      "REDUCTION_SERVER_COUNT         : 0\n",
      "REDUCTION_SERVER_MACHINE_TYPE  : n1-highcpu-16\n",
      "TF_GPU_THREAD_COUNT            : 4\n"
     ]
    }
   ],
   "source": [
    "if ACCELERATOR == \"a100\":\n",
    "    WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "    REPLICA_COUNT = 1\n",
    "    ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "    PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "    REDUCTION_SERVER_COUNT = 0                                                      \n",
    "    REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "    DISTRIBUTE_STRATEGY = 'single'\n",
    "elif ACCELERATOR == 't4':\n",
    "    WORKER_MACHINE_TYPE = 'n1-highcpu-16'\n",
    "    REPLICA_COUNT = 1\n",
    "    ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4'\n",
    "    PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "    DISTRIBUTE_STRATEGY = 'single'\n",
    "    REDUCTION_SERVER_COUNT = 0                                                      \n",
    "    REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "elif ACCELERATOR == 'l4':\n",
    "    WORKER_MACHINE_TYPE = \"g2-standard-16\"\n",
    "    REPLICA_COUNT = 1\n",
    "    ACCELERATOR_TYPE = 'NVIDIA_L4'\n",
    "    PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "    DISTRIBUTE_STRATEGY = 'single'\n",
    "    REDUCTION_SERVER_COUNT = 0                                                      \n",
    "    REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "elif ACCELERATOR == 'tpu':\n",
    "    WORKER_MACHINE_TYPE = \"cloud-tpu\"\n",
    "    REPLICA_COUNT = 1\n",
    "    ACCELERATOR_TYPE = 'TPU_v3'\n",
    "    PER_MACHINE_ACCELERATOR_COUNT = 8 # 8 | +32+ for TPU Pods\n",
    "    DISTRIBUTE_STRATEGY = 'single'\n",
    "    REDUCTION_SERVER_COUNT = 0                                                      \n",
    "    REDUCTION_SERVER_MACHINE_TYPE = None\n",
    "elif ACCELERATOR == \"False\":\n",
    "    WORKER_MACHINE_TYPE = 'n2-highmem-32' # 'n1-highmem-96'n | 'n2-highmem-92'\n",
    "    REPLICA_COUNT = 1\n",
    "    ACCELERATOR_TYPE = None\n",
    "    PER_MACHINE_ACCELERATOR_COUNT = 0\n",
    "    DISTRIBUTE_STRATEGY = 'single'\n",
    "    REDUCTION_SERVER_COUNT = 0                                                      \n",
    "    REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "    \n",
    "TF_GPU_THREAD_COUNT   = '4'      # '1' | '4' | '8'\n",
    "\n",
    "print(f\"WORKER_MACHINE_TYPE            : {WORKER_MACHINE_TYPE}\")\n",
    "print(f\"REPLICA_COUNT                  : {REPLICA_COUNT}\")\n",
    "print(f\"ACCELERATOR_TYPE               : {ACCELERATOR_TYPE}\")\n",
    "print(f\"PER_MACHINE_ACCELERATOR_COUNT  : {PER_MACHINE_ACCELERATOR_COUNT}\")\n",
    "print(f\"DISTRIBUTE_STRATEGY            : {DISTRIBUTE_STRATEGY}\")\n",
    "print(f\"REDUCTION_SERVER_COUNT         : {REDUCTION_SERVER_COUNT}\")\n",
    "print(f\"REDUCTION_SERVER_MACHINE_TYPE  : {REDUCTION_SERVER_MACHINE_TYPE}\")\n",
    "print(f\"TF_GPU_THREAD_COUNT            : {TF_GPU_THREAD_COUNT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1e4a8-4c1a-47ab-9962-f8ec1eb414bc",
   "metadata": {},
   "source": [
    "## set Vertex AI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e0358f36-d5d7-4752-b2ac-3bccdf54edfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : 02-scale-compare-v4\n",
      "RUN_NAME          : run-20231115-125019\n",
      "\n",
      "BASE_OUTPUT_DIR   : gs://rec-bandits-v2-hybrid-vertex-bucket/02-scale-compare-v4/run-20231115-125019\n",
      "LOG_DIR           : gs://rec-bandits-v2-hybrid-vertex-bucket/02-scale-compare-v4/run-20231115-125019/logs\n",
      "ROOT_DIR          : gs://rec-bandits-v2-hybrid-vertex-bucket/02-scale-compare-v4/run-20231115-125019/root\n",
      "ARTIFACTS_DIR     : gs://rec-bandits-v2-hybrid-vertex-bucket/02-scale-compare-v4/run-20231115-125019/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'02-scale-compare-v4'\n",
    "\n",
    "# new experiment\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "BASE_OUTPUT_DIR   = f'{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "LOG_DIR           = f\"{BASE_OUTPUT_DIR}/logs\"\n",
    "ROOT_DIR          = f\"{BASE_OUTPUT_DIR}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BASE_OUTPUT_DIR}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    experiment=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\\n\")\n",
    "print(f\"BASE_OUTPUT_DIR   : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611995c1-d451-4f97-8b87-f89551497590",
   "metadata": {},
   "source": [
    "## Create Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9aeed88-2bba-4832-abaf-f0f6d9b71e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME: projects/934903580331/locations/us-central1/tensorboards/1713342581284274176\n",
      "TB display name: 02-scale-compare-v4-run-20231115-125019\n"
     ]
    }
   ],
   "source": [
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}-{RUN_NAME}\"\n",
    "\n",
    "tensorboard = vertex_ai.Tensorboard.create(\n",
    "    display_name=TENSORBOARD_DISPLAY_NAME\n",
    "    , project=PROJECT_ID\n",
    "    , location=REGION\n",
    ")\n",
    "\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "# use existing TB instance\n",
    "# TB_RESOURCE_NAME = 'projects/934903580331/locations/us-central1/tensorboards/6924469145035603968'\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name: {tensorboard.display_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b8ed9-69f1-4aec-9dd8-c8995a265bbb",
   "metadata": {},
   "source": [
    "## Set training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9eae7385-dfff-4e3e-af00-c20fa6a05496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE_URI_02 : gcr.io/hybrid-vertex/train-perarm-feats-v2\n"
     ]
    }
   ],
   "source": [
    "# print(f\"REMOTE_IMAGE_NAME : {REMOTE_IMAGE_NAME}\")  # docker (local build)\n",
    "print(f\"IMAGE_URI_02 : {IMAGE_URI_02}\")                    # cloud build image uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd13dcc4-ab99-4e7c-9028-a7cd93b77a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SUBDIR           : vocabs\n",
      "VOCAB_FILENAME         : vocab_dict.pkl\n",
      "BATCH_SIZE             : 128\n",
      "TRAINING_LOOPS         : 100\n",
      "STEPS_PER_LOOP         : 1\n",
      "ASYNC_STEPS_PER_LOOP   : 1\n",
      "LOG_INTERVAL           : 10\n",
      "RANK_K                 : 10\n",
      "NUM_ACTIONS            : 2\n",
      "PER_ARM                : True\n",
      "AGENT_TYPE             : LinUCB\n",
      "NETWORK_TYPE           : commontower\n",
      "TIKHONOV_WEIGHT        : 0.001\n",
      "AGENT_ALPHA            : 0.1\n",
      "GLOBAL_DIM             : 64\n",
      "PER_ARM_DIM            : 64\n",
      "SPLIT                  : train\n",
      "RESUME_TRAINING        : None\n",
      "NUM_OOV_BUCKETS        : 1\n",
      "GLOBAL_EMBEDDING_SIZE  : 16\n",
      "MV_EMBEDDING_SIZE      : 32\n",
      "AGENT_ALPHA            : 0.1\n",
      "GLOBAL_LAYERS          : [64, 32, 16]\n",
      "ARM_LAYERS             : [64, 32, 16]\n",
      "COMMON_LAYERS          : [16, 8]\n",
      "LR                     : 0.05\n",
      "CHKPT_INTERVAL         : 1000\n",
      "EVAL_BATCH_SIZE        : 1\n",
      "NUM_EVAL_STEPS         : 2000\n",
      "EPSILON                : 0.01\n",
      "ENCODING_DIM           : 1\n",
      "EPS_PHASE_STEPS        : 1000\n"
     ]
    }
   ],
   "source": [
    "# vocab\n",
    "# VOCAB_SUBDIR         = \"vocabs\"\n",
    "# VOCAB_FILENAME       = \"vocab_dict.pkl\"\n",
    "\n",
    "# Set hyperparameters.\n",
    "BATCH_SIZE           = 128          # Training and prediction batch size.\n",
    "TRAINING_LOOPS       = 100          # Number of training iterations.\n",
    "STEPS_PER_LOOP       = 1            # Number of driver steps per training iteration.\n",
    "ASYNC_STEPS_PER_LOOP = 1\n",
    "LOG_INTERVAL         = 10\n",
    "LR                   = 0.05\n",
    "\n",
    "CHKPT_INTERVAL       = 1000\n",
    "EVAL_BATCH_SIZE      = 1  \n",
    "NUM_EVAL_STEPS       = 2000 #10000\n",
    "\n",
    "# Set MovieLens simulation environment parameters.\n",
    "RANK_K               = 10      # Rank for matrix factorization in the MovieLens environment; also the observation dimension.\n",
    "NUM_ACTIONS          = 2       # Number of actions (movie items) to choose from.\n",
    "PER_ARM              = True    # Use the non-per-arm version of the MovieLens environment.\n",
    "\n",
    "# ================================\n",
    "# Agent\n",
    "# ================================\n",
    "AGENT_TYPE          = 'LinUCB' # 'LinUCB' | 'LinTS |, 'epsGreedy' | 'NeuralLinUCB'\n",
    "NETWORK_TYPE        = \"commontower\" # 'commontower' | 'dotproduct'\n",
    "\n",
    "TIKHONOV_WEIGHT     = 0.001   # LinUCB Tikhonov regularization weight.\n",
    "AGENT_ALPHA         = 0.1     # LinUCB exploration parameter that multiplies the confidence intervals.\n",
    "EPSILON             = 0.01\n",
    "ENCODING_DIM        = 1\n",
    "EPS_PHASE_STEPS     = 1000\n",
    "\n",
    "# ================================\n",
    "# network params\n",
    "# ================================\n",
    "GLOBAL_LAYERS       = [64, 32, 16]\n",
    "ARM_LAYERS          = [64, 32, 16]\n",
    "COMMON_LAYERS       = [16, 8]\n",
    "\n",
    "# GLOBAL_LAYERS       = [128, 64, 32]\n",
    "# ARM_LAYERS          = [128, 64, 32]\n",
    "# COMMON_LAYERS       = [32, 16, 8]\n",
    "\n",
    "if AGENT_TYPE == 'NeuralLinUCB':\n",
    "    NETWORK_TYPE = 'commontower'\n",
    "    ENCODING_DIM = COMMON_LAYERS[-1]\n",
    "\n",
    "# ================================\n",
    "# data config\n",
    "# ================================\n",
    "GLOBAL_DIM             = 64       # 16\n",
    "PER_ARM_DIM            = 64       # 16\n",
    "NUM_OOV_BUCKETS        = 1\n",
    "GLOBAL_EMBEDDING_SIZE  = 16\n",
    "MV_EMBEDDING_SIZE      = 32       # 32\n",
    "SPLIT                  = \"train\"  # TODO - remove\n",
    "RESUME_TRAINING        = None\n",
    "\n",
    "print(f\"VOCAB_SUBDIR           : {VOCAB_SUBDIR}\")\n",
    "print(f\"VOCAB_FILENAME         : {VOCAB_FILENAME}\")\n",
    "print(f\"BATCH_SIZE             : {BATCH_SIZE}\")\n",
    "print(f\"TRAINING_LOOPS         : {TRAINING_LOOPS}\")\n",
    "print(f\"STEPS_PER_LOOP         : {STEPS_PER_LOOP}\")\n",
    "print(f\"ASYNC_STEPS_PER_LOOP   : {ASYNC_STEPS_PER_LOOP}\")\n",
    "print(f\"LOG_INTERVAL           : {LOG_INTERVAL}\")\n",
    "print(f\"RANK_K                 : {RANK_K}\")\n",
    "print(f\"NUM_ACTIONS            : {NUM_ACTIONS}\")\n",
    "print(f\"PER_ARM                : {PER_ARM}\")\n",
    "print(f\"AGENT_TYPE             : {AGENT_TYPE}\")\n",
    "print(f\"NETWORK_TYPE           : {NETWORK_TYPE}\")\n",
    "print(f\"TIKHONOV_WEIGHT        : {TIKHONOV_WEIGHT}\")\n",
    "print(f\"AGENT_ALPHA            : {AGENT_ALPHA}\")\n",
    "print(f\"GLOBAL_DIM             : {GLOBAL_DIM}\")\n",
    "print(f\"PER_ARM_DIM            : {PER_ARM_DIM}\")\n",
    "print(f\"SPLIT                  : {SPLIT}\")\n",
    "print(f\"RESUME_TRAINING        : {RESUME_TRAINING}\")\n",
    "print(f\"NUM_OOV_BUCKETS        : {NUM_OOV_BUCKETS}\")\n",
    "print(f\"GLOBAL_EMBEDDING_SIZE  : {GLOBAL_EMBEDDING_SIZE}\")\n",
    "print(f\"MV_EMBEDDING_SIZE      : {MV_EMBEDDING_SIZE}\")\n",
    "print(f\"AGENT_ALPHA            : {AGENT_ALPHA}\")\n",
    "print(f\"GLOBAL_LAYERS          : {GLOBAL_LAYERS}\")\n",
    "print(f\"ARM_LAYERS             : {ARM_LAYERS}\")\n",
    "print(f\"COMMON_LAYERS          : {COMMON_LAYERS}\")\n",
    "print(f\"LR                     : {LR}\")\n",
    "print(f\"CHKPT_INTERVAL         : {CHKPT_INTERVAL}\")\n",
    "print(f\"EVAL_BATCH_SIZE        : {EVAL_BATCH_SIZE}\")\n",
    "print(f\"NUM_EVAL_STEPS         : {NUM_EVAL_STEPS}\")\n",
    "print(f\"EPSILON                : {EPSILON}\")\n",
    "print(f\"ENCODING_DIM           : {ENCODING_DIM}\")\n",
    "print(f\"EPS_PHASE_STEPS        : {EPS_PHASE_STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4411e60-067d-4e29-92ff-12dbbc46726f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--project=hybrid-vertex',\n",
      "                              '--project_number=934903580331',\n",
      "                              '--bucket_name=rec-bandits-v2-hybrid-vertex-bucket',\n",
      "                              '--artifacts_dir=gs://rec-bandits-v2-hybrid-vertex-bucket/02-scale-compare-v4/run-20231115-125019/artifacts',\n",
      "                              '--root_dir=gs://rec-bandits-v2-hybrid-vertex-bucket/02-scale-compare-v4/run-20231115-125019/root',\n",
      "                              '--log_dir=gs://rec-bandits-v2-hybrid-vertex-bucket/02-scale-compare-v4/run-20231115-125019/logs',\n",
      "                              '--data_dir_prefix_path=data',\n",
      "                              '--vocab_prefix_path=vocabs',\n",
      "                              '--vocab_filename=vocab_dict.pkl',\n",
      "                              '--distribute=single',\n",
      "                              '--experiment_name=02-scale-compare-v4',\n",
      "                              '--experiment_run=run-20231115-125019',\n",
      "                              '--agent_type=LinUCB',\n",
      "                              '--network_type=commontower',\n",
      "                              '--batch_size=128',\n",
      "                              '--eval_batch_size=1',\n",
      "                              '--training_loops=100',\n",
      "                              '--steps_per_loop=1',\n",
      "                              '--num_eval_steps=2000',\n",
      "                              '--rank_k=10',\n",
      "                              '--num_actions=2',\n",
      "                              '--async_steps_per_loop=1',\n",
      "                              '--global_dim=64',\n",
      "                              '--per_arm_dim=64',\n",
      "                              '--split=train',\n",
      "                              '--log_interval=10',\n",
      "                              '--chkpt_interval=1000',\n",
      "                              '--num_oov_buckets=1',\n",
      "                              '--global_emb_size=16',\n",
      "                              '--mv_emb_size=32',\n",
      "                              '--agent_alpha=0.1',\n",
      "                              '--global_layers=[64, 32, 16]',\n",
      "                              '--arm_layers=[64, 32, 16]',\n",
      "                              '--common_layers=[16, 8]',\n",
      "                              '--learning_rate=0.05',\n",
      "                              '--epsilon=0.01',\n",
      "                              '--encoding_dim=1',\n",
      "                              '--eps_phase_steps=1000',\n",
      "                              '--tf_gpu_thread_count=4',\n",
      "                              '--use_gpu',\n",
      "                              '--debug_summaries'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/train-perarm-feats-v2:latest'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_T4',\n",
      "                   'machine_type': 'n1-highcpu-16'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "WORKER_ARGS = [\n",
    "    f\"--project={PROJECT_ID}\"\n",
    "    , f\"--project_number={PROJECT_NUM}\"\n",
    "    , f\"--bucket_name={BUCKET_NAME}\"\n",
    "    , f\"--artifacts_dir={ARTIFACTS_DIR}\"\n",
    "    , f\"--root_dir={ROOT_DIR}\"\n",
    "    , f\"--log_dir={LOG_DIR}\"\n",
    "    , f\"--data_dir_prefix_path={DATA_GCS_PREFIX}\"\n",
    "    , f\"--vocab_prefix_path={VOCAB_SUBDIR}\"\n",
    "    , f\"--vocab_filename={VOCAB_FILENAME}\"\n",
    "    ### job config\n",
    "    , f\"--distribute={DISTRIBUTE_STRATEGY}\"\n",
    "    , f\"--experiment_name={EXPERIMENT_NAME}\"\n",
    "    , f\"--experiment_run={RUN_NAME}\"\n",
    "    , f\"--agent_type={AGENT_TYPE}\"\n",
    "    , f\"--network_type={NETWORK_TYPE}\"\n",
    "    ### hparams\n",
    "    , f\"--batch_size={BATCH_SIZE}\"\n",
    "    , f\"--eval_batch_size={EVAL_BATCH_SIZE}\"\n",
    "    , f\"--training_loops={TRAINING_LOOPS}\"\n",
    "    , f\"--steps_per_loop={STEPS_PER_LOOP}\"\n",
    "    , f\"--num_eval_steps={NUM_EVAL_STEPS}\"\n",
    "    , f\"--rank_k={RANK_K}\"\n",
    "    , f\"--num_actions={NUM_ACTIONS}\"\n",
    "    , f\"--async_steps_per_loop={ASYNC_STEPS_PER_LOOP}\"\n",
    "    # , f\"--resume_training_loops\"\n",
    "    , f\"--global_dim={GLOBAL_DIM}\"\n",
    "    , f\"--per_arm_dim={PER_ARM_DIM}\"\n",
    "    , f\"--split={SPLIT}\"\n",
    "    , f\"--log_interval={LOG_INTERVAL}\"\n",
    "    , f\"--chkpt_interval={CHKPT_INTERVAL}\"\n",
    "    , f\"--num_oov_buckets={NUM_OOV_BUCKETS}\"\n",
    "    , f\"--global_emb_size={GLOBAL_EMBEDDING_SIZE}\"\n",
    "    , f\"--mv_emb_size={MV_EMBEDDING_SIZE}\"\n",
    "    , f\"--agent_alpha={AGENT_ALPHA}\"\n",
    "    , f\"--global_layers={GLOBAL_LAYERS}\"\n",
    "    , f\"--arm_layers={ARM_LAYERS}\"\n",
    "    , f\"--common_layers={COMMON_LAYERS}\"\n",
    "    , f\"--learning_rate={LR}\"\n",
    "    , f\"--epsilon={EPSILON}\"\n",
    "    , f\"--encoding_dim={ENCODING_DIM}\"\n",
    "    , f\"--eps_phase_steps={EPS_PHASE_STEPS}\"\n",
    "    , f\"--tf_gpu_thread_count={TF_GPU_THREAD_COUNT}\"\n",
    "    ### accelerators & profiling\n",
    "    , f\"--use_gpu\"\n",
    "    # , f\"--use_tpu\"\n",
    "    # , f\"--profiler\"\n",
    "    # , f\"--sum_grads_vars\"\n",
    "    , f\"--debug_summaries\"\n",
    "    # , f\"--cache_train\"\n",
    "]\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.per_arm_rl import train_utils\n",
    "\n",
    "WORKER_POOL_SPECS = train_utils.prepare_worker_pool_specs(\n",
    "    # image_uri=f\"{REMOTE_IMAGE_NAME}:latest\",\n",
    "    image_uri=f\"{IMAGE_URI_02}:latest\",\n",
    "    args=WORKER_ARGS,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e582ba0-71f0-4b9d-a5ff-edb7b43dc046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea381beb-3d3b-4595-bd69-b8aeda060ebf",
   "metadata": {},
   "source": [
    "# Submit trainging job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f42d2a93-7043-4433-8d0f-3c15c704419c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME: paf-bandit-run-20231115-125019\n"
     ]
    }
   ],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID\n",
    "    , location=REGION\n",
    "    , experiment=EXPERIMENT_NAME\n",
    "    # , staging_bucket=ROOT_DIR\n",
    ")\n",
    "\n",
    "JOB_NAME = f\"paf-bandit-{RUN_NAME}\"\n",
    "print(f\"JOB_NAME: {JOB_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd25c64d-de18-439d-96f6-5d8d166f4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CustomJob\n",
    "my_custom_job = vertex_ai.CustomJob(\n",
    "    display_name=JOB_NAME\n",
    "    , project=PROJECT_ID\n",
    "    , worker_pool_specs=WORKER_POOL_SPECS\n",
    "    , base_output_dir=BASE_OUTPUT_DIR\n",
    "    , staging_bucket=ROOT_DIR\n",
    "    # , location=\"asia-southeast1\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db98ac56-e829-4f47-a21f-95fdc3e21fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_custom_job.run(\n",
    "    tensorboard=TB_RESOURCE_NAME,\n",
    "    service_account=VERTEX_SA,\n",
    "    restart_job_on_worker_restart=False,\n",
    "    enable_web_access=True,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06cd97c1-9682-4bd8-a1c8-da27593cb4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: paf-bandit-run-20231115-104146\n",
      "Job Resource Name: projects/934903580331/locations/us-central1/customJobs/4921246713999523840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job Name: {my_custom_job.display_name}\")\n",
    "print(f\"Job Resource Name: {my_custom_job.resource_name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa63c92f-0a19-4184-b7c6-ea96d389712d",
   "metadata": {},
   "source": [
    "### Get link to Vertex AI Experiment console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98ec7046-1708-40f8-a589-886e96f726de",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df = vertex_ai.get_experiment_df()\n",
    "experiment_df = experiment_df[experiment_df.experiment_name == EXPERIMENT_NAME]\n",
    "experiment_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021ea34f-05c0-492f-baa5-ce6335d3e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Open the following link\", experiment_df[\"metric.lineage\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ff7630-5337-474c-9d28-fcdfab74f791",
   "metadata": {},
   "source": [
    "### GPU profiling\n",
    "\n",
    "> once training job begins, enter these commands in the Vertex interactive terminal:\n",
    "\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt -y install nvtop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6514a309-ca15-4ca3-a32d-857adb749d4e",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbef7bc-0077-48ce-9be0-85e4cf881523",
   "metadata": {},
   "source": [
    "### in-notebook TensorBoard\n",
    "\n",
    "> if `--profiler`, find `PROFILE` in the drop down:\n",
    "\n",
    "<img src=\"imgs/getting_profiler.png\" \n",
    "     align=\"center\" \n",
    "     width=\"850\"\n",
    "     height=\"850\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae6b0c00-39da-411a-8b96-b151a9c45ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_LOGS_PATH: gs://rec-bandits-v2-hybrid-vertex-bucket/02-scale-compare-v2/run-20231115-094131/logs\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "TB_LOGS_PATH = LOG_DIR\n",
    "# TB_LOGS_PATH = 'gs://rec-bandits-v2-hybrid-vertex-bucket/v2-scale-t4-v1/run-20231010-082727/logs'\n",
    "print(f\"TB_LOGS_PATH: {TB_LOGS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1763ace-2c4a-4b93-a8b9-4419f5858f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eecde86e-96e4-4389-a59b-97b069b97c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a46e78e632c3188d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a46e78e632c3188d\");\n",
       "          const url = new URL(\"/proxy/6009/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=$TB_LOGS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3bf4ac-5621-425e-b74f-5cbaa738b398",
   "metadata": {},
   "source": [
    "# Making predictions\n",
    "\n",
    "* When a policy is trained, given a new observation request (i.e. a user vector),\n",
    "* the policy will inference (produce) actions, which are the recommended movies.\n",
    "* In TF-Agents, observations are abstracted in a named tuple,\n",
    "\n",
    "```\n",
    "TimeStep(‘step_type’, ‘discount’, ‘reward’, ‘observation’)\n",
    "```\n",
    "\n",
    "> the policy maps time steps to actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7eb70de-3aa9-41ca-8f37-b7fb8b7d931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from src.perarm_features import emb_features as emb_features\n",
    "from src.perarm_features import reward_factory as reward_factory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653049f-0e5f-4055-99c7-471c5a4a665e",
   "metadata": {},
   "source": [
    "## Load eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fcbc77f1-1e3f-4803-a3f6-70377a755d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec={'bucketized_user_age': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'movie_genres': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None), 'movie_id': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'timestamp': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'user_id': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_occupation_text': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_rating': TensorSpec(shape=(None,), dtype=tf.float32, name=None)}>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT = \"val\"\n",
    "\n",
    "val_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{DATA_GCS_PREFIX}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        val_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "val_dataset = tf.data.TFRecordDataset(val_files)\n",
    "val_dataset = val_dataset.map(data_utils.parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# eval dataset\n",
    "eval_ds = val_dataset.batch(1)\n",
    "\n",
    "# if NUM_EVAL_STEPS > 0:\n",
    "#     eval_ds = eval_ds.take(NUM_EVAL_STEPS)\n",
    "\n",
    "eval_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc01ee-2857-4cf9-8369-e49156c906af",
   "metadata": {},
   "source": [
    "### Load vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4fe552cf-817c-49b1-8f4b-01860e0f85d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vocab...\n",
      "Downloaded vocab from: gs://rec-bandits-v2-hybrid-vertex-bucket/vocabs/vocab_dict.pkl\n",
      "\n",
      "'movie_id'\n",
      "'user_id'\n",
      "'user_occupation_text'\n",
      "'movie_genres'\n",
      "'bucketized_user_age'\n",
      "'max_timestamp'\n",
      "'min_timestamp'\n",
      "'timestamp_buckets'\n"
     ]
    }
   ],
   "source": [
    "EXISTING_VOCAB_FILE = f'gs://{BUCKET_NAME}/{VOCAB_SUBDIR}/{VOCAB_FILENAME}'\n",
    "print(f\"Downloading vocab...\")\n",
    "\n",
    "os.system(f'gsutil -q cp {EXISTING_VOCAB_FILE} .')\n",
    "print(f\"Downloaded vocab from: {EXISTING_VOCAB_FILE}\\n\")\n",
    "\n",
    "filehandler = open(VOCAB_FILENAME, 'rb')\n",
    "vocab_dict = pkl.load(filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "for key in vocab_dict.keys():\n",
    "    pprint(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d14f4f2-8304-40e5-b7ed-aad69b424bac",
   "metadata": {},
   "source": [
    "## load trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6d547058-6982-4588-94a7-62dda9801f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-scale-compare-v4/run-20231115-125019/artifacts/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-scale-compare-v4/run-20231115-125019/artifacts/fingerprint.pb\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-scale-compare-v4/run-20231115-125019/artifacts/policy_specs.pbtxt\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-scale-compare-v4/run-20231115-125019/artifacts/saved_model.pb\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-scale-compare-v4/run-20231115-125019/artifacts/assets/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/02-scale-compare-v4/run-20231115-125019/artifacts/variables/\n"
     ]
    }
   ],
   "source": [
    "# MODEL_DIR = \"gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230717-211248/model\"\n",
    "\n",
    "!gsutil ls $ARTIFACTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b70d44d1-2afc-47fa-8c37-dbbea6b286bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.py_tf_eager_policy.SavedModelPyTFEagerPolicy at 0x7fa4085c7430>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy\n",
    "\n",
    "trained_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n",
    "    ARTIFACTS_DIR, load_specs_from_pbtxt=True\n",
    ")\n",
    "\n",
    "trained_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff61492-f040-4418-b3df-8a1b05684bce",
   "metadata": {},
   "source": [
    "## call embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f69d531-d286-441a-aff9-d6afd3e30a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GLOBAL_EMBEDDING_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a78f2086-6cd3-4f81-8486-ac8356889e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.perarm_features.emb_features.EmbeddingModel at 0x7fa4085c7ac0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = emb_features.EmbeddingModel(\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af1cb7-a6a4-48ac-8b42-7d5cd20f51ed",
   "metadata": {},
   "source": [
    "## Run inference with trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e1612722-ce10-4766-9b73-cd3ad6ee902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFER_SIZE = 1\n",
    "dummy_arm = tf.zeros([INFER_SIZE, PER_ARM_DIM], dtype=tf.float32)\n",
    "\n",
    "for x in eval_ds.take(INFER_SIZE):\n",
    "    # get feature tensors\n",
    "    global_feat_infer = embs._get_global_context_features(x)\n",
    "    arm_feat_infer = embs._get_per_arm_features(x)\n",
    "    \n",
    "    # rewards = _get_rewards(x)\n",
    "    rewards = reward_factory._get_rewards(x)\n",
    "    \n",
    "    # reshape arm features\n",
    "    arm_feat_infer = tf.reshape(arm_feat_infer, [EVAL_BATCH_SIZE, PER_ARM_DIM]) # perarm_dim\n",
    "    concat_arm = tf.concat([arm_feat_infer, dummy_arm], axis=0)\n",
    "    \n",
    "    # flatten global\n",
    "    flat_global_infer = tf.reshape(global_feat_infer, [GLOBAL_DIM])\n",
    "    feature = {'global': flat_global_infer, 'per_arm': concat_arm}\n",
    "    \n",
    "    # get actual reward\n",
    "    actual_reward = rewards.numpy()[0]\n",
    "    \n",
    "    # build trajectory step\n",
    "    trajectory_step = train_utils._get_eval_step(feature, actual_reward)\n",
    "    \n",
    "    prediction = trained_policy.action(trajectory_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "185c1ee8-7a3c-42b2-87e0-2b27962b043c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=array(0, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=(), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=(), chosen_arm_features=array([ 9.3020797e-03, -3.2347158e-02,  4.1276325e-02,  2.6380036e-02,\n",
       "       -2.6593601e-02, -3.4350384e-02, -3.0565202e-02, -3.2029547e-02,\n",
       "        1.5862275e-02, -6.1246380e-03,  2.3106385e-02, -2.2312880e-02,\n",
       "       -6.0982481e-03,  1.2759421e-02,  1.3587091e-02,  2.4498928e-02,\n",
       "       -3.2554224e-02,  7.3992126e-03,  4.8149824e-03,  4.6623219e-02,\n",
       "        3.5907898e-02, -3.9207697e-02, -3.0699229e-02, -3.0026352e-02,\n",
       "        3.5303507e-02, -4.8878707e-02,  2.5514964e-02, -9.9024996e-03,\n",
       "        4.6273097e-03,  1.7675757e-03,  1.8049750e-02,  2.1221664e-02,\n",
       "        1.6136181e-02,  2.1091986e-02, -4.5583498e-02, -4.4766020e-02,\n",
       "       -4.7618970e-03, -2.3394451e-03, -2.1933293e-02, -3.5277437e-02,\n",
       "       -3.6237527e-02,  2.1629106e-02,  3.0160133e-02, -2.1044148e-02,\n",
       "       -1.9908870e-02, -4.3970432e-02, -2.9225349e-03,  6.3858926e-05,\n",
       "        7.1376935e-03, -2.8652405e-02, -6.6105612e-03, -5.8362372e-03,\n",
       "       -6.5492764e-03, -3.8090229e-02, -2.7260615e-02,  2.7258109e-02,\n",
       "        9.5215812e-03,  1.3103858e-03, -2.9661704e-02,  2.4495374e-02,\n",
       "        4.6968449e-02,  4.2166684e-02, -2.4063934e-02, -3.4181930e-02],\n",
       "      dtype=float32)))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3661d46-4d78-483c-90ee-20637b7ff6a7",
   "metadata": {},
   "source": [
    "# Deploy policy to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6b6d7cd3-eaa6-4fc4-b111-b42b1ff178c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "28009a88-63b4-4a8f-b1fb-4a8165bf441e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCKERNAME_02_PRED = Dockerfile_predict_mab_02e\n",
      "REPOSITORY         = rl-movielens-rec-bandits-v2\n",
      "IMAGE_NAME_02_PRED = pred-perarm-feats-02e\n",
      "IMAGE_URI_02_PRED  = gcr.io/hybrid-vertex/pred-perarm-feats-02e\n",
      "REMOTE_IMAGE_NAME  = us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-rec-bandits-v2/pred-perarm-feats-02e\n"
     ]
    }
   ],
   "source": [
    "# TODO incorporate to 00-env-setup\n",
    "DOCKERNAME_02_PRED = 'Dockerfile_predict_mab_02e'\n",
    "IMAGE_NAME_02_PRED = \"pred-perarm-feats-02e\"\n",
    "IMAGE_URI_02_PRED  = f\"gcr.io/hybrid-vertex/{IMAGE_NAME_02_PRED}\"\n",
    "REMOTE_IMAGE_NAME  = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE_NAME_02_PRED}\"\n",
    "\n",
    "print(f\"DOCKERNAME_02_PRED = {DOCKERNAME_02_PRED}\")\n",
    "print(f\"REPOSITORY         = {REPOSITORY}\")\n",
    "print(f\"IMAGE_NAME_02_PRED = {IMAGE_NAME_02_PRED}\")\n",
    "print(f\"IMAGE_URI_02_PRED  = {IMAGE_URI_02_PRED}\")\n",
    "print(f\"REMOTE_IMAGE_NAME  = {REMOTE_IMAGE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d7b4a283-9648-4489-a3ff-e0e039ea5b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_name    : trained-LinUCB-v2\n",
      "uploaded_policy : <google.cloud.aiplatform.models.Model object at 0x7fa3fbc06bc0> \n",
      "resource name: projects/934903580331/locations/us-central1/models/2107392155516403712\n"
     ]
    }
   ],
   "source": [
    "uploaded_policy = vertex_ai.Model.upload(\n",
    "    display_name=f'trained-{AGENT_TYPE}-v2',\n",
    "    artifact_uri=ARTIFACTS_DIR,\n",
    "    serving_container_image_uri=f\"{IMAGE_URI_02_PRED}:latest\",\n",
    "    # serving_container_image_uri=f\"{REMOTE_IMAGE_NAME}:latest\",\n",
    "    serving_container_predict_route=\"/predict\",\n",
    "    serving_container_health_route=\"/health\",\n",
    "    sync=True,\n",
    ")\n",
    "\n",
    "print(f\"display_name    : {uploaded_policy.display_name}\")\n",
    "print(f\"uploaded_policy : {uploaded_policy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3ec5d0d9-a75e-454f-b03d-73ee23b599fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_name : endpoint-epsGreedy-v1\n",
      "endpoint     : <google.cloud.aiplatform.models.Endpoint object at 0x7fa3fbc46f80> \n",
      "resource name: projects/934903580331/locations/us-central1/endpoints/211442683091091456\n"
     ]
    }
   ],
   "source": [
    "# endpoint = vertex_ai.Endpoint.create(\n",
    "#     display_name=f'endpoint-{AGENT_TYPE}-v1',\n",
    "#     project=PROJECT_ID,\n",
    "#     location=LOCATION,\n",
    "#     sync=True,\n",
    "# )\n",
    "\n",
    "endpoint = vertex_ai.Endpoint('211442683091091456')\n",
    "\n",
    "print(f\"display_name : {endpoint.display_name}\")\n",
    "print(f\"endpoint     : {endpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8776114e-dbd5-42fd-bcc2-5b59c960bd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_name    : endpoint-epsGreedy-v1\n",
      "\n",
      "deployed_policy : <google.cloud.aiplatform.models.Endpoint object at 0x7fa3fbc46f80> \n",
      "resource name: projects/934903580331/locations/us-central1/endpoints/211442683091091456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7fa3fbc46f80> \n",
       "resource name: projects/934903580331/locations/us-central1/endpoints/211442683091091456"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployed_policy = uploaded_policy.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=f'deployed-{AGENT_TYPE}-v1',\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    accelerator_type=None,\n",
    "    accelerator_count=0,\n",
    "    sync=True,\n",
    ")\n",
    "\n",
    "print(f\"display_name    : {deployed_policy.display_name}\\n\")\n",
    "print(f\"deployed_policy : {deployed_policy}\")\n",
    "\n",
    "deployed_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1924bb-09c4-46bb-ac70-1b27da6aa530",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in eval_ds.take(INFER_SIZE):\n",
    "\n",
    "    deployed_policy.predict(\n",
    "        instances=[x],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3eb310-b9a4-4a13-ad62-f9b2f7c84a43",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11040c4c-2291-4c40-8e62-c9956c9285c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint.delete(force=True)\n",
    "# uploaded_policy.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7002c9-69e2-4940-b84f-924984c03a54",
   "metadata": {},
   "source": [
    "**Finished**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
