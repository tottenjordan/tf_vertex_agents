{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01cffb8-8045-4314-b56a-8ac9154c6066",
   "metadata": {},
   "source": [
    "# Scale Training with Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c6cb3-7970-4fab-a102-8c8749ecd3fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29580d03-390d-4d90-a5c0-14c16338ddbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Notebook Objectives:\n",
    "* Create hyperparameter tuning and training custom container\n",
    "* Submit hyperparameter tuning job (optional)\n",
    "* Create custom prediction container\n",
    "* Submit custom container training job\n",
    "* Deploy trained model to Endpoint\n",
    "* Predict on the Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64261791-880e-41c7-b6e9-a39698c66d51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create hyperparameter tuning and training custom container\n",
    "\n",
    "Create a custom container that can be used for both hyperparameter tuning and training. The associated source code is in `src/training/`. This serves as the inner script of the custom container.\n",
    "As before, the training function is the same as [trainer.train](https://github.com/tensorflow/agents/blob/r0.8.0/tf_agents/bandits/agents/examples/v2/trainer.py#L104), but it keeps track of intermediate metric values, supports hyperparameter tuning, and (for training) saves artifacts to different locations. The training logic for hyperparameter tuning and training is the same.\n",
    "\n",
    "#### Execute hyperparameter tuning:\n",
    "- The code does not save model artifacts. It takes in command-line arguments as hyperparameter values from the Vertex AI Hyperparameter Tuning service, and reports training result metric to Vertex AI at each trial using cloudml-hypertune.\n",
    "- Note that if you decide to save model artifacts, saving them to the same directory may cause overwriting errors if you use parallel trials in the hyperparameter tuning job. The recommended approach is to save each trial's artifacts to a different sub-directory. This would also allow you to recover all the artifacts from different trials and can potentially save you from re-training.\n",
    "- Read more about hyperparameter tuning for custom containers [here](https://cloud.google.com/vertex-ai/docs/training/containers-overview#hyperparameter_tuning_with_custom_containers); read about hyperparameter tuning support [here](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview).\n",
    "\n",
    "#### Execute training:\n",
    "- The code saves model artifacts to `os.environ[\"AIP_MODEL_DIR\"]` in addition to `ARTIFACTS_DIR`, as required [here](https://github.com/googleapis/python-aiplatform/blob/v0.8.0/google/cloud/aiplatform/training_jobs.py#L2202).\n",
    "- If you want to make changes to the function, make sure to still save the trained policy as a SavedModel to clean directories, and avoid saving checkpoints and other artifacts, so that deploying the model to endpoints works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ea275-15ba-40b6-acb1-94d5d18996b1",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f515b2b9-a185-4336-8dd4-2e23e0076fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/jt-github/tf_vertex_agents\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2ae96-b5b5-4b10-9d9c-0b00d533ef46",
   "metadata": {},
   "source": [
    "### set vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd715ec5-f0fb-432b-bef4-a05a57586c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'tabv2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "463543b0-77b0-4d77-ba64-a6c84b70851e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID       = hybrid-vertex\n",
      "PROJECT_NUM      = 934903580331\n",
      "VPC_NETWORK_NAME = genai-haystack-vpc\n",
      "LOCATION         = us-central1\n",
      "REGION           = us-central1\n",
      "BQ_LOCATION      = US\n"
     ]
    }
   ],
   "source": [
    "# creds, PROJECT_ID = google.auth.default()\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "PROJECT_NUM              = !gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\"\n",
    "PROJECT_NUM              = PROJECT_NUM[0]\n",
    "\n",
    "VERTEX_SA                = f'{PROJECT_NUM}-compute@developer.gserviceaccount.com'\n",
    "\n",
    "VPC_NETWORK_NAME         = \"genai-haystack-vpc\"\n",
    "\n",
    "# locations / regions for cloud resources\n",
    "LOCATION                 = 'us-central1'        \n",
    "REGION                   = LOCATION\n",
    "BQ_LOCATION              = 'US'\n",
    "\n",
    "print(f\"PROJECT_ID       = {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM      = {PROJECT_NUM}\")\n",
    "print(f\"VPC_NETWORK_NAME = {VPC_NETWORK_NAME}\")\n",
    "print(f\"LOCATION         = {LOCATION}\")\n",
    "print(f\"REGION           = {REGION}\")\n",
    "print(f\"BQ_LOCATION      = {BQ_LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ac94b90-68ae-4729-b126-af84c3c56a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET_NAME       : tabv2-hybrid-vertex-bucket\n",
      "BUCKET_URI        : gs://tabv2-hybrid-vertex-bucket\n",
      "DATA_PATH         : gs://tabv2-hybrid-vertex-bucket/artifacts/u.data\n",
      "VPC_NETWORK_FULL  : projects/934903580331/global/networks/genai-haystack-vpc\n",
      "MY_BQ_DATASET     : tabv2_hybrid_vertex_bucket\n"
     ]
    }
   ],
   "source": [
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "# Location of the MovieLens 100K dataset's \"u.data\" file.\n",
    "DATA_PATH                = f\"{BUCKET_URI}/artifacts/u.data\"\n",
    "ARTIFACTS_DIR            = f\"{BUCKET_URI}/artifacts\"\n",
    "\n",
    "VPC_NETWORK_FULL         = f\"projects/{PROJECT_NUM}/global/networks/{VPC_NETWORK_NAME}\"\n",
    "\n",
    "MY_BQ_DATASET            = BUCKET_NAME.lower().replace(\"-\",\"_\")\n",
    "\n",
    "print(f\"BUCKET_NAME       : {BUCKET_NAME}\")\n",
    "print(f\"BUCKET_URI        : {BUCKET_URI}\")\n",
    "print(f\"DATA_PATH         : {DATA_PATH}\")\n",
    "print(f\"VPC_NETWORK_FULL  : {VPC_NETWORK_FULL}\")\n",
    "print(f\"MY_BQ_DATASET     : {MY_BQ_DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f5799-966e-4995-aae9-449768c1af48",
   "metadata": {},
   "source": [
    "### create GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "396332ce-7720-4a75-9920-e9079c39a85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://tabv2-hybrid-vertex-bucket/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'tabv2-hybrid-vertex-bucket' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "# create bucket\n",
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0efc43f0-6e84-4679-9c56-7d0c34a240fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://tabv2-hybrid-vertex-bucket/artifacts/\n",
      "                                 gs://tabv2-hybrid-vertex-bucket/hptuning/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ab55c-3ca8-4348-b902-749b19a6c57f",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e52ce64-2afe-4d2f-a43d-68775ee7cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce8b63e1-84a3-4417-adf4-ff3f0dfe9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents import TFAgent\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "from tf_agents.bandits.environments import (environment_utilities,\n",
    "                                            movielens_py_environment)\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import TFEnvironment, tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.metrics.tf_metric import TFStepMetric\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63cd20c0-7446-49df-9a1e-41a89065e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# # bigquery client\n",
    "# bqclient = bigquery.Client(\n",
    "#     project=PROJECT_ID,\n",
    "#     # location=LOCATION\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df188f63-506d-43dd-90ab-08246de887fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-samples-data/vertex-ai/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/u.data [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  1.9 MiB/  1.9 MiB]                                                \n",
      "Operation completed over 1 objects/1.9 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "SAMPLE_DATA_URI = \"gs://cloud-samples-data/vertex-ai/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/u.data\"\n",
    "\n",
    "! gsutil cp $SAMPLE_DATA_URI $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dda7603-ff83-4878-af18-66da744b9648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1979173  2023-07-04T17:52:20Z  gs://tabv2-hybrid-vertex-bucket/artifacts/u.data#1688493140988566  metageneration=1\n",
      "TOTAL: 1 objects, 1979173 bytes (1.89 MiB)\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17376b28-6faf-4e03-baf3-73126a743e52",
   "metadata": {},
   "source": [
    "## Create training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7d34603-7d82-4207-bd6a-7713a8db3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "TRAINER_SUBDIR = \"training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6fc1f84-11d0-4798-90ba-69f90f6be96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{TRAINER_SUBDIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{TRAINER_SUBDIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc82a317-4b4e-4eb9-84ca-6735fda799b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/training/policy_util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAINER_SUBDIR}/policy_util.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"The utility module for reinforcement learning policy.\"\"\"\n",
    "import collections\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "\n",
    "from tf_agents.agents import TFAgent\n",
    "from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import TFEnvironment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.metrics.tf_metric import TFStepMetric\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "def train(\n",
    "    agent: TFAgent\n",
    "    , environment: TFEnvironment\n",
    "    , training_loops: int\n",
    "    , steps_per_loop: int\n",
    "    , additional_metrics: Optional[List[TFStepMetric]] = None\n",
    "    , training_data_spec_transformation_fn: Optional[Callable[[T],T]] = None\n",
    "    , run_hyperparameter_tuning: bool = False\n",
    "    , root_dir: Optional[str] = None\n",
    "    , artifacts_dir: Optional[str] = None\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Performs `training_loops` iterations of training on the agent's policy.\n",
    "\n",
    "    Uses the `environment` as the problem formulation and source of immediate\n",
    "    feedback and the agent's algorithm, to perform `training-loops` iterations\n",
    "    of on-policy training on the policy. Has hyperparameter mode and regular\n",
    "    training mode.\n",
    "    If one or more baseline_reward_fns are provided, the regret is computed\n",
    "    against each one of them. Here is example baseline_reward_fn:\n",
    "    def baseline_reward_fn(observation, per_action_reward_fns):\n",
    "     rewards = ... # compute reward for each arm\n",
    "     optimal_action_reward = ... # take the maximum reward\n",
    "     return optimal_action_reward\n",
    "\n",
    "    Args:\n",
    "      agent: An instance of `TFAgent`.\n",
    "      environment: An instance of `TFEnvironment`.\n",
    "      training_loops: An integer indicating how many training loops should be run.\n",
    "      steps_per_loop: An integer indicating how many driver steps should be\n",
    "        executed and presented to the trainer during each training loop.\n",
    "      additional_metrics: Optional; list of metric objects to log, in addition to\n",
    "        default metrics `NumberOfEpisodes`, `AverageReturnMetric`, and\n",
    "        `AverageEpisodeLengthMetric`.\n",
    "      training_data_spec_transformation_fn: Optional; function that transforms\n",
    "        the data items before they get to the replay buffer.\n",
    "      run_hyperparameter_tuning: Optional; whether this training logic is\n",
    "        executed for the purpose of hyperparameter tuning. If so, then it does\n",
    "        not save model artifacts.\n",
    "      root_dir: Optional; path to the directory where training artifacts are\n",
    "        written; usually used for a default or auto-generated location. Do not\n",
    "        specify this argument if using hyperparameter tuning instead of training.\n",
    "      artifacts_dir: Optional; path to an extra directory where training\n",
    "        artifacts are written; usually used for a mutually agreed location from\n",
    "        which artifacts will be loaded. Do not specify this argument if using\n",
    "        hyperparameter tuning instead of training.\n",
    "\n",
    "    Returns:\n",
    "      A dict mapping metric names (eg. \"AverageReturnMetric\") to a list of\n",
    "      intermediate metric values over `training_loops` iterations of training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ====================================================\n",
    "    # get data spec\n",
    "    # ====================================================\n",
    "    if run_hyperparameter_tuning and not (root_dir is None and artifacts_dir is None):\n",
    "        raise ValueError(\n",
    "            \"Do not specify `root_dir` or `artifacts_dir` when\" +\n",
    "            \" running hyperparameter tuning.\"\n",
    "        )\n",
    "\n",
    "    if training_data_spec_transformation_fn is None:\n",
    "        data_spec = agent.policy.trajectory_spec\n",
    "    else:\n",
    "        data_spec = training_data_spec_transformation_fn(\n",
    "            agent.policy.trajectory_spec\n",
    "        )\n",
    "        \n",
    "    # ====================================================\n",
    "    # define replay buffer\n",
    "    # ====================================================\n",
    "    replay_buffer = trainer._get_replay_buffer(\n",
    "        data_spec = data_spec\n",
    "        , batch_size = environment.batch_size\n",
    "        , steps_per_loop = steps_per_loop\n",
    "        , async_steps_per_loop = 1\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # metrics\n",
    "    # ====================================================\n",
    "    # `step_metric` records the number of individual rounds of bandit interaction;\n",
    "    # that is, (number of trajectories) * batch_size.\n",
    "    \n",
    "    step_metric = tf_metrics.EnvironmentSteps()\n",
    "    \n",
    "    metrics = [\n",
    "        tf_metrics.NumberOfEpisodes()\n",
    "        , tf_metrics.AverageEpisodeLengthMetric(batch_size=environment.batch_size)\n",
    "    ]\n",
    "    if additional_metrics:\n",
    "        metrics += additional_metrics\n",
    "\n",
    "    if isinstance(environment.reward_spec(), dict):\n",
    "        metrics += [\n",
    "            tf_metrics.AverageReturnMultiMetric(\n",
    "                reward_spec=environment.reward_spec()\n",
    "                , batch_size=environment.batch_size\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        metrics += [\n",
    "            tf_metrics.AverageReturnMetric(batch_size=environment.batch_size)\n",
    "        ]\n",
    "\n",
    "    # Store intermediate metric results, indexed by metric names.\n",
    "    metric_results = collections.defaultdict(list)\n",
    "\n",
    "    # ====================================================\n",
    "    # Driver\n",
    "    # ====================================================\n",
    "    \n",
    "    if training_data_spec_transformation_fn is not None:\n",
    "        add_batch_fn = lambda data: replay_buffer.add_batch(\n",
    "            training_data_spec_transformation_fn(data)\n",
    "        )\n",
    "    else:\n",
    "        add_batch_fn = replay_buffer.add_batch\n",
    "\n",
    "    observers = [add_batch_fn, step_metric] + metrics\n",
    "\n",
    "    driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        env=environment\n",
    "        , policy=agent.collect_policy\n",
    "        , num_steps=steps_per_loop * environment.batch_size\n",
    "        , observers=observers\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # training_loop\n",
    "    # ====================================================\n",
    "    training_loop = trainer._get_training_loop(\n",
    "        driver = driver\n",
    "        , replay_buffer = replay_buffer\n",
    "        , agent = agent\n",
    "        , steps = steps_per_loop\n",
    "        , async_steps_per_loop = 1\n",
    "    )\n",
    "    if not run_hyperparameter_tuning:\n",
    "        saver = policy_saver.PolicySaver(agent.policy)\n",
    "\n",
    "    for train_step in range(training_loops):\n",
    "        training_loop(\n",
    "            train_step = train_step\n",
    "            , metrics = metrics\n",
    "        )\n",
    "        metric_utils.log_metrics(metrics)\n",
    "    \n",
    "        for metric in metrics:\n",
    "            metric.tf_summaries(train_step = step_metric.result())\n",
    "            metric_results[type(metric).__name__].append(metric.result().numpy())\n",
    "    \n",
    "    if not run_hyperparameter_tuning:\n",
    "        saver.save(root_dir)\n",
    "        saver.save(artifacts_dir)\n",
    "    \n",
    "    return metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cab82ce-0948-40f9-920a-2711ae774b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/training/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAINER_SUBDIR}/task.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"The entrypoint for training a policy.\"\"\"\n",
    "import argparse\n",
    "import functools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Union\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "import hypertune\n",
    "\n",
    "from src.training import policy_util\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.environments import environment_utilities\n",
    "from tf_agents.bandits.environments import movielens_py_environment\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "PER_ARM = False  # Use the non-per-arm version of the MovieLens environment.\n",
    "\n",
    "def get_args(\n",
    "    raw_args: List[str]\n",
    ") -> argparse.Namespace:\n",
    "    \"\"\"Parses parameters and hyperparameters for training a policy.\n",
    "\n",
    "    Args:\n",
    "      raw_args: A list of command line arguments.\n",
    "\n",
    "    Returns:\n",
    "      An argpase.Namespace object mapping (hyper)parameter names to the parsed\n",
    "      values.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--project_id\"\n",
    "        , type=str\n",
    "        , default='hybrid-vertex'\n",
    "    )\n",
    "    # Whether to execute hyperparameter tuning or training\n",
    "    parser.add_argument(\n",
    "        \"--run-hyperparameter-tuning\"\n",
    "        , action=\"store_true\"\n",
    "        , help=\"Whether to perform hyperparameter tuning instead of regular training.\"\n",
    "    )\n",
    "    # Whether to train using the best hyperparameters learned from a previous\n",
    "    # hyperparameter tuning job.\n",
    "    parser.add_argument(\n",
    "        \"--train-with-best-hyperparameters\"\n",
    "        , action=\"store_true\"\n",
    "        , help=\"Whether to train using the best hyperparameters learned from a previous hyperparameter tuning job.\"\n",
    "    )\n",
    "    # Path parameters\n",
    "    parser.add_argument(\n",
    "        \"--artifacts-dir\"\n",
    "        , type=str\n",
    "        , help=\"Extra directory where model artifacts are saved.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--profiler-dir\"\n",
    "        , default=None\n",
    "        , type=str\n",
    "        , help=\"Directory for TensorBoard Profiler artifacts.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data-path\", type=str, help=\"Path to MovieLens 100K's 'u.data' file.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--best-hyperparameters-bucket\"\n",
    "        , type=str\n",
    "        , help=\"Path to MovieLens 100K's 'u.data' file.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--best-hyperparameters-path\"\n",
    "        , type=str\n",
    "        , help=\"Path to JSON file containing the best hyperparameters.\"\n",
    "    )\n",
    "    # Hyperparameters\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\"\n",
    "        , default=8\n",
    "        , type=int\n",
    "        , help=\"Training and prediction batch size.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--training-loops\"\n",
    "        , default=4\n",
    "        , type=int\n",
    "        , help=\"Number of training iterations.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--steps-per-loop\"\n",
    "        , default=2\n",
    "        , type=int\n",
    "        , help=\"Number of driver steps per training iteration.\"\n",
    "    )\n",
    "    # MovieLens simulation environment parameters\n",
    "    parser.add_argument(\n",
    "        \"--rank-k\"\n",
    "        , default=20\n",
    "        , type=int\n",
    "        , help=\"Rank for matrix factorization in the MovieLens environment; also the observation dimension.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-actions\"\n",
    "        , default=20\n",
    "        , type=int\n",
    "        , help=\"Number of actions (movie items) to choose from.\"\n",
    "    )\n",
    "    # LinUCB agent parameters\n",
    "    parser.add_argument(\n",
    "        \"--tikhonov-weight\"\n",
    "        , default=0.001\n",
    "        , type=float\n",
    "        , help=\"LinUCB Tikhonov regularization weight.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--agent-alpha\"\n",
    "        , default=10.0\n",
    "        , type=float\n",
    "        , help=\"LinUCB exploration parameter that multiplies the confidence intervals.\"\n",
    "    )\n",
    "    return parser.parse_args(raw_args)\n",
    "\n",
    "\n",
    "def execute_task(\n",
    "    args: argparse.Namespace\n",
    "    , best_hyperparameters_blob: Union[storage.Blob, None]\n",
    "    , hypertune_client: Union[hypertune.HyperTune, None]\n",
    ") -> None:\n",
    "    \"\"\"Executes training, or hyperparameter tuning, for the policy.\n",
    "\n",
    "    Parses parameters and hyperparameters from the command line, reads best\n",
    "    hyperparameters if applicable, constructs the logical modules for RL, and\n",
    "    executes training or hyperparameter tuning. Tracks the training process\n",
    "    and resources using TensorBoard Profiler if applicable.\n",
    "\n",
    "    Args:\n",
    "      args: An argpase.Namespace object of (hyper)parameter values.\n",
    "      best_hyperparameters_blob: An object containing best hyperparameters in\n",
    "        Google Cloud Storage.\n",
    "      hypertune_client: Client for submitting hyperparameter tuning metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # [Do Not Change] Set the root directory for training artifacts.\n",
    "    root_dir = os.environ[\"AIP_MODEL_DIR\"] if not args.run_hyperparameter_tuning else \"\"\n",
    "\n",
    "    # Use best hyperparameters learned from a previous hyperparameter tuning job.\n",
    "    logging.info(args.train_with_best_hyperparameters)\n",
    "    if args.train_with_best_hyperparameters:\n",
    "        best_hyperparameters = json.loads(\n",
    "            best_hyperparameters_blob.download_as_string()\n",
    "        )\n",
    "        \n",
    "        if \"BATCH_SIZE\" in best_hyperparameters:\n",
    "            args.batch_size = best_hyperparameters[\"BATCH_SIZE\"]\n",
    "        if \"TRAINING_LOOPS\" in best_hyperparameters:\n",
    "            args.training_loops = best_hyperparameters[\"TRAINING_LOOPS\"]\n",
    "        if \"STEPS_PER_LOOP\" in best_hyperparameters:\n",
    "            args.step_per_loop = best_hyperparameters[\"STEPS_PER_LOOP\"]\n",
    "\n",
    "    # Define RL environment.\n",
    "    env = movielens_py_environment.MovieLensPyEnvironment(\n",
    "        args.data_path\n",
    "        , args.rank_k\n",
    "        , args.batch_size\n",
    "        , num_movies=args.num_actions\n",
    "        , csv_delimiter=\"\\t\"\n",
    "    )\n",
    "    environment = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "    # Define RL agent/algorithm.\n",
    "    agent = lin_ucb_agent.LinearUCBAgent(\n",
    "        time_step_spec=environment.time_step_spec()\n",
    "        , action_spec=environment.action_spec()\n",
    "        , tikhonov_weight=args.tikhonov_weight\n",
    "        , alpha=args.agent_alpha\n",
    "        , dtype=tf.float32\n",
    "        , accepts_per_arm_features=PER_ARM\n",
    "    )\n",
    "    logging.info(\"TimeStep Spec (for each batch):\\n%s\\n\", agent.time_step_spec)\n",
    "    logging.info(\"Action Spec (for each batch):\\n%s\\n\", agent.action_spec)\n",
    "    logging.info(\"Reward Spec (for each batch):\\n%s\\n\", environment.reward_spec())\n",
    "\n",
    "    # Define RL metric.\n",
    "    optimal_reward_fn = functools.partial(\n",
    "        environment_utilities.compute_optimal_reward_with_movielens_environment\n",
    "        , environment=environment\n",
    "    )\n",
    "    \n",
    "    regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n",
    "    metrics = [regret_metric]\n",
    "\n",
    "    # Perform on-policy training with the simulation MovieLens environment.\n",
    "    if args.profiler_dir is not None:\n",
    "        tf.profiler.experimental.start(args.profiler_dir)\n",
    "  \n",
    "    metric_results = policy_util.train(\n",
    "        agent=agent\n",
    "        , environment=environment\n",
    "        , training_loops=args.training_loops\n",
    "        , steps_per_loop=args.steps_per_loop\n",
    "        , additional_metrics=metrics\n",
    "        , run_hyperparameter_tuning=args.run_hyperparameter_tuning\n",
    "        , root_dir=root_dir if not args.run_hyperparameter_tuning else None\n",
    "        , artifacts_dir=args.artifacts_dir\n",
    "        if not args.run_hyperparameter_tuning else None\n",
    "    )\n",
    "    \n",
    "    if args.profiler_dir is not None:\n",
    "        tf.profiler.experimental.stop()\n",
    "\n",
    "    # Report training metrics to Vertex AI for hyperparameter tuning\n",
    "    if args.run_hyperparameter_tuning:\n",
    "        hypertune_client.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag=\"final_average_return\"\n",
    "            , metric_value=metric_results[\"AverageReturnMetric\"][-1]\n",
    "            # , global_step=args.training_loops\n",
    "        )\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Entry point for training or hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    args = get_args(sys.argv[1:])\n",
    "    \n",
    "    if args.train_with_best_hyperparameters:\n",
    "        storage_client = storage.Client(args.project_id)\n",
    "        bucket = storage_client.bucket(args.best_hyperparameters_bucket)\n",
    "        best_hyperparameters_blob = bucket.blob(args.best_hyperparameters_path)\n",
    "    \n",
    "    else:\n",
    "        best_hyperparameters_blob = None\n",
    "    \n",
    "    hypertune_client = hypertune.HyperTune() if args.run_hyperparameter_tuning else None\n",
    "\n",
    "    execute_task(\n",
    "        args = args\n",
    "        , best_hyperparameters_blob = best_hyperparameters_blob\n",
    "        , hypertune_client = hypertune_client\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Python Version = %s\", sys.version)\n",
    "    logging.info(\"TensorFlow Version = %s\", tf.__version__)\n",
    "    logging.info(\"TF_CONFIG = %s\", os.environ.get(\"TF_CONFIG\", \"Not found\"))\n",
    "    logging.info(\"DEVICES = %s\", device_lib.list_local_devices())\n",
    "    logging.info(\"Reinforcement learning task started...\")\n",
    "    \n",
    "    main()\n",
    "    \n",
    "    logging.info(\"Reinforcement learning task completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678fcb8-4b41-42aa-92ec-c3b07f7747e0",
   "metadata": {},
   "source": [
    "## Build train application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b106007a-9ed4-4237-88a9-bf4bbe459b15",
   "metadata": {},
   "source": [
    "### Create a Cloud Build YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8ed39b3c-23ba-4550-b193-5c0ed3aed61e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cloudbuild_yaml = \"\"\"steps:\n",
    "# - name: 'gcr.io/kaniko-project/executor:latest'\n",
    "#   args: ['--destination=gcr.io/{PROJECT_ID}/{HPTUNING_TRAINING_CONTAINER}:latest',\n",
    "#          '--cache=false',\n",
    "#          '--cache-ttl=99h']\n",
    "# options:\n",
    "#   machineType: 'E2_HIGHCPU_8'\"\"\".format(\n",
    "#     PROJECT_ID=PROJECT_ID\n",
    "#     , HPTUNING_TRAINING_CONTAINER=HPTUNING_TRAINING_CONTAINER\n",
    "# )\n",
    "\n",
    "# with open(\"cloudbuild.yaml\", \"w\") as fp:\n",
    "#     fp.write(cloudbuild_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a448db20-3021-4d07-9559-661be6a04eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile_$_DOCKERNAME']\n",
    "  env: ['AIP_STORAGE_URI=$_ARTIFACTS_DIR']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3180b65-d636-40a2-aaa2-7b6a797e43b3",
   "metadata": {},
   "source": [
    "### Write a Dockerfile\n",
    "* Use the [cloudml-hypertune](https://github.com/GoogleCloudPlatform/cloudml-hypertune) Python package to report training metrics to Vertex AI for hyperparameter tuning\n",
    "* Use the Google [Cloud Storage client library](https://cloud.google.com/storage/docs/reference/libraries) to read the best hyperparameters learned from a previous hyperarameter tuning job during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20db1e12-1e16-4ec9-89c7-2a3b181cb715",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCKERNAME = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59ea1b9d-6237-4e12-9b86-9a5ec23d7459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile_train\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile_{DOCKERNAME}\n",
    "\n",
    "# Specifies base image and tag.\n",
    "FROM gcr.io/google-appengine/python\n",
    "\n",
    "WORKDIR /root\n",
    "\n",
    "# Installs additional packages.\n",
    "RUN pip3 install cloudml-hypertune\n",
    "RUN pip3 install google-cloud-storage\n",
    "RUN pip3 install google-cloud-aiplatform\n",
    "RUN pip3 install tensorflow==2.11.0\n",
    "RUN pip3 install tensorboard\n",
    "RUN pip3 install tensorboard-plugin-profile\n",
    "RUN pip3 install tensorboard-plugin-wit\n",
    "RUN pip3 install tensorboard-data-server\n",
    "RUN pip3 install tensorflow-io\n",
    "RUN pip3 install tf-agents==0.16.0\n",
    "RUN pip3 install matplotlib\n",
    "RUN pip3 install urllib3\n",
    "\n",
    "# Copies training code to the Docker image.\n",
    "COPY src/training /root/src/training\n",
    "\n",
    "# Sets up the entry point to invoke the task.\n",
    "ENTRYPOINT [\"python3\", \"-m\", \"src.training.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0092f87b-c113-4325-9696-658ee6cf5860",
   "metadata": {},
   "source": [
    "#### Build the custom container with Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9eafefba-a58f-46aa-a331-fc0abea93702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export DOCKERNAME=train\n",
      "export IMAGE_URI=gcr.io/hybrid-vertex/hptuning-training-custom-container\n",
      "export FILE_LOCATION=./\n",
      "export MACHINE_TYPE=e2-highcpu-32\n",
      "export ARTIFACTS_DIR=gs://tabv2-hybrid-vertex-bucket/artifacts\n"
     ]
    }
   ],
   "source": [
    "HPTUNING_TRAINING_CONTAINER = \"hptuning-training-custom-container\"\n",
    "\n",
    "# Docker definitions for training\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{HPTUNING_TRAINING_CONTAINER}'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './'\n",
    "\n",
    "print(f\"export DOCKERNAME={DOCKERNAME}\")\n",
    "print(f\"export IMAGE_URI={IMAGE_URI}\")\n",
    "print(f\"export FILE_LOCATION={FILE_LOCATION}\")\n",
    "print(f\"export MACHINE_TYPE={MACHINE_TYPE}\")\n",
    "print(f\"export ARTIFACTS_DIR={ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2f64125-fd4a-459b-9034-2d084f85a095",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 90 file(s) totalling 1.7 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://hybrid-vertex_cloudbuild/source/1688493237.698893-3c143636e1724055b1fb940d8e81d62c.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/global/builds/fb220090-58d2-4aa9-8c75-a489b22495af].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/fb220090-58d2-4aa9-8c75-a489b22495af?project=934903580331 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"fb220090-58d2-4aa9-8c75-a489b22495af\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1688493237.698893-3c143636e1724055b1fb940d8e81d62c.tgz#1688493238157819\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1688493237.698893-3c143636e1724055b1fb940d8e81d62c.tgz#1688493238157819...\n",
      "/ [1 files][584.2 KiB/584.2 KiB]                                                \n",
      "Operation completed over 1 objects/584.2 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  1.872MB\n",
      "Step 1/16 : FROM gcr.io/google-appengine/python\n",
      "latest: Pulling from google-appengine/python\n",
      "Digest: sha256:c6480acd38ca4605e0b83f5196ab6fe8a8b59a0288a7b8216c42dbc45b5de8f6\n",
      "Status: Downloaded newer image for gcr.io/google-appengine/python:latest\n",
      " ---> ce284ab20159\n",
      "Step 2/16 : WORKDIR /root\n",
      " ---> Running in 684cccfde1f7\n",
      "Removing intermediate container 684cccfde1f7\n",
      " ---> dcce325693fa\n",
      "Step 3/16 : RUN pip3 install cloudml-hypertune\n",
      " ---> Running in 5a282fe96c39\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Building wheels for collected packages: cloudml-hypertune\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3993 sha256=6963c3805b90b5d16288f9ae2b035bdf9aaefa1f69eebbd9798f57f2d4e96ed1\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Successfully built cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/opt/python3.7/bin/python3.7 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 5a282fe96c39\n",
      " ---> e98dbd627b63\n",
      "Step 4/16 : RUN pip3 install google-cloud-storage\n",
      " ---> Running in 34e3e2414bc2\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.11.1-py3-none-any.whl (120 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting requests<3.0.0dev,>=2.18.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB)\n",
      "Collecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-2.21.0-py2.py3-none-any.whl (182 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5\n",
      "  Downloading protobuf-4.23.3-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.59.1-py2.py3-none-any.whl (224 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting six>=1.9.0\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting urllib3<2.0\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (171 kB)\n",
      "Installing collected packages: pyasn1, urllib3, six, rsa, pyasn1-modules, protobuf, idna, charset-normalizer, certifi, cachetools, requests, googleapis-common-protos, google-auth, google-crc32c, google-api-core, google-resumable-media, google-cloud-core, google-cloud-storage\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mSuccessfully installed cachetools-5.3.1 certifi-2023.5.7 charset-normalizer-3.1.0 google-api-core-2.11.1 google-auth-2.21.0 google-cloud-core-2.3.2 google-cloud-storage-2.10.0 google-crc32c-1.5.0 google-resumable-media-2.5.0 googleapis-common-protos-1.59.1 idna-3.4 protobuf-4.23.3 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-2.31.0 rsa-4.9 six-1.16.0 urllib3-1.26.16\n",
      "\u001b[91mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/opt/python3.7/bin/python3.7 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 34e3e2414bc2\n",
      " ---> ab6b3de55ca2\n",
      "Step 5/16 : RUN pip3 install google-cloud-aiplatform\n",
      " ---> Running in 0d420612e141\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.27.0-py2.py3-none-any.whl (2.6 MB)\n",
      "Collecting shapely<2.0.0\n",
      "  Downloading Shapely-1.8.5.post1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/python3.7/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.10.0)\n",
      "Collecting google-cloud-bigquery<4.0.0dev,>=1.15.0\n",
      "  Downloading google_cloud_bigquery-3.11.3-py2.py3-none-any.whl (219 kB)\n",
      "Collecting packaging>=14.3\n",
      "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.10.1-py2.py3-none-any.whl (321 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/python3.7/lib/python3.7/site-packages (from google-cloud-aiplatform) (4.23.3)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/python3.7/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.11.1)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "  Downloading proto_plus-1.22.3-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/python3.7/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.31.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/python3.7/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.59.1)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /opt/python3.7/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.21.0)\n",
      "Collecting grpcio<2.0dev,>=1.33.2\n",
      "  Downloading grpcio-1.56.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2\n",
      "  Downloading grpcio_status-1.56.0-py3-none-any.whl (5.1 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/python3.7/lib/python3.7/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/python3.7/lib/python3.7/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.16)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/python3.7/lib/python3.7/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/python3.7/lib/python3.7/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/python3.7/lib/python3.7/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.1)\n",
      "Collecting python-dateutil<3.0dev,>=2.7.2\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/python3.7/lib/python3.7/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/python3.7/lib/python3.7/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.2)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "  Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/python3.7/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/python3.7/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python3.7/lib/python3.7/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python3.7/lib/python3.7/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/python3.7/lib/python3.7/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.1.0)\n",
      "Installing collected packages: grpcio, grpcio-status, python-dateutil, proto-plus, packaging, grpc-google-iam-v1, shapely, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "Successfully installed google-cloud-aiplatform-1.27.0 google-cloud-bigquery-3.11.3 google-cloud-resource-manager-1.10.1 grpc-google-iam-v1-0.12.6 grpcio-1.56.0 grpcio-status-1.56.0 packaging-23.1 proto-plus-1.22.3 python-dateutil-2.8.2 shapely-1.8.5.post1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/opt/python3.7/bin/python3.7 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 0d420612e141\n",
      " ---> e5c84926f755\n",
      "Step 6/16 : RUN pip3 install tensorflow==2.11.0\n",
      " ---> Running in f99b6275b0c1\n",
      "Collecting tensorflow==2.11.0\n",
      "  Downloading tensorflow-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: setuptools in /opt/python3.7/lib/python3.7/site-packages (from tensorflow==2.11.0) (40.2.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting numpy>=1.20\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/python3.7/lib/python3.7/site-packages (from tensorflow==2.11.0) (1.16.0)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "Requirement already satisfied: packaging in /opt/python3.7/lib/python3.7/site-packages (from tensorflow==2.11.0) (23.1)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/python3.7/lib/python3.7/site-packages (from tensorflow==2.11.0) (1.56.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/python3.7/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.31.1)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-68.0.0-py3-none-any.whl (804 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.31.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.21.0)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/python3.7/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/python3.7/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/python3.7/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/python3.7/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.26.16)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/python3.7/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/python3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Installing collected packages: zipp, typing-extensions, oauthlib, requests-oauthlib, MarkupSafe, importlib-metadata, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, setuptools, protobuf, numpy, markdown, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 40.2.0\n",
      "    Uninstalling setuptools-40.2.0:\n",
      "      Successfully uninstalled setuptools-40.2.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.3\n",
      "    Uninstalling protobuf-4.23.3:\n",
      "      Successfully uninstalled protobuf-4.23.3\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.56.0 requires protobuf>=4.21.6, but you have protobuf 3.19.6 which is incompatible.\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mSuccessfully installed MarkupSafe-2.1.3 absl-py-1.4.0 astunparse-1.6.3 flatbuffers-23.5.26 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 h5py-3.8.0 importlib-metadata-6.7.0 keras-2.11.0 libclang-16.0.0 markdown-3.4.3 numpy-1.21.6 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 requests-oauthlib-1.3.1 setuptools-68.0.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 typing-extensions-4.7.1 werkzeug-2.2.3 wrapt-1.15.0 zipp-3.15.0\n",
      "\u001b[91mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/opt/python3.7/bin/python3.7 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container f99b6275b0c1\n",
      " ---> b0937d705f75\n",
      "Step 7/16 : RUN pip3 install tensorboard\n",
      " ---> Running in c7c9d6349c2e\n",
      "Requirement already satisfied: tensorboard in /opt/python3.7/lib/python3.7/site-packages (2.11.2)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard) (3.19.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard) (2.31.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard) (0.31.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard) (1.21.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard) (2.21.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard) (68.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard) (3.4.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard) (2.2.3)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard) (1.56.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/python3.7/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/python3.7/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/python3.7/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/python3.7/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.16)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/python3.7/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/python3.7/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/python3.7/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard) (6.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/python3.7/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/python3.7/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (4.7.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/python3.7/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/python3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/python3.7/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/python3.7/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/opt/python3.7/bin/python3.7 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container c7c9d6349c2e\n",
      " ---> 310b28ed044e\n",
      "Step 8/16 : RUN pip3 install tensorboard-plugin-profile\n",
      " ---> Running in 338e05fdc093\n",
      "Collecting tensorboard-plugin-profile\n",
      "  Downloading tensorboard_plugin_profile-2.13.0-py3-none-any.whl (5.4 MB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard-plugin-profile) (2.2.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard-plugin-profile) (68.0.0)\n",
      "Collecting gviz-api>=1.9.0\n",
      "  Downloading gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard-plugin-profile) (3.19.6)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/python3.7/lib/python3.7/site-packages (from tensorboard-plugin-profile) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/python3.7/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard-plugin-profile) (2.1.3)\n",
      "Installing collected packages: gviz-api, tensorboard-plugin-profile\n",
      "Successfully installed gviz-api-1.10.0 tensorboard-plugin-profile-2.13.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/opt/python3.7/bin/python3.7 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 338e05fdc093\n",
      " ---> c659e4014367\n",
      "Step 9/16 : RUN pip3 install tensorboard-plugin-wit\n",
      " ---> Running in 41e364508fc6\n",
      "Requirement already satisfied: tensorboard-plugin-wit in /opt/python3.7/lib/python3.7/site-packages (1.8.1)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/opt/python3.7/bin/python3.7 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 41e364508fc6\n",
      " ---> bd4395d85876\n",
      "Step 10/16 : RUN pip3 install tensorboard-data-server\n",
      " ---> Running in 04149d2091f7\n",
      "Requirement already satisfied: tensorboard-data-server in /opt/python3.7/lib/python3.7/site-packages (0.6.1)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/opt/python3.7/bin/python3.7 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 04149d2091f7\n",
      " ---> e0c743cbce06\n",
      "Step 11/16 : RUN pip3 install tensorflow-io\n",
      " ---> Running in 9e32a9522089\n",
      "Collecting tensorflow-io\n",
      "  Downloading tensorflow_io-0.32.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (28.0 MB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.32.0 in /opt/python3.7/lib/python3.7/site-packages (from tensorflow-io) (0.32.0)\n",
      "Installing collected packages: tensorflow-io\n",
      "Successfully installed tensorflow-io-0.32.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/opt/python3.7/bin/python3.7 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 9e32a9522089\n",
      " ---> d06d4e9d0d41\n",
      "Step 12/16 : RUN pip3 install tf-agents==0.16.0\n",
      " ---> Running in e56133d7d07c\n",
      "Collecting tf-agents==0.16.0\n",
      "  Downloading tf_agents-0.16.0-py3-none-any.whl (1.4 MB)\n",
      "Collecting gym<=0.23.0,>=0.17.0\n",
      "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/python3.7/lib/python3.7/site-packages (from tf-agents==0.16.0) (4.7.1)\n",
      "Collecting gin-config>=0.4.0\n",
      "  Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/python3.7/lib/python3.7/site-packages (from tf-agents==0.16.0) (1.21.6)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /opt/python3.7/lib/python3.7/site-packages (from tf-agents==0.16.0) (1.4.0)\n",
      "Collecting pygame==2.1.3\n",
      "  Downloading pygame-2.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
      "Collecting tensorflow-probability~=0.19.0\n",
      "  Downloading tensorflow_probability-0.19.0-py2.py3-none-any.whl (6.7 MB)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/python3.7/lib/python3.7/site-packages (from tf-agents==0.16.0) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.11.3 in /opt/python3.7/lib/python3.7/site-packages (from tf-agents==0.16.0) (3.19.6)\n",
      "Collecting pillow\n",
      "  Downloading Pillow-9.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Collecting cloudpickle>=1.3\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/python3.7/lib/python3.7/site-packages (from tf-agents==0.16.0) (1.15.0)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in /opt/python3.7/lib/python3.7/site-packages (from gym<=0.23.0,>=0.17.0->tf-agents==0.16.0) (6.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/python3.7/lib/python3.7/site-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents==0.16.0) (3.15.0)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\n",
      "Collecting decorator\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: gast>=0.3.2 in /opt/python3.7/lib/python3.7/site-packages (from tensorflow-probability~=0.19.0->tf-agents==0.16.0) (0.4.0)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (PEP 517): started\n",
      "  Building wheel for gym (PEP 517): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697642 sha256=bf25d9f19b4d757452cf6b343dd7fb34d7816007f33836cae10642e662efb6e8\n",
      "  Stored in directory: /root/.cache/pip/wheels/f0/7e/16/4d727df048fdb96518ec5c02266e55b98bc398837353852a6a\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, dm-tree, decorator, cloudpickle, tensorflow-probability, pygame, pillow, gym, gin-config, tf-agents\n",
      "Successfully installed cloudpickle-2.2.1 decorator-5.1.1 dm-tree-0.1.8 gin-config-0.5.0 gym-0.23.0 gym-notices-0.0.8 pillow-9.5.0 pygame-2.1.3 tensorflow-probability-0.19.0 tf-agents-0.16.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/opt/python3.7/bin/python3.7 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container e56133d7d07c\n",
      " ---> df1c6844b453\n",
      "Step 13/16 : RUN pip3 install matplotlib\n",
      " ---> Running in 6b2e97df3434\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "Collecting pyparsing>=2.2.1\n",
      "  Downloading pyparsing-3.1.0-py3-none-any.whl (102 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/python3.7/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python3.7/lib/python3.7/site-packages (from matplotlib) (1.21.6)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/python3.7/lib/python3.7/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/python3.7/lib/python3.7/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/python3.7/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (4.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/python3.7/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, matplotlib\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mSuccessfully installed cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.4 matplotlib-3.5.3 pyparsing-3.1.0\n",
      "\u001b[91mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/opt/python3.7/bin/python3.7 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 6b2e97df3434\n",
      " ---> b3dd0d499b65\n",
      "Step 14/16 : RUN pip3 install urllib3\n",
      " ---> Running in a48465f27624\n",
      "Requirement already satisfied: urllib3 in /opt/python3.7/lib/python3.7/site-packages (1.26.16)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/opt/python3.7/bin/python3.7 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container a48465f27624\n",
      " ---> c4cc1a7a89d0\n",
      "Step 15/16 : COPY src/training /root/src/training\n",
      " ---> 732d0fe51c65\n",
      "Step 16/16 : ENTRYPOINT [\"python3\", \"-m\", \"src.training.task\"]\n",
      " ---> Running in fe4de4ae67a8\n",
      "Removing intermediate container fe4de4ae67a8\n",
      " ---> 10a578bba042\n",
      "Successfully built 10a578bba042\n",
      "Successfully tagged gcr.io/hybrid-vertex/hptuning-training-custom-container:latest\n",
      "PUSH\n",
      "Pushing gcr.io/hybrid-vertex/hptuning-training-custom-container\n",
      "The push refers to repository [gcr.io/hybrid-vertex/hptuning-training-custom-container]\n",
      "54abacd9663d: Preparing\n",
      "41934c19d7f2: Preparing\n",
      "90cdd7943790: Preparing\n",
      "ea9278e0ca2c: Preparing\n",
      "06b13187ea57: Preparing\n",
      "6099b2617a51: Preparing\n",
      "eab5b55eff1a: Preparing\n",
      "2ade96a9d2c2: Preparing\n",
      "60f77d54aa03: Preparing\n",
      "fa270f848875: Preparing\n",
      "2e6576a7c5e4: Preparing\n",
      "41ea16034b07: Preparing\n",
      "cf5fbdabd9b6: Preparing\n",
      "087d7553d285: Preparing\n",
      "16919ab89eca: Preparing\n",
      "74bcef7f7402: Preparing\n",
      "bc9e931c388e: Preparing\n",
      "20896f2c3dd8: Preparing\n",
      "7b80c69caf34: Preparing\n",
      "3bbec54fac0c: Preparing\n",
      "4006ffa4c683: Preparing\n",
      "844d958e8cbe: Preparing\n",
      "84ff92691f90: Preparing\n",
      "b49bce339f97: Preparing\n",
      "dcb7197db903: Preparing\n",
      "2ade96a9d2c2: Waiting\n",
      "60f77d54aa03: Waiting\n",
      "fa270f848875: Waiting\n",
      "74bcef7f7402: Waiting\n",
      "2e6576a7c5e4: Waiting\n",
      "bc9e931c388e: Waiting\n",
      "20896f2c3dd8: Waiting\n",
      "41ea16034b07: Waiting\n",
      "7b80c69caf34: Waiting\n",
      "eab5b55eff1a: Waiting\n",
      "cf5fbdabd9b6: Waiting\n",
      "087d7553d285: Waiting\n",
      "6099b2617a51: Waiting\n",
      "3bbec54fac0c: Waiting\n",
      "b49bce339f97: Waiting\n",
      "dcb7197db903: Waiting\n",
      "16919ab89eca: Waiting\n",
      "4006ffa4c683: Waiting\n",
      "844d958e8cbe: Waiting\n",
      "84ff92691f90: Waiting\n",
      "54abacd9663d: Pushed\n",
      "41934c19d7f2: Pushed\n",
      "6099b2617a51: Pushed\n",
      "eab5b55eff1a: Pushed\n",
      "60f77d54aa03: Pushed\n",
      "90cdd7943790: Pushed\n",
      "2ade96a9d2c2: Pushed\n",
      "41ea16034b07: Pushed\n",
      "06b13187ea57: Pushed\n",
      "087d7553d285: Layer already exists\n",
      "cf5fbdabd9b6: Pushed\n",
      "16919ab89eca: Layer already exists\n",
      "74bcef7f7402: Layer already exists\n",
      "bc9e931c388e: Layer already exists\n",
      "20896f2c3dd8: Layer already exists\n",
      "7b80c69caf34: Layer already exists\n",
      "3bbec54fac0c: Layer already exists\n",
      "4006ffa4c683: Layer already exists\n",
      "844d958e8cbe: Layer already exists\n",
      "b49bce339f97: Layer already exists\n",
      "84ff92691f90: Layer already exists\n",
      "ea9278e0ca2c: Pushed\n",
      "dcb7197db903: Layer already exists\n",
      "2e6576a7c5e4: Pushed\n",
      "fa270f848875: Pushed\n",
      "latest: digest: sha256:7dbe6ce2aeaa75c7df83600038956ac59b8c0eb973636860518ba0db92bd95fc size: 5558\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                                             STATUS\n",
      "fb220090-58d2-4aa9-8c75-a489b22495af  2023-07-04T17:53:58+00:00  4M17S     gs://hybrid-vertex_cloudbuild/source/1688493237.698893-3c143636e1724055b1fb940d8e81d62c.tgz  gcr.io/hybrid-vertex/hptuning-training-custom-container (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# ! gcloud builds submit --config cloudbuild.yaml\n",
    "\n",
    "! gcloud builds submit --config cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION,_ARTIFACTS_DIR=$ARTIFACTS_DIR \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307bb759-0a14-4746-8d81-f846e318b1ed",
   "metadata": {},
   "source": [
    "## Submit hyperparameter tuning job [optional]\n",
    "* Submit a hyperparameter training job with the custom container. Read more details for using Python packages as an alternative to using custom containers in the example shown [here](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#create)\n",
    "* Define the hyperparameter(s), max trial count, parallel trial count, parameter search algorithm, machine spec, accelerators, worker pool, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b8fcea2-db92-42ec-98ab-5f42d91da2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPTUNING_RESULT_PATH: hptuning/result.json\n"
     ]
    }
   ],
   "source": [
    "# Execute hyperparameter tuning instead of regular training\n",
    "RUN_HYPERPARAMETER_TUNING          = True\n",
    "TRAIN_WITH_BEST_HYPERPARAMETERS    = False  # Do not train.\n",
    "\n",
    "# Directory to store the best hyperparameter(s) in `BUCKET_NAME` and locally (temporarily)\n",
    "HPTUNING_RESULT_DIR                = \"hptuning/\"\n",
    "\n",
    "# Path to the file containing the best hyperparameter(s).\n",
    "HPTUNING_RESULT_PATH               = os.path.join(HPTUNING_RESULT_DIR, \"result.json\")\n",
    "\n",
    "print(f\"HPTUNING_RESULT_PATH: {HPTUNING_RESULT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd35da25-ef51-4fe3-8676-62d72af83a1b",
   "metadata": {},
   "source": [
    "### Vertex Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b8e1f6a-4f0f-4b36-9a4e-bc63c1aa8e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : build-local-hpt\n",
      "RUN_NAME          : run-20230704-175916\n",
      "LOG_DIR           : gs://tabv2-hybrid-vertex-bucket/build-local-hpt/run-20230704-175916/tb-logs\n",
      "ROOT_DIR          : gs://tabv2-hybrid-vertex-bucket/build-local-hpt/run-20230704-175916/root\n",
      "ARTIFACTS_DIR     : gs://tabv2-hybrid-vertex-bucket/build-local-hpt/run-20230704-175916/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'build-local-hpt'\n",
    "\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "LOG_DIR           = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/tb-logs\"\n",
    "ROOT_DIR          = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e30e962-435a-42d8-aefc-82c7cd2d3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID\n",
    "    , location=REGION\n",
    "    , staging_bucket=BUCKET_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4bc165-fd04-4b45-9e8a-a43702ffe9b6",
   "metadata": {},
   "source": [
    "### helper function: create training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24a06982-6061-411b-8131-3ee3ef4373ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hyperparameter_tuning_job_sample(\n",
    "    project: str\n",
    "    , display_name: str\n",
    "    , image_uri: str\n",
    "    , args: List[str]\n",
    "    , max_trial_count: int\n",
    "    , parallel_trial_count: int\n",
    "    , location: str = \"us-central1\"\n",
    "    , api_endpoint: str = \"us-central1-aiplatform.googleapis.com\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a hyperparameter tuning job using a custom container.\n",
    "\n",
    "    Args:\n",
    "        project: GCP project ID.\n",
    "        display_name: GCP console display name for the hyperparameter tuning job in\n",
    "            Vertex AI.\n",
    "        image_uri: URI to the hyperparameter tuning container image in Container\n",
    "            Registry.\n",
    "        args: Arguments passed to the container.\n",
    "        location: Service location.\n",
    "        api_endpoint: API endpoint, eg. `<location>-aiplatform.googleapis.com`.\n",
    "\n",
    "    Returns:\n",
    "        A string of the hyperparameter tuning job ID.\n",
    "    \"\"\"\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    \n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "\n",
    "    # ====================================================\n",
    "    # study_spec\n",
    "    # ====================================================\n",
    "    # Metric based on which to evaluate which combination of hyperparameter(s) to choose\n",
    "    metric = {\n",
    "        \"metric_id\": \"final_average_return\"  # Metric you report to Vertex AI.\n",
    "        , \"goal\": aiplatform.gapic.StudySpec.MetricSpec.GoalType.MAXIMIZE,\n",
    "    }\n",
    "\n",
    "    # ====================================================\n",
    "    # Hyperparameter(s) to tune\n",
    "    # ====================================================\n",
    "    training_loops = {\n",
    "        \"parameter_id\": \"training-loops\"\n",
    "        , \"discrete_value_spec\": {\"values\": [4, 16]}\n",
    "        , \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE\n",
    "    }\n",
    "    steps_per_loop = {\n",
    "        \"parameter_id\": \"steps-per-loop\"\n",
    "        , \"discrete_value_spec\": {\"values\": [1, 2]}\n",
    "        , \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE\n",
    "    }\n",
    "\n",
    "    # ====================================================\n",
    "    # worker_pool_spec\n",
    "    # ====================================================\n",
    "    machine_spec = {\n",
    "        \"machine_type\": \"n1-standard-32\"\n",
    "        , \"accelerator_type\": aiplatform.gapic.AcceleratorType.ACCELERATOR_TYPE_UNSPECIFIED\n",
    "        , \"accelerator_count\": None\n",
    "    }\n",
    "    worker_pool_spec = {\n",
    "        \"machine_spec\": machine_spec\n",
    "        , \"replica_count\": 1\n",
    "        , \"container_spec\": {\n",
    "            \"image_uri\": image_uri\n",
    "            , \"args\": args\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # ====================================================\n",
    "    # hyperparameter_tuning_job\n",
    "    # ====================================================\n",
    "    hyperparameter_tuning_job = {\n",
    "        \"display_name\": display_name\n",
    "        , \"max_trial_count\": max_trial_count\n",
    "        , \"parallel_trial_count\": parallel_trial_count\n",
    "        , \"study_spec\": {\n",
    "            \"metrics\": [metric]\n",
    "            , \"parameters\": [training_loops, steps_per_loop]\n",
    "            , \"algorithm\": aiplatform.gapic.StudySpec.Algorithm.RANDOM_SEARCH\n",
    "        }\n",
    "        , \"trial_job_spec\": {\"worker_pool_specs\": [worker_pool_spec]}\n",
    "    }\n",
    "    parent = f\"projects/{project}/locations/{location}\"\n",
    "\n",
    "    # ====================================================\n",
    "    # Create job via client\n",
    "    # ====================================================\n",
    "    response = client.create_hyperparameter_tuning_job(\n",
    "        parent=parent\n",
    "        , hyperparameter_tuning_job=hyperparameter_tuning_job\n",
    "    )\n",
    "    job_id = response.name.split(\"/\")[-1]\n",
    "    print(\"Job ID:\", job_id)\n",
    "    print(\"Job config:\", response)\n",
    "    \n",
    "#     # ====================================================\n",
    "#     # Create job via SDK\n",
    "#     # ====================================================\n",
    "#     metric_spec = {\"final_average_return\": \"maximize\"}\n",
    "    \n",
    "#     parameter_spec = {\n",
    "#         \"training-loops\": hpt.DiscreteParameterSpec(values=[4, 16], scale=\"linear\")\n",
    "#         , \"steps-per-loop\": hpt.DiscreteParameterSpec(values=[1, 2], scale=\"linear\")\n",
    "#     }\n",
    "#     my_custom_job = aiplatform.CustomJob(\n",
    "#         display_name=display_name\n",
    "#         , worker_pool_specs=worker_pool_spec\n",
    "#         , staging_bucket=ROOT_DIR\n",
    "#     )\n",
    "    \n",
    "#     hp_job = aiplatform.HyperparameterTuningJob(\n",
    "#         display_name=display_name\n",
    "#         , custom_job=my_custom_job\n",
    "#         , metric_spec=metric_spec\n",
    "#         , parameter_spec=parameter_spec\n",
    "#         , max_trial_count=hyperparameter_tuning_job[\"max_trial_count\"]\n",
    "#         , parallel_trial_count=hyperparameter_tuning_job[\"parallel_trial_count\"]\n",
    "#     )\n",
    "\n",
    "#     hp_job.run(sync=False)\n",
    "\n",
    "    return job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9943e85e-a149-439d-ba74-96a2492e2e3a",
   "metadata": {},
   "source": [
    "### set training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bf22fa7-bf80-4748-8d94-8ef06b41fac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE       : 8\n",
      "TRAINING_LOOPS   : 5\n",
      "STEPS_PER_LOOP   : 2\n",
      "RANK_K           : 20\n",
      "NUM_ACTIONS      : 20\n",
      "PER_ARM          : False\n",
      "TIKHONOV_WEIGHT  : 0.001\n",
      "AGENT_ALPHA      : 10.0\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters.\n",
    "BATCH_SIZE       = 8      # Training and prediction batch size.\n",
    "TRAINING_LOOPS   = 5      # Number of training iterations.\n",
    "STEPS_PER_LOOP   = 2      # Number of driver steps per training iteration.\n",
    "\n",
    "# Set MovieLens simulation environment parameters.\n",
    "RANK_K           = 20     # Rank for matrix factorization in the MovieLens environment; also the observation dimension.\n",
    "NUM_ACTIONS      = 20     # Number of actions (movie items) to choose from.\n",
    "PER_ARM          = False  # Use the non-per-arm version of the MovieLens environment.\n",
    "\n",
    "# Set agent parameters.\n",
    "TIKHONOV_WEIGHT  = 0.001   # LinUCB Tikhonov regularization weight.\n",
    "AGENT_ALPHA      = 10.0    # LinUCB exploration parameter that multiplies the confidence intervals.\n",
    "\n",
    "print(f\"BATCH_SIZE       : {BATCH_SIZE}\")\n",
    "print(f\"TRAINING_LOOPS   : {TRAINING_LOOPS}\")\n",
    "print(f\"STEPS_PER_LOOP   : {STEPS_PER_LOOP}\")\n",
    "print(f\"RANK_K           : {RANK_K}\")\n",
    "print(f\"NUM_ACTIONS      : {NUM_ACTIONS}\")\n",
    "print(f\"PER_ARM          : {PER_ARM}\")\n",
    "print(f\"TIKHONOV_WEIGHT  : {TIKHONOV_WEIGHT}\")\n",
    "print(f\"AGENT_ALPHA      : {AGENT_ALPHA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7e44014-daa3-4e15-9137-8de3e7517288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"args: ['--data-path=gs://tabv2-hybrid-vertex-bucket/artifacts/u.data', \"\n",
      " \"'--batch-size=8', '--rank-k=20', '--num-actions=20', \"\n",
      " \"'--tikhonov-weight=0.001', '--agent-alpha=10.0', '--training-loops=5', \"\n",
      " \"'--steps-per-loop=2', '--run-hyperparameter-tuning']\")\n"
     ]
    }
   ],
   "source": [
    "args = [\n",
    "    f\"--data-path={DATA_PATH}\"\n",
    "    , f\"--batch-size={BATCH_SIZE}\"\n",
    "    , f\"--rank-k={RANK_K}\"\n",
    "    , f\"--num-actions={NUM_ACTIONS}\"\n",
    "    , f\"--tikhonov-weight={TIKHONOV_WEIGHT}\"\n",
    "    , f\"--agent-alpha={AGENT_ALPHA}\"\n",
    "    , f\"--training-loops={TRAINING_LOOPS}\"\n",
    "    , f\"--steps-per-loop={STEPS_PER_LOOP}\"\n",
    "]\n",
    "\n",
    "if RUN_HYPERPARAMETER_TUNING:\n",
    "    args.append(\"--run-hyperparameter-tuning\")\n",
    "    \n",
    "elif TRAIN_WITH_BEST_HYPERPARAMETERS:\n",
    "    args.append(\"--train-with-best-hyperparameters\")\n",
    "    \n",
    "from pprint import pprint\n",
    "pprint(f\"args: {args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77da3fea-4c70-4b85-8135-47d90b5484aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 7805303371929223168\n",
      "Job config: name: \"projects/934903580331/locations/us-central1/hyperparameterTuningJobs/7805303371929223168\"\n",
      "display_name: \"movielens-hp-tuning-job\"\n",
      "study_spec {\n",
      "  metrics {\n",
      "    metric_id: \"final_average_return\"\n",
      "    goal: MAXIMIZE\n",
      "  }\n",
      "  parameters {\n",
      "    parameter_id: \"training-loops\"\n",
      "    discrete_value_spec {\n",
      "      values: 4.0\n",
      "      values: 16.0\n",
      "    }\n",
      "    scale_type: UNIT_LINEAR_SCALE\n",
      "  }\n",
      "  parameters {\n",
      "    parameter_id: \"steps-per-loop\"\n",
      "    discrete_value_spec {\n",
      "      values: 1.0\n",
      "      values: 2.0\n",
      "    }\n",
      "    scale_type: UNIT_LINEAR_SCALE\n",
      "  }\n",
      "  algorithm: RANDOM_SEARCH\n",
      "}\n",
      "max_trial_count: 4\n",
      "parallel_trial_count: 2\n",
      "trial_job_spec {\n",
      "  worker_pool_specs {\n",
      "    machine_spec {\n",
      "      machine_type: \"n1-standard-32\"\n",
      "    }\n",
      "    replica_count: 1\n",
      "    disk_spec {\n",
      "      boot_disk_type: \"pd-ssd\"\n",
      "      boot_disk_size_gb: 100\n",
      "    }\n",
      "    container_spec {\n",
      "      image_uri: \"gcr.io/hybrid-vertex/hptuning-training-custom-container:latest\"\n",
      "      args: \"--data-path=gs://tabv2-hybrid-vertex-bucket/artifacts/u.data\"\n",
      "      args: \"--batch-size=8\"\n",
      "      args: \"--rank-k=20\"\n",
      "      args: \"--num-actions=20\"\n",
      "      args: \"--tikhonov-weight=0.001\"\n",
      "      args: \"--agent-alpha=10.0\"\n",
      "      args: \"--training-loops=5\"\n",
      "      args: \"--steps-per-loop=2\"\n",
      "      args: \"--run-hyperparameter-tuning\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "state: JOB_STATE_PENDING\n",
      "create_time {\n",
      "  seconds: 1688493593\n",
      "  nanos: 570007000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1688493593\n",
      "  nanos: 570007000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'7805303371929223168'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_id = create_hyperparameter_tuning_job_sample(\n",
    "    project=PROJECT_ID\n",
    "    , display_name=\"movielens-hp-tuning-job\"\n",
    "    , image_uri=f\"gcr.io/{PROJECT_ID}/{HPTUNING_TRAINING_CONTAINER}:latest\"\n",
    "    , args=args\n",
    "    , max_trial_count=4\n",
    "    , parallel_trial_count = 2\n",
    "    , location=REGION\n",
    "    , api_endpoint=f\"{REGION}-aiplatform.googleapis.com\"\n",
    ")\n",
    "\n",
    "job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a471d3-ab80-437c-a35f-b523fb2ab545",
   "metadata": {},
   "source": [
    "#### Check hyperparameter tuning job status\n",
    "* Read more about managing jobs [here](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#manage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "349823d9-3822-47f0-b2bd-0813c42f5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameter_tuning_job_sample(\n",
    "    project: str,\n",
    "    hyperparameter_tuning_job_id: str,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    ") -> aiplatform.HyperparameterTuningJob:\n",
    "    \"\"\"\n",
    "    Gets the current status of a hyperparameter tuning job.\n",
    "\n",
    "    Args:\n",
    "        project: GCP project ID.\n",
    "        hyperparameter_tuning_job_id: Hyperparameter tuning job ID.\n",
    "        location: Service location.\n",
    "        api_endpoint: API endpoint, eg. `-aiplatform.googleapis.com`.\n",
    "\n",
    "    Returns:\n",
    "        Details of the hyperparameter tuning job, such as its running status,\n",
    "        results of its trials, etc.\n",
    "    \"\"\"\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    \n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "    \n",
    "    name = client.hyperparameter_tuning_job_path(\n",
    "        project=project\n",
    "        , location=location\n",
    "        , hyperparameter_tuning_job=hyperparameter_tuning_job_id\n",
    "    )\n",
    "    \n",
    "    response = client.get_hyperparameter_tuning_job(name=name)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bb83155-4edb-4424-b28e-a801771b0e38",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current job status: JOB_STATE_RUNNING.\n",
      "Current job status: JOB_STATE_RUNNING.\n",
      "Current job status: JOB_STATE_RUNNING.\n",
      "Current job status: JOB_STATE_RUNNING.\n",
      "Current job status: JOB_STATE_RUNNING.\n",
      "Current job status: JOB_STATE_RUNNING.\n",
      "Current job status: JOB_STATE_RUNNING.\n",
      "Current job status: JOB_STATE_RUNNING.\n",
      "Current job status: JOB_STATE_RUNNING.\n",
      "Current job status: JOB_STATE_RUNNING.\n",
      "Current job status: JOB_STATE_RUNNING.\n",
      "Job succeeded.\n",
      "Job Time: 0:16:06.651853\n",
      "Trials: [id: \"1\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"steps-per-loop\"\n",
      "  value {\n",
      "    number_value: 2.0\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"training-loops\"\n",
      "  value {\n",
      "    number_value: 4.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"final_average_return\"\n",
      "    value: 1.399999976158142\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1688493604\n",
      "  nanos: 286909341\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1688493714\n",
      "}\n",
      ", id: \"2\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"steps-per-loop\"\n",
      "  value {\n",
      "    number_value: 2.0\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"training-loops\"\n",
      "  value {\n",
      "    number_value: 16.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"final_average_return\"\n",
      "    value: 0.5\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1688493604\n",
      "  nanos: 286990603\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1688493715\n",
      "}\n",
      ", id: \"3\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"steps-per-loop\"\n",
      "  value {\n",
      "    number_value: 1.0\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"training-loops\"\n",
      "  value {\n",
      "    number_value: 4.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"final_average_return\"\n",
      "    value: 0.5\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1688494091\n",
      "  nanos: 682720332\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1688494205\n",
      "}\n",
      ", id: \"4\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"steps-per-loop\"\n",
      "  value {\n",
      "    number_value: 2.0\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"training-loops\"\n",
      "  value {\n",
      "    number_value: 16.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"final_average_return\"\n",
      "    value: 1.570489187339237e-15\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1688494091\n",
      "  nanos: 682835153\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1688494199\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "trials = None\n",
    "\n",
    "while True:\n",
    "    response = get_hyperparameter_tuning_job_sample(\n",
    "        project=PROJECT_ID\n",
    "        , hyperparameter_tuning_job_id=job_id\n",
    "        , location=REGION\n",
    "        , api_endpoint=f\"{REGION}-aiplatform.googleapis.com\"\n",
    "    )\n",
    "    \n",
    "    if response.state.name == 'JOB_STATE_SUCCEEDED':\n",
    "        print(\"Job succeeded.\\nJob Time:\", response.update_time - response.create_time)\n",
    "        trials = response.trials\n",
    "        print(\"Trials:\", trials)\n",
    "        break\n",
    "    elif response.state.name == \"JOB_STATE_FAILED\":\n",
    "        print(\"Job failed.\")\n",
    "        break\n",
    "    elif response.state.name == \"JOB_STATE_CANCELLED\":\n",
    "        print(\"Job cancelled.\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Current job status: {response.state.name}.\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a06fed-f0e3-4136-8e11-913455cf0485",
   "metadata": {},
   "source": [
    "#### Find the best combination(s) hyperparameter(s) for each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f47b4ad-8a43-4ca3-b78b-d24d23ccc20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter value(s):\n",
      "Metric=final_average_return: [{'steps-per-loop': 2.0, 'training-loops': 16.0}]\n"
     ]
    }
   ],
   "source": [
    "if trials:\n",
    "    # Dict mapping from metric names to the best metric values seen so far\n",
    "    best_objective_values = dict.fromkeys(\n",
    "        [metric.metric_id for metric in trials[0].final_measurement.metrics]\n",
    "        , -np.inf\n",
    "    )\n",
    "    # Dict mapping from metric names to a list of the best combination(s) of\n",
    "    # hyperparameter(s). Each combination is a dict mapping from hyperparameter\n",
    "    # names to their values.\n",
    "    best_params = defaultdict(list)\n",
    "    for trial in trials:\n",
    "        # `final_measurement` and `parameters` are `RepeatedComposite` objects.\n",
    "        # Reference the structure above to extract the value of your interest.\n",
    "        for metric in trial.final_measurement.metrics:\n",
    "            params = {\n",
    "                param.parameter_id: param.value for param in trial.parameters\n",
    "            }\n",
    "            if metric.value > best_objective_values[metric.metric_id]:\n",
    "                best_params[metric.metric_id] = [params]\n",
    "            elif metric.value == best_objective_values[metric.metric_id]:\n",
    "                best_params[param.parameter_id].append(params)  # Handle cases where multiple hyperparameter values lead to the same performance.\n",
    "    print(\"Best hyperparameter value(s):\")\n",
    "    for metric, params in best_params.items():\n",
    "        print(f\"Metric={metric}: {sorted(params)}\")\n",
    "else:\n",
    "    print(\"No hyperparameter tuning job trials found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f615339b-fd88-4239-9c67-27157ade7ea6",
   "metadata": {},
   "source": [
    "#### Convert a combination of best hyperparameter(s) for a metric of interest to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0574bda0-f1f9-438f-b4b9-67754a769bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf $HPTUNING_RESULT_DIR\n",
    "! mkdir $HPTUNING_RESULT_DIR\n",
    "\n",
    "with open(HPTUNING_RESULT_PATH, \"w\") as f:\n",
    "    json.dump(best_params[\"final_average_return\"][0], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a63794-61bb-415a-b08d-aebbb35abeb2",
   "metadata": {},
   "source": [
    "#### Upload the best hyperparameter(s) to GCS for use in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4453f578-85de-4e4d-b0ec-0be1bd89be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(HPTUNING_RESULT_PATH)\n",
    "blob.upload_from_filename(HPTUNING_RESULT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc06d414-96dc-4f00-a9f9-f66f1e00669c",
   "metadata": {},
   "source": [
    "# Create custom prediction container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19025cc9-4b13-4c87-a138-395da7d92388",
   "metadata": {},
   "source": [
    "As with training, create a custom prediction container. This container handles the TF-Agents specific logic that is different from a regular TensorFlow Model. Specifically, it finds the predicted action using a trained policy. The associated source code is in `src/prediction/`.\n",
    "See other options for Vertex AI predictions [here](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions).\n",
    "\n",
    "#### Serve predictions:\n",
    "- Use [`tensorflow.saved_model.load`](https://www.tensorflow.org/agents/api_docs/python/tf_agents/policies/PolicySaver#usage), instead of [`tf_agents.policies.policy_loader.load`](https://github.com/tensorflow/agents/blob/r0.8.0/tf_agents/policies/policy_loader.py#L26), to load the trained policy, because the latter produces an object of type [`SavedModelPyTFEagerPolicy`](https://github.com/tensorflow/agents/blob/402b8aa81ca1b578ec1f687725d4ccb4115386d2/tf_agents/policies/py_tf_eager_policy.py#L137) whose `action()` is not compatible for use here.\n",
    "- Note that prediction requests contain only observation data but not reward. This is because: The prediction task is a standalone request that doesn't require prior knowledge of the system state. Meanwhile, end users only know what they observe at the moment. Reward is a piece of information that comes after the action has been made, so the end users would not have knowledge of said reward. In handling prediction requests, you create a [`TimeStep`](https://www.tensorflow.org/agents/api_docs/python/tf_agents/trajectories/TimeStep) object (consisting of `observation`, `reward`, `discount`, `step_type`) using the [`restart()`](https://www.tensorflow.org/agents/api_docs/python/tf_agents/trajectories/restart) function which takes in an `observation`. This function creates the *first* TimeStep in a trajectory of steps, where reward is 0, discount is 1 and step_type is marked as the first timestep. In other words, each prediction request forms the first `TimeStep` in a brand new trajectory.\n",
    "- For the prediction response, avoid using NumPy-typed values; instead, convert them to native Python values using methods such as [`tolist()`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.tolist.html) as opposed to `list()`.\n",
    "- There exists a prestart script in `src/prediction`. FastAPI executes this script before starting up the server. The `PORT` environment variable is set to equal `AIP_HTTP_PORT` in order to run FastAPI on the same port expected by Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57f6f6b1-cc85-44f5-94b3-4d6ee802a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_SUBFOLDER = 'prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c454ef00-550a-4d79-84c0-e5922577b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{PRED_SUBFOLDER}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{PRED_SUBFOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55c5c86a-0faa-4b9e-a70d-e47c8a104af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/prediction/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PRED_SUBFOLDER}/main.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Prediction server that uses a trained policy to give predicted actions.\"\"\"\n",
    "import os\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi import Request\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_agents\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "_model = tf.compat.v2.saved_model.load(os.environ[\"AIP_STORAGE_URI\"])\n",
    "\n",
    "\n",
    "@app.get(os.environ[\"AIP_HEALTH_ROUTE\"], status_code=200)\n",
    "def health():\n",
    "    \"\"\"\n",
    "    Handles server health check requests.\n",
    "\n",
    "    Returns:\n",
    "      An empty dict.\n",
    "    \"\"\"\n",
    "    return {}\n",
    "\n",
    "\n",
    "@app.post(os.environ[\"AIP_PREDICT_ROUTE\"])\n",
    "async def predict(request: Request):\n",
    "    \"\"\"\n",
    "    Handles prediction requests.\n",
    "\n",
    "    Unpacks observations in prediction requests and queries the trained policy for\n",
    "    predicted actions.\n",
    "\n",
    "    Args:\n",
    "      request: Incoming prediction requests that contain observations.\n",
    "\n",
    "    Returns:\n",
    "      A dict with the key `predictions` mapping to a list of predicted actions\n",
    "      corresponding to each observation in the prediction request.\n",
    "    \"\"\"\n",
    "    body = await request.json()\n",
    "    instances = body[\"instances\"]\n",
    "\n",
    "    predictions = []\n",
    "    for index, instance in enumerate(instances):\n",
    "        # Unpack request body and reconstruct TimeStep. Rewards default to 0.\n",
    "        batch_size = len(instance[\"observation\"])\n",
    "        \n",
    "        time_step = tf_agents.trajectories.restart(\n",
    "            observation=instance[\"observation\"]\n",
    "            , batch_size=tf.convert_to_tensor([batch_size])\n",
    "        )\n",
    "        policy_step = _model.action(time_step)\n",
    "\n",
    "        predictions.append(\n",
    "            {f\"PolicyStep {index}\": policy_step.action.numpy().tolist()}\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"predictions\": predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8e3a99d-a3c7-414c-a074-2d1c415218cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/prediction/prestart.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PRED_SUBFOLDER}/prestart.sh\n",
    "#!/bin/bash\n",
    "export PORT=$AIP_HTTP_PORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f52725ca-310b-4011-be73-37c63453a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloudbuild_yaml = \"\"\"steps:\n",
    "# - name: 'gcr.io/kaniko-project/executor:latest'\n",
    "#   args: ['--destination=gcr.io/{PROJECT_ID}/{PREDICTION_CONTAINER}:latest',\n",
    "#          '--cache=false',\n",
    "#          '--cache-ttl=99h']\n",
    "#   env: ['AIP_STORAGE_URI={ARTIFACTS_DIR}']\n",
    "# options:\n",
    "#   machineType: 'E2_HIGHCPU_8'\"\"\".format(\n",
    "#     PROJECT_ID=PROJECT_ID\n",
    "#     , PREDICTION_CONTAINER=PREDICTION_CONTAINER\n",
    "#     , ARTIFACTS_DIR=ARTIFACTS_DIR\n",
    "# )\n",
    "\n",
    "# with open(\"cloudbuild.yaml\", \"w\") as fp:\n",
    "#     fp.write(cloudbuild_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f28a8f5-24bb-428b-99f9-01815e522d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile cloudbuild.yaml\n",
    "\n",
    "# steps:\n",
    "# - name: 'gcr.io/cloud-builders/docker'\n",
    "#   args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile.$_DOCKERNAME']\n",
    "#   env: [f'AIP_STORAGE_URI={ARTIFACTS_DIR}']\n",
    "# images:\n",
    "# - '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b1a2fd-689d-441e-b309-b43ba36246f5",
   "metadata": {},
   "source": [
    "#### Define dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65838998-455d-4168-be00-54b04512b59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pred_requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile pred_requirements.txt\n",
    "\n",
    "numpy\n",
    "six\n",
    "typing-extensions~=4.6.3\n",
    "pillow\n",
    "tf-agents==0.16.0\n",
    "tensorflow==2.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f58356e-938d-4199-a7be-1a384d174c5a",
   "metadata": {},
   "source": [
    "#### Write a Dockerfile\n",
    "\n",
    "Note: leave the server directory `app`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cec9808f-cfe2-471f-826e-1b92f8696cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCKERNAME = 'pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe9e2c4b-5bf6-428a-bd32-196ecc30fc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile_pred\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile_{DOCKERNAME}\n",
    "\n",
    "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.10\n",
    "\n",
    "COPY src/prediction /app\n",
    "COPY pred_requirements.txt /app/requirements.txt\n",
    "\n",
    "RUN pip3 install -r /app/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba9f72-fcbb-4595-bc52-152cd3306ca3",
   "metadata": {},
   "source": [
    "#### Build the prediction container with Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "320eaaf5-b02a-4e02-88ed-6de388d75a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export DOCKERNAME=pred\n",
      "export IMAGE_URI=gcr.io/hybrid-vertex/prediction-custom-container\n",
      "export FILE_LOCATION=./\n",
      "export MACHINE_TYPE=e2-highcpu-32\n",
      "export ARTIFACTS_DIR=gs://tabv2-hybrid-vertex-bucket/build-local-hpt/run-20230704-175916/artifacts\n"
     ]
    }
   ],
   "source": [
    "PREDICTION_CONTAINER = \"prediction-custom-container\"\n",
    "\n",
    "# Docker definitions for training\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{PREDICTION_CONTAINER}'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './'\n",
    "\n",
    "print(f\"export DOCKERNAME={DOCKERNAME}\")\n",
    "print(f\"export IMAGE_URI={IMAGE_URI}\")\n",
    "print(f\"export FILE_LOCATION={FILE_LOCATION}\")\n",
    "print(f\"export MACHINE_TYPE={MACHINE_TYPE}\")\n",
    "print(f\"export ARTIFACTS_DIR={ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc09f920-69fb-4886-82ae-0e5b5fd20652",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 90 file(s) totalling 1.7 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://hybrid-vertex_cloudbuild/source/1688494704.164302-cc4a0e3eb5b741a9bc6e9a089022cc76.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/global/builds/03fc998d-8dee-426a-86f8-e1e5d98b8e99].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/03fc998d-8dee-426a-86f8-e1e5d98b8e99?project=934903580331 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"03fc998d-8dee-426a-86f8-e1e5d98b8e99\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1688494704.164302-cc4a0e3eb5b741a9bc6e9a089022cc76.tgz#1688494704640971\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1688494704.164302-cc4a0e3eb5b741a9bc6e9a089022cc76.tgz#1688494704640971...\n",
      "/ [1 files][584.2 KiB/584.2 KiB]                                                \n",
      "Operation completed over 1 objects/584.2 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  1.872MB\n",
      "Step 1/4 : FROM tiangolo/uvicorn-gunicorn-fastapi:python3.10\n",
      "python3.10: Pulling from tiangolo/uvicorn-gunicorn-fastapi\n",
      "bba7bb10d5ba: Already exists\n",
      "ec2b820b8e87: Already exists\n",
      "284f2345db05: Already exists\n",
      "fea23129f080: Already exists\n",
      "7c62c924b8a6: Already exists\n",
      "c48db0ed1df2: Pulling fs layer\n",
      "f614a567a403: Pulling fs layer\n",
      "00c5a00c6bc2: Pulling fs layer\n",
      "d7e169822811: Pulling fs layer\n",
      "57f5fe696490: Pulling fs layer\n",
      "5dfc4916ae9b: Pulling fs layer\n",
      "7aa93b4dad04: Pulling fs layer\n",
      "7a65e405b400: Pulling fs layer\n",
      "b4adc05b4c22: Pulling fs layer\n",
      "c3f059905ac6: Pulling fs layer\n",
      "c7f33bde337b: Pulling fs layer\n",
      "cfb5e2673c80: Pulling fs layer\n",
      "c1c864382177: Pulling fs layer\n",
      "ed32b5ed8af6: Pulling fs layer\n",
      "c3f059905ac6: Waiting\n",
      "57f5fe696490: Waiting\n",
      "c7f33bde337b: Waiting\n",
      "5dfc4916ae9b: Waiting\n",
      "cfb5e2673c80: Waiting\n",
      "7aa93b4dad04: Waiting\n",
      "c1c864382177: Waiting\n",
      "7a65e405b400: Waiting\n",
      "ed32b5ed8af6: Waiting\n",
      "b4adc05b4c22: Waiting\n",
      "d7e169822811: Waiting\n",
      "f614a567a403: Verifying Checksum\n",
      "f614a567a403: Download complete\n",
      "00c5a00c6bc2: Download complete\n",
      "d7e169822811: Verifying Checksum\n",
      "d7e169822811: Download complete\n",
      "5dfc4916ae9b: Verifying Checksum\n",
      "5dfc4916ae9b: Download complete\n",
      "c48db0ed1df2: Verifying Checksum\n",
      "c48db0ed1df2: Download complete\n",
      "7aa93b4dad04: Verifying Checksum\n",
      "7aa93b4dad04: Download complete\n",
      "7a65e405b400: Verifying Checksum\n",
      "7a65e405b400: Download complete\n",
      "57f5fe696490: Verifying Checksum\n",
      "57f5fe696490: Download complete\n",
      "b4adc05b4c22: Verifying Checksum\n",
      "b4adc05b4c22: Download complete\n",
      "c3f059905ac6: Verifying Checksum\n",
      "c3f059905ac6: Download complete\n",
      "c7f33bde337b: Verifying Checksum\n",
      "c7f33bde337b: Download complete\n",
      "ed32b5ed8af6: Verifying Checksum\n",
      "ed32b5ed8af6: Download complete\n",
      "c1c864382177: Verifying Checksum\n",
      "c1c864382177: Download complete\n",
      "cfb5e2673c80: Download complete\n",
      "c48db0ed1df2: Pull complete\n",
      "f614a567a403: Pull complete\n",
      "00c5a00c6bc2: Pull complete\n",
      "d7e169822811: Pull complete\n",
      "57f5fe696490: Pull complete\n",
      "5dfc4916ae9b: Pull complete\n",
      "7aa93b4dad04: Pull complete\n",
      "7a65e405b400: Pull complete\n",
      "b4adc05b4c22: Pull complete\n",
      "c3f059905ac6: Pull complete\n",
      "c7f33bde337b: Pull complete\n",
      "cfb5e2673c80: Pull complete\n",
      "c1c864382177: Pull complete\n",
      "ed32b5ed8af6: Pull complete\n",
      "Digest: sha256:8609f71c5df133343805afeabea9360673d32375e6c1c452d874415405a45668\n",
      "Status: Downloaded newer image for tiangolo/uvicorn-gunicorn-fastapi:python3.10\n",
      " ---> f6b8b7a55462\n",
      "Step 2/4 : COPY src/prediction /app\n",
      " ---> b518a6f3c8a3\n",
      "Step 3/4 : COPY pred_requirements.txt /app/requirements.txt\n",
      " ---> 298aefcd48ca\n",
      "Step 4/4 : RUN pip3 install -r /app/requirements.txt\n",
      " ---> Running in 033449908be3\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "      17.6/17.6 MB 81.0 MB/s eta 0:00:00\n",
      "Collecting six\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting typing-extensions~=4.6.3\n",
      "  Downloading typing_extensions-4.6.3-py3-none-any.whl (31 kB)\n",
      "Collecting pillow\n",
      "  Downloading Pillow-10.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "      3.4/3.4 MB 85.4 MB/s eta 0:00:00\n",
      "Collecting tf-agents==0.16.0\n",
      "  Downloading tf_agents-0.16.0-py3-none-any.whl (1.4 MB)\n",
      "      1.4/1.4 MB 73.3 MB/s eta 0:00:00\n",
      "Collecting tensorflow==2.11.0\n",
      "  Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "      588.3/588.3 MB 3.1 MB/s eta 0:00:00\n",
      "Collecting tensorflow-probability~=0.19.0\n",
      "  Downloading tensorflow_probability-0.19.0-py2.py3-none-any.whl (6.7 MB)\n",
      "      6.7/6.7 MB 81.7 MB/s eta 0:00:00\n",
      "Collecting gin-config>=0.4.0\n",
      "  Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "      61.3/61.3 kB 12.2 MB/s eta 0:00:00\n",
      "Collecting absl-py>=0.6.1\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "      126.5/126.5 kB 23.8 MB/s eta 0:00:00\n",
      "Collecting pygame==2.1.3\n",
      "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
      "      13.7/13.7 MB 82.3 MB/s eta 0:00:00\n",
      "Collecting cloudpickle>=1.3\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting gym<=0.23.0,>=0.17.0\n",
      "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
      "      624.4/624.4 kB 55.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "      78.4/78.4 kB 13.7 MB/s eta 0:00:00\n",
      "Collecting protobuf>=3.11.3\n",
      "  Downloading protobuf-4.23.3-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "      304.5/304.5 kB 41.3 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "      1.7/1.7 MB 81.7 MB/s eta 0:00:00\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "      2.4/2.4 MB 87.8 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "      65.5/65.5 kB 12.6 MB/s eta 0:00:00\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "      4.8/4.8 MB 93.3 MB/s eta 0:00:00\n",
      "Collecting protobuf>=3.11.3\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "      1.1/1.1 MB 76.4 MB/s eta 0:00:00\n",
      "Collecting packaging\n",
      "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
      "      48.9/48.9 kB 9.7 MB/s eta 0:00:00\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "      22.9/22.9 MB 80.0 MB/s eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "      57.5/57.5 kB 10.7 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "      6.0/6.0 MB 95.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow==2.11.0->-r /app/requirements.txt (line 7)) (65.5.1)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "      439.2/439.2 kB 47.1 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "      5.2/5.2 MB 81.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.11.0->-r /app/requirements.txt (line 7)) (0.40.0)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "      93.9/93.9 kB 18.5 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "      781.3/781.3 kB 61.0 MB/s eta 0:00:00\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "      62.6/62.6 kB 11.8 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "      4.9/4.9 MB 102.6 MB/s eta 0:00:00\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "      242.5/242.5 kB 36.7 MB/s eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.21.0-py2.py3-none-any.whl (182 kB)\n",
      "      182.1/182.1 kB 30.5 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting decorator\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "      152.8/152.8 kB 27.1 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "      181.3/181.3 kB 30.4 MB/s eta 0:00:00\n",
      "Collecting urllib3<2.0\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "      143.1/143.1 kB 27.6 MB/s eta 0:00:00\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
      "      199.3/199.3 kB 32.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r /app/requirements.txt (line 7)) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r /app/requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r /app/requirements.txt (line 7)) (2.1.3)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "      83.9/83.9 kB 16.4 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "      151.7/151.7 kB 25.6 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697642 sha256=922bf9ddd47c2ad02250215dea83afdfe06a14e127c17415af7fbb0dbee1622e\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
      "Successfully built gym\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, gym-notices, gin-config, flatbuffers, dm-tree, wrapt, werkzeug, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, pygame, pyasn1, protobuf, pillow, packaging, oauthlib, numpy, markdown, keras, grpcio, gast, decorator, cloudpickle, charset-normalizer, cachetools, absl-py, tensorflow-probability, rsa, requests, pyasn1-modules, opt-einsum, h5py, gym, google-pasta, astunparse, tf-agents, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 charset-normalizer-3.1.0 cloudpickle-2.2.1 decorator-5.1.1 dm-tree-0.1.8 flatbuffers-23.5.26 gast-0.4.0 gin-config-0.5.0 google-auth-2.21.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.56.0 gym-0.23.0 gym-notices-0.0.8 h5py-3.9.0 keras-2.11.0 libclang-16.0.0 markdown-3.4.3 numpy-1.25.0 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.1 pillow-10.0.0 protobuf-3.19.6 pyasn1-0.5.0 pyasn1-modules-0.3.0 pygame-2.1.3 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 six-1.16.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.32.0 tensorflow-probability-0.19.0 termcolor-2.3.0 tf-agents-0.16.0 typing-extensions-4.6.3 urllib3-1.26.16 werkzeug-2.3.6 wrapt-1.15.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 033449908be3\n",
      " ---> 68f23864ad4c\n",
      "Successfully built 68f23864ad4c\n",
      "Successfully tagged gcr.io/hybrid-vertex/prediction-custom-container:latest\n",
      "PUSH\n",
      "Pushing gcr.io/hybrid-vertex/prediction-custom-container\n",
      "The push refers to repository [gcr.io/hybrid-vertex/prediction-custom-container]\n",
      "2d0bb4911102: Preparing\n",
      "6594212d009d: Preparing\n",
      "883e773e9a21: Preparing\n",
      "9a48496d4218: Preparing\n",
      "63005d52920c: Preparing\n",
      "a058891c99b5: Preparing\n",
      "bfefd28d7932: Preparing\n",
      "1a0034ab58ab: Preparing\n",
      "cd2590e8c46f: Preparing\n",
      "e0dc9e5978eb: Preparing\n",
      "f34c93965828: Preparing\n",
      "a3d4e6ad7606: Preparing\n",
      "6fdaa4c39756: Preparing\n",
      "f0131a301c07: Preparing\n",
      "baef981f2696: Preparing\n",
      "65f4e45c2715: Preparing\n",
      "737cec522037: Preparing\n",
      "037f26f86912: Preparing\n",
      "e67fb4bad8f4: Preparing\n",
      "a058891c99b5: Waiting\n",
      "964529c819bb: Preparing\n",
      "2f98f42985b1: Preparing\n",
      "332b199f36eb: Preparing\n",
      "bfefd28d7932: Waiting\n",
      "1a0034ab58ab: Waiting\n",
      "65f4e45c2715: Waiting\n",
      "737cec522037: Waiting\n",
      "e0dc9e5978eb: Waiting\n",
      "037f26f86912: Waiting\n",
      "e67fb4bad8f4: Waiting\n",
      "f34c93965828: Waiting\n",
      "964529c819bb: Waiting\n",
      "a3d4e6ad7606: Waiting\n",
      "2f98f42985b1: Waiting\n",
      "6fdaa4c39756: Waiting\n",
      "332b199f36eb: Waiting\n",
      "f0131a301c07: Waiting\n",
      "baef981f2696: Waiting\n",
      "9a48496d4218: Layer already exists\n",
      "63005d52920c: Layer already exists\n",
      "bfefd28d7932: Layer already exists\n",
      "a058891c99b5: Layer already exists\n",
      "1a0034ab58ab: Layer already exists\n",
      "cd2590e8c46f: Layer already exists\n",
      "e0dc9e5978eb: Layer already exists\n",
      "f34c93965828: Layer already exists\n",
      "a3d4e6ad7606: Layer already exists\n",
      "6fdaa4c39756: Layer already exists\n",
      "baef981f2696: Layer already exists\n",
      "f0131a301c07: Layer already exists\n",
      "883e773e9a21: Pushed\n",
      "65f4e45c2715: Layer already exists\n",
      "6594212d009d: Pushed\n",
      "737cec522037: Layer already exists\n",
      "e67fb4bad8f4: Layer already exists\n",
      "037f26f86912: Layer already exists\n",
      "964529c819bb: Layer already exists\n",
      "2f98f42985b1: Layer already exists\n",
      "332b199f36eb: Layer already exists\n",
      "2d0bb4911102: Pushed\n",
      "latest: digest: sha256:4ccc387b22cb0967c62165791567544b04a82155b16c7501984e296e5ceb987b size: 4923\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                                      STATUS\n",
      "03fc998d-8dee-426a-86f8-e1e5d98b8e99  2023-07-04T18:18:24+00:00  3M20S     gs://hybrid-vertex_cloudbuild/source/1688494704.164302-cc4a0e3eb5b741a9bc6e9a089022cc76.tgz  gcr.io/hybrid-vertex/prediction-custom-container (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# ! gcloud builds submit --config cloudbuild.yaml\n",
    "\n",
    "! gcloud builds submit --config cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION,_ARTIFACTS_DIR=$ARTIFACTS_DIR \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe724c1-2dcb-4d10-8cec-fb56fb088120",
   "metadata": {},
   "source": [
    "### Submit custom container training job\n",
    "\n",
    "- Note again that the bucket must be in the same regional location as the service location and it should not be multi-regional.\n",
    "- Read more of CustomContainerTrainingJob's source code [here](https://github.com/googleapis/python-aiplatform/blob/v0.8.0/google/cloud/aiplatform/training_jobs.py#L2153).\n",
    "- Like with local execution, you can use TensorBoard Profiler to track the training process and resources, and visualize the corresponding artifacts using the command: `%tensorboard --logdir $PROFILER_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98865883-8f1e-49c8-8172-82fcfa6dfe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59c73f27-f79b-403e-9b77-1bfdc843c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_HYPERPARAMETER_TUNING = False  # Execute regular training instead of hyperparameter tuning.\n",
    "TRAIN_WITH_BEST_HYPERPARAMETERS = True  # @param {type:\"bool\"} Whether to use learned hyperparameters in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c28c490b-6764-4bc7-8552-10fea73a0d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('args: '\n",
      " \"['--artifacts-dir=gs://tabv2-hybrid-vertex-bucket/build-local-hpt/run-20230704-175916/artifacts', \"\n",
      " \"'--data-path=gs://tabv2-hybrid-vertex-bucket/artifacts/u.data', \"\n",
      " \"'--batch-size=8', '--rank-k=20', '--num-actions=20', \"\n",
      " \"'--tikhonov-weight=0.001', '--agent-alpha=10.0', \"\n",
      " \"'--train-with-best-hyperparameters', \"\n",
      " \"'--best-hyperparameters-bucket=tabv2-hybrid-vertex-bucket', \"\n",
      " \"'--best-hyperparameters-path=hptuning/result.json']\")\n"
     ]
    }
   ],
   "source": [
    "args = [\n",
    "    f\"--artifacts-dir={ARTIFACTS_DIR}\"\n",
    "    , f\"--data-path={DATA_PATH}\"\n",
    "    , f\"--batch-size={BATCH_SIZE}\"\n",
    "    , f\"--rank-k={RANK_K}\"\n",
    "    , f\"--num-actions={NUM_ACTIONS}\"\n",
    "    , f\"--tikhonov-weight={TIKHONOV_WEIGHT}\"\n",
    "    , f\"--agent-alpha={AGENT_ALPHA}\"\n",
    "    # , f\"--project_id={PROJECT_ID}\"\n",
    "]\n",
    "\n",
    "if RUN_HYPERPARAMETER_TUNING:\n",
    "    args.append(\"--run-hyperparameter-tuning\")\n",
    "elif TRAIN_WITH_BEST_HYPERPARAMETERS:\n",
    "    args.append(\"--train-with-best-hyperparameters\")\n",
    "    args.append(f\"--best-hyperparameters-bucket={BUCKET_NAME}\")\n",
    "    args.append(f\"--best-hyperparameters-path={HPTUNING_RESULT_PATH}\")\n",
    "    \n",
    "from pprint import pprint\n",
    "pprint(f\"args: {args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d60f019a-87c8-4e51-9178-d20ff648d216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Spec: container_spec {\n",
      "  image_uri: \"gcr.io/hybrid-vertex/prediction-custom-container:latest\"\n",
      "  predict_route: \"/predict\"\n",
      "  health_route: \"/health\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=\"train-movielens\",\n",
    "    container_uri=f\"gcr.io/{PROJECT_ID}/{HPTUNING_TRAINING_CONTAINER}:latest\",\n",
    "    command=[\"python3\", \"-m\", \"src.training.task\"] + args,  # Pass in training arguments, including hyperparameters.\n",
    "    model_serving_container_image_uri=f\"gcr.io/{PROJECT_ID}/{PREDICTION_CONTAINER}:latest\",\n",
    "    model_serving_container_predict_route=\"/predict\",\n",
    "    model_serving_container_health_route=\"/health\")\n",
    "\n",
    "print(\"Training Spec:\", job._managed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ac5e9dd-1557-4d61-8e95-b27a4200211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = job.run(\n",
    "    model_display_name=f\"{PREFIX}-movielens-model\",\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_type=\"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "    accelerator_count=0,\n",
    "    sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "db62f96b-b436-4342-b751-a54b5df08eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model display name: tabv2-movielens-model\n",
      "Model ID: 7328722187173494784\n"
     ]
    }
   ],
   "source": [
    "print(\"Model display name:\", model.display_name)\n",
    "print(\"Model ID:\", model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8878e9-399d-4c10-a0d6-4d6dc4ea3be7",
   "metadata": {},
   "source": [
    "### Deploy trained model to an Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ca9d1e0-492e-42c6-ba55-d80deaad65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b501650-6f91-4365-ba59-d4e84ee6f7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint display name: tabv2-movielens-model_endpoint\n",
      "Endpoint ID: 1406596429320814592\n"
     ]
    }
   ],
   "source": [
    "print(\"Endpoint display name:\", endpoint.display_name)\n",
    "print(\"Endpoint ID:\", endpoint.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb45c9-2482-4338-bd2c-c6d5e142018c",
   "metadata": {},
   "source": [
    "### Predict on the Endpoint\n",
    "- Put prediction input(s) into a list named `instances`. The observation should of dimension (BATCH_SIZE, RANK_K). Read more about the MovieLens simulation environment observation [here](https://github.com/tensorflow/agents/blob/v0.8.0/tf_agents/bandits/environments/movielens_py_environment.py#L32-L138).\n",
    "- Read more about the endpoint prediction API [here](https://cloud.google.com/sdk/gcloud/reference/ai/endpoints/predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "758e7a3b-80a6-4b74-9db1-c2675c6f37f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[{'PolicyStep 0': [4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0]}], deployed_model_id='1512032997354766336', model_version_id='1', model_resource_name='projects/934903580331/locations/us-central1/models/7328722187173494784', explanations=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.predict(\n",
    "    instances=[\n",
    "        {\"observation\": [list(np.ones(20)) for _ in range(8)]},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9634a-80d0-4058-a88d-aa0e566573ba",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67abc70e-6546-45e2-aba2-c1e65ba8b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete endpoint resource\n",
    "# ! gcloud ai endpoints delete $endpoint.name --quiet --region $REGION\n",
    "\n",
    "# # Delete model resource\n",
    "# ! gcloud ai models delete $model.name --quiet\n",
    "\n",
    "# # Delete Cloud Storage objects that were created\n",
    "# ! gsutil -m rm -r $ARTIFACTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cae30da-3748-4d75-bfba-08c501930d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
