{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "393e41d1-768c-4950-9731-dbeda4420e86",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RL-specific MLOps workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc6491c-c073-451c-a680-03a1dea08841",
   "metadata": {},
   "source": [
    "**Critical steps for an end-to-end, RL-based MLOps workflow:**\n",
    "\n",
    "* RL-specific implementation for training and prediction\n",
    "* Simulation for initial training data, prediction requests and re-training\n",
    "* Closing of the feedback loop from prediction results back to training\n",
    "* Customizable, reusable and shareable KFP components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c178db-5e85-4beb-ba4d-b4f59c788d43",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846740bf-c26d-4513-9716-bac8dd87737f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "In this notebook, we will locally build and test all components of an RL-specifc MLOps workflow\n",
    "\n",
    "**Components of (re)train-deploy pipeline:**\n",
    "* Generator to generate MovieLens simulation data\n",
    "* Ingester to ingest data\n",
    "* Trainer to train the RL policy\n",
    "* Deployer to deploy the trained policy to a Vertex AI endpoint\n",
    "\n",
    "**Helper modules for simulating production traffic/monitoring:**\n",
    "* `Simulator` (which utilizes Cloud Functions, Cloud Scheduler and Pub/Sub) to send simulated MovieLens prediction requests, \n",
    "* `Logger` to asynchronously log prediction inputs and results (which utilizes Cloud Functions, Pub/Sub and a hook in the prediction code)\n",
    "* `Trigger` to trigger recurrent re-training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06403df9-86d4-4774-869e-f799a007ccfe",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4aa46b6-2fe8-4935-966d-c8d358087720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/jt-github/tf_vertex_agents\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d9ae5b-9aca-4378-9adb-acc651f7ac41",
   "metadata": {},
   "source": [
    "### set vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630ae362-377c-404e-8b25-f755cb1eeae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'e2ev4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "084dc1e6-1530-424e-8581-d72c3ffdd5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID       = hybrid-vertex\n",
      "PROJECT_NUM      = 934903580331\n",
      "VPC_NETWORK_NAME = ucaip-haystack-vpc-network\n",
      "LOCATION         = us-central1\n",
      "REGION           = us-central1\n",
      "BQ_LOCATION      = us\n"
     ]
    }
   ],
   "source": [
    "# creds, PROJECT_ID = google.auth.default()\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "PROJECT_NUM              = !gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\"\n",
    "PROJECT_NUM              = PROJECT_NUM[0]\n",
    "\n",
    "VERTEX_SA                = f'{PROJECT_NUM}-compute@developer.gserviceaccount.com'\n",
    "\n",
    "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
    "\n",
    "# locations / regions for cloud resources\n",
    "LOCATION                 = 'us-central1'        \n",
    "REGION                   = LOCATION\n",
    "BQ_LOCATION              = 'us'\n",
    "\n",
    "print(f\"PROJECT_ID       = {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM      = {PROJECT_NUM}\")\n",
    "print(f\"VPC_NETWORK_NAME = {VPC_NETWORK_NAME}\")\n",
    "print(f\"LOCATION         = {LOCATION}\")\n",
    "print(f\"REGION           = {REGION}\")\n",
    "print(f\"BQ_LOCATION      = {BQ_LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d15db91c-f1fc-4243-91da-cc40f1bc468e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET_NAME         : e2ev4-hybrid-vertex-bucket\n",
      "BUCKET_URI          : gs://e2ev4-hybrid-vertex-bucket\n",
      "RAW_DATA_PATH       : gs://e2ev4-hybrid-vertex-bucket/raw_data/u.data\n",
      "DATA_PATH           : gs://e2ev4-hybrid-vertex-bucket/artifacts/u.data\n",
      "VPC_NETWORK_FULL    : projects/934903580331/global/networks/ucaip-haystack-vpc-network\n",
      "BIGQUERY_DATASET_ID : hybrid-vertex.movielens_dataset_e2ev4\n",
      "BIGQUERY_TABLE_ID   : hybrid-vertex.movielens_dataset_e2ev4.training_dataset\n"
     ]
    }
   ],
   "source": [
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "# Location of the MovieLens 100K dataset's \"u.data\" file.\n",
    "RAW_DATA_PATH            = f\"{BUCKET_URI}/raw_data/u.data\"\n",
    "DATA_PATH                = f\"{BUCKET_URI}/artifacts/u.data\"\n",
    "ARTIFACTS_DIR            = f\"{BUCKET_URI}/artifacts\"\n",
    "\n",
    "VPC_NETWORK_FULL         = f\"projects/{PROJECT_NUM}/global/networks/{VPC_NETWORK_NAME}\"\n",
    "\n",
    "# BigQuery parameters (used for the Generator, Ingester, Logger)\n",
    "BIGQUERY_DATASET_ID      = f\"{PROJECT_ID}.movielens_dataset_{PREFIX}\"\n",
    "BIGQUERY_TABLE_ID        = f\"{BIGQUERY_DATASET_ID}.training_dataset\"\n",
    "\n",
    "print(f\"BUCKET_NAME         : {BUCKET_NAME}\")\n",
    "print(f\"BUCKET_URI          : {BUCKET_URI}\")\n",
    "print(f\"RAW_DATA_PATH       : {RAW_DATA_PATH}\")\n",
    "print(f\"DATA_PATH           : {DATA_PATH}\")\n",
    "print(f\"VPC_NETWORK_FULL    : {VPC_NETWORK_FULL}\")\n",
    "print(f\"BIGQUERY_DATASET_ID : {BIGQUERY_DATASET_ID}\")\n",
    "print(f\"BIGQUERY_TABLE_ID   : {BIGQUERY_TABLE_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3a8ab7-ffe4-462d-a51b-ff0c046bca79",
   "metadata": {},
   "source": [
    "### create GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d363c6d7-92f7-49da-9078-729ccd751a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bucket\n",
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9176e373-8f6d-4cb9-bf05-2c0c132776c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d29b5-42c8-49b0-9d4a-30d29b550eca",
   "metadata": {},
   "source": [
    "### copy sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "439b744b-d543-458e-b22c-46d0afa5e331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-samples-data/vertex-ai/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/u.data [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  1.9 MiB/  1.9 MiB]                                                \n",
      "Operation completed over 1 objects/1.9 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "SAMPLE_DATA_URI = \"gs://cloud-samples-data/vertex-ai/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/u.data\"\n",
    "\n",
    "! gsutil cp $SAMPLE_DATA_URI $RAW_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622daea-3085-42c7-85bb-314e95a0d790",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "742bffc7-6068-42ce-b142-ddebf018b20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abbe6983-510f-4dd2-9502-7bbf78cbbd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, List, Optional, TypeVar, NamedTuple, Any\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage, bigquery\n",
    "\n",
    "# Pipelines\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "# from google_cloud_pipeline_components.experimental.custom_job import utils\n",
    "from google_cloud_pipeline_components.v1.custom_job import utils\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "# Kubeflow SDK\n",
    "# TODO: fix these\n",
    "from kfp.v2 import dsl, compiler\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "# from kfp.v2.google import client as pipelines_client\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents import TFAgent\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "from tf_agents.bandits.environments import (environment_utilities,\n",
    "                                            movielens_py_environment)\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import TFEnvironment, tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.metrics.tf_metric import TFStepMetric\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b107dc8f-1516-4538-98fb-31934adc11c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfp version: 2.0.1\n",
      "google_cloud_pipeline_components version: 2.0.0\n",
      "vertex_ai SDK version: 1.26.0\n",
      "bigquery SDK version: 3.11.1\n"
     ]
    }
   ],
   "source": [
    "print(f'kfp version: {kfp.__version__}')\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\"\n",
    "print(f'vertex_ai SDK version: {aiplatform.__version__}')\n",
    "print(f'bigquery SDK version: {bigquery.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d0898c-3659-40be-b004-bed8570923d8",
   "metadata": {},
   "source": [
    "### initialize SDK clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8c38956-db5d-46dd-bace-8ecce9b7367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# # bigquery client\n",
    "bqclient = bigquery.Client(\n",
    "    project=PROJECT_ID,\n",
    "    # location=LOCATION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59856683-c575-4ddf-93a2-3117cc1b743e",
   "metadata": {},
   "source": [
    "### set addition parameters\n",
    "\n",
    "* `BigQuery` parameters (used for the `Generator`, `Ingester`, `Logger`)\n",
    "* `Dataset` parameters (TFRecords)\n",
    "* `Logger` parameters (also used for the` Logger` hook in the prediction container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03342c78-a851-457a-bd7a-d36c5386469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIGQUERY_TMP_FILE     : tmp.json\n",
      "BIGQUERY_MAX_ROWS     : 5\n",
      "TFRECORD_FILE         : gs://e2ev4-hybrid-vertex-bucket/trainer_input_path/data.tfrecord\n",
      "LOGGER_PUBSUB_TOPIC   : logger-pubsub-topic-e2ev4\n",
      "LOGGER_CLOUD_FUNCTION : logger-cloud-function-e2ev4\n"
     ]
    }
   ],
   "source": [
    "# BigQuery parameters\n",
    "BIGQUERY_TMP_FILE = (\n",
    "    \"tmp.json\"         # Temporary file for storing data to be loaded into BigQuery.\n",
    ")\n",
    "BIGQUERY_MAX_ROWS = 5  # Maximum number of rows of data in BigQuery to ingest.\n",
    "\n",
    "# Dataset parameters\n",
    "TFRECORD_FILE = (\n",
    "    f\"{BUCKET_URI}/trainer_input_path/data.tfrecord\"  # TFRecord file to be used for training.\n",
    ")\n",
    "\n",
    "# Logger parameters \n",
    "LOGGER_PUBSUB_TOPIC = f\"logger-pubsub-topic-{PREFIX}\"  # Pub/Sub topic name for the Logger\n",
    "LOGGER_CLOUD_FUNCTION = f\"logger-cloud-function-{PREFIX}\"  # Cloud Functions name for the Logger\n",
    "\n",
    "print(f\"BIGQUERY_TMP_FILE     : {BIGQUERY_TMP_FILE}\")\n",
    "print(f\"BIGQUERY_MAX_ROWS     : {BIGQUERY_MAX_ROWS}\")\n",
    "print(f\"TFRECORD_FILE         : {TFRECORD_FILE}\")\n",
    "print(f\"LOGGER_PUBSUB_TOPIC   : {LOGGER_PUBSUB_TOPIC}\")\n",
    "print(f\"LOGGER_CLOUD_FUNCTION : {LOGGER_CLOUD_FUNCTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa3e9db-78b5-4fa1-a49f-8dd405d03a03",
   "metadata": {},
   "source": [
    "## RL components\n",
    "\n",
    "* Create the `Generator` to generate MovieLens simulation data\n",
    "* Create the `Ingester` to ingest data\n",
    "* Create the `Trainer` to train the RL policy\n",
    "* Create the `Deployer` to deploy the trained policy to a Vertex AI endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a6062-a6c8-423b-ac85-f38d6231a1c6",
   "metadata": {},
   "source": [
    "### Generator component\n",
    "\n",
    "* Create the `Generator` component to generate the initial set of training data using a MovieLens simulation environment and a random data-collecting policy\n",
    "* Store the generated data in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e336b381-f380-4285-96cc-3747f348ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "COMPONENT_SUBDIR = \"generator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de9d0915-5ee2-41a6-bc0e-94809cb836fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the generator subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "234443f7-1e2a-4b23-9ff1-e67d42bfa8ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/generator/generator_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}/generator_component.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, InputPath, \n",
    "    Model, Output, OutputPath, component, Metrics\n",
    ")\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"tensorflow/tensorflow:2.12.0\",\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-bigquery\"\n",
    "        , \"tensorflow==2.12.0\"\n",
    "        , \"tf-agents==0.16.0\"\n",
    "    ],\n",
    "    # output_component_file=\"./pipelines/train_custom_model.yaml\",\n",
    ")\n",
    "def generate_movielens_dataset_for_bigquery(\n",
    "    project_id: str\n",
    "    , raw_data_path: str\n",
    "    , batch_size: int\n",
    "    , rank_k: int\n",
    "    , num_actions: int\n",
    "    , driver_steps: int\n",
    "    , bigquery_tmp_file: str\n",
    "    , bigquery_dataset_id: str\n",
    "    , bigquery_location: str\n",
    "    , bigquery_table_id: str\n",
    ") -> NamedTuple(\"Outputs\", [\n",
    "    (\"bigquery_dataset_id\", str)\n",
    "    , (\"bigquery_location\", str)\n",
    "    , (\"bigquery_table_id\", str)\n",
    "]):\n",
    "    \"\"\"\n",
    "    Generates BigQuery training data using a MovieLens simulation environment.\n",
    "\n",
    "    Serves as the Generator pipeline component:\n",
    "    1. Generates `trajectories.Trajectory` data by applying a random policy on\n",
    "        MovieLens simulation environment.\n",
    "    2. Converts `trajectories.Trajectory` data to JSON format.\n",
    "    3. Loads JSON-formatted data into BigQuery.\n",
    "\n",
    "    This function is to be built into a Kubeflow Pipelines (KFP) component. As a\n",
    "    result, this function must be entirely self-contained. This means that the\n",
    "    import statements and helper functions must reside within itself.\n",
    "\n",
    "    Args:\n",
    "      project_id: GCP project ID. This is required because otherwise the BigQuery\n",
    "        client will use the ID of the tenant GCP project created as a result of\n",
    "        KFP, which doesn't have proper access to BigQuery.\n",
    "      raw_data_path: Path to MovieLens 100K's \"u.data\" file.\n",
    "      batch_size: Batch size of environment generated quantities eg. rewards.\n",
    "      rank_k: Rank for matrix factorization in the MovieLens environment; also\n",
    "        the observation dimension.\n",
    "      num_actions: Number of actions (movie items) to choose from.\n",
    "      driver_steps: Number of steps to run per batch.\n",
    "      bigquery_tmp_file: Path to a JSON file containing the training dataset.\n",
    "      bigquery_dataset_id: A string of the BigQuery dataset ID in the format of\n",
    "        \"project.dataset\".\n",
    "      bigquery_location: A string of the BigQuery dataset location.\n",
    "      bigquery_table_id: A string of the BigQuery table ID in the format of\n",
    "        \"project.dataset.table\".\n",
    "\n",
    "    Returns:\n",
    "      A NamedTuple of (`bigquery_dataset_id`, `bigquery_location`,\n",
    "        `bigquery_table_id`).\n",
    "    \"\"\"\n",
    "    # pylint: disable=g-import-not-at-top\n",
    "    import collections\n",
    "    import json\n",
    "    from typing import Any, Dict\n",
    "    import logging\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    from tf_agents import replay_buffers\n",
    "    from tf_agents import trajectories\n",
    "    from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "    from tf_agents.bandits.environments import movielens_py_environment\n",
    "    from tf_agents.drivers import dynamic_step_driver\n",
    "    from tf_agents.environments import tf_py_environment\n",
    "    from tf_agents.policies import random_tf_policy\n",
    "\n",
    "    def generate_simulation_data(\n",
    "        raw_data_path: str\n",
    "        , batch_size: int\n",
    "        , rank_k: int\n",
    "        , num_actions: int\n",
    "        , driver_steps: int\n",
    "    ) -> replay_buffers.TFUniformReplayBuffer:\n",
    "        \"\"\"\n",
    "        Generates `trajectories.Trajectory` data from the simulation environment.\n",
    "\n",
    "        Constructs a MovieLens simulation environment, and generates a set of\n",
    "        `trajectories.Trajectory` data using a random policy.\n",
    "\n",
    "        Args:\n",
    "          raw_data_path: Path to MovieLens 100K's \"u.data\" file.\n",
    "          batch_size: Batch size of environment generated quantities eg. rewards.\n",
    "          rank_k: Rank for matrix factorization in the MovieLens environment; also\n",
    "            the observation dimension.\n",
    "          num_actions: Number of actions (movie items) to choose from.\n",
    "          driver_steps: Number of steps to run per batch.\n",
    "\n",
    "        Returns:\n",
    "          A replay buffer holding randomly generated`trajectories.Trajectory` data.\n",
    "        \"\"\"\n",
    "        # Create MovieLens simulation environment.\n",
    "        env = movielens_py_environment.MovieLensPyEnvironment(\n",
    "            raw_data_path,\n",
    "            rank_k,\n",
    "            batch_size,\n",
    "            num_movies=num_actions,\n",
    "            csv_delimiter=\"\\t\"\n",
    "        )\n",
    "        environment = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "        # Define random policy for collecting data.\n",
    "        random_policy = random_tf_policy.RandomTFPolicy(\n",
    "            action_spec=environment.action_spec()\n",
    "            , time_step_spec=environment.time_step_spec()\n",
    "        )\n",
    "\n",
    "        # Use replay buffer and observers to keep track of Trajectory data.\n",
    "        data_spec = random_policy.trajectory_spec\n",
    "        replay_buffer = trainer._get_replay_buffer(\n",
    "            data_spec\n",
    "            , environment.batch_size\n",
    "            , driver_steps\n",
    "            , 1\n",
    "        )\n",
    "        observers = [replay_buffer.add_batch]\n",
    "\n",
    "        # Run driver to apply the random policy in the simulation environment.\n",
    "        driver = dynamic_step_driver.DynamicStepDriver(\n",
    "            env=environment\n",
    "            , policy=random_policy\n",
    "            , num_steps=driver_steps * environment.batch_size\n",
    "            , observers=observers\n",
    "        )\n",
    "        driver.run()\n",
    "\n",
    "        return replay_buffer\n",
    "\n",
    "    def build_dict_from_trajectory(\n",
    "        trajectory: trajectories.Trajectory\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Builds a dict from `trajectory` data.\n",
    "\n",
    "        Args:\n",
    "          trajectory: A `trajectories.Trajectory` object.\n",
    "\n",
    "        Returns:\n",
    "          A dict holding the same data as `trajectory`.\n",
    "        \"\"\"\n",
    "        trajectory_dict = {\n",
    "            \"step_type\": trajectory.step_type.numpy().tolist()\n",
    "            , \"observation\": [\n",
    "                {\"observation_batch\": batch} for batch in trajectory.observation.numpy().tolist()]\n",
    "            , \"action\": trajectory.action.numpy().tolist()\n",
    "            , \"policy_info\": trajectory.policy_info\n",
    "            , \"next_step_type\": trajectory.next_step_type.numpy().tolist()\n",
    "            , \"reward\": trajectory.reward.numpy().tolist()\n",
    "            , \"discount\": trajectory.discount.numpy().tolist()\n",
    "        }\n",
    "        \n",
    "        return trajectory_dict\n",
    "\n",
    "    def write_replay_buffer_to_file(\n",
    "        replay_buffer: replay_buffers.TFUniformReplayBuffer\n",
    "        , batch_size: int\n",
    "        , dataset_file: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Writes replay buffer data to a file, each JSON in one line.\n",
    "\n",
    "        Each `trajectories.Trajectory` object in `replay_buffer` will be written as\n",
    "        one line to the `dataset_file` in JSON format. I.e., the `dataset_file`\n",
    "        would be a newline-delimited JSON file.\n",
    "\n",
    "        Args:\n",
    "          replay_buffer: A `replay_buffers.TFUniformReplayBuffer` holding\n",
    "            `trajectories.Trajectory` objects.\n",
    "          batch_size: Batch size of environment generated quantities eg. rewards.\n",
    "          dataset_file: File path. Will be overwritten if already exists.\n",
    "        \"\"\"\n",
    "        dataset = replay_buffer.as_dataset(sample_batch_size=batch_size)\n",
    "        dataset_size = replay_buffer.num_frames().numpy()\n",
    "\n",
    "        with open(dataset_file, \"w\") as f:\n",
    "            for example in dataset.take(count=dataset_size):\n",
    "                traj_dict = build_dict_from_trajectory(example[0])\n",
    "                f.write(json.dumps(traj_dict) + \"\\n\")\n",
    "\n",
    "    def load_dataset_into_bigquery(\n",
    "        project_id: str\n",
    "        , dataset_file: str\n",
    "        , bigquery_dataset_id: str\n",
    "        , bigquery_location: str\n",
    "        , bigquery_table_id: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Loads training dataset into BigQuery table.\n",
    "\n",
    "        Loads training dataset of `trajectories.Trajectory` in newline delimited\n",
    "        JSON into a BigQuery dataset and table, using a BigQuery client.\n",
    "\n",
    "        Args:\n",
    "          project_id: GCP project ID. This is required because otherwise the\n",
    "            BigQuery client will use the ID of the tenant GCP project created as a\n",
    "            result of KFP, which doesn't have proper access to BigQuery.\n",
    "          dataset_file: Path to a JSON file containing the training dataset.\n",
    "          bigquery_dataset_id: A string of the BigQuery dataset ID in the format of\n",
    "            \"project.dataset\".\n",
    "          bigquery_location: A string of the BigQuery dataset location.\n",
    "          bigquery_table_id: A string of the BigQuery table ID in the format of\n",
    "            \"project.dataset.table\".\n",
    "        \"\"\"\n",
    "        # Construct a BigQuery client object.\n",
    "        client = bigquery.Client(project=project_id)\n",
    "\n",
    "        # Construct a full Dataset object to send to the API.\n",
    "        dataset = bigquery.Dataset(bigquery_dataset_id)\n",
    "\n",
    "        # Specify the geographic location where the dataset should reside.\n",
    "        dataset.location = bigquery_location\n",
    "\n",
    "        # Create the dataset, or get the dataset if it exists.\n",
    "        dataset = client.create_dataset(dataset, exists_ok=True, timeout=30)\n",
    "        _BIGQUERY_DATASET_ID = dataset.full_dataset_id\n",
    "        logging.info(f\"_BIGQUERY_DATASET_ID: {_BIGQUERY_DATASET_ID}\")\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            schema=[\n",
    "                bigquery.SchemaField(\"step_type\", \"INT64\", mode=\"REPEATED\")\n",
    "                , bigquery.SchemaField(\n",
    "                    \"observation\"\n",
    "                    , \"RECORD\"\n",
    "                    , mode=\"REPEATED\"\n",
    "                    , fields=[\n",
    "                        bigquery.SchemaField(\n",
    "                            \"observation_batch\", \"FLOAT64\", \"REPEATED\"\n",
    "                        )\n",
    "                    ]\n",
    "                )                \n",
    "                , bigquery.SchemaField(\"action\", \"INT64\", mode=\"REPEATED\")\n",
    "                , bigquery.SchemaField(\"policy_info\", \"FLOAT64\", mode=\"REPEATED\")\n",
    "                , bigquery.SchemaField(\"next_step_type\", \"INT64\", mode=\"REPEATED\")\n",
    "                , bigquery.SchemaField(\"reward\", \"FLOAT64\", mode=\"REPEATED\")\n",
    "                , bigquery.SchemaField(\"discount\", \"FLOAT64\", mode=\"REPEATED\")\n",
    "            ]\n",
    "            , source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "        )\n",
    "\n",
    "        with open(dataset_file, \"rb\") as source_file:\n",
    "            load_job = client.load_table_from_file(\n",
    "                source_file\n",
    "                , bigquery_table_id\n",
    "                , job_config=job_config\n",
    "            )\n",
    "\n",
    "        load_job.result()  # Wait for the job to complete.\n",
    "\n",
    "    replay_buffer = generate_simulation_data(\n",
    "        raw_data_path=raw_data_path\n",
    "        , batch_size=batch_size\n",
    "        , rank_k=rank_k\n",
    "        , num_actions=num_actions\n",
    "        , driver_steps=driver_steps\n",
    "    )\n",
    "\n",
    "    write_replay_buffer_to_file(\n",
    "        replay_buffer=replay_buffer\n",
    "        , batch_size=batch_size\n",
    "        , dataset_file=bigquery_tmp_file\n",
    "    )\n",
    "\n",
    "    load_dataset_into_bigquery(\n",
    "        project_id\n",
    "        , bigquery_tmp_file\n",
    "        , bigquery_dataset_id\n",
    "        , bigquery_location\n",
    "        , bigquery_table_id\n",
    "    )\n",
    "        \n",
    "    return (\n",
    "        bigquery_dataset_id\n",
    "        , bigquery_location\n",
    "        , bigquery_table_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbacc7d2-95bb-4fd3-99cf-02d79cec026a",
   "metadata": {},
   "source": [
    "### Ingester component\n",
    "\n",
    "* Create the `Ingester` component to ingest data from BigQuery, package them as `tf.train.Example` objects, and output TFRecord files\n",
    "* Read more about `tf.train.Example` and TFRecord [here](https://www.tensorflow.org/tutorials/load_data/tfrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3f8522e-17d6-4ed2-8082-93ce0f10bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENT_SUBDIR = \"ingester\"\n",
    "\n",
    "# Make the generator subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a5fc702-b339-4cd1-935a-4003f7385aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/ingester/ingester_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}/ingester_component.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"\n",
    "The Ingester component for ingesting BigQuery data into TFRecords.\n",
    "\"\"\"\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, InputPath, \n",
    "    Model, Output, OutputPath, component, Metrics\n",
    ")\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"tensorflow/tensorflow:2.12.0\",\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-bigquery\"\n",
    "        , \"tensorflow==2.12.0\"\n",
    "    ],\n",
    "    # output_component_file=\"./pipelines/train_custom_model.yaml\",\n",
    ")\n",
    "def ingest_bigquery_dataset_into_tfrecord(\n",
    "    project_id: str\n",
    "    , bigquery_table_id: str\n",
    "    , tfrecord_file_input: str\n",
    "    , tfrecord_file: OutputPath(str)\n",
    "    , bigquery_max_rows: int = None\n",
    "# ) -> NamedTuple(\"Outputs\", [\n",
    "    # (\"tfrecord_file\", str)]\n",
    "):\n",
    "    \"\"\"\n",
    "    Ingests data from BigQuery, formats them and outputs TFRecord files.\n",
    "\n",
    "    Serves as the Ingester pipeline component:\n",
    "    1. Reads data in BigQuery that contains 7 pieces of data: `step_type`,\n",
    "      `observation`, `action`, `policy_info`, `next_step_type`, `reward`,\n",
    "      `discount`.\n",
    "    2. Packages the data as `tf.train.Example` objects and outputs them as\n",
    "      TFRecord files.\n",
    "\n",
    "    This function is to be built into a Kubeflow Pipelines (KFP) component. As a\n",
    "    result, this function must be entirely self-contained. This means that the\n",
    "    import statements and helper functions must reside within itself.\n",
    "\n",
    "    Args:\n",
    "      project_id: GCP project ID. This is required because otherwise the BigQuery\n",
    "        client will use the ID of the tenant GCP project created as a result of\n",
    "        KFP, which doesn't have proper access to BigQuery.\n",
    "      bigquery_table_id: A string of the BigQuery table ID in the format of\n",
    "        \"project.dataset.table\".\n",
    "      tfrecord_file_input: Path to file to write the ingestion result TFRecords.\n",
    "      bigquery_max_rows: Optional; maximum number of rows to ingest.\n",
    "\n",
    "    Returns:\n",
    "      A NamedTuple of the path to the output TFRecord file.\n",
    "    \"\"\"\n",
    "    # pylint: disable=g-import-not-at-top\n",
    "    import logging\n",
    "    import collections\n",
    "    from typing import Optional\n",
    "    from google.cloud import bigquery\n",
    "    import tensorflow as tf\n",
    "\n",
    "    def read_data_from_bigquery(\n",
    "        project_id: str\n",
    "        , bigquery_table_id: str\n",
    "        , bigquery_max_rows: Optional[int]\n",
    "    ) -> bigquery.table.RowIterator:\n",
    "        \"\"\"\n",
    "        Reads data from BigQuery at `bigquery_table_id` and creates an iterator.\n",
    "\n",
    "        The table contains 7 columns that form `trajectories.Trajectory` objects:\n",
    "        `step_type`, `observation`, `action`, `policy_info`, `next_step_type`,\n",
    "        `reward`, `discount`.\n",
    "\n",
    "        Args:\n",
    "          project_id: GCP project ID. This is required because otherwise the\n",
    "            BigQuery client will use the ID of the tenant GCP project created as a\n",
    "            result of KFP, which doesn't have proper access to BigQuery.\n",
    "          bigquery_table_id: A string of the BigQuery table ID in the format of\n",
    "            \"project.dataset.table\".\n",
    "          bigquery_max_rows: Optional; maximum number of rows to fetch.\n",
    "\n",
    "        Returns:\n",
    "          A row iterator over all data at `bigquery_table_id`.\n",
    "        \"\"\"\n",
    "        # Construct a BigQuery client object.\n",
    "        client = bigquery.Client(project=project_id)\n",
    "\n",
    "        # Get dataset.\n",
    "        query_job = client.query(\n",
    "            f\"\"\"\n",
    "            SELECT * FROM {bigquery_table_id}\n",
    "            \"\"\"\n",
    "        )\n",
    "        table = query_job.result(max_results=bigquery_max_rows)\n",
    "\n",
    "        return table\n",
    "\n",
    "    def _bytes_feature(tensor: tf.Tensor) -> tf.train.Feature:\n",
    "        \"\"\"\n",
    "        Returns a `tf.train.Feature` with bytes from `tensor`.\n",
    "\n",
    "        Args:\n",
    "          tensor: A `tf.Tensor` object.\n",
    "\n",
    "        Returns:\n",
    "          A `tf.train.Feature` object containing bytes that represent the content of\n",
    "          `tensor`.\n",
    "        \"\"\"\n",
    "        value = tf.io.serialize_tensor(tensor)\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        \n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def build_example(data_row: bigquery.table.Row) -> tf.train.Example:\n",
    "        \"\"\"\n",
    "        Builds a `tf.train.Example` from `data_row` content.\n",
    "\n",
    "        Args:\n",
    "          data_row: A `bigquery.table.Row` object that contains 7 pieces of data:\n",
    "            `step_type`, `observation`, `action`, `policy_info`, `next_step_type`,\n",
    "            `reward`, `discount`. Each piece of data except `observation` is a 1D\n",
    "            array; `observation` is a 1D array of `{\"observation_batch\": 1D array}.`\n",
    "\n",
    "        Returns:\n",
    "          A `tf.train.Example` object holding the same data as `data_row`.\n",
    "        \"\"\"\n",
    "        feature = {\n",
    "            \"step_type\":\n",
    "                _bytes_feature(data_row.get(\"step_type\")),\n",
    "            \"observation\":\n",
    "                _bytes_feature([\n",
    "                    observation[\"observation_batch\"]\n",
    "                    for observation in data_row.get(\"observation\")\n",
    "                ]),\n",
    "            \"action\":\n",
    "                _bytes_feature(data_row.get(\"action\")),\n",
    "            \"policy_info\":\n",
    "                _bytes_feature(data_row.get(\"policy_info\")),\n",
    "            \"next_step_type\":\n",
    "                _bytes_feature(data_row.get(\"next_step_type\")),\n",
    "            \"reward\":\n",
    "                _bytes_feature(data_row.get(\"reward\")),\n",
    "            \"discount\":\n",
    "                _bytes_feature(data_row.get(\"discount\")),\n",
    "        }\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature)\n",
    "        )\n",
    "        return example_proto\n",
    "\n",
    "    def write_tfrecords(\n",
    "        tfrecord_file: str,\n",
    "        table: bigquery.table.RowIterator\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Writes the row data in `table` into TFRecords in `tfrecord_file`.\n",
    "\n",
    "        Args:\n",
    "          tfrecord_file: Path to file to write the TFRecords.\n",
    "          table: A row iterator over all data to be written.\n",
    "        \"\"\"\n",
    "        with tf.io.TFRecordWriter(tfrecord_file) as writer:\n",
    "            for data_row in table:\n",
    "                example = build_example(data_row)\n",
    "                writer.write(example.SerializeToString())\n",
    "\n",
    "    table = read_data_from_bigquery(\n",
    "        project_id=project_id\n",
    "        , bigquery_table_id=bigquery_table_id\n",
    "        , bigquery_max_rows=bigquery_max_rows\n",
    "    )\n",
    "    logging.info(f\"table: {table}\")\n",
    "    logging.info(\"writting TF Records...\")\n",
    "\n",
    "    write_tfrecords(tfrecord_file_input, table)\n",
    "    \n",
    "    logging.info(\"writting output_parameter_path...\")\n",
    "    with open(tfrecord_file, \"w\") as f:\n",
    "        f.write(f\"{tfrecord_file_input}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467d1678-5a69-4e76-bf17-f4cff841c5aa",
   "metadata": {},
   "source": [
    "### Trainer component\n",
    "\n",
    "* Create the `Trainer` component to train a RL policy on the training dataset, and then submit a remote custom training job to Vertex AI. * This component trains a policy using the TF-Agents LinUCB agent on the MovieLens simulation dataset, \n",
    "* Saves the trained policy as a SavedModel\n",
    "\n",
    "> The Trainer performs *off-policy training*, where you train a policy on a static set of pre-collected data records containing information including observation, action and reward. For a data record, the policy in training might not output the same action given the observation in that data record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ab6ee4d-8d08-4fc8-8b6b-f1c642651688",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENT_SUBDIR = \"trainer\"\n",
    "\n",
    "# Make the generator subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8aa58b4-4148-4584-9857-bcf804047125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}/trainer_component.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"The Trainer component for training a policy on TFRecord files.\"\"\"\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, InputPath, \n",
    "    Model, Output, OutputPath, component, Metrics\n",
    ")\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"tensorflow/tensorflow:2.12.0\",\n",
    "    packages_to_install=[\n",
    "        \"tf-agents==0.16.0\"\n",
    "        , \"tensorflow==2.12.0\"\n",
    "    ],\n",
    "    # output_component_file=\"./pipelines/train_custom_model.yaml\",\n",
    ")\n",
    "def train_reinforcement_learning_policy(\n",
    "    training_artifacts_dir_input: str\n",
    "    , tfrecord_file: str\n",
    "    , num_epochs: int\n",
    "    , rank_k: int\n",
    "    , num_actions: int\n",
    "    , tikhonov_weight: float\n",
    "    , agent_alpha: float\n",
    "    , training_artifacts_dir: OutputPath(str)\n",
    "    , training_artifact_dir: OutputPath(Artifact)\n",
    "):\n",
    "# ) -> NamedTuple(\"Outputs\", [\n",
    "#     (\"training_artifacts_dir\", str),]\n",
    "    \"\"\"\n",
    "    Implements off-policy training for a policy on dataset of TFRecord files.\n",
    "\n",
    "    The Trainer's task is to submit a remote training job to Vertex AI, with the\n",
    "    training logic of a specified custom training container. The task will be\n",
    "    handled by: `kfp.v2.google.experimental.run_as_aiplatform_custom_job` (which\n",
    "    takes in the component made from this placeholder function)\n",
    "\n",
    "    This function is to be built into a Kubeflow Pipelines (KFP) component. As a\n",
    "    result, this function must be entirely self-contained. This means that the\n",
    "    import statements and helper functions must reside within itself.\n",
    "\n",
    "    Args:\n",
    "      training_artifacts_dir: Path to store the Trainer artifacts (trained\n",
    "        policy).\n",
    "      tfrecord_file: Path to file to write the ingestion result TFRecords.\n",
    "      num_epochs: Number of training epochs.\n",
    "      rank_k: Rank for matrix factorization in the MovieLens environment; also\n",
    "        the observation dimension.\n",
    "      num_actions: Number of actions (movie items) to choose from.\n",
    "      tikhonov_weight: LinUCB Tikhonov regularization weight of the Trainer.\n",
    "      agent_alpha: LinUCB exploration parameter that multiplies the confidence\n",
    "        intervals of the Trainer.\n",
    "\n",
    "    Returns:\n",
    "      A NamedTuple of (`training_artifacts_dir`).\n",
    "    \"\"\"\n",
    "    # pylint: disable=g-import-not-at-top\n",
    "    import collections\n",
    "    from typing import Dict, List, NamedTuple  # pylint: disable=redefined-outer-name,reimported\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    from tf_agents import agents\n",
    "    from tf_agents import policies\n",
    "    from tf_agents import trajectories\n",
    "    from tf_agents.bandits.agents import lin_ucb_agent\n",
    "    from tf_agents.policies import policy_saver\n",
    "    from tf_agents.specs import tensor_spec\n",
    "\n",
    "    import logging\n",
    "\n",
    "    per_arm = False  # Using the non-per-arm version of the MovieLens environment.\n",
    "\n",
    "    # Mapping from feature name to serialized value\n",
    "    feature_description = {\n",
    "        \"step_type\": tf.io.FixedLenFeature((), tf.string)\n",
    "        , \"observation\": tf.io.FixedLenFeature((), tf.string)\n",
    "        , \"action\": tf.io.FixedLenFeature((), tf.string)\n",
    "        , \"policy_info\": tf.io.FixedLenFeature((), tf.string)\n",
    "        , \"next_step_type\": tf.io.FixedLenFeature((), tf.string)\n",
    "        , \"reward\": tf.io.FixedLenFeature((), tf.string)\n",
    "        , \"discount\": tf.io.FixedLenFeature((), tf.string)\n",
    "    }\n",
    "\n",
    "    def _parse_record(raw_record: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Parses a serialized `tf.train.Example` proto.\n",
    "\n",
    "        Args:\n",
    "          raw_record: A serialized data record of a `tf.train.Example` proto.\n",
    "\n",
    "        Returns:\n",
    "          A dict mapping feature names to values as `tf.Tensor` objects of type\n",
    "          string containing serialized protos, following `feature_description`.\n",
    "        \"\"\"\n",
    "        return tf.io.parse_single_example(raw_record, feature_description)\n",
    "\n",
    "    def build_trajectory(\n",
    "        parsed_record: Dict[str, tf.Tensor]\n",
    "        , policy_info: policies.utils.PolicyInfo\n",
    "    ) -> trajectories.Trajectory:\n",
    "        \"\"\"\n",
    "        Builds a `trajectories.Trajectory` object from `parsed_record`.\n",
    "\n",
    "        Args:\n",
    "          parsed_record: A dict mapping feature names to values as `tf.Tensor`\n",
    "            objects of type string containing serialized protos.\n",
    "          policy_info: Policy information specification.\n",
    "\n",
    "        Returns:\n",
    "          A `trajectories.Trajectory` object that contains values as de-serialized\n",
    "          `tf.Tensor` objects from `parsed_record`.\n",
    "        \"\"\"\n",
    "        return trajectories.Trajectory(\n",
    "            step_type=tf.expand_dims(\n",
    "                tf.io.parse_tensor(parsed_record[\"step_type\"], out_type=tf.int32)\n",
    "                , axis=1)\n",
    "            , observation=tf.expand_dims(\n",
    "                tf.io.parse_tensor(\n",
    "                    parsed_record[\"observation\"], out_type=tf.float32)\n",
    "                , axis=1)\n",
    "            , action=tf.expand_dims(\n",
    "                tf.io.parse_tensor(parsed_record[\"action\"], out_type=tf.int32)\n",
    "                , axis=1)\n",
    "            , policy_info=policy_info\n",
    "            , next_step_type=tf.expand_dims(\n",
    "                tf.io.parse_tensor(\n",
    "                    parsed_record[\"next_step_type\"], out_type=tf.int32)\n",
    "                , axis=1)\n",
    "            , reward=tf.expand_dims(\n",
    "                tf.io.parse_tensor(parsed_record[\"reward\"], out_type=tf.float32)\n",
    "                , axis=1)\n",
    "            , discount=tf.expand_dims(\n",
    "                tf.io.parse_tensor(parsed_record[\"discount\"], out_type=tf.float32)\n",
    "                , axis=1)\n",
    "        )\n",
    "\n",
    "    def train_policy_on_trajectory(\n",
    "        agent: agents.TFAgent\n",
    "        , tfrecord_file: str\n",
    "        , num_epochs: int\n",
    "  ) -> NamedTuple(\"TrainOutputs\", [\n",
    "        (\"policy\", policies.TFPolicy)\n",
    "        , (\"train_loss\", Dict[str, List[float]])\n",
    "    ]):\n",
    "        \"\"\"\n",
    "        Trains the policy in `agent` on the dataset of `tfrecord_file`.\n",
    "\n",
    "        Parses `tfrecord_file` as `tf.train.Example` objects, packages them into\n",
    "        `trajectories.Trajectory` objects, and trains the agent's policy on these\n",
    "        trajectory objects.\n",
    "\n",
    "        Args:\n",
    "          agent: A TF-Agents agent that carries the policy to train.\n",
    "          tfrecord_file: Path to the TFRecord file containing the training dataset.\n",
    "          num_epochs: Number of epochs to train the policy.\n",
    "\n",
    "        Returns:\n",
    "          A NamedTuple of (a trained TF-Agents policy, a dict mapping from\n",
    "          \"epoch<i>\" to lists of loss values produced at each training step).\n",
    "        \"\"\"\n",
    "        raw_dataset = tf.data.TFRecordDataset([tfrecord_file])\n",
    "        parsed_dataset = raw_dataset.map(_parse_record)\n",
    "\n",
    "        train_loss = collections.defaultdict(list)\n",
    "        for epoch in range(num_epochs):\n",
    "            for parsed_record in parsed_dataset:\n",
    "                trajectory = build_trajectory(parsed_record, agent.policy.info_spec)\n",
    "                loss, _ = agent.train(trajectory)\n",
    "                train_loss[f\"epoch{epoch + 1}\"].append(loss.numpy())\n",
    "\n",
    "        train_outputs = collections.namedtuple(\n",
    "            \"TrainOutputs\"\n",
    "            , [\"policy\", \"train_loss\"]\n",
    "        )\n",
    "        return train_outputs(agent.policy, train_loss)\n",
    "\n",
    "    def execute_training_and_save_policy(\n",
    "        training_artifacts_dir: str\n",
    "        , tfrecord_file: str\n",
    "        , num_epochs: int\n",
    "        , rank_k: int\n",
    "        , num_actions: int\n",
    "        , tikhonov_weight: float\n",
    "        , agent_alpha: float\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Executes training for the policy and saves the policy.\n",
    "\n",
    "        Args:\n",
    "          training_artifacts_dir: Path to store the Trainer artifacts (trained\n",
    "            policy).\n",
    "          tfrecord_file: Path to file to write the ingestion result TFRecords.\n",
    "          num_epochs: Number of training epochs.\n",
    "          rank_k: Rank for matrix factorization in the MovieLens environment; also\n",
    "            the observation dimension.\n",
    "          num_actions: Number of actions (movie items) to choose from.\n",
    "          tikhonov_weight: LinUCB Tikhonov regularization weight of the Trainer.\n",
    "          agent_alpha: LinUCB exploration parameter that multiplies the confidence\n",
    "            intervals of the Trainer.\n",
    "        \"\"\"\n",
    "        # Define time step and action specs for one batch.\n",
    "        time_step_spec = trajectories.TimeStep(\n",
    "            step_type=tensor_spec.TensorSpec(\n",
    "                shape=(), dtype=tf.int32, name=\"step_type\"\n",
    "            )\n",
    "            , reward=tensor_spec.TensorSpec(\n",
    "                shape=(), dtype=tf.float32, name=\"reward\"\n",
    "            )\n",
    "            , discount=tensor_spec.BoundedTensorSpec(\n",
    "                shape=(), dtype=tf.float32, name=\"discount\", minimum=0.\n",
    "                , maximum=1.\n",
    "            )\n",
    "            , observation=tensor_spec.TensorSpec(\n",
    "                shape=(rank_k,), dtype=tf.float32\n",
    "                , name=\"observation\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        action_spec = tensor_spec.BoundedTensorSpec(\n",
    "            shape=()\n",
    "            , dtype=tf.int32\n",
    "            , name=\"action\"\n",
    "            , minimum=0\n",
    "            , maximum=num_actions - 1\n",
    "        )\n",
    "\n",
    "        # Define RL agent/algorithm.\n",
    "        agent = lin_ucb_agent.LinearUCBAgent(\n",
    "            time_step_spec=time_step_spec\n",
    "            , action_spec=action_spec\n",
    "            , tikhonov_weight=tikhonov_weight\n",
    "            , alpha=agent_alpha\n",
    "            , dtype=tf.float32\n",
    "            , accepts_per_arm_features=per_arm\n",
    "        )\n",
    "        agent.initialize()\n",
    "        logging.info(\"TimeStep Spec (for each batch):\\n%s\\n\", agent.time_step_spec)\n",
    "        logging.info(\"Action Spec (for each batch):\\n%s\\n\", agent.action_spec)\n",
    "\n",
    "        # Perform off-policy training.\n",
    "        policy, _ = train_policy_on_trajectory(\n",
    "            agent=agent\n",
    "            , tfrecord_file=tfrecord_file\n",
    "            , num_epochs=num_epochs\n",
    "        )\n",
    "\n",
    "        # Save trained policy.\n",
    "        saver = policy_saver.PolicySaver(policy)\n",
    "        saver.save(training_artifacts_dir_input)\n",
    "\n",
    "    execute_training_and_save_policy(\n",
    "        training_artifacts_dir=training_artifacts_dir_input\n",
    "        , tfrecord_file=tfrecord_file\n",
    "        , num_epochs=num_epochs\n",
    "        , rank_k=rank_k\n",
    "        , num_actions=num_actions\n",
    "        , tikhonov_weight=tikhonov_weight\n",
    "        , agent_alpha=agent_alpha\n",
    "    )\n",
    "    \n",
    "    logging.info(\"writting training_artifacts_dir...\")\n",
    "    with open(training_artifacts_dir, \"w\") as f:\n",
    "        f.write(f\"{training_artifacts_dir_input}\")\n",
    "        \n",
    "    logging.info(\"writting training_artifact_dir...\")\n",
    "    with open(training_artifact_dir, \"w\") as f:\n",
    "        f.write(f\"{training_artifacts_dir_input}\")\n",
    "    \n",
    "    # return (training_artifacts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a8d04d1-349a-4c1b-9560-c3b60d1b0ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_MACHINE_TYPE = 'n1-standard-4'\n",
    "TRAINING_ACCELERATOR_TYPE = 'ACCELERATOR_TYPE_UNSPECIFIED'\n",
    "TRAINING_ACCELERATOR_COUNT = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32e51c-e893-47ae-b912-9a7bc7d5b4e0",
   "metadata": {},
   "source": [
    "### Convert self-contained training component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c904c74-1c2b-4b61-8a5f-a55522eeabde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.v1.custom_job import \\\n",
    "    create_custom_training_job_from_component\n",
    "\n",
    "custom_job_op = create_custom_training_job_from_component(\n",
    "    train_reinforcement_learning_policy,\n",
    "    display_name=f'custom-train-job-pipe-{PREFIX}',\n",
    "    machine_type=TRAINING_MACHINE_TYPE,\n",
    "    accelerator_type=TRAINING_ACCELERATOR_TYPE,\n",
    "    accelerator_count=TRAINING_ACCELERATOR_COUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bbacc29-f799-4645-ae00-22158bf9c84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kfp.components.yaml_component.YamlComponent at 0x7f3d189b12a0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_job_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49092fe1-bb80-41b5-a3d2-81a698aaff20",
   "metadata": {},
   "source": [
    "## Prediction Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60b96e0e-1dec-46f5-8291-02d3bfdeb8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction container parameters\n",
    "PREDICTION_CONTAINER = \"prediction_container\"\n",
    "# PREDICTION_CONTAINER_DIR = \"src/prediction_container\"\n",
    "\n",
    "DOCKERNAME = 'prediction'\n",
    "\n",
    "# Make the generator subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{PREDICTION_CONTAINER}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{PREDICTION_CONTAINER}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfd04a3-f158-42b8-b12d-7498de7cd57f",
   "metadata": {},
   "source": [
    "### CloudBuild YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77426aae-ae8e-4b46-a36b-3b0d358a1e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/prediction_container/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PREDICTION_CONTAINER}/cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile_$_DOCKERNAME']\n",
    "  env: ['AIP_STORAGE_URI=$_ARTIFACTS_DIR', 'PROJECT_ID=$_PROJECT_ID', 'LOGGER_PUBSUB_TOPIC=$_LOGGER_PUBSUB_TOPIC']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afa5b748-ed6d-4e48-bbc8-9305a404205d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/prediction_container/Dockerfile_prediction\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PREDICTION_CONTAINER}/Dockerfile_{DOCKERNAME}\n",
    "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.10\n",
    "\n",
    "COPY ./ /app/\n",
    "\n",
    "RUN pip3 install -r /app/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f16ea89-d7f3-472a-9b40-fd34f996202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/prediction_container/prestart.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PREDICTION_CONTAINER}/prestart.sh\n",
    "#!/bin/bash\n",
    "export PORT=$AIP_HTTP_PORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01dda25f-7d3a-4975-8532-46a6b5c99d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/prediction_container/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PREDICTION_CONTAINER}/requirements.txt\n",
    "google-cloud-pubsub\n",
    "pillow\n",
    "tf-agents==0.16.0\n",
    "tensorflow==2.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d2d5293-e967-4fe8-8a87-7f55e982661b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/prediction_container/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PREDICTION_CONTAINER}/main.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Prediction server that uses a trained policy to give predicted actions.\"\"\"\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import fastapi\n",
    "\n",
    "from google.cloud import pubsub_v1\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_agents\n",
    "from tf_agents import policies\n",
    "\n",
    "\n",
    "app = fastapi.FastAPI()\n",
    "app_vars = {\"trained_policy\": None}\n",
    "\n",
    "\n",
    "def _startup_event() -> None:\n",
    "    \"\"\"\n",
    "    Loads the trained policy at startup\n",
    "    \"\"\"\n",
    "    app_vars[\"trained_policy\"] = tf.saved_model.load(\n",
    "        os.environ[\"AIP_STORAGE_URI\"]\n",
    "    )\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event() -> None:\n",
    "    \"\"\"\n",
    "    Loads the trained policy at startup\n",
    "    \"\"\"\n",
    "    _startup_event()\n",
    "\n",
    "def _health() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Handles server health check requests.\n",
    "    \n",
    "    Returns:\n",
    "      An empty dict.\n",
    "    \"\"\"\n",
    "    return {}\n",
    "\n",
    "\n",
    "@app.get(os.environ[\"AIP_HEALTH_ROUTE\"], status_code=200)\n",
    "def health() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Handles server health check requests\n",
    "\n",
    "    Returns:\n",
    "      An empty dict.\n",
    "    \"\"\"\n",
    "    return _health()\n",
    "\n",
    "\n",
    "def _message_logger_via_pubsub(\n",
    "    project_id: str\n",
    "    , logger_pubsub_topic: str\n",
    "    , observations: List[Dict[str, List[List[float]]]]\n",
    "    , predicted_actions: List[Dict[str, List[float]]]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Send a message to the Pub/Sub topic which triggers the Logger.\n",
    "\n",
    "    Package observations and the corresponding predicted actions in a message JSON\n",
    "    and send to Pub/Sub topic.\n",
    "\n",
    "    Args:\n",
    "      project_id: GCP project ID.\n",
    "      logger_pubsub_topic: Name of Pub/Sub topic that triggers the Logger.\n",
    "      observations: List of `{\"observation\": <observation>}` in the prediction\n",
    "        request.\n",
    "      predicted_actions: List of `{\"predicted_action\": <predicted_action>}`\n",
    "        corresponding to the observations.\n",
    "    \"\"\"\n",
    "    # Create message with observations and predicted actions.\n",
    "    message_json = json.dumps({\n",
    "        \"observations\": observations\n",
    "        , \"predicted_actions\": predicted_actions\n",
    "    })\n",
    "    message_bytes = message_json.encode(\"utf-8\")\n",
    "\n",
    "    # Instantiate a Pub/Sub client.\n",
    "    publisher = pubsub_v1.PublisherClient()\n",
    "\n",
    "    # Get the Logger's Pub/Sub topic.\n",
    "    topic_path = publisher.topic_path(project_id, logger_pubsub_topic)\n",
    "\n",
    "    # Send message.\n",
    "    publish_future = publisher.publish(topic_path, data=message_bytes)\n",
    "    publish_future.result()\n",
    "\n",
    "def _predict(\n",
    "    instances: List[Dict[str, List[List[float]]]]\n",
    "    , trained_policy: policies.TFPolicy\n",
    ") -> Dict[str, List[Dict[str, List[int]]]]:\n",
    "    \"\"\"\n",
    "    Gets predictions for the observations in `instances`; triggers the Logger.\n",
    "\n",
    "    Unpacks observations in `instances` and queries the trained policy for\n",
    "    predicted actions. Triggers the Logger with observations and predicted\n",
    "    actions.\n",
    "\n",
    "    Args:\n",
    "      instances: List of `{\"observation\": <observation>}` for which to generate\n",
    "        predictions.\n",
    "      trained_policy: Trained policy to generate predictions.\n",
    "\n",
    "    Returns:\n",
    "      A dict with the key \"predictions\" mapping to a list of predicted actions\n",
    "      corresponding to each observation in the prediction request.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    predicted_actions = []\n",
    "    for index, instance in enumerate(instances):\n",
    "        # Unpack observation and reconstruct TimeStep. Rewards default to 0.\n",
    "        batch_size = len(instance[\"observation\"])\n",
    "        time_step = tf_agents.trajectories.restart(\n",
    "            observation=instance[\"observation\"]\n",
    "            , batch_size=tf.convert_to_tensor([batch_size]))\n",
    "        policy_step = trained_policy.action(time_step)\n",
    "\n",
    "        predicted_action = policy_step.action.numpy().tolist()\n",
    "        predictions.append(\n",
    "            {f\"PolicyStep {index}\": predicted_action}\n",
    "        )\n",
    "        predicted_actions.append({\"predicted_action\": predicted_action})\n",
    "\n",
    "    # Trigger the Logger to log prediction inputs and results.\n",
    "    _message_logger_via_pubsub(\n",
    "        project_id=os.environ[\"PROJECT_ID\"]\n",
    "        , logger_pubsub_topic=os.environ[\"LOGGER_PUBSUB_TOPIC\"]\n",
    "        , observations=instances\n",
    "        , predicted_actions=predicted_actions\n",
    "    )\n",
    "    return {\"predictions\": predictions}\n",
    "\n",
    "@app.post(os.environ[\"AIP_PREDICT_ROUTE\"])\n",
    "async def predict(\n",
    "    request: fastapi.Request\n",
    ") -> Dict[str, List[Dict[str, List[int]]]]:\n",
    "    \"\"\"\n",
    "    Handles prediction requests.\n",
    "\n",
    "    Unpacks observations in prediction requests and queries the trained policy for\n",
    "    predicted actions.\n",
    "\n",
    "    Args:\n",
    "      request: Incoming prediction requests that contain observations.\n",
    "\n",
    "    Returns:\n",
    "      A dict with the key \"predictions\" mapping to a list of predicted actions\n",
    "      corresponding to each observation in the prediction request.\n",
    "    \"\"\"\n",
    "    body = await request.json()\n",
    "    instances = body[\"instances\"]\n",
    "    \n",
    "    return _predict(\n",
    "        instances, app_vars[\"trained_policy\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c133b1d-8055-4207-99aa-1e18a7e6ed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export DOCKERNAME=prediction\n",
      "export IMAGE_URI=gcr.io/hybrid-vertex/prediction_container\n",
      "export FILE_LOCATION=./src/prediction_container\n",
      "export MACHINE_TYPE=e2-highcpu-32\n",
      "export ARTIFACTS_DIR=gs://e2ev4-hybrid-vertex-bucket/artifacts\n"
     ]
    }
   ],
   "source": [
    "# Docker definitions for training\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{PREDICTION_CONTAINER}'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = f'./{REPO_DOCKER_PATH_PREFIX}/{PREDICTION_CONTAINER}'\n",
    "\n",
    "print(f\"export DOCKERNAME={DOCKERNAME}\")\n",
    "print(f\"export IMAGE_URI={IMAGE_URI}\")\n",
    "print(f\"export FILE_LOCATION={FILE_LOCATION}\")\n",
    "print(f\"export MACHINE_TYPE={MACHINE_TYPE}\")\n",
    "print(f\"export ARTIFACTS_DIR={ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45868153-59fb-475b-9191-def13a372a81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! gcloud builds submit --config {REPO_DOCKER_PATH_PREFIX}/{PREDICTION_CONTAINER}/cloudbuild.yaml -q \\\n",
    "#     --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION,_ARTIFACTS_DIR=$ARTIFACTS_DIR,_PROJECT_ID=$PROJECT_ID,_LOGGER_PUBSUB_TOPIC=$LOGGER_PUBSUB_TOPIC \\\n",
    "#     --timeout=2h \\\n",
    "#     --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf9dda4-7279-4415-b409-c83058c4a785",
   "metadata": {},
   "source": [
    "## Create and run pipeline\n",
    "\n",
    "> Here, we build a \"startup\" pipeline that generates randomly sampled training data (with the Generator) as the first step. This pipeline runs only once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae1a57-eb15-4248-8b45-a215a250b62f",
   "metadata": {},
   "source": [
    "### set pipeline args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccd9972f-c298-4a9c-aa9c-e89f3fc65ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_CACHING              : True\n",
      "CPU_LIMIT                   : 8\n",
      "MEMORY_LIMIT                : 8G\n",
      "TRAINING_ARTIFACTS_DIR      : gs://e2ev4-hybrid-vertex-bucket/artifacts\n",
      "TRAINING_REPLICA_COUNT      : 1\n",
      "TRAINING_MACHINE_TYPE       : n1-standard-4\n",
      "TRAINING_ACCELERATOR_TYPE   : ACCELERATOR_TYPE_UNSPECIFIED\n",
      "TRAINING_ACCELERATOR_COUNT  : 0\n",
      "TRAINED_POLICY_DISPLAY_NAME : movielens-trained-policy\n",
      "TRAFFIC_SPLIT               : {'0': 100}\n",
      "ENDPOINT_DISPLAY_NAME       : movielens-endpoint\n",
      "ENDPOINT_MACHINE_TYPE       : n1-standard-4\n",
      "ENDPOINT_REPLICA_COUNT      : 1\n",
      "ENDPOINT_ACCELERATOR_TYPE   : ACCELERATOR_TYPE_UNSPECIFIED\n",
      "ENDPOINT_ACCELERATOR_COUNT  : 0\n"
     ]
    }
   ],
   "source": [
    "ENABLE_CACHING = True\n",
    "CPU_LIMIT = \"8\"    # vCPUs\n",
    "MEMORY_LIMIT = \"8G\"\n",
    "\n",
    "# =====================================\n",
    "# Trainer parameters\n",
    "# =====================================\n",
    "# Root directory for training artifacts.\n",
    "TRAINING_ARTIFACTS_DIR = (\n",
    "    f\"{BUCKET_URI}/artifacts\"\n",
    ")\n",
    "# Type of machine & number of replica to run the custom training job\n",
    "TRAINING_REPLICA_COUNT = 1\n",
    "TRAINING_MACHINE_TYPE = (\n",
    "    \"n1-standard-4\"\n",
    ")\n",
    "# Type and count of accelerators to run the custom training job.\n",
    "TRAINING_ACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"\n",
    "TRAINING_ACCELERATOR_COUNT = 0\n",
    "\n",
    "# =====================================\n",
    "# Deployer parameters\n",
    "# =====================================\n",
    "# Display name of the uploaded and deployed policy.\n",
    "TRAINED_POLICY_DISPLAY_NAME = (\n",
    "    \"movielens-trained-policy\" \n",
    ")\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "ENDPOINT_DISPLAY_NAME = \"movielens-endpoint\"                # Display name of the prediction endpoint.\n",
    "ENDPOINT_MACHINE_TYPE = \"n1-standard-4\"                     # Type of machine of the prediction endpoint.\n",
    "ENDPOINT_REPLICA_COUNT = 1                                  # Number of replicas of prediction endpoint.\n",
    "ENDPOINT_ACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"  # Type of accelerators for prediction endpoint\n",
    "ENDPOINT_ACCELERATOR_COUNT = 0                              # Number of accelerators for prediction endpoint\n",
    "\n",
    "print(f\"ENABLE_CACHING              : {ENABLE_CACHING}\")\n",
    "print(f\"CPU_LIMIT                   : {CPU_LIMIT}\")\n",
    "print(f\"MEMORY_LIMIT                : {MEMORY_LIMIT}\")\n",
    "print(f\"TRAINING_ARTIFACTS_DIR      : {TRAINING_ARTIFACTS_DIR}\")\n",
    "print(f\"TRAINING_REPLICA_COUNT      : {TRAINING_REPLICA_COUNT}\")\n",
    "print(f\"TRAINING_MACHINE_TYPE       : {TRAINING_MACHINE_TYPE}\")\n",
    "print(f\"TRAINING_ACCELERATOR_TYPE   : {TRAINING_ACCELERATOR_TYPE}\")\n",
    "print(f\"TRAINING_ACCELERATOR_COUNT  : {TRAINING_ACCELERATOR_COUNT}\")\n",
    "print(f\"TRAINED_POLICY_DISPLAY_NAME : {TRAINED_POLICY_DISPLAY_NAME}\")\n",
    "print(f\"TRAFFIC_SPLIT               : {TRAFFIC_SPLIT}\")\n",
    "print(f\"ENDPOINT_DISPLAY_NAME       : {ENDPOINT_DISPLAY_NAME}\")\n",
    "print(f\"ENDPOINT_MACHINE_TYPE       : {ENDPOINT_MACHINE_TYPE}\")\n",
    "print(f\"ENDPOINT_REPLICA_COUNT      : {ENDPOINT_REPLICA_COUNT}\")\n",
    "print(f\"ENDPOINT_ACCELERATOR_TYPE   : {ENDPOINT_ACCELERATOR_TYPE}\")\n",
    "print(f\"ENDPOINT_ACCELERATOR_COUNT  : {ENDPOINT_ACCELERATOR_COUNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86c3fb46-daaa-427f-8349-aa1b6e7d8ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_TAG: movielens-pipeline-startup-v17\n",
      "PIPELINE_NAME: tfab-e2ev4-movielens-pipeline-startup-v17\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_VERSION = 'v17'\n",
    "\n",
    "PIPELINE_TAG = f'movielens-pipeline-startup-{PIPELINE_VERSION}'\n",
    "print(\"PIPELINE_TAG:\", PIPELINE_TAG)\n",
    "\n",
    "PIPELINE_NAME = f'tfab-{PREFIX}-{PIPELINE_TAG}'.replace('_', '-')\n",
    "print(\"PIPELINE_NAME:\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e42806f4-2b9a-4ed8-98b2-0cf584815dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)\n",
    "\n",
    "from src.generator import generator_component\n",
    "from src.ingester import ingester_component\n",
    "# from src.trainer import trainer_component\n",
    "\n",
    "@kfp.v2.dsl.pipeline(\n",
    "    name=f'{PIPELINE_NAME}'.replace('_', '-')\n",
    ")\n",
    "def pipeline(\n",
    "    # Pipeline configs\n",
    "    project_id: str,\n",
    "    raw_data_path: str,\n",
    "    training_artifacts_dir: str,\n",
    "    pipeline_version: str,\n",
    "    # BigQuery configs\n",
    "    bigquery_dataset_id: str,\n",
    "    bigquery_location: str,\n",
    "    bigquery_table_id: str,\n",
    "    bigquery_max_rows: int = 10000,\n",
    "    # TF-Agents RL configs\n",
    "    batch_size: int = 8,\n",
    "    rank_k: int = 20,\n",
    "    num_actions: int = 20,\n",
    "    driver_steps: int = 3,\n",
    "    num_epochs: int = 5,\n",
    "    tikhonov_weight: float = 0.01,\n",
    "    agent_alpha: float = 10,\n",
    "):\n",
    "    \"\"\"Authors a RL pipeline for MovieLens movie recommendation system.\n",
    "\n",
    "    Integrates the Generator, Ingester, Trainer and Deployer components. This\n",
    "    pipeline generates initial training data with a random policy and runs once\n",
    "    as the initiation of the system.\n",
    "\n",
    "    Args:\n",
    "      project_id: GCP project ID. This is required because otherwise the BigQuery\n",
    "        client will use the ID of the tenant GCP project created as a result of\n",
    "        KFP, which doesn't have proper access to BigQuery.\n",
    "      raw_data_path: Path to MovieLens 100K's \"u.data\" file.\n",
    "      training_artifacts_dir: Path to store the Trainer artifacts (trained policy).\n",
    "\n",
    "      bigquery_dataset: A string of the BigQuery dataset ID in the format of\n",
    "        \"project.dataset\".\n",
    "      bigquery_location: A string of the BigQuery dataset location.\n",
    "      bigquery_table_id: A string of the BigQuery table ID in the format of\n",
    "        \"project.dataset.table\".\n",
    "      bigquery_max_rows: Optional; maximum number of rows to ingest.\n",
    "\n",
    "      batch_size: Optional; batch size of environment generated quantities eg.\n",
    "        rewards.\n",
    "      rank_k: Optional; rank for matrix factorization in the MovieLens environment;\n",
    "        also the observation dimension.\n",
    "      num_actions: Optional; number of actions (movie items) to choose from.\n",
    "      driver_steps: Optional; number of steps to run per batch.\n",
    "      num_epochs: Optional; number of training epochs.\n",
    "      tikhonov_weight: Optional; LinUCB Tikhonov regularization weight of the\n",
    "        Trainer.\n",
    "      agent_alpha: Optional; LinUCB exploration parameter that multiplies the\n",
    "        confidence intervals of the Trainer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========================================================================\n",
    "    # generator\n",
    "    # ========================================================================\n",
    "    \n",
    "    generate_task = (\n",
    "        generator_component.generate_movielens_dataset_for_bigquery(\n",
    "            project_id = project_id\n",
    "            , raw_data_path = raw_data_path\n",
    "            , batch_size = batch_size\n",
    "            , rank_k = rank_k\n",
    "            , num_actions = num_actions\n",
    "            , driver_steps = driver_steps\n",
    "            , bigquery_tmp_file = BIGQUERY_TMP_FILE\n",
    "            , bigquery_dataset_id = bigquery_dataset_id\n",
    "            , bigquery_location = bigquery_location\n",
    "            , bigquery_table_id = bigquery_table_id\n",
    "        )\n",
    "        .set_display_name(\"generator\")\n",
    "        .set_caching_options(False)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # ingester\n",
    "    # ========================================================================\n",
    "    \n",
    "    ingest_task = (\n",
    "        ingester_component.ingest_bigquery_dataset_into_tfrecord(\n",
    "            project_id = project_id\n",
    "            , bigquery_table_id = generate_task.outputs['bigquery_table_id']\n",
    "            , tfrecord_file_input = TFRECORD_FILE\n",
    "            , bigquery_max_rows = bigquery_max_rows\n",
    "        )\n",
    "        .set_display_name(\"ingester\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "            \n",
    "    # ========================================================================\n",
    "    # trainer\n",
    "    # ========================================================================\n",
    "\n",
    "    train_task = (\n",
    "        custom_job_op(\n",
    "            training_artifacts_dir_input=training_artifacts_dir\n",
    "            , tfrecord_file=ingest_task.outputs[\"tfrecord_file\"]\n",
    "            , num_epochs=num_epochs\n",
    "            , rank_k=rank_k\n",
    "            , num_actions=num_actions\n",
    "            , tikhonov_weight=tikhonov_weight\n",
    "            , agent_alpha=agent_alpha\n",
    "            , project=PROJECT_ID\n",
    "            , location=REGION\n",
    "        )\n",
    "        .set_display_name(\"trainer\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "            \n",
    "    # ========================================================================\n",
    "    # deployer\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Upload the trained policy as a model.\n",
    "    model_upload_op = (\n",
    "        gcc_aip.ModelUploadOp(\n",
    "            project=project_id\n",
    "            , display_name=TRAINED_POLICY_DISPLAY_NAME\n",
    "            , artifact_uri=train_task.outputs[\"training_artifacts_dir\"]\n",
    "            , serving_container_image_uri=f\"gcr.io/{PROJECT_ID}/{PREDICTION_CONTAINER}:latest\"\n",
    "        )\n",
    "        .set_display_name(\"register model\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    # Create a Vertex AI endpoint\n",
    "    endpoint_create_op = (\n",
    "        gcc_aip.EndpointCreateOp(\n",
    "            project=project_id, display_name=ENDPOINT_DISPLAY_NAME\n",
    "        )\n",
    "        .set_display_name(\"create endpoint\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    # Deploy the uploaded, trained policy to the created endpoint\n",
    "    deploy_model_op = (\n",
    "        gcc_aip.ModelDeployOp(\n",
    "            endpoint=endpoint_create_op.outputs[\"endpoint\"]\n",
    "            , model=model_upload_op.outputs[\"model\"]\n",
    "            , deployed_model_display_name=TRAINED_POLICY_DISPLAY_NAME\n",
    "            , traffic_split=TRAFFIC_SPLIT\n",
    "            , dedicated_resources_machine_type=ENDPOINT_MACHINE_TYPE\n",
    "            , dedicated_resources_accelerator_type=ENDPOINT_ACCELERATOR_TYPE\n",
    "            , dedicated_resources_accelerator_count=ENDPOINT_ACCELERATOR_COUNT\n",
    "            , dedicated_resources_min_replica_count=ENDPOINT_REPLICA_COUNT\n",
    "        )\n",
    "        .set_display_name(\"deploy model\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01360ca8-808a-48db-ab80-753817a2eff7",
   "metadata": {},
   "source": [
    "### compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a934714a-f318-4f7f-94a9-6676feae4b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -f custom_container_pipeline_spec.json\n",
    "\n",
    "PIPELINE_JSON_SPEC_LOCAL = \"custom_pipeline_spec.json\"\n",
    "\n",
    "! rm -f $PIPELINE_JSON_SPEC_LOCAL\n",
    "\n",
    "kfp.v2.compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=PIPELINE_JSON_SPEC_LOCAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e02abeb-ec29-4ab2-9b24-7c472fa5f5ca",
   "metadata": {},
   "source": [
    "### save pipeline spec json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2bdd563-112e-494a-a021-77235022cb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT: gs://e2ev4-hybrid-vertex-bucket/pipeline\n",
      "PIPELINES_FILEPATH: gs://e2ev4-hybrid-vertex-bucket/pipeline/pipeline_spec.json\n"
     ]
    }
   ],
   "source": [
    "# !gsutil cp custom_container_pipeline_spec.json $PIPELINE_ROOT_PATH/pipeline_spec.json\n",
    "\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline\"\n",
    "print(\"PIPELINE_ROOT:\", PIPELINE_ROOT)\n",
    "\n",
    "PIPELINES_FILEPATH = f'{PIPELINE_ROOT}/pipeline_spec.json'\n",
    "print(\"PIPELINES_FILEPATH:\", PIPELINES_FILEPATH)\n",
    "\n",
    "!gsutil -q cp $PIPELINE_JSON_SPEC_LOCAL $PIPELINES_FILEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80a84d3d-97a4-4fc6-be35-f7583f13d73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://e2ev4-hybrid-vertex-bucket/pipeline/pipeline_spec.json\n",
      "gs://e2ev4-hybrid-vertex-bucket/pipeline/934903580331/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a876e8b5-6796-4304-b03e-74f095b59326",
   "metadata": {},
   "source": [
    "## Submit pipeline to Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bca8e63e-a370-45dd-8330-2346185b5132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline run job.\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=f\"{PIPELINE_NAME}-startup\"\n",
    "    , template_path=PIPELINE_JSON_SPEC_LOCAL\n",
    "    , pipeline_root=PIPELINE_ROOT\n",
    "    , failure_policy='fast'\n",
    "    , parameter_values={\n",
    "        # Pipeline configs\n",
    "        \"project_id\": PROJECT_ID\n",
    "        , \"raw_data_path\": RAW_DATA_PATH\n",
    "        , \"training_artifacts_dir\": TRAINING_ARTIFACTS_DIR\n",
    "        , \"pipeline_version\" : PIPELINE_VERSION\n",
    "        # BigQuery configs\n",
    "        , \"bigquery_dataset_id\": BIGQUERY_DATASET_ID\n",
    "        , \"bigquery_location\": BQ_LOCATION\n",
    "        , \"bigquery_table_id\": BIGQUERY_TABLE_ID\n",
    "    }\n",
    "    , enable_caching=ENABLE_CACHING,\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=False\n",
    "    , service_account=VERTEX_SA\n",
    "    , network=VPC_NETWORK_FULL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3e4e8f-3242-4287-a7f1-4d2b6d68f029",
   "metadata": {},
   "source": [
    "## Create Simulator \n",
    "\n",
    "* sends simulated MovieLens prediction requests\n",
    "\n",
    "Create the Simulator to [obtain observations](https://github.com/tensorflow/agents/blob/v0.8.0/tf_agents/bandits/environments/movielens_py_environment.py#L118-L125) from the MovieLens simulation environment, formats them, and sends prediction requests to the Vertex AI endpoint.\n",
    "\n",
    "The workflow is: Cloud Scheduler --> Pub/Sub --> Cloud Functions --> Endpoint\n",
    "\n",
    "In production, this Simulator logic can be modified to that of gathering real-world input features as observations, getting prediction results from the endpoint and communicating those results to real-world users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "723feda4-34d7-4c95-bb4c-d5061e399c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulator parameters\n",
    "SIMULATOR_PUBSUB_TOPIC = (\n",
    "    \"simulator-pubsub-topic\"  # Pub/Sub topic name for the Simulator.\n",
    ")\n",
    "SIMULATOR_CLOUD_FUNCTION = (\n",
    "    \"simulator-cloud-function\"  # Cloud Functions name for the Simulator.\n",
    ")\n",
    "SIMULATOR_SCHEDULER_JOB = (\n",
    "    \"simulator-scheduler-job\"  # Cloud Scheduler cron job name for the Simulator.\n",
    ")\n",
    "SIMULATOR_SCHEDULE = \"*/5 * * * *\"  # Cloud Scheduler cron job schedule for the Simulator. Eg. \"*/5 * * * *\" means every 5 mins.\n",
    "SIMULATOR_SCHEDULER_MESSAGE = (\n",
    "    \"simulator-message\"  # Cloud Scheduler message for the Simulator.\n",
    ")\n",
    "# TF-Agents RL configs\n",
    "BATCH_SIZE = 8\n",
    "RANK_K = 20\n",
    "NUM_ACTIONS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dad9b62e-8974-4759-ab17-497bbd61a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENT_SUBDIR = \"simulator\"\n",
    "\n",
    "# Make the generator subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "476e9027-0614-4a6d-90cd-c6a4d4465b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/simulator/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}/main.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"The Simulator component for sending recurrent prediction requests.\"\"\"\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Dict\n",
    "\n",
    "import dataclasses\n",
    "from google import cloud  # For patch of google.cloud.aiplatform to work.\n",
    "from google.cloud import aiplatform  # For using the module.  # pylint: disable=unused-import\n",
    "import tensorflow as tf  # For tf_agents to work.  # pylint: disable=unused-import\n",
    "from tf_agents.bandits.environments import movielens_py_environment\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class EnvVars:\n",
    "    \"\"\"A class containing environment variables and their values.\n",
    "\n",
    "    Attributes:\n",
    "      project_id: A string of the GCP project ID.\n",
    "      region: A string of the GCP service region.\n",
    "      endpoint_id: A string of the Vertex AI endpoint ID.\n",
    "      raw_data_path: A string of the path to MovieLens 100K's \"u.data\" file.\n",
    "      rank_k: An integer of the rank for matrix factorization in the MovieLens\n",
    "        environment; also the observation dimension.\n",
    "      batch_size: A integer of the batch size of environment generated quantities.\n",
    "      num_actions: A integer of the number of actions (movie items) to choose\n",
    "        from.\n",
    "    \"\"\"\n",
    "    project_id: str\n",
    "    region: str\n",
    "    endpoint_id: str\n",
    "    raw_data_path: str\n",
    "    rank_k: int\n",
    "    batch_size: int\n",
    "    num_actions: int\n",
    "\n",
    "\n",
    "def get_env_vars() -> EnvVars:\n",
    "    \"\"\"\n",
    "    Gets a set of environment variables necessary for `simulate`.\n",
    "\n",
    "    Returns:\n",
    "      A `EnvVars` of environment variables for configuring `simulate`.\n",
    "    \"\"\"\n",
    "    return EnvVars(\n",
    "        project_id=os.getenv(\"PROJECT_ID\")\n",
    "        , region=os.getenv(\"REGION\")\n",
    "        , endpoint_id=os.getenv(\"ENDPOINT_ID\")\n",
    "        , raw_data_path=os.getenv(\"RAW_DATA_PATH\")\n",
    "        , rank_k=int(os.getenv(\"RANK_K\"))\n",
    "        , batch_size=int(os.getenv(\"BATCH_SIZE\"))\n",
    "        , num_actions=int(os.getenv(\"NUM_ACTIONS\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def simulate(event: Dict[str, Any], context) -> None:  # pylint: disable=unused-argument\n",
    "    \"\"\"\n",
    "    Gets observations and sends prediction requests to endpoints.\n",
    "\n",
    "    Queries the MovieLens simulation environment for observations and sends\n",
    "    prediction requests with the observations to the Vertex endpoint.\n",
    "\n",
    "    Serves as the Simulator and the entrypoint of Cloud Functions.\n",
    "\n",
    "    Note: In production, this function can be modified to hold the logic of\n",
    "    gathering real-world input features as observations, getting prediction\n",
    "    results from the endpoint and communicating those results to real-world\n",
    "    users.\n",
    "\n",
    "    Args:\n",
    "      event: Triggering event of this function.\n",
    "      context: Trigerring context of this function.\n",
    "        This is of type `functions_v1.context.Context` but not specified since\n",
    "        it is not importable for a local environment that wants to run unit\n",
    "        tests.\n",
    "    \"\"\"\n",
    "    env_vars = get_env_vars()\n",
    "\n",
    "    # Create MovieLens simulation environment.\n",
    "    env = movielens_py_environment.MovieLensPyEnvironment(\n",
    "        env_vars.raw_data_path, env_vars.rank_k, env_vars.batch_size\n",
    "        , num_movies=env_vars.num_actions, csv_delimiter=\"\\t\"\n",
    "    )\n",
    "\n",
    "    # Get environment observation.\n",
    "    observation_array = env._observe()  # pylint: disable=protected-access\n",
    "    # Convert to nested list to be sent to the endpoint for prediction.\n",
    "    observation = [\n",
    "        list(observation_batch) for observation_batch in observation_array\n",
    "    ]\n",
    "\n",
    "    cloud.aiplatform.init(\n",
    "        project=env_vars.project_id, location=env_vars.region\n",
    "    )\n",
    "    endpoint = cloud.aiplatform.Endpoint(env_vars.endpoint_id)\n",
    "\n",
    "    # Send prediction request to endpoint and get prediction result.\n",
    "    predictions = endpoint.predict(\n",
    "        instances=[\n",
    "            {\"observation\": observation},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    logging.info(\"prediction result: %s\", predictions[0])\n",
    "    logging.info(\"prediction model ID: %s\", predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b21f8f8-49c5-4a31-8e25-416960503c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/simulator/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}/requirements.txt\n",
    "dataclasses\n",
    "google-cloud-aiplatform\n",
    "tensorflow==2.12.0\n",
    "pillow\n",
    "tf-agents==0.16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569a68b6-3b51-4171-a3e1-58cce040d740",
   "metadata": {},
   "source": [
    "### Create Pub/Sub topic\n",
    "* Read more about creating Pub/Sub topics [here](https://cloud.google.com/functions/docs/tutorials/pubsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2be7b08d-e4f5-475d-b334-4dfa165ffddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created topic [projects/hybrid-vertex/topics/simulator-pubsub-topic].\n"
     ]
    }
   ],
   "source": [
    "! gcloud pubsub topics create $SIMULATOR_PUBSUB_TOPIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58afed1a-dd08-4823-b6a2-d1b862f1b03e",
   "metadata": {},
   "source": [
    "### Set up a recurrent Cloud Scheduler job for the Pub/Sub topic\n",
    "* Read more about possible ways to create cron jobs [here](https://cloud.google.com/scheduler/docs/creating#gcloud).\n",
    "* Read about the cron job schedule format [here](https://man7.org/linux/man-pages/man5/crontab.5.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3da736cd-fecd-4070-a959-f2fd605eeeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulator-scheduler-job --schedule=*/5 * * * * --topic=simulator-pubsub-topic --message-body=simulator-message\n"
     ]
    }
   ],
   "source": [
    "scheduler_job_args = \" \".join(\n",
    "    [\n",
    "        SIMULATOR_SCHEDULER_JOB,\n",
    "        f\"--schedule='{SIMULATOR_SCHEDULE}'\",\n",
    "        f\"--topic={SIMULATOR_PUBSUB_TOPIC}\",\n",
    "        f\"--message-body={SIMULATOR_SCHEDULER_MESSAGE}\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "! echo $scheduler_job_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15aafa39-f001-483e-bb03-9c46618f9d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: projects/hybrid-vertex/locations/us-central1/jobs/simulator-scheduler-job\n",
      "pubsubTarget:\n",
      "  data: c2ltdWxhdG9yLW1lc3NhZ2U=\n",
      "  topicName: projects/hybrid-vertex/topics/simulator-pubsub-topic\n",
      "retryConfig:\n",
      "  maxBackoffDuration: 3600s\n",
      "  maxDoublings: 16\n",
      "  maxRetryDuration: 0s\n",
      "  minBackoffDuration: 5s\n",
      "schedule: '*/5 * * * *'\n",
      "state: ENABLED\n",
      "timeZone: Etc/UTC\n",
      "userUpdateTime: '2023-07-05T10:48:05Z'\n"
     ]
    }
   ],
   "source": [
    "! gcloud scheduler jobs create pubsub $scheduler_job_args --location=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe57b85-1281-43dc-9328-8838a1c1801b",
   "metadata": {},
   "source": [
    "### Define Simulator logic in Cloud Function to be triggered periodically, deploy this Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b378328-742f-46de-ac8f-51fb3455bbcb",
   "metadata": {},
   "source": [
    "* Specify dependencies of the Function in `src/simulator/requirements.txt`.\n",
    "* Read more about the available configurable arguments for deploying a Function [here](https://cloud.google.com/sdk/gcloud/reference/functions/deploy). For instance, based on the complexity of your Function, you may want to adjust its memory and timeout.\n",
    "* Note that the environment variables in `ENV_VARS` should be comma-separated; there should not be additional spaces, or other characters in between. Read more about setting/updating/deleting environment variables [here](https://cloud.google.com/functions/docs/env-var).\n",
    "* Read more about sending predictions to Vertex endpoints [here](https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3972b5cc-ee33-4955-9464-76a1df166688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "ENDPOINT_ID          DISPLAY_NAME\n",
      "1283028914544836608  movielens-endpoint \n",
      "\n",
      "ENDPOINT_ID=1283028914544836608\n"
     ]
    }
   ],
   "source": [
    "endpoints = ! gcloud ai endpoints list --region=$REGION \\\n",
    "    --filter=display_name=$ENDPOINT_DISPLAY_NAME\n",
    "\n",
    "print(\"\\n\".join(endpoints), \"\\n\")\n",
    "\n",
    "ENDPOINT_ID = endpoints[2].split(\" \")[0]\n",
    "print(f\"ENDPOINT_ID={ENDPOINT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f23d51f-7ab6-44d4-960b-06d9736d0466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID=hybrid-vertex,REGION=us-central1,ENDPOINT_ID=1283028914544836608,RAW_DATA_PATH=gs://e2ev4-hybrid-vertex-bucket/raw_data/u.data,BATCH_SIZE=8,RANK_K=20,NUM_ACTIONS=20\n"
     ]
    }
   ],
   "source": [
    "ENV_VARS = \",\".join(\n",
    "    [\n",
    "        f\"PROJECT_ID={PROJECT_ID}\",\n",
    "        f\"REGION={REGION}\",\n",
    "        f\"ENDPOINT_ID={ENDPOINT_ID}\",\n",
    "        f\"RAW_DATA_PATH={RAW_DATA_PATH}\",\n",
    "        f\"BATCH_SIZE={BATCH_SIZE}\",\n",
    "        f\"RANK_K={RANK_K}\",\n",
    "        f\"NUM_ACTIONS={NUM_ACTIONS}\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "! echo $ENV_VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29183c65-4d78-4bf1-ade9-a689b34dc2fc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying function (may take a while - up to 2 minutes)...                    \n",
      "For Cloud Build Logs, visit: https://console.cloud.google.com/cloud-build/builds;region=us-central1/16b62bcf-69ab-47b3-af80-24084472b02e?project=934903580331\n",
      "Deploying function (may take a while - up to 2 minutes)...done.                \n",
      "availableMemoryMb: 512\n",
      "buildId: 16b62bcf-69ab-47b3-af80-24084472b02e\n",
      "buildName: projects/934903580331/locations/us-central1/builds/16b62bcf-69ab-47b3-af80-24084472b02e\n",
      "dockerRegistry: CONTAINER_REGISTRY\n",
      "entryPoint: simulate\n",
      "environmentVariables:\n",
      "  BATCH_SIZE: '8'\n",
      "  ENDPOINT_ID: '1283028914544836608'\n",
      "  NUM_ACTIONS: '20'\n",
      "  PROJECT_ID: hybrid-vertex\n",
      "  RANK_K: '20'\n",
      "  RAW_DATA_PATH: gs://e2ev4-hybrid-vertex-bucket/raw_data/u.data\n",
      "  REGION: us-central1\n",
      "eventTrigger:\n",
      "  eventType: google.pubsub.topic.publish\n",
      "  failurePolicy: {}\n",
      "  resource: projects/hybrid-vertex/topics/simulator-pubsub-topic\n",
      "  service: pubsub.googleapis.com\n",
      "ingressSettings: ALLOW_ALL\n",
      "labels:\n",
      "  deployment-tool: cli-gcloud\n",
      "maxInstances: 3000\n",
      "name: projects/hybrid-vertex/locations/us-central1/functions/simulator-cloud-function\n",
      "runtime: python310\n",
      "serviceAccountEmail: hybrid-vertex@appspot.gserviceaccount.com\n",
      "sourceArchiveUrl: gs://e2ev4-hybrid-vertex-bucket/us-central1-projects/hybrid-vertex/locations/us-central1/functions/simulator-cloud-function-ifhgqhijljip.zip\n",
      "status: ACTIVE\n",
      "timeout: 200s\n",
      "updateTime: '2023-07-05T11:10:23.098Z'\n",
      "versionId: '2'\n"
     ]
    }
   ],
   "source": [
    "! gcloud functions deploy $SIMULATOR_CLOUD_FUNCTION --ingress-settings=all \\\n",
    "    --region=$REGION \\\n",
    "    --trigger-topic=$SIMULATOR_PUBSUB_TOPIC \\\n",
    "    --runtime=python310 \\\n",
    "    --memory=2048MB \\\n",
    "    --timeout=200s \\\n",
    "    --source=src/simulator \\\n",
    "    --entry-point=simulate \\\n",
    "    --stage-bucket=$BUCKET_NAME \\\n",
    "    --update-env-vars=$ENV_VARS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10967524-3cca-471a-a0f5-ca44563d679d",
   "metadata": {},
   "source": [
    "## Create Logger\n",
    "\n",
    "> asynchronously log prediction inputs and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba57cf-a32d-4a28-a041-10e65aff76bd",
   "metadata": {},
   "source": [
    "**The Logger** \n",
    "* gets environment feedback as rewards from the MovieLens simulation environment based on prediction observations and predicted actions, formulates trajectory data, and stores said data back to BigQuery \n",
    "* closes the RL feedback loop from prediction to training data, and allows re-training of the policy on new training data\n",
    "* triggered by a hook in the prediction code. At each prediction request, the prediction code messages a Pub/Sub topic, which triggers the Logger code.\n",
    "\n",
    "The workflow is: prediction container code (at prediction request) --> Pub/Sub --> Cloud Functions (logging predictions back to BigQuery)\n",
    "\n",
    "In production, this Logger logic can be modified to that of gathering real-world feedback (rewards) based on observations and predicted actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e50e36d4-6cb6-4e30-bbb3-2735c6a07c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENT_SUBDIR = \"logger\"\n",
    "\n",
    "# Make the generator subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5a89234-e9e7-4000-bd64-b4de28e7c1cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/logger/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}/main.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"The Logger component for logging prediction inputs and results.\"\"\"\n",
    "import base64\n",
    "import dataclasses\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import tensorflow as tf\n",
    "from tf_agents import trajectories\n",
    "from tf_agents.bandits.environments import movielens_py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class EnvVars:\n",
    "    \"\"\"\n",
    "    A class containing environment variables and their values.\n",
    "\n",
    "    Attributes:\n",
    "      project_id: A string of the GCP project ID.\n",
    "      raw_data_path: A string of the path to MovieLens 100K's \"u.data\" file.\n",
    "      batch_size: A integer of the batch size of environment generated quantities.\n",
    "      rank_k: An integer of the rank for matrix factorization in the MovieLens\n",
    "        environment; also the observation dimension.\n",
    "      num_actions: A integer of the number of actions (movie items) to choose\n",
    "        from.\n",
    "      bigquery_tmp_file: Path to a JSON file containing the training dataset.\n",
    "      bigquery_dataset_id: A string of the BigQuery dataset ID as\n",
    "        `project_id.dataset_id`.\n",
    "      bigquery_location: A string of the BigQuery dataset region.\n",
    "      bigquery_table_id: A string of the BigQuery table ID as\n",
    "        `project_id.dataset_id.table_id`.\n",
    "    \"\"\"\n",
    "    project_id: str\n",
    "    raw_data_path: str\n",
    "    batch_size: int\n",
    "    rank_k: int\n",
    "    num_actions: int\n",
    "    bigquery_tmp_file: str\n",
    "    bigquery_dataset_id: str\n",
    "    bigquery_location: str\n",
    "    bigquery_table_id: str\n",
    "\n",
    "\n",
    "def get_env_vars() -> EnvVars:\n",
    "    \"\"\"\n",
    "    Gets a set of environment variables necessary for `log`.\n",
    "\n",
    "    Returns:\n",
    "      A `EnvVars` of environment variables for configuring `log`.\n",
    "    \"\"\"\n",
    "    return EnvVars(\n",
    "        project_id=os.getenv(\"PROJECT_ID\"),\n",
    "        raw_data_path=os.getenv(\"RAW_DATA_PATH\"),\n",
    "        batch_size=int(os.getenv(\"BATCH_SIZE\")),\n",
    "        rank_k=int(os.getenv(\"RANK_K\")),\n",
    "        num_actions=int(os.getenv(\"NUM_ACTIONS\")),\n",
    "        bigquery_tmp_file=os.getenv(\"BIGQUERY_TMP_FILE\"),\n",
    "        bigquery_dataset_id=os.getenv(\"BIGQUERY_DATASET_ID\"),\n",
    "        bigquery_location=os.getenv(\"BIGQUERY_LOCATION\"),\n",
    "        bigquery_table_id=os.getenv(\"BIGQUERY_TABLE_ID\")\n",
    "    )\n",
    "\n",
    "\n",
    "def replace_observation_in_time_step(\n",
    "    original_time_step: trajectories.TimeStep\n",
    "    , observation: tf.Tensor\n",
    ") -> trajectories.TimeStep:\n",
    "    \"\"\"\n",
    "    Returns a `trajectories.TimeStep` with the observation field replaced.\n",
    "\n",
    "    Args:\n",
    "      original_time_step: The original `trajectories.TimeStep` in which the\n",
    "        `observation` will be filled in.\n",
    "      observation: A single, batched observation.\n",
    "\n",
    "    Returns:\n",
    "      A `trajectories.TimeStep` with `observation` filled into\n",
    "      `original_time_step`.\n",
    "    \"\"\"\n",
    "    return trajectories.TimeStep(\n",
    "        step_type=original_time_step[0]\n",
    "        , reward=original_time_step[1]\n",
    "        , discount=original_time_step[2]\n",
    "        , observation=observation\n",
    "    )\n",
    "\n",
    "\n",
    "def get_trajectory_from_environment(\n",
    "    environment: tf_py_environment.TFPyEnvironment\n",
    "    , observation: List[List[float]]\n",
    "    , predicted_action: int\n",
    ") -> trajectories.Trajectory:\n",
    "    \"\"\"\n",
    "    Gets trajectory data from `environment` based on observation and action.\n",
    "\n",
    "    Aligns `environment` observation to `observation` so that its feedback align\n",
    "    with `observation`. The `trajectories.Trajectory` object contains time step\n",
    "    information before and after applying `predicted_action` and feedback in the\n",
    "    form of a reward.\n",
    "\n",
    "    In production, this function can be replaced to actually pull feedback from\n",
    "    some real-world environment.\n",
    "\n",
    "    Args:\n",
    "      environment: A TF-Agents environment that holds observations, apply actions\n",
    "        and returns rewards.\n",
    "      observation: A single, batched observation.\n",
    "      predicted_action: A predicted action corresponding to the observation.\n",
    "\n",
    "    Returns:\n",
    "      A dict holding the same data as `trajectory`.\n",
    "    \"\"\"\n",
    "    environment.reset()\n",
    "\n",
    "    # Align environment to observation.\n",
    "    original_time_step = environment.current_time_step()\n",
    "    time_step = replace_observation_in_time_step(original_time_step, observation)\n",
    "    environment._time_step = time_step  # pylint: disable=protected-access\n",
    "\n",
    "    # Apply predicted action to environment.\n",
    "    environment.step(action=predicted_action)\n",
    "\n",
    "    # Get next time step.\n",
    "    next_time_step = environment.current_time_step()\n",
    "\n",
    "    # Get trajectory as an encapsulation of all feedback from the environment.\n",
    "    trajectory = trajectories.from_transition(\n",
    "        time_step=time_step,\n",
    "        action_step=trajectories.PolicyStep(action=predicted_action),\n",
    "        next_time_step=next_time_step\n",
    "    )\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def build_dict_from_trajectory(\n",
    "    trajectory: trajectories.Trajectory) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Builds a dict from `trajectory` data.\n",
    "\n",
    "    Args:\n",
    "      trajectory: A `trajectories.Trajectory` object.\n",
    "\n",
    "    Returns:\n",
    "      A dict holding the same data as `trajectory`.\n",
    "    \"\"\"\n",
    "    trajectory_dict = {\n",
    "        \"step_type\": trajectory.step_type.numpy().tolist()\n",
    "        , \"observation\": [\n",
    "            {\"observation_batch\": batch} for batch in trajectory.observation.numpy().tolist()\n",
    "        ]\n",
    "        , \"action\": trajectory.action.numpy().tolist()\n",
    "        , \"policy_info\": trajectory.policy_info\n",
    "        , \"next_step_type\": trajectory.next_step_type.numpy().tolist()\n",
    "        , \"reward\": trajectory.reward.numpy().tolist()\n",
    "        , \"discount\": trajectory.discount.numpy().tolist()\n",
    "    }\n",
    "    return trajectory_dict\n",
    "\n",
    "\n",
    "def write_trajectories_to_file(\n",
    "    dataset_file: str,\n",
    "    environment: tf_py_environment.TFPyEnvironment,\n",
    "    observations: List[Dict[str, List[List[float]]]],\n",
    "    predicted_actions: List[Dict[str, List[float]]]) -> None:\n",
    "    \"\"\"\n",
    "    Writes trajectory data to a file, each JSON in one line.\n",
    "\n",
    "    Gets `trajectories.Trajectory` objects that encapsulate environment\n",
    "    feedback eg. rewards based on `observations` and `predicted_actions`.\n",
    "    Each `trajectories.Trajectory` object gets written as one line to\n",
    "    `dataset_file` in JSON format. I.e., the `dataset_file` would be a\n",
    "    newline-delimited JSON file.\n",
    "\n",
    "    Args:\n",
    "      dataset_file: Path to a JSON file containing the training dataset.\n",
    "      environment: A TF-Agents environment that holds observations, apply actions\n",
    "        and returns rewards.\n",
    "      observations: List of `{\"observation\": <observation>}` in the prediction\n",
    "        request.\n",
    "      predicted_actions: List of `{\"predicted_action\": <predicted_action>}`\n",
    "        corresponding to the observations.\n",
    "    \"\"\"\n",
    "    with open(dataset_file, \"w\") as f:\n",
    "        for observation, predicted_action in zip(observations, predicted_actions):\n",
    "            trajectory = get_trajectory_from_environment(\n",
    "                environment=environment\n",
    "                , observation=tf.constant(observation[\"observation\"])\n",
    "                , predicted_action=tf.constant(predicted_action[\"predicted_action\"]))\n",
    "            trajectory_dict = build_dict_from_trajectory(trajectory)\n",
    "            f.write(json.dumps(trajectory_dict) + \"\\n\")\n",
    "\n",
    "\n",
    "def append_dataset_to_bigquery(\n",
    "    project_id: str,\n",
    "    dataset_file: str,\n",
    "    bigquery_dataset_id: str,\n",
    "    bigquery_location: str,\n",
    "    bigquery_table_id: str) -> None:\n",
    "    \"\"\"\n",
    "    Appends training dataset to BigQuery table.\n",
    "\n",
    "    Appends training dataset of `trajectories.Trajectory` in newline delimited\n",
    "    JSON to a BigQuery dataset and table, using a BigQuery client.\n",
    "\n",
    "    Args:\n",
    "      project_id: GCP project ID. This is required because otherwise the\n",
    "        BigQuery client will use the ID of the tenant GCP project created as a\n",
    "        result of KFP, which doesn't have proper access to BigQuery.\n",
    "      dataset_file: Path to a JSON file containing the training dataset.\n",
    "      bigquery_dataset_id: A string of the BigQuery dataset ID in the format of\n",
    "        \"project.dataset\".\n",
    "      bigquery_location: A string of the BigQuery dataset location.\n",
    "      bigquery_table_id: A string of the BigQuery table ID in the format of\n",
    "        \"project.dataset.table\".\n",
    "    \"\"\"\n",
    "    # Construct a BigQuery client object.\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Construct a full Dataset object to send to the API.\n",
    "    dataset = bigquery.Dataset(bigquery_dataset_id)\n",
    "\n",
    "    # Specify the geographic location where the dataset should reside.\n",
    "    dataset.location = bigquery_location\n",
    "\n",
    "    # Create the dataset, or get the dataset if it exists.\n",
    "    dataset = client.create_dataset(dataset, exists_ok=True, timeout=30)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND\n",
    "        , schema=[\n",
    "            bigquery.SchemaField(\"step_type\", \"INT64\", mode=\"REPEATED\")\n",
    "            , bigquery.SchemaField(\n",
    "                \"observation\"\n",
    "                , \"RECORD\"\n",
    "                , mode=\"REPEATED\"\n",
    "                , fields=[\n",
    "                    bigquery.SchemaField(\n",
    "                        \"observation_batch\", \"FLOAT64\", \"REPEATED\"\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            , bigquery.SchemaField(\"action\", \"INT64\", mode=\"REPEATED\")\n",
    "            , bigquery.SchemaField(\"policy_info\", \"FLOAT64\", mode=\"REPEATED\")\n",
    "            , bigquery.SchemaField(\"next_step_type\", \"INT64\", mode=\"REPEATED\")\n",
    "            , bigquery.SchemaField(\"reward\", \"FLOAT64\", mode=\"REPEATED\")\n",
    "            , bigquery.SchemaField(\"discount\", \"FLOAT64\", mode=\"REPEATED\")\n",
    "        ]\n",
    "        , source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "    )\n",
    "\n",
    "    with open(dataset_file, \"rb\") as source_file:\n",
    "        load_job = client.load_table_from_file(\n",
    "            source_file, bigquery_table_id, job_config=job_config\n",
    "        )\n",
    "\n",
    "    load_job.result()  # Wait for the job to complete.\n",
    "\n",
    "\n",
    "def log_prediction_to_bigquery(event: Dict[str, Any], context) -> None:  # pylint: disable=unused-argument\n",
    "    \"\"\"\n",
    "    Logs prediction inputs and results to BigQuery.\n",
    "\n",
    "    Queries the MovieLens simulation environment for rewards and other info based\n",
    "    on observations and predicted actions, and logs trajectory data to BigQuery.\n",
    "\n",
    "    Serves as the Logger and the entrypoint of Cloud Functions. The Logger closes\n",
    "    the feedback loop from prediction results to training data, and allows\n",
    "    re-training of the policy with new training data.\n",
    "\n",
    "    Note: In production, this function can be modified to hold the logic of\n",
    "    gathering real-world feedback for observations and predicted actions,\n",
    "    formulating trajectory data, and storing back into BigQuery.\n",
    "\n",
    "    Args:\n",
    "      event: Triggering event of this function.\n",
    "      context: Trigerring context of this function.\n",
    "        This is of type `functions_v1.context.Context` but not specified since\n",
    "        it is not importable for a local environment that wants to run unit\n",
    "        tests.\n",
    "    \"\"\"\n",
    "    env_vars = get_env_vars()\n",
    "    # Get a file path with permission for writing.\n",
    "    dataset_file = os.path.join(tempfile.gettempdir(), env_vars.bigquery_tmp_file)\n",
    "\n",
    "    data_bytes = base64.b64decode(event[\"data\"])\n",
    "    data_json = data_bytes.decode(\"utf-8\")\n",
    "    data = json.loads(data_json)\n",
    "    observations = data[\"observations\"]\n",
    "    predicted_actions = data[\"predicted_actions\"]\n",
    "\n",
    "    # Create MovieLens simulation environment.\n",
    "    env = movielens_py_environment.MovieLensPyEnvironment(\n",
    "        env_vars.raw_data_path\n",
    "        , env_vars.rank_k\n",
    "        , env_vars.batch_size\n",
    "        , num_movies=env_vars.num_actions\n",
    "        , csv_delimiter=\"\\t\"\n",
    "    )\n",
    "    environment = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "    # Get environment feedback and write trajectory data.\n",
    "    write_trajectories_to_file(\n",
    "        dataset_file=dataset_file\n",
    "        , environment=environment\n",
    "        , observations=observations\n",
    "        , predicted_actions=predicted_actions\n",
    "    )\n",
    "\n",
    "    # Add trajectory data as new training data to BigQuery.\n",
    "    append_dataset_to_bigquery(\n",
    "        project_id=env_vars.project_id\n",
    "        , dataset_file=dataset_file\n",
    "        , bigquery_dataset_id=env_vars.bigquery_dataset_id\n",
    "        , bigquery_location=env_vars.bigquery_location\n",
    "        , bigquery_table_id=env_vars.bigquery_table_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d85fb026-c14c-4656-9bfe-3d756e936155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/logger/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{COMPONENT_SUBDIR}/requirements.txt\n",
    "google-cloud-bigquery\n",
    "tensorflow==2.12.0\n",
    "pillow\n",
    "tf-agents==0.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "94d8ca56-66d8-4e38-996f-2c422210c16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID=hybrid-vertex,RAW_DATA_PATH=gs://e2ev4-hybrid-vertex-bucket/raw_data/u.data,BATCH_SIZE=8,RANK_K=20,NUM_ACTIONS=20,BIGQUERY_TMP_FILE=tmp.json,BIGQUERY_DATASET_ID=hybrid-vertex.movielens_dataset_e2ev4,BIGQUERY_LOCATION=us,BIGQUERY_TABLE_ID=hybrid-vertex.movielens_dataset_e2ev4.training_dataset\n"
     ]
    }
   ],
   "source": [
    "ENV_VARS = \",\".join(\n",
    "    [\n",
    "        f\"PROJECT_ID={PROJECT_ID}\",\n",
    "        f\"RAW_DATA_PATH={RAW_DATA_PATH}\",\n",
    "        f\"BATCH_SIZE={BATCH_SIZE}\",\n",
    "        f\"RANK_K={RANK_K}\",\n",
    "        f\"NUM_ACTIONS={NUM_ACTIONS}\",\n",
    "        f\"BIGQUERY_TMP_FILE={BIGQUERY_TMP_FILE}\",\n",
    "        f\"BIGQUERY_DATASET_ID={BIGQUERY_DATASET_ID}\",\n",
    "        f\"BIGQUERY_LOCATION={BQ_LOCATION}\",\n",
    "        f\"BIGQUERY_TABLE_ID={BIGQUERY_TABLE_ID}\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "! echo $ENV_VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06cf3087-6b80-4bd1-a21a-dd75d9f4c374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying function (may take a while - up to 2 minutes)...                    \n",
      "For Cloud Build Logs, visit: https://console.cloud.google.com/cloud-build/builds;region=us-central1/a7e6fce8-2452-4f64-96be-182dbbb7d426?project=934903580331\n",
      "Deploying function (may take a while - up to 2 minutes)...done.                \n",
      "availableMemoryMb: 512\n",
      "buildId: a7e6fce8-2452-4f64-96be-182dbbb7d426\n",
      "buildName: projects/934903580331/locations/us-central1/builds/a7e6fce8-2452-4f64-96be-182dbbb7d426\n",
      "dockerRegistry: CONTAINER_REGISTRY\n",
      "entryPoint: log_prediction_to_bigquery\n",
      "environmentVariables:\n",
      "  BATCH_SIZE: '8'\n",
      "  BIGQUERY_DATASET_ID: hybrid-vertex.movielens_dataset_e2ev4\n",
      "  BIGQUERY_LOCATION: us\n",
      "  BIGQUERY_TABLE_ID: hybrid-vertex.movielens_dataset_e2ev4.training_dataset\n",
      "  BIGQUERY_TMP_FILE: tmp.json\n",
      "  NUM_ACTIONS: '20'\n",
      "  PROJECT_ID: hybrid-vertex\n",
      "  RANK_K: '20'\n",
      "  RAW_DATA_PATH: gs://e2ev4-hybrid-vertex-bucket/raw_data/u.data\n",
      "eventTrigger:\n",
      "  eventType: google.pubsub.topic.publish\n",
      "  failurePolicy: {}\n",
      "  resource: projects/hybrid-vertex/topics/logger-pubsub-topic-e2ev4\n",
      "  service: pubsub.googleapis.com\n",
      "ingressSettings: ALLOW_ALL\n",
      "labels:\n",
      "  deployment-tool: cli-gcloud\n",
      "maxInstances: 3000\n",
      "name: projects/hybrid-vertex/locations/us-central1/functions/logger-cloud-function-e2ev4\n",
      "runtime: python310\n",
      "serviceAccountEmail: hybrid-vertex@appspot.gserviceaccount.com\n",
      "sourceArchiveUrl: gs://e2ev4-hybrid-vertex-bucket/us-central1-projects/hybrid-vertex/locations/us-central1/functions/logger-cloud-function-e2ev4-wvyutyyljzxm.zip\n",
      "status: ACTIVE\n",
      "timeout: 200s\n",
      "updateTime: '2023-07-05T11:24:43.791Z'\n",
      "versionId: '2'\n"
     ]
    }
   ],
   "source": [
    "! gcloud functions deploy $LOGGER_CLOUD_FUNCTION --ingress-settings=all \\\n",
    "    --region=$REGION \\\n",
    "    --trigger-topic=$LOGGER_PUBSUB_TOPIC \\\n",
    "    --runtime=python310 \\\n",
    "    --memory=2048MB \\\n",
    "    --timeout=200s \\\n",
    "    --source=src/logger \\\n",
    "    --entry-point=log_prediction_to_bigquery \\\n",
    "    --stage-bucket=$BUCKET_NAME \\\n",
    "    --update-env-vars=$ENV_VARS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4aa28d-4f65-4fb6-b893-339d0529c257",
   "metadata": {},
   "source": [
    "## Create (re)training pipeline\n",
    "\n",
    "* The Trigger recurrently re-runs the pipeline to re-train the policy on new training data, using `kfp.v2.google.client.AIPlatformClient.create_schedule_from_job_spec`\n",
    "* Create a pipeline for orchestration on Vertex Pipelines, and a Cloud Scheduler job that recurrently triggers the pipeline\n",
    "* The method also automatically creates a Cloud Function that acts as an intermediary between the Scheduler and Pipelines. You can find the source code [here](https://github.com/kubeflow/pipelines/blob/v1.7.0-alpha.3/sdk/python/kfp/v2/google/client/client.py#L347-L391).\n",
    "\n",
    "When the Simulator sends prediction requests to the endpoint, the Logger is triggered by the hook in the prediction code to log prediction results to BigQuery, as new training data. As this pipeline has a recurrent schedule, it utlizes the new training data in training a new policy, therefore closing the feedback loop. Theoretically speaking, if you set the pipeline scheduler to be infinitely frequent, then you would be approaching real-time, continuous training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "333d184f-faaa-4f87-8ce7-d58b9d153674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRIGGER_SCHEDULE = \"*/30 * * * *\"  # Schedule to trigger the pipeline. Eg. \"*/30 * * * *\" means every 30 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a1a28cda-9c08-422d-b4fc-d7c4dd3d814f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_TAG: movie-pipe-retraining-v1\n",
      "PIPELINE_NAME: tfab-e2ev4-movie-pipe-retraining-v1\n"
     ]
    }
   ],
   "source": [
    "RE_TRAIN_PIPE_VERSION = 'v1'\n",
    "\n",
    "PIPELINE_TAG = f'movie-pipe-retraining-{RE_TRAIN_PIPE_VERSION}'\n",
    "print(\"PIPELINE_TAG:\", PIPELINE_TAG)\n",
    "\n",
    "PIPELINE_NAME = f'tfab-{PREFIX}-{PIPELINE_TAG}'.replace('_', '-')\n",
    "print(\"PIPELINE_NAME:\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b78d07e8-4f1f-422d-9ceb-3e5b5716b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)\n",
    "\n",
    "from src.generator import generator_component\n",
    "from src.ingester import ingester_component\n",
    "# from src.trainer import trainer_component\n",
    "\n",
    "@kfp.v2.dsl.pipeline(\n",
    "    name=f'{PIPELINE_NAME}'.replace('_', '-')\n",
    ")\n",
    "def pipeline(\n",
    "    # Pipeline configs\n",
    "    project_id: str,\n",
    "    training_artifacts_dir: str,\n",
    "    # BigQuery configs\n",
    "    bigquery_table_id: str,\n",
    "    bigquery_max_rows: int = 10000,\n",
    "    # TF-Agents RL configs\n",
    "    rank_k: int = 20,\n",
    "    num_actions: int = 20,\n",
    "    num_epochs: int = 5,\n",
    "    tikhonov_weight: float = 0.01,\n",
    "    agent_alpha: float = 10,\n",
    "):  \n",
    "    # ========================================================================\n",
    "    # ingester\n",
    "    # ========================================================================\n",
    "    # Run the Ingester component.\n",
    "    ingest_task = (\n",
    "        ingester_component.ingest_bigquery_dataset_into_tfrecord(\n",
    "            project_id=project_id,\n",
    "            bigquery_table_id=bigquery_table_id,\n",
    "            bigquery_max_rows=bigquery_max_rows,\n",
    "            tfrecord_file_input=TFRECORD_FILE,\n",
    "        )\n",
    "    )\n",
    "            \n",
    "    # ========================================================================\n",
    "    # trainer\n",
    "    # ========================================================================\n",
    "\n",
    "    train_task = (\n",
    "        custom_job_op(\n",
    "            training_artifacts_dir_input=training_artifacts_dir\n",
    "            , tfrecord_file=ingest_task.outputs[\"tfrecord_file\"]\n",
    "            , num_epochs=num_epochs\n",
    "            , rank_k=rank_k\n",
    "            , num_actions=num_actions\n",
    "            , tikhonov_weight=tikhonov_weight\n",
    "            , agent_alpha=agent_alpha\n",
    "            , project=PROJECT_ID\n",
    "            , location=REGION\n",
    "        )\n",
    "        .set_display_name(\"trainer\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # deployer\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Upload the trained policy as a model.\n",
    "    model_upload_op = (\n",
    "        gcc_aip.ModelUploadOp(\n",
    "            project=project_id\n",
    "            , display_name=TRAINED_POLICY_DISPLAY_NAME\n",
    "            , artifact_uri=train_task.outputs[\"training_artifacts_dir\"]\n",
    "            , serving_container_image_uri=f\"gcr.io/{PROJECT_ID}/{PREDICTION_CONTAINER}:latest\"\n",
    "        )\n",
    "        .set_display_name(\"register model\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    # Create a Vertex AI endpoint\n",
    "    endpoint_create_op = (\n",
    "        gcc_aip.EndpointCreateOp(\n",
    "            project=project_id\n",
    "            , display_name=ENDPOINT_DISPLAY_NAME\n",
    "        )\n",
    "        .set_display_name(\"create endpoint\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    # Deploy the uploaded, trained policy to the created endpoint\n",
    "    deploy_model_op = (\n",
    "        gcc_aip.ModelDeployOp(\n",
    "            endpoint=endpoint_create_op.outputs[\"endpoint\"]\n",
    "            , model=model_upload_op.outputs[\"model\"]\n",
    "            , deployed_model_display_name=TRAINED_POLICY_DISPLAY_NAME\n",
    "            , traffic_split=TRAFFIC_SPLIT\n",
    "            , dedicated_resources_machine_type=ENDPOINT_MACHINE_TYPE\n",
    "            , dedicated_resources_accelerator_type=ENDPOINT_ACCELERATOR_TYPE\n",
    "            , dedicated_resources_accelerator_count=ENDPOINT_ACCELERATOR_COUNT\n",
    "            , dedicated_resources_min_replica_count=ENDPOINT_REPLICA_COUNT\n",
    "        )\n",
    "        .set_display_name(\"deploy model\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e89946f-8ece-4c58-aaf0-10f048f2289d",
   "metadata": {},
   "source": [
    "### compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d53cefda-0210-435c-bca3-1a13d478e6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -f custom_container_pipeline_spec.json\n",
    "\n",
    "RT_PIPELINE_JSON_SPEC_LOCAL = \"retraining_pipeline_spec.json\"\n",
    "\n",
    "! rm -f $RT_PIPELINE_JSON_SPEC_LOCAL\n",
    "\n",
    "kfp.v2.compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=RT_PIPELINE_JSON_SPEC_LOCAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a1c602-3a5a-4001-a06d-4b5130dbbabf",
   "metadata": {},
   "source": [
    "### save json spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d77d8d42-e0d9-46fc-912f-bf62972765e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT_PIPELINE_ROOT: gs://e2ev4-hybrid-vertex-bucket/retrain_pipeline\n",
      "RT_PIPELINES_FILEPATH: gs://e2ev4-hybrid-vertex-bucket/retrain_pipeline/retraining_pipeline_spec.json\n"
     ]
    }
   ],
   "source": [
    "RT_PIPELINE_ROOT = f\"{BUCKET_URI}/retrain_pipeline\"\n",
    "print(\"RT_PIPELINE_ROOT:\", RT_PIPELINE_ROOT)\n",
    "\n",
    "RT_PIPELINES_FILEPATH = f'{RT_PIPELINE_ROOT}/retraining_pipeline_spec.json'\n",
    "print(\"RT_PIPELINES_FILEPATH:\", RT_PIPELINES_FILEPATH)\n",
    "\n",
    "!gsutil -q cp $RT_PIPELINE_JSON_SPEC_LOCAL $RT_PIPELINES_FILEPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe57d9-e0bc-4e26-907c-a341a41a44e6",
   "metadata": {},
   "source": [
    "### submit pipeline to Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1518cd22-7d65-43a8-8759-9eb60a01557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline run job.\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=f\"{PIPELINE_NAME}-retrain\"\n",
    "    , template_path=RT_PIPELINE_JSON_SPEC_LOCAL\n",
    "    , pipeline_root=RT_PIPELINE_ROOT\n",
    "    , failure_policy='fast'\n",
    "    , parameter_values={\n",
    "        # Pipeline configs\n",
    "        \"project_id\": PROJECT_ID\n",
    "        , \"training_artifacts_dir\": TRAINING_ARTIFACTS_DIR\n",
    "        # BigQuery configs\n",
    "        , \"bigquery_table_id\": BIGQUERY_TABLE_ID\n",
    "    }\n",
    "    , enable_caching=False\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=False\n",
    "    , service_account=VERTEX_SA\n",
    "    # , network=VPC_NETWORK_FULL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef967d-6b7f-478e-bcf3-ec6b3191c7a4",
   "metadata": {},
   "source": [
    "## clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ea79e-4dcf-4196-859c-dfcf57d30c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf custom_pipeline_spec.json"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
