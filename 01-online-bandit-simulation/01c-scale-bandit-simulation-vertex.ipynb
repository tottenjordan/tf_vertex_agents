{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01cffb8-8045-4314-b56a-8ac9154c6066",
   "metadata": {},
   "source": [
    "# Scale Matrix-factorization-based simulation for Contextual Bandits with Vertex AI Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c6cb3-7970-4fab-a102-8c8749ecd3fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29580d03-390d-4d90-a5c0-14c16338ddbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Notebook Objectives:\n",
    "* Create hyperparameter tuning and training custom container\n",
    "* Submit hyperparameter tuning job (optional)\n",
    "* Create custom prediction container\n",
    "* Submit custom container training job\n",
    "* Deploy trained model to Endpoint\n",
    "* Predict on the Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64261791-880e-41c7-b6e9-a39698c66d51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**TODO** - fix vars -Create hyperparameter tuning and training custom container\n",
    "\n",
    "Create a custom container that can be used for both hyperparameter tuning and training. The associated source code is in `src/per_arm_rl/`. This serves as the inner script of the custom container.\n",
    "As before, the training function is the same as [trainer.train](https://github.com/tensorflow/agents/blob/r0.8.0/tf_agents/bandits/agents/examples/v2/trainer.py#L104), but it keeps track of intermediate metric values, supports hyperparameter tuning, and (for training) saves artifacts to different locations. The training logic for hyperparameter tuning and training is the same.\n",
    "\n",
    "**Execute hyperparameter tuning:**\n",
    "* The code does not save model artifacts. It takes in command-line arguments as hyperparameter values from the Vertex AI Hyperparameter Tuning service, and reports training result metric to Vertex AI at each trial using cloudml-hypertune.\n",
    "* Note that if you decide to save model artifacts, saving them to the same directory may cause overwriting errors if you use parallel trials in the hyperparameter tuning job. The recommended approach is to save each trial's artifacts to a different sub-directory. This would also allow you to recover all the artifacts from different trials and can potentially save you from re-training.\n",
    "* Read more about hyperparameter tuning for custom containers [here](https://cloud.google.com/vertex-ai/docs/training/containers-overview#hyperparameter_tuning_with_custom_containers); read about hyperparameter tuning support [here](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview).\n",
    "\n",
    "**Execute training:**\n",
    "* The code saves model artifacts to `os.environ[\"AIP_MODEL_DIR\"]` in addition to `ARTIFACTS_DIR`, as required [here](https://github.com/googleapis/python-aiplatform/blob/v0.8.0/google/cloud/aiplatform/training_jobs.py#L2202).\n",
    "* If you want to make changes to the function, make sure to still save the trained policy as a SavedModel to clean directories\n",
    "* avoid saving checkpoints and other artifacts, so that deploying the model to endpoints works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ea275-15ba-40b6-acb1-94d5d18996b1",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2ae96-b5b5-4b10-9d9c-0b00d533ef46",
   "metadata": {},
   "source": [
    "### set vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd715ec5-f0fb-432b-bef4-a05a57586c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX: rec-bandits-v2\n"
     ]
    }
   ],
   "source": [
    "# PREFIX = 'mabv1'\n",
    "VERSION        = \"v2\"                       # TODO\n",
    "PREFIX         = f'rec-bandits-{VERSION}'   # TODO\n",
    "\n",
    "print(f\"PREFIX: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd6fddd-38f5-4e8f-9beb-54b7cc631290",
   "metadata": {},
   "source": [
    "**run the next cell to populate env vars**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82643015-091b-444a-83a6-e24100a48a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"rec-bandits-v2\"\n",
      "VERSION                  = \"v2\"\n",
      "\n",
      "BUCKET_NAME              = \"rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "BUCKET_URI               = \"gs://rec-bandits-v2-hybrid-vertex-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "DATA_PATH_KFP_DEMO       = \"gs://rec-bandits-v2-hybrid-vertex-bucket/data/kfp_demo_data/u.data\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_NAME    = \"mvlens_rec_bandits_v2\"\n",
      "BIGQUERY_TABLE_NAME      = \"training_dataset\"\n",
      "\n",
      "REPOSITORY               = \"rl-movielens-rec-bandits-v2\"\n",
      "\n",
      "DOCKERNAME_01            = \"Dockerfile_train_my_perarm_env\"\n",
      "IMAGE_NAME_01            = \"train-my-perarm-env-v2\"\n",
      "IMAGE_URI_01             = \"gcr.io/hybrid-vertex/train-my-perarm-env-v2\"\n",
      "\n",
      "DOCKERNAME_02            = \"Dockerfile_perarm_feats\"\n",
      "IMAGE_NAME_02            = \"train-perarm-feats-v2\"\n",
      "IMAGE_URI_02             = \"gcr.io/hybrid-vertex/train-perarm-feats-v2\"\n",
      "\n",
      "DOCKERNAME_03            = \"Dockerfile_ranking_bandit\"\n",
      "IMAGE_NAME_03            = \"train-rank-bandit-v2\"\n",
      "IMAGE_URI_03             = \"gcr.io/hybrid-vertex/train-rank-bandit-v2\"\n",
      "\n",
      "DOCKERNAME_04            = \"Dockerfile_train_mab_e2e\"\n",
      "IMAGE_NAME_04            = \"train-mab-e2e-v2\"\n",
      "IMAGE_URI_04             = \"gcr.io/hybrid-vertex/train-mab-e2e-v2\"\n",
      "\n",
      "DOCKERNAME_04_pred       = \"Dockerfile_pred_mab_e2e\"\n",
      "IMAGE_NAME_04_pred       = \"pred-mab-e2e-v2\"\n",
      "IMAGE_URI_04_pred        = \"gcr.io/hybrid-vertex/pred-mab-e2e-v2\"\n",
      "\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-rec-bandits-v2/local_docker_tfa\"\n",
      "\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "RL_SUB_DIR               = \"per_arm_rl\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ab55c-3ca8-4348-b902-749b19a6c57f",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e52ce64-2afe-4d2f-a43d-68775ee7cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce8b63e1-84a3-4417-adf4-ff3f0dfe9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents import TFAgent\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "from tf_agents.bandits.environments import (environment_utilities,\n",
    "                                            movielens_py_environment)\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import TFEnvironment, tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.metrics.tf_metric import TFStepMetric\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "# GPU\n",
    "from numba import cuda \n",
    "import gc\n",
    "\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f8aa37b-acaa-479f-aba2-b4fc8eaa7974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18ecec03-be21-41d9-b1da-e94ea3a0a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# my project\n",
    "from src.per_arm_rl import data_utils\n",
    "from src.per_arm_rl import data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63cd20c0-7446-49df-9a1e-41a89065e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dda7603-ff83-4878-af18-66da744b9648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/ml-ratings-100k-full.tfrecord\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/kfp_demo_data/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/listwise-3n-train/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/listwise-3n-val/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/listwise-5n-train/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/listwise-5n-val/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/train/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/data/val/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls $DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678fcb8-4b41-42aa-92ec-c3b07f7747e0",
   "metadata": {},
   "source": [
    "## Build train application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd895298-c435-414d-b84a-908cf0932ba4",
   "metadata": {},
   "source": [
    "### Vertex Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6322612-2a66-4b05-b600-dbae4d307efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : scale-my-mf-env-hpt-rec-bandits-v2\n",
      "RUN_NAME          : run-20231114-144856\n",
      "BASE_OUTPUT_DIR   : gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-hpt-rec-bandits-v2/run-20231114-144856\n",
      "LOG_DIR           : gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-hpt-rec-bandits-v2/run-20231114-144856/logs\n",
      "ROOT_DIR          : gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-hpt-rec-bandits-v2/run-20231114-144856/root\n",
      "ARTIFACTS_DIR     : gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-hpt-rec-bandits-v2/run-20231114-144856/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'scale-my-mf-env-hpt-{PREFIX}'\n",
    "\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "BASE_OUTPUT_DIR   = f'gs://{BUCKET_NAME}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "LOG_DIR           = f\"{BASE_OUTPUT_DIR}/logs\"\n",
    "ROOT_DIR          = f\"{BASE_OUTPUT_DIR}/root\"                               # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BASE_OUTPUT_DIR}/artifacts\"                          # Where the trained model will be saved and restored.\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\")\n",
    "print(f\"BASE_OUTPUT_DIR   : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307bb759-0a14-4746-8d81-f846e318b1ed",
   "metadata": {},
   "source": [
    "## Prepare (hpt) training job for Vertex AI\n",
    "* Submit a hyperparameter training job with the custom container. Read more details for using Python packages as an alternative to using custom containers in the example shown [here](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#create)\n",
    "* Define the hyperparameter(s), max trial count, parallel trial count, parameter search algorithm, machine spec, accelerators, worker pool, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b8fcea2-db92-42ec-98ab-5f42d91da2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPTUNING_RESULT_DIR  : hptuning\n",
      "HPTUNING_RESULT_FILE : result.json\n",
      "HPTUNING_RESULT_PATH : scale-my-mf-env-hpt-rec-bandits-v2/run-20231114-144856/hptuning/result.json\n",
      "HPTUNING_RESULT_URI  : gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-hpt-rec-bandits-v2/run-20231114-144856/hptuning/result.json\n"
     ]
    }
   ],
   "source": [
    "# Execute hyperparameter tuning instead of regular training\n",
    "RUN_HYPERPARAMETER_TUNING          = True\n",
    "TRAIN_WITH_BEST_HYPERPARAMETERS    = False  # Do not train.\n",
    "\n",
    "# Directory to store the best hyperparameter(s) in `BUCKET_NAME` and locally (temporarily)\n",
    "HPTUNING_RESULT_DIR                = \"hptuning\"\n",
    "HPTUNING_RESULT_FILE               = \"result.json\"\n",
    "HPTUNING_RESULT_PATH               = f\"{EXPERIMENT_NAME}/{RUN_NAME}/{HPTUNING_RESULT_DIR}/{HPTUNING_RESULT_FILE}\"\n",
    "HPTUNING_RESULT_URI                = f\"{BUCKET_URI}/{HPTUNING_RESULT_PATH}\"\n",
    "\n",
    "print(f\"HPTUNING_RESULT_DIR  : {HPTUNING_RESULT_DIR}\")\n",
    "print(f\"HPTUNING_RESULT_FILE : {HPTUNING_RESULT_FILE}\")\n",
    "print(f\"HPTUNING_RESULT_PATH : {HPTUNING_RESULT_PATH}\")\n",
    "print(f\"HPTUNING_RESULT_URI  : {HPTUNING_RESULT_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3e816-d9f3-4b91-9327-fdc27eba54a4",
   "metadata": {},
   "source": [
    "### Accelerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "831ae50e-d4c7-4a8a-bb13-40279a7c2946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCELERATOR: t4\n"
     ]
    }
   ],
   "source": [
    "ACCELERATOR = \"t4\" # str: \"a100\" | \"t4\" | None | l4\n",
    "ACCELERATOR = str(ACCELERATOR)\n",
    "print(f\"ACCELERATOR: {ACCELERATOR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0f7958f-5520-45f0-b069-3d3aaaf2a0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKER_MACHINE_TYPE            : n1-highcpu-16\n",
      "REPLICA_COUNT                  : 1\n",
      "ACCELERATOR_TYPE               : NVIDIA_TESLA_T4\n",
      "PER_MACHINE_ACCELERATOR_COUNT  : 1\n",
      "DISTRIBUTE_STRATEGY            : single\n",
      "REDUCTION_SERVER_COUNT         : 0\n",
      "REDUCTION_SERVER_MACHINE_TYPE  : n1-highcpu-16\n",
      "TF_GPU_THREAD_COUNT            : 4\n"
     ]
    }
   ],
   "source": [
    "if ACCELERATOR == \"a100\":\n",
    "    WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "    REPLICA_COUNT = 1\n",
    "    ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "    PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "    REDUCTION_SERVER_COUNT = 0                                                      \n",
    "    REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "    DISTRIBUTE_STRATEGY = 'single'\n",
    "elif ACCELERATOR == 't4':\n",
    "    WORKER_MACHINE_TYPE = 'n1-highcpu-16'\n",
    "    REPLICA_COUNT = 1\n",
    "    ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4'\n",
    "    PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "    DISTRIBUTE_STRATEGY = 'single'\n",
    "    REDUCTION_SERVER_COUNT = 0                                                      \n",
    "    REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "elif ACCELERATOR == 'l4':\n",
    "    WORKER_MACHINE_TYPE = \"g2-standard-16\"\n",
    "    REPLICA_COUNT = 1\n",
    "    ACCELERATOR_TYPE = 'NVIDIA_L4'\n",
    "    PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "    DISTRIBUTE_STRATEGY = 'single'\n",
    "    REDUCTION_SERVER_COUNT = 0                                                      \n",
    "    REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "elif ACCELERATOR == 'tpu':\n",
    "    WORKER_MACHINE_TYPE = \"cloud-tpu\"\n",
    "    REPLICA_COUNT = 1\n",
    "    ACCELERATOR_TYPE = 'TPU_v3'\n",
    "    PER_MACHINE_ACCELERATOR_COUNT = 8 # 8 | +32+ for TPU Pods\n",
    "    DISTRIBUTE_STRATEGY = 'single'\n",
    "    REDUCTION_SERVER_COUNT = 0                                                      \n",
    "    REDUCTION_SERVER_MACHINE_TYPE = None\n",
    "elif ACCELERATOR == \"False\":\n",
    "    WORKER_MACHINE_TYPE = 'n2-highmem-32' # 'n1-highmem-96'n | 'n2-highmem-92'\n",
    "    REPLICA_COUNT = 1\n",
    "    ACCELERATOR_TYPE = None\n",
    "    PER_MACHINE_ACCELERATOR_COUNT = 0\n",
    "    DISTRIBUTE_STRATEGY = 'single'\n",
    "    REDUCTION_SERVER_COUNT = 0                                                      \n",
    "    REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "    \n",
    "TF_GPU_THREAD_COUNT   = '4'      # '1' | '4' | '8'\n",
    "\n",
    "print(f\"WORKER_MACHINE_TYPE            : {WORKER_MACHINE_TYPE}\")\n",
    "print(f\"REPLICA_COUNT                  : {REPLICA_COUNT}\")\n",
    "print(f\"ACCELERATOR_TYPE               : {ACCELERATOR_TYPE}\")\n",
    "print(f\"PER_MACHINE_ACCELERATOR_COUNT  : {PER_MACHINE_ACCELERATOR_COUNT}\")\n",
    "print(f\"DISTRIBUTE_STRATEGY            : {DISTRIBUTE_STRATEGY}\")\n",
    "print(f\"REDUCTION_SERVER_COUNT         : {REDUCTION_SERVER_COUNT}\")\n",
    "print(f\"REDUCTION_SERVER_MACHINE_TYPE  : {REDUCTION_SERVER_MACHINE_TYPE}\")\n",
    "print(f\"TF_GPU_THREAD_COUNT            : {TF_GPU_THREAD_COUNT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5213898-3252-4eaf-a50f-1166e640a2d5",
   "metadata": {},
   "source": [
    "### Create Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32e1fd75-c50c-453c-8cc6-b85416836648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME: projects/934903580331/locations/us-central1/tensorboards/4213966274381742080\n",
      "TB display name: scale-my-mf-env-hpt-rec-bandits-v2-run-20231114-144856\n"
     ]
    }
   ],
   "source": [
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}-{RUN_NAME}\"\n",
    "\n",
    "tensorboard = aiplatform.Tensorboard.create(\n",
    "    display_name=TENSORBOARD_DISPLAY_NAME\n",
    "    , project=PROJECT_ID\n",
    "    , location=REGION\n",
    ")\n",
    "\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name: {tensorboard.display_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c14bc7-7fcb-402f-adbf-8cb35e039edd",
   "metadata": {},
   "source": [
    "### Set training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d74207b8-dbb5-4baf-8c86-f8f0b594e8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE       : 128\n",
      "TRAINING_LOOPS   : 200\n",
      "STEPS_PER_LOOP   : 2\n",
      "RANK_K           : 20\n",
      "NUM_ACTIONS      : 20\n",
      "PER_ARM          : True\n",
      "TIKHONOV_WEIGHT  : 0.001\n",
      "AGENT_ALPHA      : 10.0\n",
      "CHKPT_INTERVAL   : 199\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters.\n",
    "BATCH_SIZE       = 128       # Training and prediction batch size.\n",
    "TRAINING_LOOPS   = 200      # Number of training iterations.\n",
    "STEPS_PER_LOOP   = 2       # Number of driver steps per training iteration.\n",
    "\n",
    "# Set MovieLens simulation environment parameters.\n",
    "RANK_K           = 20      # Rank for matrix factorization in the MovieLens environment; also the observation dimension.\n",
    "NUM_ACTIONS      = 20      # Number of actions (movie items) to choose from.\n",
    "PER_ARM          = True    # Use the non-per-arm version of the MovieLens environment.\n",
    "\n",
    "# Set agent parameters.\n",
    "TIKHONOV_WEIGHT  = 0.001   # LinUCB Tikhonov regularization weight.\n",
    "AGENT_ALPHA      = 10.0    # LinUCB exploration parameter that multiplies the confidence intervals.\n",
    "\n",
    "CHKPT_INTERVAL       = TRAINING_LOOPS - 1\n",
    "\n",
    "print(f\"BATCH_SIZE       : {BATCH_SIZE}\")\n",
    "print(f\"TRAINING_LOOPS   : {TRAINING_LOOPS}\")\n",
    "print(f\"STEPS_PER_LOOP   : {STEPS_PER_LOOP}\")\n",
    "print(f\"RANK_K           : {RANK_K}\")\n",
    "print(f\"NUM_ACTIONS      : {NUM_ACTIONS}\")\n",
    "print(f\"PER_ARM          : {PER_ARM}\")\n",
    "print(f\"TIKHONOV_WEIGHT  : {TIKHONOV_WEIGHT}\")\n",
    "print(f\"AGENT_ALPHA      : {AGENT_ALPHA}\")\n",
    "print(f\"CHKPT_INTERVAL   : {CHKPT_INTERVAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03b3cc89-1401-480c-8d4e-1422227860f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--data-path=gs://rec-bandits-v2-hybrid-vertex-bucket/data',\n",
      "                              '--bucket_name=rec-bandits-v2-hybrid-vertex-bucket',\n",
      "                              '--data_gcs_prefix=data',\n",
      "                              '--data_path=gs://rec-bandits-v2-hybrid-vertex-bucket/data',\n",
      "                              '--project_number=934903580331',\n",
      "                              '--batch-size=128',\n",
      "                              '--rank-k=20',\n",
      "                              '--num-actions=20',\n",
      "                              '--tikhonov-weight=0.001',\n",
      "                              '--agent-alpha=10.0',\n",
      "                              '--training_loops=200',\n",
      "                              '--steps-per-loop=2',\n",
      "                              '--distribute=single',\n",
      "                              '--artifacts_dir=gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-hpt-rec-bandits-v2/run-20231114-144856/artifacts',\n",
      "                              '--root_dir=gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-hpt-rec-bandits-v2/run-20231114-144856/root',\n",
      "                              '--log_dir=gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-hpt-rec-bandits-v2/run-20231114-144856/logs',\n",
      "                              '--experiment_name=scale-my-mf-env-hpt-rec-bandits-v2',\n",
      "                              '--experiment_run=run-20231114-144856',\n",
      "                              '--tf_gpu_thread_count=4',\n",
      "                              '--chkpt_interval=199',\n",
      "                              '--sum_grads_vars',\n",
      "                              '--debug_summaries',\n",
      "                              '--use_gpu',\n",
      "                              '--run-hyperparameter-tuning'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/train-my-perarm-env-v2:latest'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_T4',\n",
      "                   'machine_type': 'n1-highcpu-16'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "WORKER_ARGS = [\n",
    "    f\"--data-path={DATA_PATH}\"               # TODO - remove duplicate arg\n",
    "    , f\"--bucket_name={BUCKET_NAME}\"\n",
    "    , f\"--data_gcs_prefix={DATA_GCS_PREFIX}\"\n",
    "    , f\"--data_path={DATA_PATH}\"\n",
    "    , f\"--project_number={PROJECT_NUM}\"\n",
    "    , f\"--batch-size={BATCH_SIZE}\"\n",
    "    , f\"--rank-k={RANK_K}\"\n",
    "    , f\"--num-actions={NUM_ACTIONS}\"\n",
    "    , f\"--tikhonov-weight={TIKHONOV_WEIGHT}\"\n",
    "    , f\"--agent-alpha={AGENT_ALPHA}\"\n",
    "    , f\"--training_loops={TRAINING_LOOPS}\"\n",
    "    , f\"--steps-per-loop={STEPS_PER_LOOP}\"\n",
    "    , f\"--distribute={DISTRIBUTE_STRATEGY}\"\n",
    "    , f\"--artifacts_dir={ARTIFACTS_DIR}\"\n",
    "    , f\"--root_dir={ROOT_DIR}\"\n",
    "    , f\"--log_dir={LOG_DIR}\"\n",
    "    , f\"--experiment_name={EXPERIMENT_NAME}\"\n",
    "    , f\"--experiment_run={RUN_NAME}\"\n",
    "    , f\"--tf_gpu_thread_count={TF_GPU_THREAD_COUNT}\"\n",
    "    , f\"--chkpt_interval={CHKPT_INTERVAL}\"\n",
    "    # , f\"--profiler\"\n",
    "    , f\"--sum_grads_vars\"\n",
    "    , f\"--debug_summaries\"\n",
    "    , f\"--use_gpu\"\n",
    "    # , f\"--use_tpu\"\n",
    "]\n",
    "\n",
    "if RUN_HYPERPARAMETER_TUNING:\n",
    "    WORKER_ARGS.append(\"--run-hyperparameter-tuning\")\n",
    "    \n",
    "elif TRAIN_WITH_BEST_HYPERPARAMETERS:\n",
    "    WORKER_ARGS.append(\"--train-with-best-hyperparameters\")\n",
    "    \n",
    "from src.per_arm_rl import train_utils\n",
    "\n",
    "WORKER_POOL_SPECS = train_utils.prepare_worker_pool_specs(\n",
    "    image_uri=f\"{IMAGE_URI_01}:latest\",\n",
    "    args=WORKER_ARGS,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c8ee96-89e0-45d5-a1a6-14a3997358dd",
   "metadata": {},
   "source": [
    "### Define parameter spec\n",
    "\n",
    "Next, define the 1parameter_spec1, which is a dictionary specifying the parameters you want to optimize. The **dictionary key** is the string you assigned to the command line argument for each hyperparameter, and the **dictionary value** is the parameter specification.\n",
    "\n",
    "For each hyperparameter, you need to define the `Type` as well as the bounds for the values that the tuning service will try. Hyperparameters can be of type `Double`, `Integer`, `Categorical`, or `Discrete`. If you select the type `Double` or `Integer`, you need to provide a minimum and maximum value. And if you select `Categorical` or `Discrete` you need to provide the values. For the `Double` and `Integer` types, you also need to provide the scaling value. Learn more about [Using an Appropriate Scale](https://www.youtube.com/watch?v=cSoK_6Rkbfg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c289fe7-edb7-4b7b-ad21-49e8e8946b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary representing parameters to optimize.\n",
    "# The dictionary key is the parameter_id, which is passed into your training\n",
    "# job as a command line argument,\n",
    "# And the dictionary value is the parameter specification of the metric.\n",
    "\n",
    "parameter_spec = {\n",
    "    # \"steps-per-loop\": hpt.DiscreteParameterSpec(values=[2, 4], scale=None),\n",
    "    \"batch-size\": hpt.DiscreteParameterSpec(values=[16, 32, 128], scale=None),\n",
    "    \"num-actions\": hpt.DiscreteParameterSpec(values=[8, 24, 32], scale=None),\n",
    "    # \"training-loops\": hpt.DiscreteParameterSpec(values=[4, 6, 8], scale=None),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c99738c-0b2c-4a48-9025-21f82b634617",
   "metadata": {},
   "source": [
    "The final spec to define is `metric_spec`, which is a dictionary representing the metric to optimize. The dictionary key is the `hyperparameter_metric_tag` that you set in your training application code, and the value is the optimization goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b926ea3b-1631-476c-a7a5-fe2f5841f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary representing metrics to optimize.\n",
    "# The dictionary key is the metric_id, which is reported by your training job,\n",
    "# And the dictionary value is the optimization goal of the metric.\n",
    "metric_spec = {\"final_average_return\": \"maximize\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2576033c-f80f-44ed-bfa7-c2a594bef0ec",
   "metadata": {},
   "source": [
    "## [1] Submit (hpt) train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "657b600d-770c-4acd-844e-9b6620213a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME: 01d-hpt-run-20231114-144856\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    # staging_bucket=ROOT_DIR,\n",
    "    # experiment_tensorboard=TB_RESOURCE_NAME,\n",
    ")\n",
    "\n",
    "JOB_NAME = f\"01d-hpt-{RUN_NAME}\"\n",
    "print(f\"JOB_NAME: {JOB_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8135819-0b02-40db-a8b9-3e924660630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CustomJob\n",
    "my_custom_hpt_job = aiplatform.CustomJob(\n",
    "    display_name=JOB_NAME\n",
    "    , project=PROJECT_ID\n",
    "    , worker_pool_specs=WORKER_POOL_SPECS\n",
    "    , base_output_dir=BASE_OUTPUT_DIR\n",
    "    , staging_bucket=ROOT_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e62b4a-0f73-473d-9394-ec74688a302f",
   "metadata": {},
   "source": [
    "Then, create and run a HyperparameterTuningJob.\n",
    "\n",
    "> see [source code](https://github.com/googleapis/python-aiplatform/blob/main/google/cloud/aiplatform/hyperparameter_tuning.py)\n",
    "\n",
    "There are a few arguments to note:\n",
    "\n",
    "* `max_trial_count`: Sets an upper bound on the number of trials the service will run. The recommended practice is to start with a smaller number of trials and get a sense of how impactful your chosen hyperparameters are before scaling up.\n",
    "\n",
    "* `parallel_trial_count`: If you use parallel trials, the service provisions multiple training processing clusters. The worker pool spec that you specify when creating the job is used for each individual training cluster. Increasing the number of parallel trials reduces the amount of time the hyperparameter tuning job takes to run; however, it can reduce the effectiveness of the job overall. This is because the default tuning strategy uses results of previous trials to inform the assignment of values in subsequent trials.\n",
    "\n",
    "* `search_algorithm`: The available search algorithms are grid, random, or default (None). The default option applies Bayesian optimization to search the space of possible hyperparameter values and is the recommended algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb39f023-77fb-4624-aa67-7873a026e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run HyperparameterTuningJob\n",
    "\n",
    "hp_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=JOB_NAME,\n",
    "    custom_job=my_custom_hpt_job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=6,\n",
    "    parallel_trial_count=6,\n",
    "    project=PROJECT_ID,\n",
    "    search_algorithm=\"random\",\n",
    ")\n",
    "\n",
    "hp_job.run(\n",
    "    sync=False\n",
    "    , service_account=VERTEX_SA\n",
    "    , restart_job_on_worker_restart = False \n",
    "    , enable_web_access = True\n",
    "    , tensorboard = TB_RESOURCE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7246e29b-f37f-458c-a63a-01cfdf71224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: 01d-hpt-run-20231114-144856\n",
      "Job Resource Name: projects/934903580331/locations/us-central1/hyperparameterTuningJobs/5939469248110788608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job Name: {hp_job.display_name}\")\n",
    "print(f\"Job Resource Name: {hp_job.resource_name}\\n\")\n",
    "# print(f\"Check training progress at {custom_job._dashboard_uri()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c19d4f85-5306-433d-9b2d-6f22b7ce2254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.jobs.HyperparameterTuningJob object at 0x7f4008c45300> \n",
       "resource name: projects/934903580331/locations/us-central1/hyperparameterTuningJobs/6469768104233664512"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpt_job_test = aiplatform.HyperparameterTuningJob.get(\n",
    "    resource_name=hp_job.resource_name,\n",
    ")\n",
    "hpt_job_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a1b76a3-3d33-4d0b-bc32-4a392350e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hpt_job_test.error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9184f2dd-2b47-41ef-a4ff-719cf8a6fe47",
   "metadata": {},
   "source": [
    "### View TensorBoard for HPT job\n",
    "\n",
    "<img src=\"imgs/01_hpt_tboard.png\" \n",
    "     align=\"center\" \n",
    "     width=\"850\"\n",
    "     height=\"850\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a06fed-f0e3-4136-8e11-913455cf0485",
   "metadata": {},
   "source": [
    "#### Find the best combination(s) hyperparameter(s) for each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c496d69-b89a-4e15-9342-2ca58585d17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('6', 128.0, 8.0, 1.6136982440948486)\n"
     ]
    }
   ],
   "source": [
    "best_test = (None, None, None, 0.0)\n",
    "for trial in hp_job.trials:\n",
    "    # print(trial)\n",
    "    # Keep track of the best outcome\n",
    "    if float(trial.final_measurement.metrics[0].value) > best_test[3]:\n",
    "        # print(trial.final_measurement.metrics[0].value)\n",
    "        # print(trial.parameters[0].value)\n",
    "        try:\n",
    "            best_test = (\n",
    "                trial.id,\n",
    "                trial.parameters[0].value, #.number_value,\n",
    "                trial.parameters[1].value, #.number_value,\n",
    "                trial.final_measurement.metrics[0].value,\n",
    "            )\n",
    "        except:\n",
    "            best_test = (\n",
    "                trial.id,\n",
    "                trial.parameters[0].value.number_value,\n",
    "                None,\n",
    "                trial.final_measurement.metrics[0].value,\n",
    "            )\n",
    "\n",
    "print(best_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc3a9c7e-f7a2-4cff-86cc-6d70053046a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST_HPT_DICT : {'batch-size': 128, 'num-actions': 24}\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE_best = 128 #int(best_test[1])\n",
    "NUM_ACTIONS_best = 24 #int(best_test[2])\n",
    "\n",
    "BEST_HPT_DICT = {\n",
    "    \"batch-size\":BATCH_SIZE_best,\n",
    "    \"num-actions\":NUM_ACTIONS_best\n",
    "}\n",
    "print(f\"BEST_HPT_DICT : {BEST_HPT_DICT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f615339b-fd88-4239-9c67-27157ade7ea6",
   "metadata": {},
   "source": [
    "#### Convert a combination of best hyperparameter(s) for a metric of interest to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95610efa-ba53-4864-bd6d-219e851412e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPTUNING_RESULT_DIR = \"hptuning/\"\n",
    "# HPTUNING_RESULT_PATH = os.path.join(HPTUNING_RESULT_DIR, \"result.json\")\n",
    "\n",
    "# print(f\"HPTUNING_RESULT_PATH : {HPTUNING_RESULT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03136fb1-5f98-4663-9317-26bd6bb4326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf $HPTUNING_RESULT_DIR\n",
    "# ! mkdir $HPTUNING_RESULT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0574bda0-f1f9-438f-b4b9-67754a769bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_RESULTS_FILE = \"result.json\"  # {\"batch-size\": 8.0, \"steps-per-loop\": 2.0}\n",
    "\n",
    "# with open(LOCAL_RESULTS_FILE, \"w\") as f:\n",
    "#     json.dump(best_params[\"final_average_return\"][0], f)\n",
    "\n",
    "with open(LOCAL_RESULTS_FILE, \"w\") as f:\n",
    "    json_dumps_str = json.dumps(BEST_HPT_DICT)\n",
    "    f.write(json_dumps_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a63794-61bb-415a-b08d-aebbb35abeb2",
   "metadata": {},
   "source": [
    "#### Upload the best hyperparameter(s) to GCS for use in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a3f7eab-f1ff-4cea-8359-b17516d584ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-hpt-rec-bandits-v2/run-20231114-144856/hptuning/result.json\n"
     ]
    }
   ],
   "source": [
    "!gsutil -q cp $LOCAL_RESULTS_FILE $HPTUNING_RESULT_URI\n",
    "\n",
    "!gsutil ls $HPTUNING_RESULT_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe724c1-2dcb-4d10-8cec-fb56fb088120",
   "metadata": {},
   "source": [
    "## [2] Submit custom container training job\n",
    "\n",
    "- Note again that the bucket must be in the same regional location as the service location and it should not be multi-regional.\n",
    "- Read more of CustomContainerTrainingJob's source code [here](https://github.com/googleapis/python-aiplatform/blob/v0.8.0/google/cloud/aiplatform/training_jobs.py#L2153).\n",
    "- Like with local execution, you can use TensorBoard Profiler to track the training process and resources, and visualize the corresponding artifacts using the command: `%tensorboard --logdir $PROFILER_DIR`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f5514c-4aca-42ab-afe4-f95217713724",
   "metadata": {},
   "source": [
    "### Vertex Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "421336fc-8031-45c4-afb1-3579e7b95d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : scale-my-mf-env-rec-bandits-v2\n",
      "RUN_NAME          : run-20231114-150903\n",
      "BASE_OUTPUT_DIR   : gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903\n",
      "LOG_DIR           : gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/logs\n",
      "ROOT_DIR          : gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/root\n",
      "ARTIFACTS_DIR     : gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'scale-my-mf-env-{PREFIX}'\n",
    "\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "BASE_OUTPUT_DIR   = f'gs://{BUCKET_NAME}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "LOG_DIR           = f\"{BASE_OUTPUT_DIR}/logs\"\n",
    "ROOT_DIR          = f\"{BASE_OUTPUT_DIR}/root\"                               # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BASE_OUTPUT_DIR}/artifacts\"                          # Where the trained model will be saved and restored.\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\")\n",
    "print(f\"BASE_OUTPUT_DIR   : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98865883-8f1e-49c8-8172-82fcfa6dfe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59c73f27-f79b-403e-9b77-1bfdc843c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_HYPERPARAMETER_TUNING       = False \n",
    "TRAIN_WITH_BEST_HYPERPARAMETERS = False   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b1dc2e-ec7b-424b-9a4a-de40cfdd4b51",
   "metadata": {},
   "source": [
    "### set Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe2866e1-3f4e-4bf0-a97e-760cf5c58cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME : projects/934903580331/locations/us-central1/tensorboards/2516109214863065088\n",
      "TB display name  : scale-my-mf-env-rec-bandits-v2-run-20231114-150903\n",
      "TB_ID            : 2516109214863065088\n"
     ]
    }
   ],
   "source": [
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}-{RUN_NAME}\"\n",
    "\n",
    "tensorboard = aiplatform.Tensorboard.create(\n",
    "    display_name=TENSORBOARD_DISPLAY_NAME\n",
    "    , project=PROJECT_ID\n",
    "    , location=REGION\n",
    ")\n",
    "\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "TB_ID = TB_RESOURCE_NAME.split('/')[-1]\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME : {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name  : {tensorboard.display_name}\")\n",
    "print(f\"TB_ID            : {TB_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71603efe-bfa5-4ec6-8103-cec850fb839e",
   "metadata": {},
   "source": [
    "### set training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccca6c0b-26a2-479e-a386-94a39c199f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE     : 128\n",
      "TRAINING_LOOPS : 100\n",
      "STEPS_PER_LOOP : 1\n",
      "NUM_ACTIONS    : 24\n",
      "CHKPT_INTERVAL : 20\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters.\n",
    "BATCH_SIZE       = BATCH_SIZE_best\n",
    "TRAINING_LOOPS   = 100\n",
    "STEPS_PER_LOOP   = 1\n",
    "NUM_ACTIONS      = NUM_ACTIONS_best\n",
    "\n",
    "CHKPT_INTERVAL       = TRAINING_LOOPS // 5\n",
    "\n",
    "print(f\"BATCH_SIZE     : {BATCH_SIZE}\")\n",
    "print(f\"TRAINING_LOOPS : {TRAINING_LOOPS}\")\n",
    "print(f\"STEPS_PER_LOOP : {STEPS_PER_LOOP}\")\n",
    "print(f\"NUM_ACTIONS    : {NUM_ACTIONS}\")\n",
    "print(f\"CHKPT_INTERVAL : {CHKPT_INTERVAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c28c490b-6764-4bc7-8552-10fea73a0d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--data-path=gs://rec-bandits-v2-hybrid-vertex-bucket/data',\n",
      "                              '--bucket_name=rec-bandits-v2-hybrid-vertex-bucket',\n",
      "                              '--data_gcs_prefix=data',\n",
      "                              '--data_path=gs://rec-bandits-v2-hybrid-vertex-bucket/data',\n",
      "                              '--project_number=934903580331',\n",
      "                              '--batch-size=128',\n",
      "                              '--rank-k=20',\n",
      "                              '--num-actions=24',\n",
      "                              '--tikhonov-weight=0.001',\n",
      "                              '--agent-alpha=10.0',\n",
      "                              '--training_loops=100',\n",
      "                              '--steps-per-loop=1',\n",
      "                              '--distribute=single',\n",
      "                              '--artifacts_dir=gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/artifacts',\n",
      "                              '--root_dir=gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/root',\n",
      "                              '--log_dir=gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/logs',\n",
      "                              '--experiment_name=scale-my-mf-env-rec-bandits-v2',\n",
      "                              '--experiment_run=run-20231114-150903',\n",
      "                              '--tf_gpu_thread_count=4',\n",
      "                              '--chkpt_interval=20',\n",
      "                              '--sum_grads_vars',\n",
      "                              '--debug_summaries',\n",
      "                              '--use_gpu'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/train-my-perarm-env-v2:latest'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_T4',\n",
      "                   'machine_type': 'n1-highcpu-16'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "WORKER_ARGS = [\n",
    "    f\"--data-path={DATA_PATH}\"               # TODO - remove duplicate arg\n",
    "    , f\"--bucket_name={BUCKET_NAME}\"\n",
    "    , f\"--data_gcs_prefix={DATA_GCS_PREFIX}\"\n",
    "    , f\"--data_path={DATA_PATH}\"\n",
    "    , f\"--project_number={PROJECT_NUM}\"\n",
    "    , f\"--batch-size={BATCH_SIZE}\"\n",
    "    , f\"--rank-k={RANK_K}\"\n",
    "    , f\"--num-actions={NUM_ACTIONS_best}\"\n",
    "    , f\"--tikhonov-weight={TIKHONOV_WEIGHT}\"\n",
    "    , f\"--agent-alpha={AGENT_ALPHA}\"\n",
    "    , f\"--training_loops={TRAINING_LOOPS}\"\n",
    "    , f\"--steps-per-loop={STEPS_PER_LOOP}\"\n",
    "    , f\"--distribute={DISTRIBUTE_STRATEGY}\"\n",
    "    , f\"--artifacts_dir={ARTIFACTS_DIR}\"\n",
    "    , f\"--root_dir={ROOT_DIR}\"\n",
    "    , f\"--log_dir={LOG_DIR}\"\n",
    "    , f\"--experiment_name={EXPERIMENT_NAME}\"\n",
    "    , f\"--experiment_run={RUN_NAME}\"\n",
    "    , f\"--tf_gpu_thread_count={TF_GPU_THREAD_COUNT}\"\n",
    "    , f\"--chkpt_interval={CHKPT_INTERVAL}\"\n",
    "    # , f\"--profiler\"\n",
    "    , f\"--sum_grads_vars\"\n",
    "    , f\"--debug_summaries\"\n",
    "    , f\"--use_gpu\"\n",
    "    # , f\"--use_tpu\"\n",
    "]\n",
    "\n",
    "if RUN_HYPERPARAMETER_TUNING:\n",
    "    WORKER_ARGS.append(\"--run-hyperparameter-tuning\")\n",
    "elif TRAIN_WITH_BEST_HYPERPARAMETERS:\n",
    "    WORKER_ARGS.append(\"--train-with-best-hyperparameters\")\n",
    "    WORKER_ARGS.append(f\"--best-hyperparameters-bucket={BUCKET_NAME}\")\n",
    "    WORKER_ARGS.append(f\"--best-hyperparameters-path={HPTUNING_RESULT_PATH}\")\n",
    "    \n",
    "from src.per_arm_rl import train_utils\n",
    "\n",
    "WORKER_POOL_SPECS = train_utils.prepare_worker_pool_specs(\n",
    "    image_uri=f\"{IMAGE_URI_01}:latest\",\n",
    "    args=WORKER_ARGS,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c437f0d1-f5e5-4dcc-b08c-efd964bd4f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME: mvl-best-train-run-20231114-150903\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID\n",
    "    , location=REGION\n",
    "    , experiment=EXPERIMENT_NAME\n",
    "    # , staging_bucket=ROOT_DIR\n",
    ")\n",
    "\n",
    "JOB_NAME = f\"mvl-best-train-{RUN_NAME}\"\n",
    "print(f\"JOB_NAME: {JOB_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45b07bef-7efa-4cfa-981a-59a75eaefeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CustomJob\n",
    "job = aiplatform.CustomJob(\n",
    "    display_name=JOB_NAME\n",
    "    , project=PROJECT_ID\n",
    "    , worker_pool_specs=WORKER_POOL_SPECS\n",
    "    , base_output_dir=BASE_OUTPUT_DIR\n",
    "    , staging_bucket=ROOT_DIR\n",
    "    # , location=\"asia-southeast1\" # TODO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce86a695-93f4-4d88-b8e1-d032d8fff054",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run(\n",
    "    tensorboard=TB_RESOURCE_NAME,\n",
    "    service_account=VERTEX_SA,\n",
    "    restart_job_on_worker_restart=False,\n",
    "    enable_web_access=True,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e41d7c6c-93f1-4d07-afab-b19257307ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: mvl-best-train-run-20231114-150903\n",
      "Job Resource Name: projects/934903580331/locations/us-central1/customJobs/5365823245574471680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job Name: {job.display_name}\")\n",
    "print(f\"Job Resource Name: {job.resource_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe40abf9-714f-4027-93ea-088d74792193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job.error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e44765f-46e6-4bcb-821c-d632bd11a898",
   "metadata": {},
   "source": [
    "### TensorBoard Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f444ff4d-ef4a-4c3e-81b7-2931b5f6e4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/logs/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/logs/events.out.tfevents.1699974892.6a7c11bc622b.1.0.v2\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66789dee-8c0b-42ec-9a39-ab06d57e412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "113ed591-2570-4d6a-a779-2754e012225b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c3333cb11bdb4e87\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c3333cb11bdb4e87\");\n",
       "          const url = new URL(\"/proxy/6007/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=$LOG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8878e9-399d-4c10-a0d6-4d6dc4ea3be7",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "* When a policy is trained, given a new observation request (i.e. a user vector), \n",
    "* the policy will inference (produce) actions, which are the recommended movies. \n",
    "* In TF-Agents, observations are abstracted in a named tuple,\n",
    "\n",
    "```\n",
    "TimeStep(step_type, discount, reward, observation)\n",
    "```\n",
    "\n",
    "* the policy map time steps to actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7cf849fa-ed47-489a-827a-5981329e5285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_OUTPUT_DIR: gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903\n",
      "CHKPOINT_DIR: gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/checkpoints\n"
     ]
    }
   ],
   "source": [
    "CHKPOINT_DIR = f\"{BASE_OUTPUT_DIR}/checkpoints\"\n",
    "\n",
    "print(f\"BASE_OUTPUT_DIR: {BASE_OUTPUT_DIR}\")\n",
    "print(f\"CHKPOINT_DIR: {CHKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6d2e592-5486-4b21-8274-aaec9a890dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/checkpoints/\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/checkpoints/checkpoint\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/checkpoints/ckpt-1.data-00000-of-00001\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/checkpoints/ckpt-1.index\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/checkpoints/ckpt-2.data-00000-of-00001\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/checkpoints/ckpt-2.index\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/checkpoints/ckpt-3.data-00000-of-00001\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/checkpoints/ckpt-3.index\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/checkpoints/ckpt-4.data-00000-of-00001\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/checkpoints/ckpt-4.index\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/checkpoints/ckpt-5.data-00000-of-00001\n",
      "gs://rec-bandits-v2-hybrid-vertex-bucket/scale-my-mf-env-rec-bandits-v2/run-20231114-150903/checkpoints/ckpt-5.index\n"
     ]
    }
   ],
   "source": [
    "# ARTIFACTS_DIR = \"gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230717-211248/model\"\n",
    "\n",
    "!gsutil ls $CHKPOINT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b8b465f-f764-49fb-b035-8f610b6c756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_policy = tf.saved_model.load(CHKPOINT_DIR)\n",
    "# trained_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dfa1889f-608e-4666-b286-ed77a1fc92bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded policy at step: %d -1\n"
     ]
    }
   ],
   "source": [
    "# train_step = trained_policy.get_train_step()\n",
    "# print('Loaded policy at step: %d', train_step.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f22e67b2-c0de-47ea-97f6-e4d84b41bcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.per_arm_rl import my_per_arm_py_env as my_per_arm_py_env\n",
    "\n",
    "env = my_per_arm_py_env.MyMovieLensPerArmPyEnvironment(\n",
    "    project_number = PROJECT_NUM\n",
    "    , data_path = DATA_PATH\n",
    "    , bucket_name = BUCKET_NAME\n",
    "    , data_gcs_prefix = DATA_GCS_PREFIX\n",
    "    , user_age_lookup_dict = data_config.USER_AGE_LOOKUP\n",
    "    , user_occ_lookup_dict = data_config.USER_OCC_LOOKUP\n",
    "    , movie_gen_lookup_dict = data_config.MOVIE_GEN_LOOKUP\n",
    "    , num_users = data_config.MOVIELENS_NUM_USERS\n",
    "    , num_movies = data_config.MOVIELENS_NUM_MOVIES\n",
    "    , rank_k = RANK_K\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , num_actions = NUM_ACTIONS\n",
    ")\n",
    "\n",
    "environment = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2e542503-f5bb-40dc-bdb0-6dacb0afdedf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 9, 11, 0, 12, 5, 7, 13]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_array = environment._observe()\n",
    "\n",
    "time_step = tf_agents.trajectories.restart(\n",
    "    observation=observation_array,\n",
    "    batch_size=tf.convert_to_tensor([BATCH_SIZE]),\n",
    ")\n",
    "\n",
    "action_step = trained_policy.action(time_step)\n",
    "\n",
    "action_step.action.numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055c931e-d69e-47cb-b7af-0d85b8ad03bd",
   "metadata": {},
   "source": [
    "### debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ae456bf-6d03-41bb-b6bb-f5c345a5bbcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# observation_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b8d6ede1-3c60-4e0c-9e2e-411861fe3b40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4ff21900-0965-49a0-a805-7929fb06263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be7cc8dc-ba10-4b11-8243-628d16e86825",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# observation_array = environment._observe()\n",
    "# observation_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ab7ef60-8f8e-4db9-ac0d-dfb04c8a8357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# time_step = tf_agents.trajectories.restart(\n",
    "#     observation=observation_array,\n",
    "#     batch_size=tf.convert_to_tensor([BATCH_SIZE]),\n",
    "# )\n",
    "# time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24d7badc-6a24-481c-8729-36ad898f2755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# action_step = trained_policy.action(time_step)\n",
    "# action_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "575e930a-e229-4417-b50f-0d805623cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_step.action.numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9634a-80d0-4058-a88d-aa0e566573ba",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67abc70e-6546-45e2-aba2-c1e65ba8b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete endpoint resource\n",
    "# ! gcloud ai endpoints delete $endpoint.name --quiet --region $REGION\n",
    "\n",
    "# # Delete model resource\n",
    "# ! gcloud ai models delete $model.name --quiet\n",
    "\n",
    "# # Delete Cloud Storage objects that were created\n",
    "# ! gsutil -m rm -r $ARTIFACTS_DIR"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
