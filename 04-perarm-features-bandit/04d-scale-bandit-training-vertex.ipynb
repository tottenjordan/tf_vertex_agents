{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4699e32-3938-4c72-9eb7-4fe36bea46a0",
   "metadata": {},
   "source": [
    "# Scaling bandit training with Vertex AI \n",
    "\n",
    "**prerequisites:**\n",
    "* build training image in `04b-build-training-image` noteook\n",
    "\n",
    "**Recommendation**\n",
    "\n",
    "When profiling a train job, we don't need to do a full train. \n",
    "\n",
    "> We just need to get multiple iterations of going through the entire Agent graph (i.e., from data iterator --> agent.train a few times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725d9fa8-ad43-49b4-8bf5-75fda5e337fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiplatform SDK version: 1.26.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28447656-866e-4403-92ae-e2b3700a71bb",
   "metadata": {},
   "source": [
    "## setup notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38e75f3e-73b9-4f14-8aff-08d3e4ea849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/tf_vertex_agents/04-perarm-features-bandit\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6546a-ec55-46eb-a2fe-c4e8b6a5ad9d",
   "metadata": {},
   "source": [
    "### Load env config\n",
    "* use the prefix from `00-env-setup` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c928177-d72f-4715-a149-91c9a0a9f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'mabv1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "314139a3-e896-4f55-acd7-9daac475e98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"mabv1\"\n",
      "VERSION                  = \"v1\"\n",
      "\n",
      "BUCKET_NAME              = \"mabv1-hybrid-vertex-bucket\"\n",
      "BUCKET_URI               = \"gs://mabv1-hybrid-vertex-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://mabv1-hybrid-vertex-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_ID      = \"hybrid-vertex.movielens_dataset_mabv1\"\n",
      "BIGQUERY_TABLE_ID        = \"hybrid-vertex.movielens_dataset_mabv1.training_dataset\"\n",
      "\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "RL_SUB_DIR               = \"per_arm_rl\"\n",
      "\n",
      "REPOSITORY               = \"rl-movielens-mabv1\"\n",
      "IMAGE_NAME               = \"train-perarm-feats-v1\"\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-mabv1/train-perarm-feats-v1\"\n",
      "DOCKERNAME               = \"Dockerfile_perarm_feats\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd4a5c39-b56c-4422-9124-c58cbb9951dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil iam ch serviceAccount:{VERTEX_SA}:roles/storage.objects.get $BUCKET_URI\n",
    "# ! gsutil iam ch serviceAccount:{VERTEX_SA}:roles/storage.objects.get $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce57c0ef-7e44-4c0b-afb2-adb1021c6e0e",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c9f1333-417a-4d12-b28d-c5183b5b4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2490c9ee-9a23-4903-a659-228268a31081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "import pickle as pkl\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "786af747-66b9-46ab-aef5-2f3dfe8cc564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.per_arm_rl import data_utils\n",
    "from src.per_arm_rl import train_utils\n",
    "from src.per_arm_rl import data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a53e33e-c682-43db-9de6-240721914a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "vertex_ai.init(project=PROJECT_ID,location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ec42ad-e318-4e9c-b1fe-4f875c9e3574",
   "metadata": {},
   "source": [
    "# Vertex Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d534c68-7ab2-4e15-bdf4-220aab5a6531",
   "metadata": {},
   "source": [
    "## job compute\n",
    "\n",
    "Set the variable `TRAIN_COMPUTE` to configure the compute resources for the VMs you will use for for training.\n",
    "\n",
    "**Machine Type:**\n",
    "* `n1-standard`: 3.75GB of memory per vCPU.\n",
    "* `n1-highmem`: 6.5GB of memory per vCPU\n",
    "* `n1-highcpu`: 0.9 GB of memory per vCPU\n",
    "* `vCPUs`: number of `[2, 4, 8, 16, 32, 64, 96 ]`\n",
    "\n",
    "**Note:** The following is not supported for training:\n",
    "\n",
    "* `standard`: 2 vCPUs\n",
    "* `highcpu`: 2, 4 and 8 vCPUs\n",
    "\n",
    "> Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs.\n",
    "\n",
    "relevant docs: \n",
    "* [Configure compute resources for training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types) for more details\n",
    "* [Machine series comparison](https://cloud.google.com/compute/docs/machine-resource#machine_type_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0700c7d-4c38-491d-90a5-7cf052753d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_GPU: t4\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = \"t4\" # str: \"a100\" | \"t4\" | None\n",
    "USE_GPU = str(USE_GPU)\n",
    "print(f\"USE_GPU: {USE_GPU}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd78debb-2363-4ed6-a8cf-7da7ae44a050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKER_MACHINE_TYPE            : n1-standard-16\n",
      "REPLICA_COUNT                  : 1\n",
      "ACCELERATOR_TYPE               : NVIDIA_TESLA_T4\n",
      "PER_MACHINE_ACCELERATOR_COUNT  : 1\n",
      "DISTRIBUTE_STRATEGY            : single\n",
      "REDUCTION_SERVER_COUNT         : 0\n",
      "REDUCTION_SERVER_MACHINE_TYPE  : n1-highcpu-16\n"
     ]
    }
   ],
   "source": [
    "if USE_GPU == \"a100\":\n",
    "    WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "    REPLICA_COUNT = 1\n",
    "    ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "    PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "    REDUCTION_SERVER_COUNT = 0                                                      \n",
    "    REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "    DISTRIBUTE_STRATEGY = 'single'\n",
    "elif USE_GPU == 't4':\n",
    "    WORKER_MACHINE_TYPE = 'n1-standard-16'\n",
    "    REPLICA_COUNT = 1\n",
    "    ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4' # NVIDIA_TESLA_T4 NVIDIA_TESLA_V100\n",
    "    PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "    DISTRIBUTE_STRATEGY = 'single'\n",
    "    REDUCTION_SERVER_COUNT = 0                                                      \n",
    "    REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "elif USE_GPU == \"False\":\n",
    "    WORKER_MACHINE_TYPE = 'n2-highmem-32' # 'n1-highmem-96'n | 'n2-highmem-92'\n",
    "    REPLICA_COUNT = 1\n",
    "    ACCELERATOR_TYPE = None\n",
    "    PER_MACHINE_ACCELERATOR_COUNT = 0\n",
    "    DISTRIBUTE_STRATEGY = 'single'\n",
    "    REDUCTION_SERVER_COUNT = 0                                                      \n",
    "    REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "\n",
    "print(f\"WORKER_MACHINE_TYPE            : {WORKER_MACHINE_TYPE}\")\n",
    "print(f\"REPLICA_COUNT                  : {REPLICA_COUNT}\")\n",
    "print(f\"ACCELERATOR_TYPE               : {ACCELERATOR_TYPE}\")\n",
    "print(f\"PER_MACHINE_ACCELERATOR_COUNT  : {PER_MACHINE_ACCELERATOR_COUNT}\")\n",
    "print(f\"DISTRIBUTE_STRATEGY            : {DISTRIBUTE_STRATEGY}\")\n",
    "print(f\"REDUCTION_SERVER_COUNT         : {REDUCTION_SERVER_COUNT}\")\n",
    "print(f\"REDUCTION_SERVER_MACHINE_TYPE  : {REDUCTION_SERVER_MACHINE_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1e4a8-4c1a-47ab-9962-f8ec1eb414bc",
   "metadata": {},
   "source": [
    "## set Vertex AI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0358f36-d5d7-4752-b2ac-3bccdf54edfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME   : scale-mod-v1\n",
      "RUN_NAME          : run-20230824-145013\n",
      "\n",
      "BASE_OUTPUT_DIR   : gs://mabv1-hybrid-vertex-bucket/scale-mod-v1/run-20230824-145013\n",
      "LOG_DIR           : gs://mabv1-hybrid-vertex-bucket/scale-mod-v1/run-20230824-145013/logs\n",
      "ROOT_DIR          : gs://mabv1-hybrid-vertex-bucket/scale-mod-v1/run-20230824-145013/root\n",
      "ARTIFACTS_DIR     : gs://mabv1-hybrid-vertex-bucket/scale-mod-v1/run-20230824-145013/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'scale-mod-v1'\n",
    "\n",
    "# new experiment\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME          = f'run-{invoke_time}'\n",
    "\n",
    "BASE_OUTPUT_DIR   = f'{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "LOG_DIR           = f\"{BASE_OUTPUT_DIR}/logs\"\n",
    "ROOT_DIR          = f\"{BASE_OUTPUT_DIR}/root\"       # Root directory for writing logs/summaries/checkpoints.\n",
    "ARTIFACTS_DIR     = f\"{BASE_OUTPUT_DIR}/artifacts\"  # Where the trained model will be saved and restored.\n",
    "\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    experiment=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "print(f\"EXPERIMENT_NAME   : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME          : {RUN_NAME}\\n\")\n",
    "print(f\"BASE_OUTPUT_DIR   : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"LOG_DIR           : {LOG_DIR}\")\n",
    "print(f\"ROOT_DIR          : {ROOT_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR     : {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611995c1-d451-4f97-8b87-f89551497590",
   "metadata": {},
   "source": [
    "## Create Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9aeed88-2bba-4832-abaf-f0f6d9b71e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME: projects/934903580331/locations/us-central1/tensorboards/139743529843818496\n",
      "TB display name: scale-mod-v1-run-20230824-145013\n"
     ]
    }
   ],
   "source": [
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}-{RUN_NAME}\"\n",
    "\n",
    "tensorboard = vertex_ai.Tensorboard.create(\n",
    "    display_name=TENSORBOARD_DISPLAY_NAME\n",
    "    , project=PROJECT_ID\n",
    "    , location=REGION\n",
    ")\n",
    "\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "# use existing TB instance\n",
    "# TB_RESOURCE_NAME = 'projects/934903580331/locations/us-central1/tensorboards/6924469145035603968'\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name: {tensorboard.display_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b8ed9-69f1-4aec-9dd8-c8995a265bbb",
   "metadata": {},
   "source": [
    "## Set training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9eae7385-dfff-4e3e-af00-c20fa6a05496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOTE_IMAGE_NAME : us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-mabv1/train-perarm-feats-v1\n"
     ]
    }
   ],
   "source": [
    "print(f\"REMOTE_IMAGE_NAME : {REMOTE_IMAGE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd13dcc4-ab99-4e7c-9028-a7cd93b77a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SUBDIR           : vocabs\n",
      "VOCAB_FILENAME         : vocab_dict.pkl\n",
      "BATCH_SIZE             : 128\n",
      "TRAINING_LOOPS         : 300\n",
      "STEPS_PER_LOOP         : 1\n",
      "ASYNC_STEPS_PER_LOOP   : 1\n",
      "LOG_INTERVAL           : 50\n",
      "RANK_K                 : 10\n",
      "NUM_ACTIONS            : 2\n",
      "PER_ARM                : True\n",
      "AGENT_TYPE             : epsGreedy\n",
      "NETWORK_TYPE           : commontower\n",
      "TIKHONOV_WEIGHT        : 0.001\n",
      "AGENT_ALPHA            : 0.1\n",
      "GLOBAL_DIM             : 64\n",
      "PER_ARM_DIM            : 64\n",
      "SPLIT                  : train\n",
      "RESUME_TRAINING        : None\n",
      "NUM_OOV_BUCKETS        : 1\n",
      "GLOBAL_EMBEDDING_SIZE  : 16\n",
      "MV_EMBEDDING_SIZE      : 32\n",
      "AGENT_ALPHA            : 0.1\n",
      "GLOBAL_LAYERS          : [64, 32, 16]\n",
      "ARM_LAYERS             : [64, 32, 16]\n",
      "COMMON_LAYERS          : [16, 8]\n",
      "LR                     : 0.05\n",
      "CHKPT_INTERVAL         : 1000\n",
      "EVAL_BATCH_SIZE        : 1\n",
      "NUM_EVAL_STEPS         : 19000\n",
      "EPSILON                : 0.01\n",
      "ENCODING_DIM           : 1\n",
      "EPS_PHASE_STEPS        : 1000\n"
     ]
    }
   ],
   "source": [
    "# vocab\n",
    "# VOCAB_SUBDIR         = \"vocabs\"\n",
    "# VOCAB_FILENAME       = \"vocab_dict.pkl\"\n",
    "\n",
    "# Set hyperparameters.\n",
    "BATCH_SIZE           = 128          # Training and prediction batch size.\n",
    "TRAINING_LOOPS       = 300          # Number of training iterations.\n",
    "STEPS_PER_LOOP       = 1            # Number of driver steps per training iteration.\n",
    "ASYNC_STEPS_PER_LOOP = 1\n",
    "LOG_INTERVAL         = 50\n",
    "LR                   = 0.05\n",
    "\n",
    "CHKPT_INTERVAL       = 1000\n",
    "EVAL_BATCH_SIZE      = 1  \n",
    "NUM_EVAL_STEPS       = 19000 #10000\n",
    "\n",
    "# Set MovieLens simulation environment parameters.\n",
    "RANK_K               = 10      # Rank for matrix factorization in the MovieLens environment; also the observation dimension.\n",
    "NUM_ACTIONS          = 2       # Number of actions (movie items) to choose from.\n",
    "PER_ARM              = True    # Use the non-per-arm version of the MovieLens environment.\n",
    "\n",
    "# ================================\n",
    "# Agent\n",
    "# ================================\n",
    "AGENT_TYPE          = 'epsGreedy' # 'LinUCB' | 'LinTS |, 'epsGreedy' | 'NeuralLinUCB'\n",
    "NETWORK_TYPE        = \"commontower\" # 'commontower' | 'dotproduct'\n",
    "\n",
    "if AGENT_TYPE == 'NeuralLinUCB':\n",
    "    NETWORK_TYPE = 'commontower'\n",
    "\n",
    "TIKHONOV_WEIGHT     = 0.001   # LinUCB Tikhonov regularization weight.\n",
    "AGENT_ALPHA         = 0.1     # LinUCB exploration parameter that multiplies the confidence intervals.\n",
    "EPSILON             = 0.01\n",
    "ENCODING_DIM        = 1\n",
    "EPS_PHASE_STEPS     = 1000\n",
    "\n",
    "# ================================\n",
    "# network params\n",
    "# ================================\n",
    "GLOBAL_LAYERS       = [64, 32, 16]\n",
    "ARM_LAYERS          = [64, 32, 16]\n",
    "COMMON_LAYERS       = [16, 8]\n",
    "\n",
    "# ================================\n",
    "# data config\n",
    "# ================================\n",
    "GLOBAL_DIM             = 64       # 16\n",
    "PER_ARM_DIM            = 64       # 16\n",
    "NUM_OOV_BUCKETS        = 1\n",
    "GLOBAL_EMBEDDING_SIZE  = 16\n",
    "MV_EMBEDDING_SIZE      = 32       # 32\n",
    "SPLIT                  = \"train\"  # TODO - remove\n",
    "RESUME_TRAINING        = None\n",
    "\n",
    "print(f\"VOCAB_SUBDIR           : {VOCAB_SUBDIR}\")\n",
    "print(f\"VOCAB_FILENAME         : {VOCAB_FILENAME}\")\n",
    "print(f\"BATCH_SIZE             : {BATCH_SIZE}\")\n",
    "print(f\"TRAINING_LOOPS         : {TRAINING_LOOPS}\")\n",
    "print(f\"STEPS_PER_LOOP         : {STEPS_PER_LOOP}\")\n",
    "print(f\"ASYNC_STEPS_PER_LOOP   : {ASYNC_STEPS_PER_LOOP}\")\n",
    "print(f\"LOG_INTERVAL           : {LOG_INTERVAL}\")\n",
    "print(f\"RANK_K                 : {RANK_K}\")\n",
    "print(f\"NUM_ACTIONS            : {NUM_ACTIONS}\")\n",
    "print(f\"PER_ARM                : {PER_ARM}\")\n",
    "print(f\"AGENT_TYPE             : {AGENT_TYPE}\")\n",
    "print(f\"NETWORK_TYPE           : {NETWORK_TYPE}\")\n",
    "print(f\"TIKHONOV_WEIGHT        : {TIKHONOV_WEIGHT}\")\n",
    "print(f\"AGENT_ALPHA            : {AGENT_ALPHA}\")\n",
    "print(f\"GLOBAL_DIM             : {GLOBAL_DIM}\")\n",
    "print(f\"PER_ARM_DIM            : {PER_ARM_DIM}\")\n",
    "print(f\"SPLIT                  : {SPLIT}\")\n",
    "print(f\"RESUME_TRAINING        : {RESUME_TRAINING}\")\n",
    "print(f\"NUM_OOV_BUCKETS        : {NUM_OOV_BUCKETS}\")\n",
    "print(f\"GLOBAL_EMBEDDING_SIZE  : {GLOBAL_EMBEDDING_SIZE}\")\n",
    "print(f\"MV_EMBEDDING_SIZE      : {MV_EMBEDDING_SIZE}\")\n",
    "print(f\"AGENT_ALPHA            : {AGENT_ALPHA}\")\n",
    "print(f\"GLOBAL_LAYERS          : {GLOBAL_LAYERS}\")\n",
    "print(f\"ARM_LAYERS             : {ARM_LAYERS}\")\n",
    "print(f\"COMMON_LAYERS          : {COMMON_LAYERS}\")\n",
    "print(f\"LR                     : {LR}\")\n",
    "print(f\"CHKPT_INTERVAL         : {CHKPT_INTERVAL}\")\n",
    "print(f\"EVAL_BATCH_SIZE        : {EVAL_BATCH_SIZE}\")\n",
    "print(f\"NUM_EVAL_STEPS         : {NUM_EVAL_STEPS}\")\n",
    "print(f\"EPSILON                : {EPSILON}\")\n",
    "print(f\"ENCODING_DIM           : {ENCODING_DIM}\")\n",
    "print(f\"EPS_PHASE_STEPS        : {EPS_PHASE_STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4411e60-067d-4e29-92ff-12dbbc46726f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--project=hybrid-vertex',\n",
      "                              '--project_number=934903580331',\n",
      "                              '--bucket_name=mabv1-hybrid-vertex-bucket',\n",
      "                              '--artifacts_dir=gs://mabv1-hybrid-vertex-bucket/scale-mod-v1/run-20230824-145013/artifacts',\n",
      "                              '--root_dir=gs://mabv1-hybrid-vertex-bucket/scale-mod-v1/run-20230824-145013/root',\n",
      "                              '--log_dir=gs://mabv1-hybrid-vertex-bucket/scale-mod-v1/run-20230824-145013/logs',\n",
      "                              '--data_dir_prefix_path=data',\n",
      "                              '--vocab_prefix_path=vocabs',\n",
      "                              '--vocab_filename=vocab_dict.pkl',\n",
      "                              '--distribute=single',\n",
      "                              '--experiment_name=scale-mod-v1',\n",
      "                              '--experiment_run=run-20230824-145013',\n",
      "                              '--agent_type=epsGreedy',\n",
      "                              '--network_type=commontower',\n",
      "                              '--batch_size=128',\n",
      "                              '--eval_batch_size=1',\n",
      "                              '--training_loops=300',\n",
      "                              '--steps_per_loop=1',\n",
      "                              '--num_eval_steps=19000',\n",
      "                              '--rank_k=10',\n",
      "                              '--num_actions=2',\n",
      "                              '--async_steps_per_loop=1',\n",
      "                              '--global_dim=64',\n",
      "                              '--per_arm_dim=64',\n",
      "                              '--split=train',\n",
      "                              '--log_interval=50',\n",
      "                              '--chkpt_interval=1000',\n",
      "                              '--num_oov_buckets=1',\n",
      "                              '--global_emb_size=16',\n",
      "                              '--mv_emb_size=32',\n",
      "                              '--agent_alpha=0.1',\n",
      "                              '--global_layers=[64, 32, 16]',\n",
      "                              '--arm_layers=[64, 32, 16]',\n",
      "                              '--common_layers=[16, 8]',\n",
      "                              '--learning_rate=0.05',\n",
      "                              '--epsilon=0.01',\n",
      "                              '--encoding_dim=1',\n",
      "                              '--eps_phase_steps=1000',\n",
      "                              '--use_gpu'],\n",
      "                     'image_uri': 'us-central1-docker.pkg.dev/hybrid-vertex/rl-movielens-mabv1/train-perarm-feats-v1'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_T4',\n",
      "                   'machine_type': 'n1-standard-16'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "WORKER_ARGS = [\n",
    "    f\"--project={PROJECT_ID}\"\n",
    "    , f\"--project_number={PROJECT_NUM}\"\n",
    "    , f\"--bucket_name={BUCKET_NAME}\"\n",
    "    , f\"--artifacts_dir={ARTIFACTS_DIR}\"\n",
    "    , f\"--root_dir={ROOT_DIR}\"\n",
    "    , f\"--log_dir={LOG_DIR}\"\n",
    "    , f\"--data_dir_prefix_path={DATA_GCS_PREFIX}\"\n",
    "    , f\"--vocab_prefix_path={VOCAB_SUBDIR}\"\n",
    "    , f\"--vocab_filename={VOCAB_FILENAME}\"\n",
    "    ### job config\n",
    "    , f\"--distribute={DISTRIBUTE_STRATEGY}\"\n",
    "    , f\"--experiment_name={EXPERIMENT_NAME}\"\n",
    "    , f\"--experiment_run={RUN_NAME}\"\n",
    "    , f\"--agent_type={AGENT_TYPE}\"\n",
    "    , f\"--network_type={NETWORK_TYPE}\"\n",
    "    ### hparams\n",
    "    , f\"--batch_size={BATCH_SIZE}\"\n",
    "    , f\"--eval_batch_size={EVAL_BATCH_SIZE}\"\n",
    "    , f\"--training_loops={TRAINING_LOOPS}\"\n",
    "    , f\"--steps_per_loop={STEPS_PER_LOOP}\"\n",
    "    , f\"--num_eval_steps={NUM_EVAL_STEPS}\"\n",
    "    , f\"--rank_k={RANK_K}\"\n",
    "    , f\"--num_actions={NUM_ACTIONS}\"\n",
    "    , f\"--async_steps_per_loop={ASYNC_STEPS_PER_LOOP}\"\n",
    "    # , f\"--resume_training_loops\"\n",
    "    , f\"--global_dim={GLOBAL_DIM}\"\n",
    "    , f\"--per_arm_dim={PER_ARM_DIM}\"\n",
    "    , f\"--split={SPLIT}\"\n",
    "    , f\"--log_interval={LOG_INTERVAL}\"\n",
    "    , f\"--chkpt_interval={CHKPT_INTERVAL}\"\n",
    "    , f\"--num_oov_buckets={NUM_OOV_BUCKETS}\"\n",
    "    , f\"--global_emb_size={GLOBAL_EMBEDDING_SIZE}\"\n",
    "    , f\"--mv_emb_size={MV_EMBEDDING_SIZE}\"\n",
    "    , f\"--agent_alpha={AGENT_ALPHA}\"\n",
    "    , f\"--global_layers={GLOBAL_LAYERS}\"\n",
    "    , f\"--arm_layers={ARM_LAYERS}\"\n",
    "    , f\"--common_layers={COMMON_LAYERS}\"\n",
    "    , f\"--learning_rate={LR}\"\n",
    "    , f\"--epsilon={EPSILON}\"\n",
    "    , f\"--encoding_dim={ENCODING_DIM}\"\n",
    "    , f\"--eps_phase_steps={EPS_PHASE_STEPS}\"\n",
    "    ### accelerators & profiling\n",
    "    , f\"--use_gpu\"\n",
    "    # , f\"--use_tpu\"\n",
    "    # , f\"--profiler\"\n",
    "]\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.per_arm_rl import train_utils\n",
    "\n",
    "WORKER_POOL_SPECS = train_utils.prepare_worker_pool_specs(\n",
    "    image_uri=f\"{REMOTE_IMAGE_NAME}\",\n",
    "    args=WORKER_ARGS,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e582ba0-71f0-4b9d-a5ff-edb7b43dc046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea381beb-3d3b-4595-bd69-b8aeda060ebf",
   "metadata": {},
   "source": [
    "# Submit trainging job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f42d2a93-7043-4433-8d0f-3c15c704419c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME: paf-bandit-run-20230824-145013\n"
     ]
    }
   ],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID\n",
    "    , location=REGION\n",
    "    , experiment=EXPERIMENT_NAME\n",
    "    # , staging_bucket=ROOT_DIR\n",
    ")\n",
    "\n",
    "JOB_NAME = f\"paf-bandit-{RUN_NAME}\"\n",
    "print(f\"JOB_NAME: {JOB_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd25c64d-de18-439d-96f6-5d8d166f4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CustomJob\n",
    "my_custom_job = vertex_ai.CustomJob(\n",
    "    display_name=JOB_NAME\n",
    "    , project=PROJECT_ID\n",
    "    , worker_pool_specs=WORKER_POOL_SPECS\n",
    "    , base_output_dir=BASE_OUTPUT_DIR\n",
    "    , staging_bucket=ROOT_DIR\n",
    "    # , location=\"asia-southeast1\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db98ac56-e829-4f47-a21f-95fdc3e21fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_custom_job.run(\n",
    "    tensorboard=TB_RESOURCE_NAME,\n",
    "    service_account=VERTEX_SA,\n",
    "    restart_job_on_worker_restart=False,\n",
    "    enable_web_access=True,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6514a309-ca15-4ca3-a32d-857adb749d4e",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbef7bc-0077-48ce-9be0-85e4cf881523",
   "metadata": {},
   "source": [
    "### in-notebook TensorBoard\n",
    "\n",
    "> if `--profiler`, find `PROFILE` in the drop down:\n",
    "\n",
    "<img src=\"./imgs/getting_profiler.png\" \n",
    "     align=\"center\" \n",
    "     width=\"850\"\n",
    "     height=\"850\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae6b0c00-39da-411a-8b96-b151a9c45ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_LOGS_PATH: gs://mabv1-hybrid-vertex-bucket/scale-mod-v1/run-20230824-121858/logs\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "TB_LOGS_PATH = LOG_DIR\n",
    "print(f\"TB_LOGS_PATH: {TB_LOGS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1763ace-2c4a-4b93-a8b9-4419f5858f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eecde86e-96e4-4389-a59b-97b069b97c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-24dec2e324713950\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-24dec2e324713950\");\n",
       "          const url = new URL(\"/proxy/6007/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=$TB_LOGS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3bf4ac-5621-425e-b74f-5cbaa738b398",
   "metadata": {},
   "source": [
    "# Making predictions\n",
    "\n",
    "* When a policy is trained, given a new observation request (i.e. a user vector),\n",
    "* the policy will inference (produce) actions, which are the recommended movies.\n",
    "* In TF-Agents, observations are abstracted in a named tuple,\n",
    "\n",
    "```\n",
    "TimeStep(‘step_type’, ‘discount’, ‘reward’, ‘observation’)\n",
    "```\n",
    "\n",
    "> the policy maps time steps to actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7eb70de-3aa9-41ca-8f37-b7fb8b7d931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from src.perarm_features import emb_features as emb_features\n",
    "from src.perarm_features import reward_factory as reward_factory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653049f-0e5f-4055-99c7-471c5a4a665e",
   "metadata": {},
   "source": [
    "## Load eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcbc77f1-1e3f-4803-a3f6-70377a755d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec={'bucketized_user_age': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'movie_genres': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None), 'movie_id': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'timestamp': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'user_id': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_occupation_text': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_rating': TensorSpec(shape=(None,), dtype=tf.float32, name=None)}>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT = \"val\"\n",
    "\n",
    "val_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{DATA_GCS_PREFIX}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        val_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "val_dataset = tf.data.TFRecordDataset(val_files)\n",
    "val_dataset = val_dataset.map(data_utils.parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# eval dataset\n",
    "eval_ds = val_dataset.batch(1)\n",
    "\n",
    "# if NUM_EVAL_STEPS > 0:\n",
    "#     eval_ds = eval_ds.take(NUM_EVAL_STEPS)\n",
    "\n",
    "eval_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc01ee-2857-4cf9-8369-e49156c906af",
   "metadata": {},
   "source": [
    "### Load vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fe552cf-817c-49b1-8f4b-01860e0f85d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vocab...\n",
      "Downloaded vocab from: gs://mabv1-hybrid-vertex-bucket/vocabs/vocab_dict.pkl\n",
      "\n",
      "'movie_id'\n",
      "'user_id'\n",
      "'user_occupation_text'\n",
      "'movie_genres'\n",
      "'bucketized_user_age'\n",
      "'max_timestamp'\n",
      "'min_timestamp'\n",
      "'timestamp_buckets'\n"
     ]
    }
   ],
   "source": [
    "EXISTING_VOCAB_FILE = f'gs://{BUCKET_NAME}/{VOCAB_SUBDIR}/{VOCAB_FILENAME}'\n",
    "print(f\"Downloading vocab...\")\n",
    "\n",
    "os.system(f'gsutil -q cp {EXISTING_VOCAB_FILE} .')\n",
    "print(f\"Downloaded vocab from: {EXISTING_VOCAB_FILE}\\n\")\n",
    "\n",
    "filehandler = open(VOCAB_FILENAME, 'rb')\n",
    "vocab_dict = pkl.load(filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "for key in vocab_dict.keys():\n",
    "    pprint(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d14f4f2-8304-40e5-b7ed-aad69b424bac",
   "metadata": {},
   "source": [
    "## load trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d547058-6982-4588-94a7-62dda9801f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://mabv1-hybrid-vertex-bucket/scale-mod-v1/run-20230824-121858/artifacts/\n",
      "gs://mabv1-hybrid-vertex-bucket/scale-mod-v1/run-20230824-121858/artifacts/fingerprint.pb\n",
      "gs://mabv1-hybrid-vertex-bucket/scale-mod-v1/run-20230824-121858/artifacts/policy_specs.pbtxt\n",
      "gs://mabv1-hybrid-vertex-bucket/scale-mod-v1/run-20230824-121858/artifacts/saved_model.pb\n",
      "gs://mabv1-hybrid-vertex-bucket/scale-mod-v1/run-20230824-121858/artifacts/assets/\n",
      "gs://mabv1-hybrid-vertex-bucket/scale-mod-v1/run-20230824-121858/artifacts/variables/\n"
     ]
    }
   ],
   "source": [
    "# MODEL_DIR = \"gs://mabv1-hybrid-vertex-bucket/scale-perarm-hpt/run-20230717-211248/model\"\n",
    "\n",
    "!gsutil ls $ARTIFACTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b70d44d1-2afc-47fa-8c37-dbbea6b286bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.py_tf_eager_policy.SavedModelPyTFEagerPolicy at 0x7f7991e24190>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy\n",
    "\n",
    "trained_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n",
    "    ARTIFACTS_DIR, load_specs_from_pbtxt=True\n",
    ")\n",
    "\n",
    "trained_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff61492-f040-4418-b3df-8a1b05684bce",
   "metadata": {},
   "source": [
    "## call embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f69d531-d286-441a-aff9-d6afd3e30a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GLOBAL_EMBEDDING_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a78f2086-6cd3-4f81-8486-ac8356889e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.perarm_features.emb_features.EmbeddingModel at 0x7f799199efb0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = emb_features.EmbeddingModel(\n",
    "    vocab_dict = vocab_dict,\n",
    "    num_oov_buckets = NUM_OOV_BUCKETS,\n",
    "    global_emb_size = GLOBAL_EMBEDDING_SIZE,\n",
    "    mv_emb_size = MV_EMBEDDING_SIZE,\n",
    ")\n",
    "\n",
    "embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af1cb7-a6a4-48ac-8b42-7d5cd20f51ed",
   "metadata": {},
   "source": [
    "## Run inference with trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1612722-ce10-4766-9b73-cd3ad6ee902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFER_SIZE = 1\n",
    "dummy_arm = tf.zeros([INFER_SIZE, PER_ARM_DIM], dtype=tf.float32)\n",
    "\n",
    "for x in eval_ds.take(INFER_SIZE):\n",
    "    # get feature tensors\n",
    "    global_feat_infer = embs._get_global_context_features(x)\n",
    "    arm_feat_infer = embs._get_per_arm_features(x)\n",
    "    \n",
    "    # rewards = _get_rewards(x)\n",
    "    rewards = reward_factory._get_rewards(x)\n",
    "    \n",
    "    # reshape arm features\n",
    "    arm_feat_infer = tf.reshape(arm_feat_infer, [EVAL_BATCH_SIZE, PER_ARM_DIM]) # perarm_dim\n",
    "    concat_arm = tf.concat([arm_feat_infer, dummy_arm], axis=0)\n",
    "    \n",
    "    # flatten global\n",
    "    flat_global_infer = tf.reshape(global_feat_infer, [GLOBAL_DIM])\n",
    "    feature = {'global': flat_global_infer, 'per_arm': concat_arm}\n",
    "    \n",
    "    # get actual reward\n",
    "    actual_reward = rewards.numpy()[0]\n",
    "    \n",
    "    # build trajectory step\n",
    "    trajectory_step = train_utils._get_eval_step(feature, actual_reward)\n",
    "    \n",
    "    prediction = trained_policy.action(trajectory_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "185c1ee8-7a3c-42b2-87e0-2b27962b043c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=array(0, dtype=int32), state=(), info=PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=array([3.4261932, 3.345249 ], dtype=float32), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=(), chosen_arm_features=array([-0.00077925,  0.04027135, -0.03941175, -0.03021237,  0.03059468,\n",
       "       -0.03124018,  0.0269785 , -0.04453319, -0.03894002,  0.01960157,\n",
       "        0.02454543, -0.00041627,  0.00531179, -0.01486508,  0.03092239,\n",
       "        0.02424474, -0.03802389,  0.02251568,  0.04327125, -0.03721147,\n",
       "        0.01682815, -0.02249478,  0.04773061, -0.04053724,  0.01620258,\n",
       "       -0.02433126,  0.04081773, -0.01725332, -0.0302623 ,  0.00458778,\n",
       "       -0.00797267,  0.0396576 , -0.03479489, -0.03826151,  0.01921891,\n",
       "       -0.03527061, -0.01330353, -0.00986265, -0.04157568,  0.03914535,\n",
       "        0.0329074 , -0.00862693, -0.01705732,  0.03421475, -0.01511655,\n",
       "        0.0367487 ,  0.01149056,  0.00785396, -0.00081193,  0.01323238,\n",
       "        0.03936524, -0.04055817, -0.03350972,  0.03084621, -0.03323904,\n",
       "       -0.01903235, -0.00984392, -0.01885979,  0.0348522 ,  0.03835208,\n",
       "        0.01779189, -0.04397999, -0.04074303, -0.01322315], dtype=float32)))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7002c9-69e2-4940-b84f-924984c03a54",
   "metadata": {},
   "source": [
    "**Finished**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
