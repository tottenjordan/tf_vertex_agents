{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385307d6-2058-47ec-8da0-57f7ef5c43d6",
   "metadata": {},
   "source": [
    "# Contextual Multi Armed Bandits with ✨Generative✨ Reward Functions\n",
    "\n",
    "Inspiration from this idea - somewhat of a LLM distillation method:\n",
    "https://arxiv.org/abs/2303.00001\n",
    "\n",
    "### Reward Design with Language Models\n",
    "##### Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh\n",
    "`Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by prompting a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of the desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user's objectives and outperform RL agents trained with reward functions learned via supervised learning`\n",
    "\n",
    "In this we will use PALM Text Bison. We will do the following\n",
    "\n",
    "1. Generate \"trajectory\" training examples where we have a batch of n users, shown a randomly selected batch of n_actions movies\n",
    "2. Take the raw textual data of those features and format a prompt asking text bison to rate the movie (float) on a 0-5 rating scale\n",
    "3. These rewards are then used as we generte the reward and update the trajectory\n",
    "4. Either generate the rest of these examples online (suggested for scale), or generate as-you-train\n",
    "5. Create a deep network for an epsilon-greed multi-armed bandit agent to explore/exlpoit the rewards generated by text-bison\n",
    "\n",
    "\n",
    "Future ideas\n",
    "\n",
    "- When tempurature is set to high and/or token count is extended, it can be interesting to see it rationalize the ratings. \n",
    "\n",
    "- Mapreduce multiple prompts with...\n",
    "\n",
    "- Mixture-of-experts approach\n",
    "\n",
    "# Train Bandits with per-arm features\n",
    "\n",
    "**Exploring linear and nonlinear** (e.g., those with neural network-based value functions) bandit methods for recommendations using TF-Agents\n",
    "\n",
    "> Neural linear bandits provide a nice way to leverage the representation power of deep learning and the bandit approach for uncertainty measure and efficient exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fd257-b98b-426a-a2cd-024429b014f1",
   "metadata": {},
   "source": [
    "## Load notebook config\n",
    "\n",
    "* use the prefix defined in `00-env-setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f39c9d08-d118-4013-a47f-88450f49f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'mabv1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca59d6da-77fa-49d5-b5ee-7c7384056fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # staging GCS\n",
    "# import subprocess\n",
    "\n",
    "# GCP_PROJECTS             = !gcloud config get-value project\n",
    "# PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# # GCS bucket and paths\n",
    "# BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "# BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "# # config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "# # print(config.n)\n",
    "# # exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "317d08e2-da4a-440e-99e0-b2618f5ee86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"wortz-project-352116\"\n",
      "PROJECT_NUM              = \"679926387543\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"679926387543-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"mabv1\"\n",
      "VERSION                  = \"v1\"\n",
      "\n",
      "BUCKET_NAME              = \"mabv1-wortz-project-352116-bucket\"\n",
      "BUCKET_URI               = \"gs://mabv1-wortz-project-352116-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://mabv1-wortz-project-352116-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/679926387543/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_ID      = \"wortz-project-352116.movielens_dataset_mabv1\"\n",
      "BIGQUERY_TABLE_ID        = \"wortz-project-352116.movielens_dataset_mabv1.training_dataset\"\n",
      "\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "RL_SUB_DIR               = \"per_arm_rl\"\n",
      "REPOSITORY               = \"rl-movielens-mabv1\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['gsutil', 'cat', 'gs://mabv1-wortz-project-352116-bucket/config/notebook_env.py'], returncode=0)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "GCP_PROJECTS             = \"wortz-project-352116\"\n",
    "PROJECT_ID               = \"wortz-project-352116\"\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "LOCATION                 = \"us-central1\"\n",
    "\n",
    "# config = subprocess.run([f\"gsutil\", \"cat\", f\"{BUCKET_URI}/config/notebook_env.py\"])\n",
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8647360c-1577-4b7d-beca-26b34512a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROJECT_ID               = \"wortz-project-352116\"\n",
    "PROJECT_NUM              = \"679926387543\"\n",
    "LOCATION                 = \"us-central1\"\n",
    "\n",
    "REGION                   = \"us-central1\"\n",
    "BQ_LOCATION              = \"US\"\n",
    "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
    "\n",
    "VERTEX_SA                = \"679926387543-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "PREFIX                   = \"mabv1\"\n",
    "VERSION                  = \"v1\"\n",
    "\n",
    "BUCKET_NAME              = \"mabv1-wortz-project-352116-bucket\"\n",
    "BUCKET_URI               = \"gs://mabv1-wortz-project-352116-bucket\"\n",
    "DATA_GCS_PREFIX          = \"data\"\n",
    "DATA_PATH                = \"gs://mabv1-wortz-project-352116-bucket/data\"\n",
    "VOCAB_SUBDIR             = \"vocabs\"\n",
    "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
    "\n",
    "VPC_NETWORK_FULL         = \"projects/679926387543/global/networks/ucaip-haystack-vpc-network\"\n",
    "\n",
    "BIGQUERY_DATASET_ID      = \"wortz-project-352116.movielens_dataset_mabv1\"\n",
    "BIGQUERY_TABLE_ID        = \"wortz-project-352116.movielens_dataset_mabv1.training_dataset\"\n",
    "\n",
    "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
    "RL_SUB_DIR               = \"per_arm_rl\"\n",
    "REPOSITORY               = \"rl-movielens-mabv1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "908f6b95-b539-4a9f-a836-840d26ea3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # staging GCS\n",
    "# GCP_PROJECTS             = !gcloud config get-value project\n",
    "# PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# # GCS bucket and paths\n",
    "# BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "# BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "# config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "# print(config.n)\n",
    "# exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c690a9-e2bd-4759-ba41-4e2469098aee",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4d0dfe4-695c-4dd4-9f24-67f7488ce1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c669f1a1-1af7-4efb-ab2d-6bf3b3847991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pprint import pprint\n",
    "import pickle as pkl\n",
    "# import torch\n",
    "# from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "#we set mem growth so we only use what gpu is needed - to make room for torch/LLaMA\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "gpus\n",
    "\n",
    "# from tf_agents.agents import TFAgent\n",
    "\n",
    "# from tf_agents.bandits.environments import stationary_stochastic_per_arm_py_environment as p_a_env\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "# from tf_agents.drivers import dynamic_step_driver\n",
    "# from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "# from tf_agents.bandits.agents import lin_ucb_agent\n",
    "# from tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\n",
    "from tf_agents.bandits.agents import neural_epsilon_greedy_agent\n",
    "from tf_agents.bandits.agents import neural_linucb_agent\n",
    "from tf_agents.bandits.networks import global_and_arm_feature_network\n",
    "from tf_agents.bandits.policies import policy_utilities\n",
    "\n",
    "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "# GPU\n",
    "from numba import cuda \n",
    "import gc\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# this repo\n",
    "# from src.per_arm_rl import data_utils\n",
    "\n",
    "# tf exceptions and vars\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dcd57a-4d4c-4375-90ae-4cce63ae36f5",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947313c8-76c7-478e-b972-8e9a31c0e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.per_arm_rl import data_utils\n",
    "\n",
    "def get_all_features():\n",
    "    \n",
    "    feats = {\n",
    "        # user - global context features\n",
    "        'user_id': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n",
    "        'user_rating': tf.io.FixedLenFeature(shape=(), dtype=tf.float32),\n",
    "        'bucketized_user_age': tf.io.FixedLenFeature(shape=(), dtype=tf.float32),\n",
    "        'user_occupation_text': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n",
    "        'user_occupation_label': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n",
    "        'user_zip_code': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n",
    "        'user_gender': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n",
    "        'timestamp': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n",
    "\n",
    "        # movie - per arm features\n",
    "        'movie_id': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n",
    "        'movie_title': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n",
    "        'movie_genres': tf.io.FixedLenFeature(shape=(1,), dtype=tf.int64),\n",
    "    }\n",
    "    \n",
    "    return feats\n",
    "\n",
    "def parse_tfrecord(example):\n",
    "    \"\"\"\n",
    "    Reads a serialized example from GCS and converts to tfrecord\n",
    "    \"\"\"\n",
    "    feats = get_all_features()\n",
    "    \n",
    "    example = tf.io.parse_example(\n",
    "        example,\n",
    "        feats\n",
    "        # features=feats\n",
    "    )\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29e58dd7-ab2b-419f-9771-bf1e98db758b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4969d3e-1fc0-45db-8a69-aa6b342019de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = cuda.get_current_device()\n",
    "# device.reset()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "274e7f4a-1802-4946-888e-876638f5c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a866b1-85b9-43e6-9546-edfbbf886bce",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5b953-14c0-42ed-a511-77147a1bc0ac",
   "metadata": {},
   "source": [
    "### Read TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d0157c8-a04c-4dbd-b6d9-a1ede97687a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c934c06-bf08-4c7f-b0cc-0de04ef3515c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://mabv1-wortz-project-352116-bucket/data/train/ml-ratings-100k-train.tfrecord']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT = \"train\" # \"train\" | \"val\"\n",
    "\n",
    "train_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{DATA_GCS_PREFIX}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7511e4d-bf81-4800-bde7-8b16dec9aeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([35.], dtype=float32)>,\n",
      " 'movie_genres': <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[7]])>,\n",
      " 'movie_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'898'], dtype=object)>,\n",
      " 'movie_title': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Postman, The (1997)'], dtype=object)>,\n",
      " 'timestamp': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([885409515])>,\n",
      " 'user_gender': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'False'], dtype=object)>,\n",
      " 'user_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'681'], dtype=object)>,\n",
      " 'user_occupation_label': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([14])>,\n",
      " 'user_occupation_text': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'marketing'], dtype=object)>,\n",
      " 'user_rating': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([4.], dtype=float32)>,\n",
      " 'user_zip_code': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'97208'], dtype=object)>}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "train_dataset = train_dataset.map(parse_tfrecord)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8959967-522e-41c8-9a1b-050ca8bc191f",
   "metadata": {},
   "source": [
    "### get vocab\n",
    "\n",
    "**TODO:** \n",
    "* streamline vocab calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b9169bc-d6dc-497e-9dff-6ebb175282ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATE_VOCABS: False\n"
     ]
    }
   ],
   "source": [
    "GENERATE_VOCABS = False\n",
    "print(f\"GENERATE_VOCABS: {GENERATE_VOCABS}\")\n",
    "\n",
    "VOCAB_SUBDIR   = \"vocabs\"\n",
    "VOCAB_FILENAME = \"vocab_dict.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434bab4-81c2-4147-aad8-b47dd0d31f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The ID of your GCS object\n",
    "    # source_blob_name = \"storage-object-name\"\n",
    "\n",
    "    # The path to which the file should be downloaded\n",
    "    # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Construct a client side representation of a blob.\n",
    "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "    # any content from Google Cloud Storage. As we don't need additional data,\n",
    "    # using `Bucket.blob` is preferred here.\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(\n",
    "        \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n",
    "            source_blob_name, bucket_name, destination_file_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3ccf137-7a72-42e7-aa89-3c81a99cf40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vocab...\n",
      "Downloaded vocab from: gs://mabv1-wortz-project-352116-bucket/vocabs/vocab_dict.pkl\n",
      "\n",
      "'movie_id'\n",
      "'user_id'\n",
      "'user_occupation_text'\n",
      "'movie_genres'\n",
      "'bucketized_user_age'\n",
      "'max_timestamp'\n",
      "'min_timestamp'\n",
      "'timestamp_buckets'\n"
     ]
    }
   ],
   "source": [
    "if not GENERATE_VOCABS:\n",
    "\n",
    "    EXISTING_VOCAB_FILE = f'gs://{BUCKET_NAME}/{VOCAB_SUBDIR}/{VOCAB_FILENAME}'\n",
    "    print(f\"Downloading vocab...\")\n",
    "    \n",
    "    # os.system(f'gsutil -q cp {EXISTING_VOCAB_FILE} .')\n",
    "    download_blob(BUCKET_NAME, f\"{VOCAB_SUBDIR}/{VOCAB_FILENAME}\", VOCAB_FILENAME)\n",
    "    print(f\"Downloaded vocab from: {EXISTING_VOCAB_FILE}\\n\")\n",
    "\n",
    "    filehandler = open(VOCAB_FILENAME, 'rb')\n",
    "    vocab_dict = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "    for key in vocab_dict.keys():\n",
    "        pprint(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccaef62-882a-46ff-a1b1-3837e69fdf74",
   "metadata": {},
   "source": [
    "## helper functions\n",
    "\n",
    "**TODO:**\n",
    "* modularize in a train_utils or similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cba2bb14-bf94-466b-b60f-8c7d96c7aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_outer_dimension(x):\n",
    "    \"\"\"Adds an extra outer dimension.\"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        for key, value in x.items():\n",
    "            x[key] = tf.expand_dims(value, 1)\n",
    "        return x\n",
    "    return tf.expand_dims(x, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b941063b-ad48-4817-aef0-9afa8a444632",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits with Per-Arm Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28770b8d-836b-448d-8dd1-203d76fc6d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "nest = tf.nest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0138a295-2b0d-4359-8496-af8552ec8cff",
   "metadata": {},
   "source": [
    "## Preprocessing layers for global and arm features\n",
    "\n",
    "The preproccesing layers will ultimately feed the two functions described below, both of which will ultimately feed the `Environment`\n",
    "\n",
    "`global_context_sampling_fn`: \n",
    "* A function that outputs a random 1d array or list of ints or floats\n",
    "* This output is the global context. Its shape and type must be consistent across calls.\n",
    "\n",
    "`arm_context_sampling_fn`: \n",
    "* A function that outputs a random 1 array or list of ints or floats (same type as the output of `global_context_sampling_fn`). * This output is the per-arm context. Its shape must be consistent across calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8d77956-635c-438a-916a-185eec52f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OOV_BUCKETS        = 1\n",
    "GLOBAL_EMBEDDING_SIZE  = 16\n",
    "MV_EMBEDDING_SIZE      = 32 #32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142e63e-0a20-4d51-997c-7a4733517f7e",
   "metadata": {},
   "source": [
    "### global context (user) features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195acd92-06b6-42e4-bef7-798fd09da856",
   "metadata": {},
   "source": [
    "#### user ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c28e887b-421a-4603-8899-87071056783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_input_layer = tf.keras.Input(\n",
    "    name=\"user_id\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.string\n",
    ")\n",
    "\n",
    "user_id_lookup = tf.keras.layers.StringLookup(\n",
    "    max_tokens=len(vocab_dict['user_id']) + NUM_OOV_BUCKETS,\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    mask_token=None,\n",
    "    vocabulary=vocab_dict['user_id'],\n",
    ")(user_id_input_layer)\n",
    "\n",
    "user_id_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['user_id']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_id_lookup)\n",
    "\n",
    "user_id_embedding = tf.reduce_sum(user_id_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_id_input_layer)\n",
    "# global_features.append(user_id_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d6a0fe7-26cb-4c62-a3ef-17f98e6ccddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'681'], shape=(1,), dtype=string)\n",
      "tf.Tensor(\n",
      "[[-0.0430223   0.04316026 -0.03505059  0.04390352  0.02443171 -0.01438611\n",
      "   0.01287739  0.03737282 -0.0031909   0.04948736  0.04524032 -0.00800508\n",
      "   0.04057597  0.02142993  0.03996793  0.02976451]], shape=(1, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_user_id_model = tf.keras.Model(inputs=user_id_input_layer, outputs=user_id_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"user_id\"])\n",
    "    print(test_user_id_model(x[\"user_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352d2227-92ec-4386-926f-df2fdb9434ec",
   "metadata": {},
   "source": [
    "#### user AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70785bf0-5ece-4875-ab72-06d9c45ea9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_age_input_layer = tf.keras.Input(\n",
    "    name=\"bucketized_user_age\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "user_age_lookup = tf.keras.layers.IntegerLookup(\n",
    "    vocabulary=vocab_dict['bucketized_user_age'],\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    oov_value=0,\n",
    ")(user_age_input_layer)\n",
    "\n",
    "user_age_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['bucketized_user_age']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_age_lookup)\n",
    "\n",
    "user_age_embedding = tf.reduce_sum(user_age_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_age_input_layer)\n",
    "# global_features.append(user_age_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e01622a-9418-4ca7-8925-9b0ebef8940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([35.], shape=(1,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.03988954 -0.01103028  0.04024919  0.01274309 -0.03701185  0.00081088\n",
      "  -0.04950501 -0.04429914 -0.04845876  0.03500685 -0.01687964 -0.03256346\n",
      "  -0.02901732  0.0246805  -0.0419592   0.00757987]], shape=(1, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_user_age_model = tf.keras.Model(inputs=user_age_input_layer, outputs=user_age_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"bucketized_user_age\"])\n",
    "    print(test_user_age_model(x[\"bucketized_user_age\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997ffaa8-ca92-4851-b7e3-bb06fba8958b",
   "metadata": {},
   "source": [
    "#### user OCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03e7344d-71fb-423a-89dd-f1abeb270e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_occ_input_layer = tf.keras.Input(\n",
    "    name=\"user_occupation_text\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.string\n",
    ")\n",
    "\n",
    "user_occ_lookup = tf.keras.layers.StringLookup(\n",
    "    max_tokens=len(vocab_dict['user_occupation_text']) + NUM_OOV_BUCKETS,\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    mask_token=None,\n",
    "    vocabulary=vocab_dict['user_occupation_text'],\n",
    ")(user_occ_input_layer)\n",
    "\n",
    "user_occ_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['user_occupation_text']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_occ_lookup)\n",
    "\n",
    "user_occ_embedding = tf.reduce_sum(user_occ_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_occ_input_layer)\n",
    "# global_features.append(user_occ_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39cbbc31-ca43-4f8f-a804-a4b830e99d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'marketing'], shape=(1,), dtype=string)\n",
      "tf.Tensor(\n",
      "[[-0.01944429  0.02837546  0.04978884  0.00983376  0.0448536  -0.01951627\n",
      "   0.00986196  0.03108202 -0.04180235 -0.02979856 -0.02010383 -0.02731179\n",
      "  -0.04827795 -0.04703623 -0.04032074 -0.01869842]], shape=(1, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_user_occ_model = tf.keras.Model(inputs=user_occ_input_layer, outputs=user_occ_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"user_occupation_text\"])\n",
    "    print(test_user_occ_model(x[\"user_occupation_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee0098-a48a-4de6-88bf-6219ce8c0533",
   "metadata": {},
   "source": [
    "#### user Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61a4e01a-e742-4c68-93a9-aa66eb9a5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ts_input_layer = tf.keras.Input(\n",
    "    name=\"timestamp\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.int64\n",
    ")\n",
    "\n",
    "user_ts_lookup = tf.keras.layers.Discretization(\n",
    "    vocab_dict['timestamp_buckets'].tolist()\n",
    ")(user_ts_input_layer)\n",
    "\n",
    "user_ts_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['timestamp_buckets'].tolist()) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_ts_lookup)\n",
    "\n",
    "user_ts_embedding = tf.reduce_sum(user_ts_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_ts_input_layer)\n",
    "# global_features.append(user_ts_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db99f90b-57f8-45e6-9f28-871658e17358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([885409515], shape=(1,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[-0.02315004  0.03875582 -0.01551851 -0.01455063 -0.01816684 -0.01592218\n",
      "  -0.01438422 -0.01885771  0.02474555 -0.00832601 -0.01128403 -0.02329888\n",
      "   0.00698436 -0.04715705 -0.00511736 -0.03002429]], shape=(1, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_user_ts_model = tf.keras.Model(inputs=user_ts_input_layer, outputs=user_ts_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"timestamp\"])\n",
    "    print(test_user_ts_model(x[\"timestamp\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc734ea-cb5e-4c6b-8b94-2a8853220178",
   "metadata": {},
   "source": [
    "#### define global sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff58c380-8b53-4dfa-b5b4-d36853638ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_global_context_features(x):\n",
    "    \"\"\"\n",
    "    This function generates a single global observation vector.\n",
    "    \"\"\"\n",
    "    user_id_value = x['user_id']\n",
    "    user_age_value = x['bucketized_user_age']\n",
    "    user_occ_value = x['user_occupation_text']\n",
    "    user_ts_value = x['timestamp']\n",
    "\n",
    "    _id = test_user_id_model(user_id_value) # input_tensor=tf.Tensor(shape=(4,), dtype=float32)\n",
    "    _age = test_user_age_model(user_age_value)\n",
    "    _occ = test_user_occ_model(user_occ_value)\n",
    "    _ts = test_user_ts_model(user_ts_value)\n",
    "\n",
    "    # # tmp - insepct numpy() values\n",
    "    # print(_id.numpy()) #[0])\n",
    "    # print(_age.numpy()) #[0])\n",
    "    # print(_occ.numpy()) #[0])\n",
    "    # print(_ts.numpy()) #[0])\n",
    "\n",
    "    # to numpy array\n",
    "    _id = np.array(_id.numpy())\n",
    "    _age = np.array(_age.numpy())\n",
    "    _occ = np.array(_occ.numpy())\n",
    "    _ts = np.array(_ts.numpy())\n",
    "\n",
    "    concat = np.concatenate(\n",
    "        [_id, _age, _occ, _ts], axis=-1 # -1\n",
    "    ).astype(np.float32)\n",
    "    \n",
    "    user_info = [\n",
    "                user_age_value.numpy(),\n",
    "                user_occ_value.numpy(),\n",
    "                user_ts_value.numpy(),\n",
    "                x['user_zip_code'].numpy(),\n",
    "                x['user_gender'].numpy(),\n",
    "                x['movie_title'].numpy(),\n",
    "                x['user_rating'].numpy()\n",
    "                ]\n",
    "\n",
    "    return concat, user_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d583b8f8-df19-4bfc-a963-81874d41523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    \n",
    "    iterator = iter(train_dataset.batch(5))\n",
    "    data = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d693e5b-ab19-438c-b8eb-3be677e4c621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bucketized_user_age': <tf.Tensor: shape=(5,), dtype=float32, numpy=array([35., 18., 56., 45., 35.], dtype=float32)>,\n",
       " 'movie_genres': <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
       " array([[7],\n",
       "        [4],\n",
       "        [9],\n",
       "        [4],\n",
       "        [7]])>,\n",
       " 'movie_id': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'898', b'367', b'484', b'494', b'58'], dtype=object)>,\n",
       " 'movie_title': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
       " array([b'Postman, The (1997)', b'Clueless (1995)',\n",
       "        b'Maltese Falcon, The (1941)', b'His Girl Friday (1940)',\n",
       "        b'Quiz Show (1994)'], dtype=object)>,\n",
       " 'timestamp': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([885409515, 883388887, 891249586, 878044851, 880130613])>,\n",
       " 'user_gender': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'False', b'True', b'True', b'True', b'False'], dtype=object)>,\n",
       " 'user_id': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'681', b'442', b'932', b'506', b'18'], dtype=object)>,\n",
       " 'user_occupation_label': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([14, 17,  0, 12, 11])>,\n",
       " 'user_occupation_text': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
       " array([b'marketing', b'student', b'educator', b'programmer', b'other'],\n",
       "       dtype=object)>,\n",
       " 'user_rating': <tf.Tensor: shape=(5,), dtype=float32, numpy=array([4., 2., 5., 5., 4.], dtype=float32)>,\n",
       " 'user_zip_code': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'97208', b'85282', b'06437', b'03869', b'37212'], dtype=object)>}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "557c2d7a-bac0-405b-904c-f05731abb8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-4.30223010e-02,  4.31602634e-02, -3.50505933e-02,\n",
       "          4.39035185e-02,  2.44317092e-02, -1.43861063e-02,\n",
       "          1.28773935e-02,  3.73728164e-02, -3.19089741e-03,\n",
       "          4.94873635e-02,  4.52403165e-02, -8.00508261e-03,\n",
       "          4.05759700e-02,  2.14299299e-02,  3.99679281e-02,\n",
       "          2.97645070e-02, -3.98895368e-02, -1.10302791e-02,\n",
       "          4.02491949e-02,  1.27430893e-02, -3.70118506e-02,\n",
       "          8.10883939e-04, -4.95050065e-02, -4.42991368e-02,\n",
       "         -4.84587550e-02,  3.50068472e-02, -1.68796405e-02,\n",
       "         -3.25634629e-02, -2.90173180e-02,  2.46804990e-02,\n",
       "         -4.19592038e-02,  7.57987425e-03, -1.94442868e-02,\n",
       "          2.83754580e-02,  4.97888438e-02,  9.83376428e-03,\n",
       "          4.48536016e-02, -1.95162650e-02,  9.86195728e-03,\n",
       "          3.10820229e-02, -4.18023467e-02, -2.97985561e-02,\n",
       "         -2.01038253e-02, -2.73117907e-02, -4.82779518e-02,\n",
       "         -4.70362306e-02, -4.03207429e-02, -1.86984167e-02,\n",
       "         -2.31500398e-02,  3.87558229e-02, -1.55185089e-02,\n",
       "         -1.45506263e-02, -1.81668401e-02, -1.59221776e-02,\n",
       "         -1.43842213e-02, -1.88577063e-02,  2.47455500e-02,\n",
       "         -8.32600519e-03, -1.12840310e-02, -2.32988838e-02,\n",
       "          6.98436424e-03, -4.71570492e-02, -5.11735678e-03,\n",
       "         -3.00242901e-02],\n",
       "        [ 4.69660498e-02,  2.65448354e-02,  3.07315476e-02,\n",
       "          3.04737799e-02,  3.63421105e-02,  4.17004935e-02,\n",
       "         -9.66113806e-03,  4.55860011e-02, -4.03001197e-02,\n",
       "         -2.59792563e-02, -3.86926308e-02, -4.39062826e-02,\n",
       "         -4.49776910e-02, -2.02752594e-02,  6.80737570e-03,\n",
       "          1.64921544e-02,  1.71021558e-02, -2.42292881e-03,\n",
       "          4.44087870e-02,  1.57637037e-02,  1.83954574e-02,\n",
       "          5.85645437e-03, -2.74389144e-02,  3.52374054e-02,\n",
       "         -1.54169090e-02,  2.20800973e-02,  4.47182916e-02,\n",
       "         -4.74400520e-02,  2.63927840e-02,  1.83223747e-02,\n",
       "         -1.91325676e-02, -4.24458385e-02,  4.24465574e-02,\n",
       "          8.14551115e-03,  2.49345936e-02, -1.72560811e-02,\n",
       "         -1.32654980e-03,  1.28944628e-02,  2.31189616e-02,\n",
       "          4.40431572e-02, -3.22369710e-02, -4.93860245e-02,\n",
       "         -1.09520555e-02, -4.35434952e-02, -7.37880543e-03,\n",
       "          7.55071640e-04, -4.09746394e-02,  4.13533114e-02,\n",
       "          1.32808201e-02,  3.19990776e-02, -4.64788675e-02,\n",
       "         -9.42307711e-03, -4.34667245e-02,  2.85610445e-02,\n",
       "         -4.99829426e-02,  3.86170261e-02,  7.51940161e-03,\n",
       "          2.81953812e-03,  1.80814900e-02, -4.31088358e-03,\n",
       "          1.01458319e-02, -1.32047646e-02,  5.91119379e-03,\n",
       "         -1.49211287e-02],\n",
       "        [-2.70334370e-02,  4.37513702e-02, -1.46971457e-02,\n",
       "          3.67597006e-02, -1.65954828e-02,  3.42276730e-02,\n",
       "         -4.08488289e-02, -4.23637144e-02, -2.55930424e-02,\n",
       "         -4.41064946e-02,  2.43629254e-02,  3.59648205e-02,\n",
       "         -1.92385204e-02,  4.19508927e-02,  4.58181761e-02,\n",
       "         -3.39624658e-02,  1.76376216e-02,  4.56940383e-04,\n",
       "         -1.91807393e-02,  2.93706395e-02,  6.86968490e-03,\n",
       "          4.13806699e-02,  1.56127848e-02,  4.67057712e-02,\n",
       "          3.28212120e-02, -4.20764573e-02,  2.42307298e-02,\n",
       "          2.09368132e-02,  4.76628430e-02, -2.29876172e-02,\n",
       "         -4.76946235e-02,  2.79281624e-02, -3.50389592e-02,\n",
       "         -5.76114655e-03, -3.78744677e-03, -2.81548500e-03,\n",
       "          3.51333953e-02, -4.30136696e-02, -2.43678335e-02,\n",
       "          1.76427253e-02,  3.33579667e-02,  3.14849727e-02,\n",
       "         -1.39432773e-02, -2.57920474e-04, -4.61602472e-02,\n",
       "         -2.54771002e-02,  4.50503938e-02, -3.96545418e-02,\n",
       "          2.13202722e-02, -3.07416804e-02, -3.29795368e-02,\n",
       "          8.51154327e-03, -3.36742289e-02, -9.53949615e-03,\n",
       "          3.01229991e-02,  4.64024432e-02, -1.14442334e-02,\n",
       "          5.23375347e-03,  3.97555120e-02, -3.13096642e-02,\n",
       "         -4.49394099e-02, -2.87721511e-02, -6.95101917e-05,\n",
       "          4.89707701e-02],\n",
       "        [-1.59455426e-02, -1.87172294e-02, -5.12377173e-03,\n",
       "          4.17371504e-02,  4.39611115e-02, -3.87871154e-02,\n",
       "         -2.46387124e-02,  2.19018199e-02, -4.83862422e-02,\n",
       "          3.68660800e-02, -2.59269476e-02,  2.12687254e-03,\n",
       "         -9.37020779e-03,  3.02309431e-02, -2.71360166e-02,\n",
       "         -3.44440825e-02,  4.03604656e-03, -3.03568728e-02,\n",
       "         -1.14092939e-02, -1.66295767e-02, -1.74120814e-03,\n",
       "         -1.52599588e-02,  4.11326624e-02, -1.90024618e-02,\n",
       "         -1.45124681e-02,  1.95238739e-03,  1.65074356e-02,\n",
       "          4.31598909e-02,  3.21508981e-02, -2.00832132e-02,\n",
       "         -4.44557182e-02, -9.76584107e-03,  2.17100419e-02,\n",
       "          1.21938959e-02,  4.40506674e-02,  4.88808788e-02,\n",
       "         -4.64540608e-02,  2.95942910e-02, -2.82038450e-02,\n",
       "         -5.29075786e-03,  3.29414047e-02, -3.15438285e-02,\n",
       "         -3.96598503e-03,  1.06416568e-02,  1.04829445e-02,\n",
       "         -2.41396185e-02,  1.90463327e-02, -1.52578130e-02,\n",
       "          4.66110744e-02,  1.59166940e-02,  3.60908397e-02,\n",
       "         -1.14442110e-02,  4.50668484e-03, -2.12202799e-02,\n",
       "          1.95778497e-02, -4.08638343e-02, -4.77494486e-02,\n",
       "         -2.62322277e-03,  3.71096283e-03,  4.35302407e-03,\n",
       "         -3.15465704e-02,  4.62664105e-02,  4.61545922e-02,\n",
       "         -2.99774110e-04],\n",
       "        [-3.71839851e-03, -2.61165500e-02,  4.06365134e-02,\n",
       "          5.88506460e-03,  1.02738291e-03, -2.02932358e-02,\n",
       "         -4.99955565e-03,  1.52086876e-02, -6.64261729e-03,\n",
       "          3.27788293e-04,  9.95352119e-03, -3.19846272e-02,\n",
       "         -5.88293001e-03, -2.68876553e-03,  4.99733202e-02,\n",
       "          6.40131533e-04, -3.98895368e-02, -1.10302791e-02,\n",
       "          4.02491949e-02,  1.27430893e-02, -3.70118506e-02,\n",
       "          8.10883939e-04, -4.95050065e-02, -4.42991368e-02,\n",
       "         -4.84587550e-02,  3.50068472e-02, -1.68796405e-02,\n",
       "         -3.25634629e-02, -2.90173180e-02,  2.46804990e-02,\n",
       "         -4.19592038e-02,  7.57987425e-03, -4.48017232e-02,\n",
       "         -1.97837837e-02,  3.02808024e-02, -3.75205167e-02,\n",
       "          6.48463890e-03,  3.89027931e-02,  3.89373936e-02,\n",
       "         -2.26274617e-02, -3.82860526e-02,  2.90216133e-03,\n",
       "          2.10588314e-02, -1.36240721e-02, -1.93883907e-02,\n",
       "          4.79879640e-02,  6.09869882e-03,  3.63555290e-02,\n",
       "          8.16275924e-03, -1.81722417e-02,  1.95270814e-02,\n",
       "         -2.16807853e-02, -1.01798996e-02,  2.74381749e-02,\n",
       "         -4.35418151e-02,  9.66263935e-03,  2.98919566e-02,\n",
       "          1.72713064e-02,  1.89321898e-02,  4.05734219e-02,\n",
       "          3.64482403e-03, -4.74258922e-02, -1.81389228e-02,\n",
       "         -1.99805573e-03]], dtype=float32),\n",
       " [array([35., 18., 56., 45., 35.], dtype=float32),\n",
       "  array([b'marketing', b'student', b'educator', b'programmer', b'other'],\n",
       "        dtype=object),\n",
       "  array([885409515, 883388887, 891249586, 878044851, 880130613]),\n",
       "  array([b'97208', b'85282', b'06437', b'03869', b'37212'], dtype=object),\n",
       "  array([b'False', b'True', b'True', b'True', b'False'], dtype=object),\n",
       "  array([b'Postman, The (1997)', b'Clueless (1995)',\n",
       "         b'Maltese Falcon, The (1941)', b'His Girl Friday (1940)',\n",
       "         b'Quiz Show (1994)'], dtype=object),\n",
       "  array([4., 2., 5., 5., 4.], dtype=float32)])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_global_context_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c9f64a0-a713-49bc-9043-d6e9598ecc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #check how this works with batches - new JW\n",
    "\n",
    "# batch_elem = train_dataset.batch(4)\n",
    "# _get_global_context_features(batch_elem)\n",
    "_get_global_context_features(data)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bba133ab-bf12-4b3b-926d-6d1dba940837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0430223 ,  0.04316026, -0.03505059,  0.04390352,  0.02443171,\n",
       "        -0.01438611,  0.01287739,  0.03737282, -0.0031909 ,  0.04948736,\n",
       "         0.04524032, -0.00800508,  0.04057597,  0.02142993,  0.03996793,\n",
       "         0.02976451, -0.03988954, -0.01103028,  0.04024919,  0.01274309,\n",
       "        -0.03701185,  0.00081088, -0.04950501, -0.04429914, -0.04845876,\n",
       "         0.03500685, -0.01687964, -0.03256346, -0.02901732,  0.0246805 ,\n",
       "        -0.0419592 ,  0.00757987, -0.01944429,  0.02837546,  0.04978884,\n",
       "         0.00983376,  0.0448536 , -0.01951627,  0.00986196,  0.03108202,\n",
       "        -0.04180235, -0.02979856, -0.02010383, -0.02731179, -0.04827795,\n",
       "        -0.04703623, -0.04032074, -0.01869842, -0.02315004,  0.03875582,\n",
       "        -0.01551851, -0.01455063, -0.01816684, -0.01592218, -0.01438422,\n",
       "        -0.01885771,  0.02474555, -0.00832601, -0.01128403, -0.02329888,\n",
       "         0.00698436, -0.04715705, -0.00511736, -0.03002429]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in train_dataset.batch(1).take(1):\n",
    "    test_globals = _get_global_context_features(x)[0]\n",
    "\n",
    "\n",
    "test_globals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249fa771-35d7-4d04-ab68-2b70911bac17",
   "metadata": {},
   "source": [
    "### arm preprocessing layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534b3bf1-a2ea-4bfb-8c77-efa057f4e391",
   "metadata": {},
   "source": [
    "#### movie ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa53cbe9-2616-4da4-90dc-dc5616258af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_id_input_layer = tf.keras.Input(\n",
    "    name=\"movie_id\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.string\n",
    ")\n",
    "\n",
    "mv_id_lookup = tf.keras.layers.StringLookup(\n",
    "    max_tokens=len(vocab_dict['movie_id']) + NUM_OOV_BUCKETS,\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    mask_token=None,\n",
    "    vocabulary=vocab_dict['movie_id'],\n",
    ")(mv_id_input_layer)\n",
    "\n",
    "mv_id_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['movie_id']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=MV_EMBEDDING_SIZE\n",
    ")(mv_id_lookup)\n",
    "\n",
    "mv_id_embedding = tf.reduce_sum(mv_id_embedding, axis=-2)\n",
    "\n",
    "# arm_inputs.append(mv_id_input_layer)\n",
    "# arm_features.append(mv_id_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bd19f09-a12e-4a21-a1a1-5ec5bc116559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'898'], shape=(1,), dtype=string)\n",
      "tf.Tensor(\n",
      "[[ 0.04911545 -0.01634108 -0.01234639 -0.0070103  -0.00573139 -0.00559377\n",
      "   0.01259658 -0.00546701  0.0216699   0.01893058 -0.0372486   0.00512571\n",
      "   0.02640522 -0.04102753  0.04012236 -0.03196473 -0.02491291  0.03234743\n",
      "   0.01537937  0.01648374  0.01373536  0.00863097  0.03960489 -0.04251634\n",
      "  -0.00295169  0.02778888 -0.01608081  0.03596785  0.0189887   0.04032845\n",
      "   0.00537063  0.00472279]], shape=(1, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_mv_id_model = tf.keras.Model(inputs=mv_id_input_layer, outputs=mv_id_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"movie_id\"])\n",
    "    print(test_mv_id_model(x[\"movie_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a0e97-c477-4042-b9c0-fcb0f428de0d",
   "metadata": {},
   "source": [
    "#### movie genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f04a0091-d7b0-4f90-ba7c-3eb41dd0b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_genre_input_layer = tf.keras.Input(\n",
    "    name=\"movie_genres\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "mv_genre_lookup = tf.keras.layers.IntegerLookup(\n",
    "    vocabulary=vocab_dict['movie_genres'],\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    oov_value=0,\n",
    ")(mv_genre_input_layer)\n",
    "\n",
    "mv_genre_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['movie_genres']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=MV_EMBEDDING_SIZE\n",
    ")(mv_genre_lookup)\n",
    "\n",
    "mv_genre_embedding = tf.reduce_sum(mv_genre_embedding, axis=-2)\n",
    "\n",
    "# arm_inputs.append(mv_genre_input_layer)\n",
    "# arm_features.append(mv_genre_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51701f0a-9b3e-461c-a9d9-a0c146e310ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[7]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([b'898'], shape=(1,), dtype=string)\n",
      "tf.Tensor(\n",
      "[[-0.04909344 -0.03367907  0.018834   -0.00841088  0.01948916 -0.04885552\n",
      "  -0.00617999 -0.02069405 -0.04477356  0.03419046 -0.04278373  0.00344169\n",
      "   0.04837879 -0.03245031 -0.02844121  0.02616468 -0.04857191 -0.00706835\n",
      "   0.03614764 -0.03875374 -0.02769949 -0.03707442  0.00756137  0.04047615\n",
      "  -0.00872046  0.00992896 -0.01910238 -0.01092043  0.02773506 -0.00496411\n",
      "   0.04582066  0.02387423]], shape=(1, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_mv_gen_model = tf.keras.Model(inputs=mv_genre_input_layer, outputs=mv_genre_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"movie_genres\"])\n",
    "    print(x[\"movie_id\"])\n",
    "    print(test_mv_gen_model(x[\"movie_genres\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b41cc9-63f5-4559-a943-1288be9c0892",
   "metadata": {},
   "source": [
    "#### define sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8727904e-e9b6-4005-8cf3-9da461ca88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_per_arm_features(x):\n",
    "    \"\"\"\n",
    "    This function generates a single per-arm observation vector\n",
    "    \"\"\"\n",
    "    mv_id_value = x['movie_id']\n",
    "    mv_gen_value = x['movie_genres']\n",
    "\n",
    "    _mid = test_mv_id_model(mv_id_value)\n",
    "    _mgen = test_mv_gen_model(mv_gen_value)\n",
    "\n",
    "    # to numpy array\n",
    "    _mid = np.array(_mid.numpy())\n",
    "    _mgen = np.array(_mgen.numpy())\n",
    "\n",
    "    # print(_mid)\n",
    "    # print(_mgen)\n",
    "\n",
    "    concat = np.concatenate(\n",
    "        [_mid, _mgen], axis=-1 # -1\n",
    "    ).astype(np.float32)\n",
    "    # concat = tf.concat([_mid, _mgen], axis=-1).astype(np.float32)\n",
    "\n",
    "    return concat #this is special to this example - there is only one action dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78d3c6bb-e525-4408-b321-34ac9684f881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_per_arm_features(data).shape #shape checks out at batchdim, nactions, arm feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff5764-25de-4f4c-ada3-3a417bdf22b5",
   "metadata": {},
   "source": [
    "### Create a moive lookup Table 🆕\n",
    "\n",
    "This will be used in our trajectories to randomly select a movie. Using the produced embeddings, we will also have a reward function for each combination by taking the inner product via `tf_agents.bandits.networks.global_and_arm_feature_network.create_feed_forward_dot_product_network` [link](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/create_feed_forward_dot_product_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "770c3473-622f-44eb-b512-e96c4b08a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_lookup_table = {'id': [],\n",
    "                      'movie_features': [],\n",
    "                      'movie_title': [],\n",
    "                      'movie_genres': []\n",
    "                     }\n",
    "    \n",
    "iterator = iter(train_dataset.batch(1000))\n",
    "for data in iterator:\n",
    "    _get_per_arm_features(data)\n",
    "    movie_lookup_table['id'].extend(data['movie_id'].numpy())\n",
    "    movie_lookup_table['movie_title'].extend(data['movie_title'].numpy())\n",
    "    movie_lookup_table['movie_genres'].extend(data['movie_genres'].numpy())\n",
    "    movie_lookup_table['movie_features'].extend(_get_per_arm_features(data))\n",
    "    \n",
    "#fix string ids to integers for random lookup later\n",
    "movie_lookup_table['id'] = [int(x) for x in movie_lookup_table['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66eed76c-79c2-415e-8dd9-e4ee85b5e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "movie_lookup_table = pd.DataFrame(movie_lookup_table)\n",
    "movie_lookup_table.set_index(['id'])\n",
    "\n",
    "unique_table = movie_lookup_table.groupby(['id'])[['movie_features', 'movie_title', 'movie_genres']].first().reset_index() #resetting index to get consecutive counts from min-max (no gaps)\n",
    "# unique_table = unique_table['movie_features']\n",
    "MAX_ARM_ID = len(unique_table)-1\n",
    "MIN_ARM_ID = 0\n",
    "\n",
    "# unique_table\n",
    "# print(f\"Max movie id is: {MAX_ARM_ID} \\nMin movie id is: {MIN_ARM_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8184ea44-5ae0-4986-922b-57ee76cc4783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01271609, -0.01063954, -0.04453922,  0.03591642,  0.00564051,\n",
       "        0.02092992, -0.04488032, -0.02584858, -0.04740479, -0.03002661,\n",
       "       -0.00383632,  0.03370481,  0.03419319, -0.00474702,  0.03322557,\n",
       "        0.03964141, -0.01609331, -0.00516544,  0.01496688, -0.01403636,\n",
       "        0.04762489, -0.00278686, -0.03502396, -0.04725162, -0.0474501 ,\n",
       "       -0.01647393, -0.04304332, -0.02555263,  0.04443676,  0.04496859,\n",
       "        0.01758147,  0.03720978, -0.04137355,  0.00880221,  0.04673413,\n",
       "        0.01510568,  0.00661532, -0.01288216,  0.04426864, -0.01195104,\n",
       "       -0.00610339, -0.03908002,  0.04702449,  0.03476996,  0.02464242,\n",
       "       -0.04901971, -0.00125569, -0.04959689, -0.04279599, -0.02668892,\n",
       "       -0.02922921,  0.02490618, -0.02546238, -0.03765206, -0.0131256 ,\n",
       "       -0.00520775,  0.03591717, -0.02238215,  0.01095585,  0.01336155,\n",
       "        0.01121206, -0.03909452,  0.00455445,  0.00605113], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_table.iloc[2,:]['movie_features'] #example of getting a ra movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6284375d-410c-4cd5-be8b-3889d39f98b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 64), dtype=float32, numpy=\n",
       " array([[ 0.01961209, -0.04153495, -0.03037058, -0.04911233, -0.00121395,\n",
       "         -0.00420331, -0.01246155,  0.01867643, -0.04896856,  0.04429226,\n",
       "         -0.00705642, -0.04387381, -0.03706367,  0.0153186 , -0.00388533,\n",
       "         -0.0074302 ,  0.04971996,  0.01255456,  0.02609159, -0.00529303,\n",
       "         -0.04905823,  0.00490832,  0.02905465,  0.01013665,  0.02138368,\n",
       "          0.00283175, -0.04763628,  0.039085  , -0.0075358 ,  0.01310653,\n",
       "         -0.02010018,  0.02264884, -0.04909344, -0.03367907,  0.018834  ,\n",
       "         -0.00841088,  0.01948916, -0.04885552, -0.00617999, -0.02069405,\n",
       "         -0.04477356,  0.03419046, -0.04278373,  0.00344169,  0.04837879,\n",
       "         -0.03245031, -0.02844121,  0.02616468, -0.04857191, -0.00706835,\n",
       "          0.03614764, -0.03875374, -0.02769949, -0.03707442,  0.00756137,\n",
       "          0.04047615, -0.00872046,  0.00992896, -0.01910238, -0.01092043,\n",
       "          0.02773506, -0.00496411,  0.04582066,  0.02387423]],\n",
       "       dtype=float32)>,\n",
       " [b'Sling Blade (1996)', array([7])])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_random_arm_features(movie_id):\n",
    "    movie_info = unique_table.iloc[movie_id]\n",
    "    tensor = tf.constant(movie_info['movie_features'], dtype=tf.float32)\n",
    "    return tf.reshape(tensor, [1, tensor.shape[0]]), [movie_info['movie_title'],\n",
    "                                                     movie_info['movie_genres']]\n",
    "\n",
    "get_random_arm_features(222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56f020a5-9ac8-46b7-b4b5-257a5cd03398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_set_of_arm_features(n_actions):\n",
    "    random_arm_ids = list(np.random.randint(MIN_ARM_ID, MAX_ARM_ID, n_actions))\n",
    "    features = [get_random_arm_features(x) for x in random_arm_ids]\n",
    "    just_features = [x[0] for x in features]\n",
    "    movie_info = [x[1] for x in features]\n",
    "    return tf.concat(just_features, axis=0), movie_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a334c538-6dab-4db3-9593-8621d5ef060c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 64), dtype=float32, numpy=\n",
       "array([[ 0.00206195, -0.02604058, -0.04603716,  0.04289932,  0.00313123,\n",
       "         0.0446151 , -0.04643131, -0.03961716,  0.02898718,  0.02279401,\n",
       "         0.03361676,  0.03951753, -0.00993199,  0.03707865,  0.04852705,\n",
       "        -0.00771183, -0.04796034,  0.03386337,  0.00921752,  0.03477183,\n",
       "        -0.00896877, -0.03940413, -0.04002311, -0.01100897,  0.0089735 ,\n",
       "        -0.00648671,  0.04059838,  0.03659462,  0.02175606, -0.00847901,\n",
       "        -0.03849056, -0.04923638, -0.04909344, -0.03367907,  0.018834  ,\n",
       "        -0.00841088,  0.01948916, -0.04885552, -0.00617999, -0.02069405,\n",
       "        -0.04477356,  0.03419046, -0.04278373,  0.00344169,  0.04837879,\n",
       "        -0.03245031, -0.02844121,  0.02616468, -0.04857191, -0.00706835,\n",
       "         0.03614764, -0.03875374, -0.02769949, -0.03707442,  0.00756137,\n",
       "         0.04047615, -0.00872046,  0.00992896, -0.01910238, -0.01092043,\n",
       "         0.02773506, -0.00496411,  0.04582066,  0.02387423],\n",
       "       [ 0.03543487,  0.01234521,  0.02217102, -0.02047111, -0.00810171,\n",
       "         0.0497914 ,  0.00227005,  0.04466916,  0.03226245, -0.04643449,\n",
       "         0.01679727, -0.02082038, -0.03041546, -0.00247792, -0.04240475,\n",
       "         0.00342004,  0.01348129,  0.04897363,  0.00654066,  0.03867895,\n",
       "         0.0307304 ,  0.01709144, -0.0201005 ,  0.02293474, -0.04783181,\n",
       "         0.00440451, -0.0312738 , -0.01418464, -0.00185504, -0.04369694,\n",
       "        -0.02046617,  0.01673779, -0.04460721,  0.03731959,  0.01797117,\n",
       "        -0.0177703 ,  0.02581063, -0.04508248, -0.04506424, -0.04909786,\n",
       "         0.02252001,  0.02500698,  0.03291753, -0.03441533, -0.04846109,\n",
       "         0.00031912,  0.00272926, -0.01895897, -0.04925644, -0.03167586,\n",
       "        -0.00462555,  0.04214302,  0.02915622,  0.02062919, -0.04654649,\n",
       "        -0.04697719, -0.03188027, -0.01157142, -0.03715948, -0.01354308,\n",
       "        -0.04527227,  0.02224699,  0.0471638 , -0.04266332]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_set_of_arm_features(n_actions=2)[0] #NEW - there's a tuple returned with the movies we will use for PALM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a52c9f62-04f9-4f50-912c-d2f75b5f6986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([35., 18., 56., 45., 35., 25., 25., 35.], dtype=float32), array([b'marketing', b'student', b'educator', b'programmer', b'other',\n",
      "       b'programmer', b'other', b'executive'], dtype=object), array([885409515, 883388887, 891249586, 878044851, 880130613, 892778202,\n",
      "       879959212, 877131685]), array([b'97208', b'85282', b'06437', b'03869', b'37212', b'55414',\n",
      "       b'06405', b'L1V3W'], dtype=object), array([b'False', b'True', b'True', b'True', b'False', b'True', b'False',\n",
      "       b'True'], dtype=object), array([b'Postman, The (1997)', b'Clueless (1995)',\n",
      "       b'Maltese Falcon, The (1941)', b'His Girl Friday (1940)',\n",
      "       b'Quiz Show (1994)', b\"Carlito's Way (1993)\",\n",
      "       b'Primal Fear (1996)', b'Aladdin (1992)'], dtype=object), array([4., 2., 5., 5., 4., 4., 5., 4.], dtype=float32)] [[b'Roman Holiday (1953)', array([4])], [b'Careful (1992)', array([4])], [b'Mille bolle blu (1993)', array([4])], [b'Two Bits (1995)', array([7])], [b\"What's Love Got to Do with It (1993)\", array([7])]]\n"
     ]
    }
   ],
   "source": [
    "### Look at the raw input features to format a good prompt for ranking movies\n",
    "NUM_ACTIONS = 5\n",
    "batch_size = 8\n",
    "iterator = iter(train_dataset.batch(batch_size))\n",
    "data = next(iterator)\n",
    "\n",
    "_, user_info = _get_global_context_features(data) #new - user info passes on the raw user features for prompting with PALM\n",
    "###NEW - we are getting the arm features here\n",
    "_, movie_info = get_random_set_of_arm_features(n_actions=NUM_ACTIONS)\n",
    "\n",
    "print(user_info, movie_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1f331390-f23b-4f41-a218-2a9211fba9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wed Jan 21 19:05:15 1998'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "dt = datetime.utcfromtimestamp(885409515)\n",
    "dt.ctime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c610713-7e65-4ff5-8d9f-bfabf81d6c82",
   "metadata": {},
   "source": [
    "## Quick inspection of movie info\n",
    "Here's an example of `N_ACTIONS` randomly selected movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ce9675e3-1a41-4533-b9fa-7fd02bba84c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[b'Roman Holiday (1953)', array([4])],\n",
       " [b'Careful (1992)', array([4])],\n",
       " [b'Mille bolle blu (1993)', array([4])],\n",
       " [b'Two Bits (1995)', array([7])],\n",
       " [b\"What's Love Got to Do with It (1993)\", array([7])]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf81d87-00c0-40f6-ae0e-0fa9a1e7aa7a",
   "metadata": {},
   "source": [
    "##### Feature formats info reference\n",
    "\n",
    "[BUCKETIZED AGE](https://www.tensorflow.org/datasets/catalog/movielens)\n",
    "\n",
    "\n",
    "[GENRE_LIST](https://files.grouplens.org/datasets/movielens/ml-10m-README.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a804c78a-40b3-4b4c-85ff-b8810e93192d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You are looking to watch a movie and need to review each movie based on user '\n",
      " 'demographics \\n'\n",
      " 'Here are some info on this the user: \\n'\n",
      " 'the user is age is 35-44, n\\n'\n",
      " 'and lives in zipcode 97208\\n'\n",
      " \"the user's occupation is marketing \\n\"\n",
      " 'the user previously reviewed Postman, The (1997), \\n'\n",
      " 'giving it a 4.0 out five star review during Wed Jan 21 19:05:15 1998\\n'\n",
      " '    \\n'\n",
      " 'Please rate these movies below using using \\n'\n",
      " '5 - highly recommended movie\\n'\n",
      " '4 - somewhat recommended movie\\n'\n",
      " '3 - maybe watch movie\\n'\n",
      " '2 - not a good movie\\n'\n",
      " '1 - really bad movie\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '1. Roman Holiday (1953), Comedy\\n'\n",
      " '2. Careful (1992), Comedy\\n'\n",
      " '3. Mille bolle blu (1993), Comedy\\n'\n",
      " '4. Two Bits (1995), Drama\\n'\n",
      " \"5. What's Love Got to Do with It (1993), Drama\\n\"\n",
      " ' please rate the 5 movies\\n'\n",
      " ' ensure you return the ratings as a python list of just the ratings for 5 '\n",
      " 'movies')\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "age_text_lookup = {\n",
    "'1': \"Under 18\",\n",
    "'18': \"18-24\",\n",
    "'25': \"25-34\",\n",
    "'35': \"35-44\",\n",
    "'45': \"45-49\",\n",
    "'50': \"50-55\",\n",
    "'56': \"56+\"\n",
    "}\n",
    "\n",
    "genre_list = [\n",
    "    \"Action\",\n",
    "    \"Adventure\",\n",
    "    \"Animation\",\n",
    "    \"Children's\",\n",
    "    \"Comedy\",\n",
    "    \"Crime\",\n",
    "    \"Documentary\",\n",
    "    \"Drama\",\n",
    "    \"Fantasy\",\n",
    "    \"Film-Noir\",\n",
    "    \"Horror\",\n",
    "    \"Musical\",\n",
    "    \"Mystery\",\n",
    "    \"Romance\",\n",
    "    \"Sci-Fi\",\n",
    "    \"Thriller\",\n",
    "    \"War\",\n",
    "    \"Western\",\n",
    "] #use this to lookup genres\n",
    "\n",
    "def gender_movielens_translator(elem):\n",
    "    if elem==\"True\":\n",
    "        return \"male\" \n",
    "    else:\n",
    "        return \"non-male\"\n",
    "\n",
    "rating_scale = '''\n",
    "5 - highly recommended movie\n",
    "4 - somewhat recommended movie\n",
    "3 - maybe watch movie\n",
    "2 - not a good movie\n",
    "1 - really bad movie\n",
    "'''\n",
    "\n",
    "age, occ, time, zipcode, gender, ex_movie, ex_movie_rating = user_info[0], user_info[1], user_info[2], user_info[3], user_info[4], user_info[5], user_info[6]\n",
    "\n",
    "prompts = []\n",
    "for i in range(len(age)):\n",
    "    formatted_datetime = datetime.utcfromtimestamp(time[i]).ctime()\n",
    "    gender = gender_movielens_translator(gender[i])\n",
    "    prompt = f\"\"\"You are looking to watch a movie and need to review each movie based on user demographics \n",
    "Here are some info on this the user: \n",
    "the user is age is {age_text_lookup[str(int(age[i]))]}, {gender[i]}\n",
    "and lives in zipcode {zipcode[i].decode(\"utf-8\")}\n",
    "the user's occupation is {occ[i].decode(\"utf-8\")} \n",
    "the user previously reviewed {ex_movie[i].decode(\"utf-8\")}, \n",
    "giving it a {float(ex_movie_rating[i])} out five star review during {formatted_datetime}\n",
    "    \n",
    "Please rate these movies below using using {rating_scale}\n",
    "\"\"\"\n",
    "    \n",
    "    for j, movie in enumerate(movie_info):\n",
    "        try:\n",
    "            genre = genre_list[movie[1][0]]\n",
    "        except:\n",
    "            genre = 'NA'\n",
    "        prompt += f\"\\n{j+1}. {movie[0].decode('utf-8')}, {genre}\"\n",
    "        total_movies = j+1\n",
    "    prompt += f\"\\n please rate the {total_movies} movies\"\n",
    "    prompt += f\"\\n ensure you return the ratings as a python list of just the ratings for {total_movies} movies\"\n",
    "        \n",
    "    ## next add in the movie selections\n",
    "    prompts.append(prompt)\n",
    "pprint(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a03c7f6d-9825-4923-b984-376e4ddd16b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[b'Roman Holiday (1953)', array([4])],\n",
       " [b'Careful (1992)', array([4])],\n",
       " [b'Mille bolle blu (1993)', array([4])],\n",
       " [b'Two Bits (1995)', array([7])],\n",
       " [b\"What's Love Got to Do with It (1993)\", array([7])]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb050b77-ede5-4745-a166-6ade8e401c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL_prompt(user_info, movie_info):\n",
    "    \n",
    "    age, occ, time, zipcode, gender, ex_movie, ex_movie_rating = user_info[0], user_info[1], user_info[2], user_info[3], user_info[4], user_info[5], user_info[6]\n",
    "\n",
    "    prompts = []\n",
    "    for i in range(len(age)):\n",
    "        formatted_datetime = datetime.utcfromtimestamp(time[i]).ctime()\n",
    "        gender = gender_movielens_translator(gender[i])\n",
    "        prompt = f\"\"\"CONTEXT: Pretend you are looking to watch a movie and need to review each movie on your user profile \n",
    "USER PROFILE: \n",
    "your age is {age_text_lookup[str(int(age[i]))]}, {gender[i]}\n",
    "and lives you live in zipcode {zipcode[i].decode(\"utf-8\")}\n",
    "your occupation is {occ[i].decode(\"utf-8\")} \n",
    "plus you previously reviewed {ex_movie[i].decode(\"utf-8\")}, \n",
    "giving it a {int(ex_movie_rating[i])} out 5.0 star review during {formatted_datetime}\n",
    "\n",
    "Review these movies using this scale: {rating_scale}\n",
    "Here are the movies to you need to rate: \"\"\"\n",
    "        for j, movie in enumerate(movie_info):\n",
    "            try:\n",
    "                genre = genre_list[movie[1][0]]\n",
    "            except:\n",
    "                genre = 'NA'\n",
    "            prompt += f\"\\n{j+1}. {movie[0].decode('utf-8')}, {genre}\"\n",
    "        # prompt += textwrap.dedent(f\"\\n Please rate these movies below using using this scale: {rating_scale}\")\n",
    "        # prompt += f\"Q: return the ratings of these movies like so: 3.5, 4, ... for each movie:\" #llm recency bias\n",
    "        prompts.append(prompt)\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "697199de-e949-4533-affb-a8c96563c55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CONTEXT: Pretend you are looking to watch a movie and need to review each '\n",
      " 'movie on your user profile \\n'\n",
      " 'USER PROFILE: \\n'\n",
      " 'your age is 35-44, n\\n'\n",
      " 'and lives you live in zipcode 97208\\n'\n",
      " 'your occupation is marketing \\n'\n",
      " 'plus you previously reviewed Postman, The (1997), \\n'\n",
      " 'giving it a 4 out 5.0 star review during Wed Jan 21 19:05:15 1998\\n'\n",
      " '\\n'\n",
      " 'Review these movies using this scale: \\n'\n",
      " '5 - highly recomended movie\\n'\n",
      " '4 - somewhat recommend movie\\n'\n",
      " '3 - maybe watch movie\\n'\n",
      " '2 - not a good movie\\n'\n",
      " '1 - really bad movie\\n'\n",
      " '\\n'\n",
      " 'Here are the movies to you need to rate: \\n'\n",
      " '1. Roman Holiday (1953), Comedy\\n'\n",
      " '2. Careful (1992), Comedy\\n'\n",
      " '3. Mille bolle blu (1993), Comedy\\n'\n",
      " '4. Two Bits (1995), Drama\\n'\n",
      " \"5. What's Love Got to Do with It (1993), Drama\")\n"
     ]
    }
   ],
   "source": [
    "prompts = RL_prompt(user_info, movie_info)\n",
    "\n",
    "len(prompts)\n",
    "pprint(prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3458f058-6346-4636-97db-f8fa02e89984",
   "metadata": {},
   "source": [
    "## Adding in reward function with PALM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1526b43-e69e-4d35-9145-05d0c2aaee18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am doing well today, thank you for asking! I am excited to be learning more about natural language processing and how it can be used to improve the customer experience.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Adding in reward function with PALM!\n",
    "\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "vertexai.init(project=\"wortz-project-352116\", location=\"us-central1\")\n",
    "parameters = {\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_output_tokens\": 400,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 40\n",
    "}\n",
    "llm = TextGenerationModel.from_pretrained(\"text-bison\")\n",
    "response = llm.predict(\n",
    "    \"How are you today?\",\n",
    "    **parameters\n",
    ")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2d4b74fc-90a9-45af-a2c0-a023bcfdf50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a rate limiter function\n",
    "import time\n",
    "\n",
    "def wait(secs):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            time.sleep(secs)\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5966a267-bb83-4ad5-93dc-cc569620c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@wait(1)\n",
    "def palm_rate_limited(prompt, *args, **kwargs):\n",
    "    return llm.predict(prompt, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9d27fa6e-89bd-4dbd-a91a-6731eb744b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "world\n",
       "\n",
       "```\n",
       "#include <stdio.h>\n",
       "\n",
       "int main() {\n",
       "  printf(\"Hello, world!\\n\");\n",
       "  return 0;\n",
       "}\n",
       "```\n",
       "\n",
       "Output:\n",
       "\n",
       "```\n",
       "Hello, world!\n",
       "```"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palm_rate_limited(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1bc7f2-25f0-4a4b-9fa8-1c6a2b86b952",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### WIP Adding in reward funciton with OpenLLaMA\n",
    "\n",
    "https://arxiv.org/abs/2302.13971\n",
    "\n",
    "Running this locally may improve run times for reward examples\n",
    "\n",
    "Note PEFT/LORA would be a good opition to do a pass over the entire dataset, then use fine-tuned model TODO\n",
    "\n",
    "##### Follow pip install for packages below\n",
    "\n",
    "```python\n",
    "!pip install torch transformers sentencepiece --user\n",
    "!pip install accelerate --user\n",
    "# https://github.com/pytorch/pytorch/issues/90673\n",
    "!echo Y | conda install libcusparse=11.7.3.50 -c nvidia\n",
    "!echo Y |conda install cudatoolkit=11.8 -c pytorch -c nvidia #I did a thing...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2d548fda-678d-42c8-a0b3-c96be15e3a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# model_path = \"openlm-research/open_llama_3b\"\n",
    "\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#     model_path,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# prompt = \"Q: What is the largest animal?\\nA:\"\n",
    "# input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "# input_ids = input_ids.to(\"cuda\")\n",
    "# generation_output = model.generate(input_ids=input_ids, max_new_tokens=32)\n",
    "# print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "12b4b120-e390-4848-8c95-c982d205eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_llama_predictions(prompt, max_new_tokens):\n",
    "#     input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "#     input_ids = input_ids.to(\"cuda\")\n",
    "#     generation_output = model.generate(input_ids=input_ids, max_new_tokens=max_new_tokens)\n",
    "#     return(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2b5c2f66-afdf-4390-b83a-7d1e1ca5efce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CONTEXT: Pretend you are looking to watch a movie and need to review each '\n",
      " 'movie on your user profile \\n'\n",
      " 'USER PROFILE: \\n'\n",
      " 'your age is 35-44, n\\n'\n",
      " 'and lives you live in zipcode 97208\\n'\n",
      " 'your occupation is marketing \\n'\n",
      " 'plus you previously reviewed Postman, The (1997), \\n'\n",
      " 'giving it a 4 out 5.0 star review during Wed Jan 21 19:05:15 1998\\n'\n",
      " '\\n'\n",
      " 'Review these movies using this scale: \\n'\n",
      " '5 - highly recomended movie\\n'\n",
      " '4 - somewhat recommend movie\\n'\n",
      " '3 - maybe watch movie\\n'\n",
      " '2 - not a good movie\\n'\n",
      " '1 - really bad movie\\n'\n",
      " '\\n'\n",
      " 'Here are the movies to you need to rate: \\n'\n",
      " '1. Roman Holiday (1953), Comedy\\n'\n",
      " '2. Careful (1992), Comedy\\n'\n",
      " '3. Mille bolle blu (1993), Comedy\\n'\n",
      " '4. Two Bits (1995), Drama\\n'\n",
      " \"5. What's Love Got to Do with It (1993), Drama\",\n",
      " 'CONTEXT: Pretend you are looking to watch a movie and need to review each '\n",
      " 'movie on your user profile \\n'\n",
      " 'USER PROFILE: \\n'\n",
      " 'your age is 18-24, o\\n'\n",
      " 'and lives you live in zipcode 85282\\n'\n",
      " 'your occupation is student \\n'\n",
      " 'plus you previously reviewed Clueless (1995), \\n'\n",
      " 'giving it a 2 out 5.0 star review during Mon Dec 29 09:48:07 1997\\n'\n",
      " '\\n'\n",
      " 'Review these movies using this scale: \\n'\n",
      " '5 - highly recomended movie\\n'\n",
      " '4 - somewhat recommend movie\\n'\n",
      " '3 - maybe watch movie\\n'\n",
      " '2 - not a good movie\\n'\n",
      " '1 - really bad movie\\n'\n",
      " '\\n'\n",
      " 'Here are the movies to you need to rate: \\n'\n",
      " '1. Roman Holiday (1953), Comedy\\n'\n",
      " '2. Careful (1992), Comedy\\n'\n",
      " '3. Mille bolle blu (1993), Comedy\\n'\n",
      " '4. Two Bits (1995), Drama\\n'\n",
      " \"5. What's Love Got to Do with It (1993), Drama\",\n",
      " 'CONTEXT: Pretend you are looking to watch a movie and need to review each '\n",
      " 'movie on your user profile \\n'\n",
      " 'USER PROFILE: \\n'\n",
      " 'your age is 56+, n\\n'\n",
      " 'and lives you live in zipcode 06437\\n'\n",
      " 'your occupation is educator \\n'\n",
      " 'plus you previously reviewed Maltese Falcon, The (1941), \\n'\n",
      " 'giving it a 5 out 5.0 star review during Mon Mar 30 09:19:46 1998\\n'\n",
      " '\\n'\n",
      " 'Review these movies using this scale: \\n'\n",
      " '5 - highly recomended movie\\n'\n",
      " '4 - somewhat recommend movie\\n'\n",
      " '3 - maybe watch movie\\n'\n",
      " '2 - not a good movie\\n'\n",
      " '1 - really bad movie\\n'\n",
      " '\\n'\n",
      " 'Here are the movies to you need to rate: \\n'\n",
      " '1. Roman Holiday (1953), Comedy\\n'\n",
      " '2. Careful (1992), Comedy\\n'\n",
      " '3. Mille bolle blu (1993), Comedy\\n'\n",
      " '4. Two Bits (1995), Drama\\n'\n",
      " \"5. What's Love Got to Do with It (1993), Drama\",\n",
      " 'CONTEXT: Pretend you are looking to watch a movie and need to review each '\n",
      " 'movie on your user profile \\n'\n",
      " 'USER PROFILE: \\n'\n",
      " 'your age is 45-49, -\\n'\n",
      " 'and lives you live in zipcode 03869\\n'\n",
      " 'your occupation is programmer \\n'\n",
      " 'plus you previously reviewed His Girl Friday (1940), \\n'\n",
      " 'giving it a 5 out 5.0 star review during Tue Oct 28 13:20:51 1997\\n'\n",
      " '\\n'\n",
      " 'Review these movies using this scale: \\n'\n",
      " '5 - highly recomended movie\\n'\n",
      " '4 - somewhat recommend movie\\n'\n",
      " '3 - maybe watch movie\\n'\n",
      " '2 - not a good movie\\n'\n",
      " '1 - really bad movie\\n'\n",
      " '\\n'\n",
      " 'Here are the movies to you need to rate: \\n'\n",
      " '1. Roman Holiday (1953), Comedy\\n'\n",
      " '2. Careful (1992), Comedy\\n'\n",
      " '3. Mille bolle blu (1993), Comedy\\n'\n",
      " '4. Two Bits (1995), Drama\\n'\n",
      " \"5. What's Love Got to Do with It (1993), Drama\",\n",
      " 'CONTEXT: Pretend you are looking to watch a movie and need to review each '\n",
      " 'movie on your user profile \\n'\n",
      " 'USER PROFILE: \\n'\n",
      " 'your age is 35-44, m\\n'\n",
      " 'and lives you live in zipcode 37212\\n'\n",
      " 'your occupation is other \\n'\n",
      " 'plus you previously reviewed Quiz Show (1994), \\n'\n",
      " 'giving it a 4 out 5.0 star review during Fri Nov 21 16:43:33 1997\\n'\n",
      " '\\n'\n",
      " 'Review these movies using this scale: \\n'\n",
      " '5 - highly recomended movie\\n'\n",
      " '4 - somewhat recommend movie\\n'\n",
      " '3 - maybe watch movie\\n'\n",
      " '2 - not a good movie\\n'\n",
      " '1 - really bad movie\\n'\n",
      " '\\n'\n",
      " 'Here are the movies to you need to rate: \\n'\n",
      " '1. Roman Holiday (1953), Comedy\\n'\n",
      " '2. Careful (1992), Comedy\\n'\n",
      " '3. Mille bolle blu (1993), Comedy\\n'\n",
      " '4. Two Bits (1995), Drama\\n'\n",
      " \"5. What's Love Got to Do with It (1993), Drama\",\n",
      " 'CONTEXT: Pretend you are looking to watch a movie and need to review each '\n",
      " 'movie on your user profile \\n'\n",
      " 'USER PROFILE: \\n'\n",
      " 'your age is 25-34, a\\n'\n",
      " 'and lives you live in zipcode 55414\\n'\n",
      " 'your occupation is programmer \\n'\n",
      " \"plus you previously reviewed Carlito's Way (1993), \\n\"\n",
      " 'giving it a 4 out 5.0 star review during Fri Apr 17 01:56:42 1998\\n'\n",
      " '\\n'\n",
      " 'Review these movies using this scale: \\n'\n",
      " '5 - highly recomended movie\\n'\n",
      " '4 - somewhat recommend movie\\n'\n",
      " '3 - maybe watch movie\\n'\n",
      " '2 - not a good movie\\n'\n",
      " '1 - really bad movie\\n'\n",
      " '\\n'\n",
      " 'Here are the movies to you need to rate: \\n'\n",
      " '1. Roman Holiday (1953), Comedy\\n'\n",
      " '2. Careful (1992), Comedy\\n'\n",
      " '3. Mille bolle blu (1993), Comedy\\n'\n",
      " '4. Two Bits (1995), Drama\\n'\n",
      " \"5. What's Love Got to Do with It (1993), Drama\",\n",
      " 'CONTEXT: Pretend you are looking to watch a movie and need to review each '\n",
      " 'movie on your user profile \\n'\n",
      " 'USER PROFILE: \\n'\n",
      " 'your age is 25-34, l\\n'\n",
      " 'and lives you live in zipcode 06405\\n'\n",
      " 'your occupation is other \\n'\n",
      " 'plus you previously reviewed Primal Fear (1996), \\n'\n",
      " 'giving it a 5 out 5.0 star review during Wed Nov 19 17:06:52 1997\\n'\n",
      " '\\n'\n",
      " 'Review these movies using this scale: \\n'\n",
      " '5 - highly recomended movie\\n'\n",
      " '4 - somewhat recommend movie\\n'\n",
      " '3 - maybe watch movie\\n'\n",
      " '2 - not a good movie\\n'\n",
      " '1 - really bad movie\\n'\n",
      " '\\n'\n",
      " 'Here are the movies to you need to rate: \\n'\n",
      " '1. Roman Holiday (1953), Comedy\\n'\n",
      " '2. Careful (1992), Comedy\\n'\n",
      " '3. Mille bolle blu (1993), Comedy\\n'\n",
      " '4. Two Bits (1995), Drama\\n'\n",
      " \"5. What's Love Got to Do with It (1993), Drama\",\n",
      " 'CONTEXT: Pretend you are looking to watch a movie and need to review each '\n",
      " 'movie on your user profile \\n'\n",
      " 'USER PROFILE: \\n'\n",
      " 'your age is 35-44, e\\n'\n",
      " 'and lives you live in zipcode L1V3W\\n'\n",
      " 'your occupation is executive \\n'\n",
      " 'plus you previously reviewed Aladdin (1992), \\n'\n",
      " 'giving it a 4 out 5.0 star review during Fri Oct 17 23:41:25 1997\\n'\n",
      " '\\n'\n",
      " 'Review these movies using this scale: \\n'\n",
      " '5 - highly recomended movie\\n'\n",
      " '4 - somewhat recommend movie\\n'\n",
      " '3 - maybe watch movie\\n'\n",
      " '2 - not a good movie\\n'\n",
      " '1 - really bad movie\\n'\n",
      " '\\n'\n",
      " 'Here are the movies to you need to rate: \\n'\n",
      " '1. Roman Holiday (1953), Comedy\\n'\n",
      " '2. Careful (1992), Comedy\\n'\n",
      " '3. Mille bolle blu (1993), Comedy\\n'\n",
      " '4. Two Bits (1995), Drama\\n'\n",
      " \"5. What's Love Got to Do with It (1993), Drama\"]\n"
     ]
    }
   ],
   "source": [
    "pprint(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "44200339-dc0e-41d6-8806-7ae53f55c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# get_llama_predictions(\"what is your favorite color?\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "72431627-0265-4d3d-9f12-08ab0c19a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# get_llama_predictions(prompt[0], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3c5ad486-4b76-44a5-ac8a-ed92fea7dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = prompts[0]\n",
    "# input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "# input_ids = input_ids.to(\"cuda\")\n",
    "# generation_output = model.generate(input_ids=input_ids, max_new_tokens=10)\n",
    "# print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b7223dca-5b0a-42a4-bea8-857e14649535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5, 3, 2, 3, 4'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "### PALM test prompt!\n",
    "\n",
    "rating = llm.predict(prompts[0], **parameters)\n",
    "extraction_prompt = \"extract the ratings in order in a simple comma seperated list:\"\n",
    "ratings = llm.predict(f\"{rating.text} {extraction_prompt}\", **parameters)\n",
    "ratings.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b4911b38-0cec-4f9f-874b-b37040e7f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(prompts):\n",
    "    ratings_list = []\n",
    "    for prompt in prompts:\n",
    "        rating = palm_rate_limited(prompt, **parameters)\n",
    "        extraction_prompt = \"extract the numeric-only ratings a comma seperated list:\"\n",
    "        ratings = palm_rate_limited(f\"given the output {rating.text}, {extraction_prompt}\", **parameters)\n",
    "        ratings_list.append(ratings.text)\n",
    "    return ratings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "39574548-f50e-437f-a2af-fddc565e23fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 5\n"
     ]
    }
   ],
   "source": [
    "#now try to put it together by getting ratings for a batch with multiple arms\n",
    "\n",
    "print(batch_size, NUM_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a267366d-6f6e-4b28-a4da-85dc4b3538c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unvalidated_llm_response = llm_call(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d0e6b0b2-f0ba-46bb-a7d9-1648e6a099ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5, 3, 2, 3, 4',\n",
       " '4, 3, 2, 3, 4',\n",
       " '5, 3, 3, 2, 4',\n",
       " '5, 3, 3, 2, 4',\n",
       " '5, 3, 2, 3, 4',\n",
       " '5, 3, 2, 3, 4',\n",
       " '5, 3, 2, 3, 4',\n",
       " '5, 3, 2, 3, 4']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unvalidated_llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "56487957-39af-43e6-bc52-9bae34bfc6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# ERROR 2023-08-22T03:37:47.867063105Z [resource.labels.taskName: workerpool0-0] ValueError: could not convert string to float: ' 4/5'\n",
    "# ERROR 2023-08-22T03:37:47.867039137Z [resource.labels.taskName: workerpool0-0] return [[float(y) for y in x] for x in str_list]\n",
    "\n",
    "def validate_llm_response(llm_response, n_actions):\n",
    "    \"this formats the text lists into a list of floats and also\"\n",
    "    \"TODO - handles when LLM has poor output\"\n",
    "    str_list = []\n",
    "    for resp in llm_response:\n",
    "        str_elem = [y for y in resp.split(',')]\n",
    "        if len(str_elem) != n_actions:\n",
    "             str_elem = list(np.ones(n_actions)*3) #default rating of all threes if we can't figure it out TODO\n",
    "        try:\n",
    "            [float(y) for y in str_elem] #check if we can do a float conversion\n",
    "        except:\n",
    "            str_elem = list(np.ones(n_actions)*3)\n",
    "        str_list.append(str_elem)\n",
    "    # re_clean_list = [[re.findall(r'\\d+', y) for y in x] for x in str_list]\n",
    "    try:\n",
    "        return [[float(y) for y in x] for x in str_list]\n",
    "    except:\n",
    "        return list(np.ones(n_actions)*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9e656d8f-63f2-4855-bdae-711b933ed9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5.0, 3.0, 2.0, 3.0, 4.0],\n",
       " [4.0, 3.0, 2.0, 3.0, 4.0],\n",
       " [5.0, 3.0, 3.0, 2.0, 4.0],\n",
       " [5.0, 3.0, 3.0, 2.0, 4.0],\n",
       " [5.0, 3.0, 2.0, 3.0, 4.0],\n",
       " [5.0, 3.0, 2.0, 3.0, 4.0],\n",
       " [5.0, 3.0, 2.0, 3.0, 4.0],\n",
       " [5.0, 3.0, 2.0, 3.0, 4.0]]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_rewards = validate_llm_response(unvalidated_llm_response, NUM_ACTIONS)\n",
    "llm_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3e01ba-8d4f-4d18-b07e-e4183dddd48e",
   "metadata": {},
   "source": [
    "## Finally, put it together into the LLM reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3a08075a-4cae-45d1-b0fc-cb6418cd338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_reward(user_info, movie_info, num_actions):\n",
    "    prompts = RL_prompt(user_info, movie_info)\n",
    "    unvalidated_llm_response = llm_call(prompts)\n",
    "    return validate_llm_response(unvalidated_llm_response, num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "03de09e2-5ce4-4158-8185-e7635e6be7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Look at the raw input features to format a good prompt for ranking movies\n",
    "# NUM_ACTIONS = 5\n",
    "# batch_size = 8\n",
    "# iterator = iter(train_dataset.batch(batch_size))\n",
    "# test_steps = 3\n",
    "# for _ in range(3):\n",
    "#     data = next(iterator)\n",
    "\n",
    "#     _, user_info = _get_global_context_features(data) #new - user info passes on the raw user features for prompting with PALM\n",
    "#     ###NEW - we are getting the arm features here\n",
    "#     _, movie_info = get_random_set_of_arm_features(n_actions=NUM_ACTIONS)\n",
    "\n",
    "\n",
    "#     llm_reward(user_info, movie_info, NUM_ACTIONS) #batch size by n_actions/arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1db42-81ab-46a6-b4a5-9f44d76ca4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one more validation - we will add a null tie in case of bad formatting TODO\n",
    "### should make sure we have the correct shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6836c-67b7-4fd4-917a-24ddad708edd",
   "metadata": {},
   "source": [
    "# TF-Agents implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877c79c-b6c8-4048-b1ce-05f011e8d69e",
   "metadata": {},
   "source": [
    "In TF-Agents, the *per-arm features* implementation differs from the *global-only* feature examples in the following aspects:\n",
    "* Reward is modeled not per-arm, but globally.\n",
    "* The arms are permutation invariant: it doesn’t matter which arm is arm 1 or arm 2, only their features.\n",
    "* One can have a different number of arms to choose from in every step (note that unspecified/dynamically changing number of arms will have a problem with XLA compatibility).\n",
    "\n",
    "When implementing per-arm features in TF-Bandits, the following details have to be discussed:\n",
    "* Observation spec and observations,\n",
    "* Action spec and actions,\n",
    "* Implementation of specific policies and agents.\n",
    "\n",
    "\n",
    "**TODO:**\n",
    "* outline the components and highlight their interactions, dependencies on eachother, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7ff9baaf-987d-448d-a981-742a79b581e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE  : 8\n",
      "NUM_ACTIONS : 5\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE  = 8\n",
    "NUM_ACTIONS = 5 \n",
    "\n",
    "# GLOBAL_EMBEDDING_SIZE  = 16\n",
    "# MV_EMBEDDING_SIZE      = 32 #32\n",
    "\n",
    "GLOBAL_DIM = GLOBAL_EMBEDDING_SIZE * 4 # 4 global features in this example\n",
    "PER_ARM_DIM = MV_EMBEDDING_SIZE * 2 # 2 movie features\n",
    "\n",
    "print(f\"BATCH_SIZE  : {BATCH_SIZE}\")\n",
    "print(f\"NUM_ACTIONS : {NUM_ACTIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb4902-5bb9-4587-8d29-c01d78b006be",
   "metadata": {},
   "source": [
    "## Tensor Specs\n",
    "\n",
    "**TODO:**\n",
    "* explain relationship between Tensor Specs and their Tensor counterparts\n",
    "* highlight the errors, lessons learned, and utility functions to address these"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68ebe8-116d-43b3-a6e1-4f5a5c7f4741",
   "metadata": {},
   "source": [
    "### Observation spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c4746-d01b-4ca4-aa53-ab68da54d37a",
   "metadata": {},
   "source": [
    "**This observation spec allows the user to have a global observation of fixed dimension**, and an unspecified number of *per-arm* features (also of fixed dimension)\n",
    "* The actions output by the policy are still integers as usual, and they indicate which row of the arm-features it has chosen \n",
    "* The action spec must be a single integer value without boundaries:\n",
    "\n",
    "```python\n",
    "global_spec = tensor_spec.TensorSpec([GLOBAL_DIM], tf.float32)\n",
    "per_arm_spec = tensor_spec.TensorSpec([None, PER_ARM_DIM], tf.float32)\n",
    "observation_spec = {'global': global_spec, 'per_arm': per_arm_spec}\n",
    "\n",
    "action_spec = tensor_spec.TensorSpec((), tf.int32)\n",
    "```\n",
    "> Here the only difference compared to the action spec with global features only is that the tensor spec is not bounded, as we don’t know how many arms there will be at any time step\n",
    "\n",
    "**XLA compatibility:**\n",
    "* Since dynamic tensor shapes are not compatible with XLA, the number of arm features (and consequently, number of arms for a step) cannot be dynamic. \n",
    "* One workaround is to fix the maximum number of arms for a problem, then pad the arm features in steps with fewer arms, and use action masking to indicate how many arms are actually active.\n",
    "\n",
    "```python\n",
    "per_arm_spec = tensor_spec.TensorSpec([NUM_ACTIONS, PER_ARM_DIM], tf.float32)\n",
    "\n",
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=(), dtype=tf.int32, minimum = 0, maximum = NUM_ACTIONS - 1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "36bd3b33-635a-4274-8b9e-7172696ebb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(64,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(5, 64), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_spec = {\n",
    "    'global': tf.TensorSpec([GLOBAL_DIM], tf.float32),\n",
    "    'per_arm': tf.TensorSpec([NUM_ACTIONS, PER_ARM_DIM], tf.float32) #excluding action dim here\n",
    "}\n",
    "observation_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2da92-7db2-4f42-94a7-b7bad1c8fc42",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Action spec\n",
    "\n",
    "> The time_step_spec and action_spec are specifications for the input time step and the output action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af42c7-18d7-480f-a7f3-d7da3f4840eb",
   "metadata": {},
   "source": [
    "```python\n",
    "    if (\n",
    "        not tensor_spec.is_bounded(action_spec)\n",
    "        or not tensor_spec.is_discrete(action_spec)\n",
    "        or action_spec.shape.rank > 1\n",
    "        or action_spec.shape.num_elements() != 1\n",
    "    ):\n",
    "      raise NotImplementedError(\n",
    "          'action_spec must be a BoundedTensorSpec of type int32 and shape (). '\n",
    "          'Found {}.'.format(action_spec)\n",
    "      )\n",
    "```\n",
    "\n",
    "* [src](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/policies/reward_prediction_base_policy.py#L97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "549a123c-349a-4103-b39a-4502f47d1e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(4, dtype=int32))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=[], \n",
    "    dtype=tf.int32,\n",
    "    minimum=tf.constant(0),            \n",
    "    maximum=NUM_ACTIONS-1, #n degrees of freedom and will dictate the expected mean reward spec shape\n",
    "    name=\"action_spec\"\n",
    ")\n",
    "\n",
    "action_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a51efce-9f1b-42d1-bec4-7b788e3fd7e0",
   "metadata": {},
   "source": [
    "### TimeStep spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "95f05860-0fbf-4a5a-8273-9c81761e0ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': {'global': TensorSpec(shape=(64,), dtype=tf.float32, name=None),\n",
       "                 'per_arm': TensorSpec(shape=(5, 64), dtype=tf.float32, name=None)},\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec = ts.time_step_spec(observation_spec)#, reward_spec=tf.TensorSpec([1, NUM_ACTIONS]))\n",
    "time_step_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9c197-ae9b-461d-8956-f078b929ac12",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Agent\n",
    "\n",
    "**Note** that contextual bandits form a special case of RL, where the actions taken by the agent do not alter the state of the environment \n",
    "\n",
    "> “Contextual” refers to the fact that the agent chooses among a set of actions while having knowledge of the context (environment observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aadb01-eb5c-4870-ae14-9e66624ba594",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Agent types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075e8a6-11a9-4346-8725-3653fba4bac4",
   "metadata": {},
   "source": [
    "**Possible Agent Types:**\n",
    "\n",
    "```\n",
    "AGENT_TYPE = ['LinUCB', 'LinTS', 'epsGreedy', 'NeuralLinUCB']\n",
    "```\n",
    "\n",
    "**LinearUCBAgent:** (`LinUCB`)\n",
    "* An agent implementing the Linear UCB bandit algorithm\n",
    "* (whitepaper) [A contextual bandit approach to personalized news recommendation](https://arxiv.org/abs/1003.0146)\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/lin_ucb_agent/LinearUCBAgent)\n",
    "\n",
    "**LinearThompsonSamplingAgent:** (`LinTS`)\n",
    "* Implements the Linear Thompson Sampling Agent from the paper: [Thompson Sampling for Contextual Bandits with Linear Payoffs](https://arxiv.org/abs/1209.3352)\n",
    "* the agent maintains two parameters `weight_covariances` and `parameter_estimators`, and updates them based on experience.\n",
    "* The inverse of the weight covariance parameters are updated with the outer product of the observations using the Woodbury inverse matrix update, while the parameter estimators are updated by the reward-weighted observation vectors for every action\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/linear_thompson_sampling_agent/LinearThompsonSamplingAgent)\n",
    "\n",
    "**NeuralEpsilonGreedyAgent:** (`epsGreedy`) \n",
    "* A neural network based epsilon greedy agent\n",
    "* This agent receives a neural network that it trains to predict rewards\n",
    "* The action is chosen greedily with respect to the prediction with probability `1 - epsilon`, and uniformly randomly with probability epsilon\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_epsilon_greedy_agent/NeuralEpsilonGreedyAgent)\n",
    "\n",
    "**NeuralLinUCBAgent:** (`NeuralLinUCB`)\n",
    "* An agent implementing the LinUCB algorithm on top of a neural network\n",
    "* `ENCODING_DIM` is the output dimension of the encoding network \n",
    "> * This output will be used by either a linear reward layer and epsilon greedy exploration, or by a LinUCB logic, depending on the number of training steps executed so far\n",
    "* `EPS_PHASE_STEPS` is the number training steps to run for training the encoding network before switching to `LinUCB`\n",
    "> * If negative, the encoding network is assumed to be already trained\n",
    "> * If the number of steps is less than or equal to `EPS_PHASE_STEPS`, `epsilon greedy` is used, otherwise `LinUCB`\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_linucb_agent/NeuralLinUCBAgent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8e88e-c8ea-4193-a911-0d974ef3b1a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### network types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547087d-5fad-4521-a584-cb46ce52897d",
   "metadata": {},
   "source": [
    "Which network architecture to use for the `epsGreedy` or `NeuralLinUCB` agents\n",
    "\n",
    "```\n",
    "NETWORK_TYPE = ['commontower', 'dotproduct']\n",
    "```\n",
    "\n",
    "**GlobalAndArmCommonTowerNetwork:** (`commontower`)\n",
    "* This network takes the output of the global and per-arm networks, and leads them through a common network, that in turn outputs reward estimates\n",
    "> * `GLOBAL_LAYERS` - Iterable of ints. Specifies the layers of the global tower\n",
    "> * `ARM_LAYERS` - Iterable of ints. Specifies the layers of the arm tower\n",
    "> * `COMMON_LAYERS` - Iterable of ints. Specifies the layers of the common tower\n",
    "* The network produced by this function can be used either in `GreedyRewardPredictionPolicy`, or `NeuralLinUCBPolicy`\n",
    "> * In the former case, the network must have `output_dim=1`, it is going to be an instance of `QNetwork`, and used in the policy as a reward prediction network\n",
    "> * In the latter case, the network will be an encoding network with its output consumed by a reward layer or a `LinUCB` method. The specified `output_dim` will be the encoding dimension\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmCommonTowerNetwork)\n",
    "\n",
    "**GlobalAndArmDotProductNetwork:** (`dotproduct`)\n",
    "* This network calculates the **dot product** of the output of the global and per-arm networks and returns them as reward estimates\n",
    "> * `GLOBAL_LAYERS` - Iterable of ints. Specifies the layers of the global tower\n",
    "> * `ARM_LAYERS` - Iterable of ints. Specifies the layers of the arm tower\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmDotProductNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3dc270-deb5-4e96-8276-74759a06c318",
   "metadata": {},
   "source": [
    "### define agent and network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6d817bf2-1fa6-4bae-af90-745fad996b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 8,\n",
      " 'common_layers': [100],\n",
      " 'epsilon': 0.4,\n",
      " 'global_layers': [50, 50, 50],\n",
      " 'learning_rate': 0.005,\n",
      " 'model_type': 'epsGreedy',\n",
      " 'network_type': 'dotproduct',\n",
      " 'num_actions': 5,\n",
      " 'per_arm_layers': [50, 50, 50]}\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Agents\n",
    "# ================================\n",
    "AGENT_TYPE      = 'epsGreedy' # 'LinUCB' | 'LinTS |, 'epsGreedy' | 'NeuralLinUCB'\n",
    "\n",
    "# Parameters for linear agents (LinUCB and LinTS).\n",
    "AGENT_ALPHA     = 0.1\n",
    "\n",
    "# Parameters for neural agents (NeuralEpsGreedy and NerualLinUCB).\n",
    "EPSILON         = 0.4\n",
    "LR              = 0.005\n",
    "\n",
    "# Parameters for NeuralLinUCB\n",
    "ENCODING_DIM    = 1\n",
    "EPS_PHASE_STEPS = 1000\n",
    "\n",
    "# ================================\n",
    "# Agent's Preprocess Network\n",
    "# ================================\n",
    "NETWORK_TYPE    = \"dotproduct\" # 'commontower' | 'dotproduct'\n",
    "\n",
    "if AGENT_TYPE == 'NeuralLinUCB':\n",
    "    NETWORK_TYPE = 'commontower'\n",
    "    \n",
    "\n",
    "GLOBAL_LAYERS   = [50, 50, 50]\n",
    "ARM_LAYERS      = [50, 50, 50]\n",
    "COMMON_LAYERS   = [100]\n",
    "\n",
    "observation_and_action_constraint_splitter = None\n",
    "\n",
    "HPARAMS = {  # TODO - streamline and consolidate\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"num_actions\": NUM_ACTIONS,\n",
    "    \"model_type\": AGENT_TYPE,\n",
    "    \"network_type\": NETWORK_TYPE,\n",
    "    \"global_layers\": GLOBAL_LAYERS,\n",
    "    \"per_arm_layers\": ARM_LAYERS,\n",
    "    \"common_layers\": COMMON_LAYERS,\n",
    "    \"learning_rate\": LR,\n",
    "    \"epsilon\": EPSILON,\n",
    "}\n",
    "pprint(HPARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db165067-6e9d-4b79-b675-bae69ec98c10",
   "metadata": {},
   "source": [
    "### Agent Factory\n",
    "\n",
    "**TODO:**\n",
    "* consolidate agent, network, and hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e7c79df4-f975-49fc-b598-d6fd2f6be125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick check on the inputs of the agent - this can be used to diagnose spec shape inputs\n",
      "\n",
      "time_step_spec:  TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': {'global': TensorSpec(shape=(64,), dtype=tf.float32, name=None),\n",
      "                 'per_arm': TensorSpec(shape=(5, 64), dtype=tf.float32, name=None)},\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n",
      "\n",
      "action_spec:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(4, dtype=int32))\n",
      "\n",
      "observation_spec:  {'global': TensorSpec(shape=(64,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(5, 64), dtype=tf.float32, name=None)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Quick check on the inputs of the agent - this can be used to diagnose spec shape inputs\")\n",
    "print(\"\\ntime_step_spec: \", time_step_spec)\n",
    "print(\"\\naction_spec: \", action_spec)\n",
    "print(\"\\nobservation_spec: \", observation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6cb60f0b-90b7-49ab-9c41-046ea00ab750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: OffpolicyNeuralEpsGreedyAgent\n",
      "\n",
      "Network: GlobalAndArmDotProductNetwork\n"
     ]
    }
   ],
   "source": [
    "# from tf_agents.bandits.policies import policy_utilities\n",
    "# from tf_agents.bandits.agents import greedy_reward_prediction_agent\n",
    "\n",
    "network = None\n",
    "observation_and_action_constraint_splitter = None\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "if AGENT_TYPE == 'LinUCB':\n",
    "    agent = lin_ucb_agent.LinearUCBAgent(\n",
    "        time_step_spec=time_step_spec,\n",
    "        action_spec=action_spec,\n",
    "        alpha=AGENT_ALPHA,\n",
    "        accepts_per_arm_features=True,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "elif AGENT_TYPE == 'LinTS':\n",
    "    agent = lin_ts_agent.LinearThompsonSamplingAgent(\n",
    "        time_step_spec=time_step_spec,\n",
    "        action_spec=action_spec,\n",
    "        alpha=AGENT_ALPHA,\n",
    "        observation_and_action_constraint_splitter=(\n",
    "            observation_and_action_constraint_splitter\n",
    "        ),\n",
    "        accepts_per_arm_features=True,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "elif AGENT_TYPE == 'epsGreedy':\n",
    "    # obs_spec = per_arm_tf_env.observation_spec()\n",
    "    if NETWORK_TYPE == 'commontower':\n",
    "        network = global_and_arm_feature_network.create_feed_forward_common_tower_network(\n",
    "            observation_spec = observation_spec, \n",
    "            global_layers = GLOBAL_LAYERS, \n",
    "            arm_layers = ARM_LAYERS, \n",
    "            common_layers = COMMON_LAYERS,\n",
    "            # output_dim = 1\n",
    "        )\n",
    "    elif NETWORK_TYPE == 'dotproduct':\n",
    "        network = global_and_arm_feature_network.create_feed_forward_dot_product_network(\n",
    "            observation_spec = observation_spec, \n",
    "            global_layers = GLOBAL_LAYERS, \n",
    "            arm_layers = ARM_LAYERS\n",
    "        )\n",
    "    agent = neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(\n",
    "        time_step_spec=time_step_spec,\n",
    "        action_spec=action_spec,\n",
    "        reward_network=network,\n",
    "        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=HPARAMS['learning_rate']),\n",
    "        epsilon=HPARAMS['epsilon'],\n",
    "        observation_and_action_constraint_splitter=(\n",
    "            observation_and_action_constraint_splitter\n",
    "        ),\n",
    "        accepts_per_arm_features=True,\n",
    "        emit_policy_info=policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN,\n",
    "        train_step_counter=global_step,\n",
    "        # info_fields_to_inherit_from_greedy=['predicted_rewards_mean'],\n",
    "        name='OffpolicyNeuralEpsGreedyAgent'\n",
    "    )\n",
    "\n",
    "elif AGENT_TYPE == 'NeuralLinUCB':\n",
    "    # obs_spec = per_arm_tf_env.observation_spec()\n",
    "    network = (\n",
    "        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n",
    "            observation_spec = observation_spec, \n",
    "            global_layers = GLOBAL_LAYERS, \n",
    "            arm_layers = ARM_LAYERS, \n",
    "            common_layers = COMMON_LAYERS,\n",
    "            output_dim = ENCODING_DIM\n",
    "        )\n",
    "    )\n",
    "    agent = neural_linucb_agent.NeuralLinUCBAgent(\n",
    "        time_step_spec=per_arm_tf_env.time_step_spec(),\n",
    "        action_spec=per_arm_tf_env.action_spec(),\n",
    "        encoding_network=network,\n",
    "        encoding_network_num_train_steps=EPS_PHASE_STEPS,\n",
    "        encoding_dim=ENCODING_DIM,\n",
    "        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n",
    "        alpha=1.0,\n",
    "        gamma=1.0,\n",
    "        epsilon_greedy=EPSILON,\n",
    "        accepts_per_arm_features=True,\n",
    "        debug_summaries=True,\n",
    "        summarize_grads_and_vars=True,\n",
    "        emit_policy_info=policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN,\n",
    "    )\n",
    "    \n",
    "agent.initialize() # TODO - does this go here?\n",
    "    \n",
    "print(f\"Agent: {agent.name}\\n\")\n",
    "if network:\n",
    "    print(f\"Network: {network.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d404dae-3cc3-4f81-962b-4641455ca4f2",
   "metadata": {},
   "source": [
    "## Reward function\n",
    "\n",
    "**TODO:**\n",
    "* create a baseline reward function on user features and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "df9aecd6-d20a-48a7-9ead-4da3bbcfdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _get_rewards(element):\n",
    "#     \"\"\"Calculates reward for the actions.\"\"\"\n",
    "\n",
    "#     def _calc_reward(x):\n",
    "#         \"\"\"Calculates reward for a single action.\"\"\"\n",
    "#         r0 = lambda: tf.constant(0.0)\n",
    "#         r1 = lambda: tf.constant(-10.0)\n",
    "#         r2 = lambda: tf.constant(2.0)\n",
    "#         r3 = lambda: tf.constant(3.0)\n",
    "#         r4 = lambda: tf.constant(4.0)\n",
    "#         r5 = lambda: tf.constant(10.0)\n",
    "#         c1 = tf.equal(x, 1.0)\n",
    "#         c2 = tf.equal(x, 2.0)\n",
    "#         c3 = tf.equal(x, 3.0)\n",
    "#         c4 = tf.equal(x, 4.0)\n",
    "#         c5 = tf.equal(x, 5.0)\n",
    "#         return tf.case(\n",
    "#             [(c1, r1), (c2, r2), (c3, r3),(c4, r4),(c5, r5)], \n",
    "#             default=r0, exclusive=True\n",
    "#         )\n",
    "\n",
    "#     return tf.map_fn(\n",
    "#         fn=_calc_reward, \n",
    "#         elems=element['user_rating'], \n",
    "#         dtype=tf.float32\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a18218-5062-470d-a715-86020e6a2f51",
   "metadata": {},
   "source": [
    "### New - exploring the dot product network\n",
    "\n",
    "Let's get the dot proudcut of arm/global features for the trajectories\n",
    "\n",
    "Looking at source [code](https://github.com/tensorflow/agents/blob/v0.17.0/tf_agents/bandits/networks/global_and_arm_feature_network.py#L54-L138)\n",
    "\n",
    "```python\n",
    "return GlobalAndArmDotProductNetwork(obs_spec_no_num_actions, global_network,\n",
    "                                       arm_network)\n",
    "```\n",
    "\n",
    "Leads to [here](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmDotProductNetwork#get_initial_state)\n",
    "\n",
    "Also member the config\n",
    "\n",
    "- GLOBAL_LAYERS   = [16, 4]\n",
    "- ARM_LAYERS      = [16, 4]\n",
    "- COMMON_LAYERS   = [4]\n",
    "\n",
    "```python\n",
    "network = global_and_arm_feature_network.create_feed_forward_dot_product_network(\n",
    "            observation_spec = observation_spec, \n",
    "            global_layers = GLOBAL_LAYERS, \n",
    "            arm_layers = ARM_LAYERS\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59703a-9a00-4eba-a3b7-45b21b738dd4",
   "metadata": {},
   "source": [
    "## Trajectory function\n",
    "\n",
    "**parking lot**\n",
    "* does trajectory fn need concept of `dummy_chosen_arm_features`, similar to [this](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/policies/reward_prediction_base_policy.py#L297)\n",
    "\n",
    "```python\n",
    "      dummy_chosen_arm_features = tf.nest.map_structure(\n",
    "          lambda obs: tf.zeros_like(obs[:, 0, ...]),\n",
    "          time_step.observation[bandit_spec_utils.PER_ARM_FEATURE_KEY],\n",
    "      )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bcb31c7b-f03e-4d9f-a0f8-2f8ad8318d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "def _trajectory_fn(element, batch_size): # hparams\n",
    "        \n",
    "    \"\"\"Converts a dataset element into a trajectory.\"\"\"\n",
    "    global_features, user_info = _get_global_context_features(element) #new - user info passes on the raw user features for prompting with PALM\n",
    "    ###NEW - we are getting the arm features here\n",
    "    arm_features, movie_info = get_random_set_of_arm_features(n_actions=NUM_ACTIONS)\n",
    "    # arm_features = get_random_set_of_arm_features(n_actions=NUM_ACTIONS)\n",
    "    \n",
    "    #get the dot product reward of the feed-forward network\n",
    "    reward = llm_reward(user_info, movie_info, NUM_ACTIONS)\n",
    "    \n",
    "    reward = tf.constant(reward, tf.float32)\n",
    "    \n",
    "    #chose an arm\n",
    "    best_arm_ids = tf.argmax(reward, axis=1)\n",
    "    # best_arm_ids = tf.cast(best_arm_ids, dtype=tf.int32)\n",
    "    max_rewards = tf.math.reduce_max(reward, axis=1)\n",
    "    max_rewards = _add_outer_dimension(max_rewards) # add time dim\n",
    "    chosen_arm_feats = tf.gather(arm_features, best_arm_ids) # [batch_size, arm_features]\n",
    "    \n",
    "    chosen_arm_feats = _add_outer_dimension(chosen_arm_feats)\n",
    "    # Adds a time dimension.\n",
    "    arm_features = _add_outer_dimension(arm_features)\n",
    "\n",
    "    # obs spec\n",
    "    observation = {\n",
    "        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n",
    "            _add_outer_dimension(global_features), #timedim bloat\n",
    "    }\n",
    "    \n",
    "    \n",
    "    reward = _add_outer_dimension(reward)\n",
    "    \n",
    "    ###TODO - not sure if this should actually go in the action for trajectory\n",
    "    # best_arm_ids =  _add_outer_dimension(best_arm_ids)\n",
    "    \n",
    "    dummy_rewards = tf.zeros([batch_size, 1, NUM_ACTIONS])\n",
    "    \n",
    "    policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "        chosen_arm_features=chosen_arm_feats,\n",
    "        # Pass dummy mean rewards here to match the model_spec for emitting\n",
    "        # mean rewards in policy info\n",
    "        predicted_rewards_mean=dummy_rewards\n",
    "    )\n",
    "    \n",
    "    if HPARAMS['model_type'] == 'neural_ucb':\n",
    "        policy_info = policy_info._replace(\n",
    "            predicted_rewards_optimistic=dummy_rewards\n",
    "        )\n",
    "        \n",
    "    return trajectory.single_step(\n",
    "        observation=observation,\n",
    "        action=tf.zeros_like(\n",
    "            max_rewards, dtype=tf.int32\n",
    "        ),  # Arm features are copied from policy info, put dummy zeros here\n",
    "        policy_info=policy_info,\n",
    "        reward=max_rewards,\n",
    "        discount=tf.zeros_like(max_rewards)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b3d3c-75f7-46f4-9a1b-6329e419b7f5",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb7ffc-00df-4d46-857b-86c87b78f597",
   "metadata": {},
   "source": [
    "`agent.train(experience=...)`\n",
    "\n",
    "where `experience` is a batch of trajectories data in the form of a Trajectory. \n",
    "* The structure of experience must match that of `self.training_data_spec`. \n",
    "* All tensors in experience must be shaped [batch, time, ...] where time must be equal to self.train_step_length if that property is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4cf53ca1-f827-424f-adf4-0edaf863da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "##todo - create a function that selects the best movie features along with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6332c741-cdff-4956-831f-fb68417fa52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 5)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE, NUM_ACTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b15e87-8971-4721-8edc-50d3f4695ee6",
   "metadata": {},
   "source": [
    "## Generate the training trajectories to disk\n",
    "So we don't have to sync call the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "18f674f3-faf3-4c93-8f66-c8e7b76dae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to save\n",
    "import pickle\n",
    "\n",
    "# os.mkdir('trajectories')\n",
    "\n",
    "def save_trajectories(trajectory, filename):\n",
    "    # Open a file for writing\n",
    "    with open(f\"{filename}\", \"wb\") as f:\n",
    "        # Write the dictionary to the file\n",
    "        pickle.dump(trajectory, f)\n",
    "\n",
    "    # Close the file\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0874b7f0-a777-4dda-80fb-af04709f2ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # The path to your file to upload\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # The ID of your GCS object\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Optional: set a generation-match precondition to avoid potential race conditions\n",
    "    # and data corruptions. The request to upload is aborted if the object's\n",
    "    # generation number does not match your precondition. For a destination\n",
    "    # object that does not yet exist, set the if_generation_match precondition to 0.\n",
    "    # If the destination object already exists in your bucket, set instead a\n",
    "    # generation-match precondition using its generation number.\n",
    "    generation_match_precondition = 0\n",
    "\n",
    "    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "66007340-a77b-4fa4-8276-a9fbd0d79fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:43, ?it/s]\n"
     ]
    },
    {
     "ename": "PreconditionFailed",
     "evalue": "412 POST https://storage.googleapis.com/upload/storage/v1/b/mabv1-wortz-project-352116-bucket/o?uploadType=multipart&ifGenerationMatch=0: {\n  \"error\": {\n    \"code\": 412,\n    \"message\": \"At least one of the pre-conditions you specified did not hold.\",\n    \"errors\": [\n      {\n        \"message\": \"At least one of the pre-conditions you specified did not hold.\",\n        \"domain\": \"global\",\n        \"reason\": \"conditionNotMet\",\n        \"locationType\": \"header\",\n        \"location\": \"If-Match\"\n      }\n    ]\n  }\n}\n: ('Request failed with status code', 412, 'Expected one of', <HTTPStatus.OK: 200>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidResponse\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/storage/blob.py:2540\u001b[0m, in \u001b[0;36mBlob.upload_from_file\u001b[0;34m(self, file_obj, rewind, size, content_type, num_retries, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2540\u001b[0m     created_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2544\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredefined_acl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2554\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_properties(created_json)\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/storage/blob.py:2355\u001b[0m, in \u001b[0;36mBlob._do_upload\u001b[0;34m(self, client, stream, content_type, size, num_retries, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m _MAX_MULTIPART_SIZE:\n\u001b[0;32m-> 2355\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_multipart_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2359\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredefined_acl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/storage/blob.py:1890\u001b[0m, in \u001b[0;36mBlob._do_multipart_upload\u001b[0;34m(self, client, stream, content_type, size, num_retries, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   1886\u001b[0m upload\u001b[38;5;241m.\u001b[39m_retry_strategy \u001b[38;5;241m=\u001b[39m _api_core_retry_to_resumable_media_retry(\n\u001b[1;32m   1887\u001b[0m     retry, num_retries\n\u001b[1;32m   1888\u001b[0m )\n\u001b[0;32m-> 1890\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mupload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransmit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1892\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/resumable_media/requests/upload.py:153\u001b[0m, in \u001b[0;36mMultipartUpload.transmit\u001b[0;34m(self, transport, data, metadata, content_type, timeout)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_request_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_and_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriable_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_status_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_strategy\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/resumable_media/requests/_request_helpers.py:155\u001b[0m, in \u001b[0;36mwait_and_retry\u001b[0;34m(func, get_status_code, retry_strategy)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _CONNECTION_ERROR_CLASSES \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/resumable_media/requests/upload.py:149\u001b[0m, in \u001b[0;36mMultipartUpload.transmit.<locals>.retriable_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m result \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    146\u001b[0m     method, url, data\u001b[38;5;241m=\u001b[39mpayload, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    147\u001b[0m )\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/resumable_media/_upload.py:114\u001b[0m, in \u001b[0;36mUploadBase._process_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m \u001b[43m_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_status_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_status_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/resumable_media/_helpers.py:108\u001b[0m, in \u001b[0;36mrequire_status_code\u001b[0;34m(response, status_codes, get_status_code, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m         callback()\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m common\u001b[38;5;241m.\u001b[39mInvalidResponse(\n\u001b[1;32m    109\u001b[0m         response,\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest failed with status code\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    111\u001b[0m         status_code,\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected one of\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;241m*\u001b[39mstatus_codes\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status_code\n",
      "\u001b[0;31mInvalidResponse\u001b[0m: ('Request failed with status code', 412, 'Expected one of', <HTTPStatus.OK: 200>)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPreconditionFailed\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(big_number)[\u001b[38;5;241m1\u001b[39m:big_nubmer_len]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.p\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     40\u001b[0m save_trajectories(trajectories, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jupyter/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m---> 41\u001b[0m \u001b[43mupload_blob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBUCKET_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/jupyter/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfilename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenerative-trajectories/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfilename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m big_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[116], line 25\u001b[0m, in \u001b[0;36mupload_blob\u001b[0;34m(bucket_name, source_file_name, destination_blob_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Optional: set a generation-match precondition to avoid potential race conditions\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# and data corruptions. The request to upload is aborted if the object's\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# generation number does not match your precondition. For a destination\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# object that does not yet exist, set the if_generation_match precondition to 0.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# If the destination object already exists in your bucket, set instead a\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# generation-match precondition using its generation number.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m generation_match_precondition \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mblob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_from_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_file_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_match_precondition\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/storage/blob.py:2684\u001b[0m, in \u001b[0;36mBlob.upload_from_filename\u001b[0;34m(self, filename, content_type, num_retries, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   2682\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_obj:\n\u001b[1;32m   2683\u001b[0m     total_bytes \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfstat(file_obj\u001b[38;5;241m.\u001b[39mfileno())\u001b[38;5;241m.\u001b[39mst_size\n\u001b[0;32m-> 2684\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2689\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_bytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredefined_acl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredefined_acl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/storage/blob.py:2557\u001b[0m, in \u001b[0;36mBlob.upload_from_file\u001b[0;34m(self, file_obj, rewind, size, content_type, num_retries, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_properties(created_json)\n\u001b[1;32m   2556\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m resumable_media\u001b[38;5;241m.\u001b[39mInvalidResponse \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m-> 2557\u001b[0m     \u001b[43m_raise_from_invalid_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/storage/blob.py:4373\u001b[0m, in \u001b[0;36m_raise_from_invalid_response\u001b[0;34m(error)\u001b[0m\n\u001b[1;32m   4369\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(error)\n\u001b[1;32m   4371\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 4373\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_status(response\u001b[38;5;241m.\u001b[39mstatus_code, message, response\u001b[38;5;241m=\u001b[39mresponse)\n",
      "\u001b[0;31mPreconditionFailed\u001b[0m: 412 POST https://storage.googleapis.com/upload/storage/v1/b/mabv1-wortz-project-352116-bucket/o?uploadType=multipart&ifGenerationMatch=0: {\n  \"error\": {\n    \"code\": 412,\n    \"message\": \"At least one of the pre-conditions you specified did not hold.\",\n    \"errors\": [\n      {\n        \"message\": \"At least one of the pre-conditions you specified did not hold.\",\n        \"domain\": \"global\",\n        \"reason\": \"conditionNotMet\",\n        \"locationType\": \"header\",\n        \"location\": \"If-Match\"\n      }\n    ]\n  }\n}\n: ('Request failed with status code', 412, 'Expected one of', <HTTPStatus.OK: 200>)"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.policies import policy_saver\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "ARTIFACTS_DIR = '.'\n",
    "\n",
    "\n",
    "train_step_counter = tf.compat.v1.train.get_or_create_global_step()\n",
    "saver = policy_saver.PolicySaver(\n",
    "    agent.policy, \n",
    "    train_step=train_step_counter\n",
    ")\n",
    "starting_loop = 0\n",
    "\n",
    "print(f\"saving files...\")\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "big_number = 10_000_000\n",
    "big_nubmer_len = len(str(big_number))\n",
    "\n",
    "iterator = iter(train_dataset.batch(BATCH_SIZE))\n",
    "\n",
    "for data in tqdm(iterator):\n",
    "    trajectories = _trajectory_fn(data, BATCH_SIZE)\n",
    "    filename = str(big_number)[1:big_nubmer_len]+'.p'\n",
    "    save_trajectories(trajectories, f'/home/jupyter/{filename}') \n",
    "    upload_blob(BUCKET_NAME, f'/home/jupyter/{filename}', f'generative-trajectories/{filename}')\n",
    "    big_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10ff367-1d6e-4a05-9a9b-c88a91c33e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m110"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "local-conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
