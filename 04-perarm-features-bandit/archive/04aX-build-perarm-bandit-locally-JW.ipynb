{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385307d6-2058-47ec-8da0-57f7ef5c43d6",
   "metadata": {},
   "source": [
    "# Train Bandits with per-arm features\n",
    "\n",
    "**Exploring linear and nonlinear** (e.g., those with neural network-based value functions) bandit methods for recommendations using TF-Agents\n",
    "\n",
    "> Neural linear bandits provide a nice way to leverage the representation power of deep learning and the bandit approach for uncertainty measure and efficient exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fd257-b98b-426a-a2cd-024429b014f1",
   "metadata": {},
   "source": [
    "## Load env config\n",
    "\n",
    "* use the prefix from `00-env-setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39c9d08-d118-4013-a47f-88450f49f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'mabv1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908f6b95-b539-4a9f-a836-840d26ea3b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"wortz-project-352116\"\n",
      "PROJECT_NUM              = \"679926387543\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"679926387543-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"mabv1\"\n",
      "VERSION                  = \"v1\"\n",
      "\n",
      "BUCKET_NAME              = \"mabv1-wortz-project-352116-bucket\"\n",
      "BUCKET_URI               = \"gs://mabv1-wortz-project-352116-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://mabv1-wortz-project-352116-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/679926387543/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_ID      = \"wortz-project-352116.movielens_dataset_mabv1\"\n",
      "BIGQUERY_TABLE_ID        = \"wortz-project-352116.movielens_dataset_mabv1.training_dataset\"\n",
      "\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "RL_SUB_DIR               = \"per_arm_rl\"\n",
      "REPOSITORY               = \"rl-movielens-mabv1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c690a9-e2bd-4759-ba41-4e2469098aee",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac6a5737-2d6f-4643-9243-b339449308bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to fix protobuffs I upgraded to tf 2.12\n",
    "# !pip install protobuf==3.20.3 --user\n",
    "# !pip install tensorflow==2.12.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4d0dfe4-695c-4dd4-9f24-67f7488ce1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c669f1a1-1af7-4efb-ab2d-6bf3b3847991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pprint import pprint\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents import TFAgent\n",
    "\n",
    "from tf_agents.bandits.environments import stationary_stochastic_per_arm_py_environment as p_a_env\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\n",
    "from tf_agents.bandits.agents import neural_epsilon_greedy_agent\n",
    "from tf_agents.bandits.agents import neural_linucb_agent\n",
    "from tf_agents.bandits.networks import global_and_arm_feature_network\n",
    "\n",
    "# GPU\n",
    "from numba import cuda \n",
    "import gc\n",
    "\n",
    "# my project\n",
    "from src.per_arm_rl import data_utils\n",
    "from src.per_arm_rl import data_config\n",
    "\n",
    "# tf exceptions and vars\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e58dd7-ab2b-419f-9771-bf1e98db758b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "# gpus\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4969d3e-1fc0-45db-8a69-aa6b342019de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = cuda.get_current_device()\n",
    "# device.reset()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "274e7f4a-1802-4946-888e-876638f5c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a866b1-85b9-43e6-9546-edfbbf886bce",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5b953-14c0-42ed-a511-77147a1bc0ac",
   "metadata": {},
   "source": [
    "### Read TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d0157c8-a04c-4dbd-b6d9-a1ede97687a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c934c06-bf08-4c7f-b0cc-0de04ef3515c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://mabv1-wortz-project-352116-bucket/data/val/ml-ratings-100k-val.tfrecord']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{DATA_GCS_PREFIX}/val'): # tmp TODO - \"train\"\n",
    "    if '.tfrecord' in blob.name:\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7511e4d-bf81-4800-bde7-8b16dec9aeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([25.], dtype=float32)>,\n",
      " 'movie_genres': <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[4]])>,\n",
      " 'movie_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'211'], dtype=object)>,\n",
      " 'timestamp': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([874948475])>,\n",
      " 'user_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'346'], dtype=object)>,\n",
      " 'user_occupation_text': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'other'], dtype=object)>,\n",
      " 'user_rating': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([4.], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "train_dataset = train_dataset.map(data_utils.parse_tfrecord)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4e4b2561-f45c-48d0-8c5f-f47b9eba637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW Sorting by user_id, grabbing most recent 5 movies\n",
    "\n",
    "NUM_ACTIONS = 5\n",
    "\n",
    "from tensorflow.data.experimental import group_by_window\n",
    "\n",
    "def key_f(row):\n",
    "    return tf.strings.to_number(row['user_id']\n",
    "                               ,out_type=tf.int64)  \n",
    "\n",
    "def reduce_func(key, ds):\n",
    "    \n",
    "    ds=ds\\\n",
    "    .batch(1000000)\\\n",
    "    .map(\n",
    "        lambda x: dict([(k, tf.gather(x[k], tf.argsort(x[\"timestamp\"]))) for k, v in x.items()])\n",
    "    )\\\n",
    "    .unbatch()\n",
    "\n",
    "    return ds.take(NUM_ACTIONS)\n",
    "\n",
    "t = group_by_window(key_func = key_f, reduce_func = reduce_func, window_size=100)\n",
    "\n",
    "sorted_top_ten = train_dataset.apply(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "909f16fb-445b-400d-9962-460ae49b08d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': <tf.Tensor: shape=(5,), dtype=float32, numpy=array([18., 18., 18., 18., 18.], dtype=float32)>,\n",
      " 'movie_genres': <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
      "array([[7],\n",
      "       [7],\n",
      "       [0],\n",
      "       [0],\n",
      "       [4]])>,\n",
      " 'movie_id': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'468', b'193', b'174', b'187', b'204'], dtype=object)>,\n",
      " 'timestamp': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([885544698, 885544698, 885544739, 885544739, 885544769])>,\n",
      " 'user_id': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'405', b'405', b'405', b'405', b'405'], dtype=object)>,\n",
      " 'user_occupation_text': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
      "array([b'healthcare', b'healthcare', b'healthcare', b'healthcare',\n",
      "       b'healthcare'], dtype=object)>,\n",
      " 'user_rating': <tf.Tensor: shape=(5,), dtype=float32, numpy=array([3., 4., 5., 5., 5.], dtype=float32)>}\n",
      "{'bucketized_user_age': <tf.Tensor: shape=(5,), dtype=float32, numpy=array([50., 50., 50., 50., 50.], dtype=float32)>,\n",
      " 'movie_genres': <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
      "array([[5],\n",
      "       [7],\n",
      "       [0],\n",
      "       [7],\n",
      "       [0]])>,\n",
      " 'movie_id': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'302', b'1265', b'328', b'325', b'295'], dtype=object)>,\n",
      " 'timestamp': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([887424720, 887425025, 887425025, 887425197, 887425530])>,\n",
      " 'user_id': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'655', b'655', b'655', b'655', b'655'], dtype=object)>,\n",
      " 'user_occupation_text': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
      "array([b'healthcare', b'healthcare', b'healthcare', b'healthcare',\n",
      "       b'healthcare'], dtype=object)>,\n",
      " 'user_rating': <tf.Tensor: shape=(5,), dtype=float32, numpy=array([4., 3., 2., 2., 3.], dtype=float32)>}\n",
      "{'bucketized_user_age': <tf.Tensor: shape=(5,), dtype=float32, numpy=array([45., 45., 45., 45., 45.], dtype=float32)>,\n",
      " 'movie_genres': <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
      "array([[0],\n",
      "       [7],\n",
      "       [4],\n",
      "       [4],\n",
      "       [4]])>,\n",
      " 'movie_id': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'328', b'98', b'514', b'480', b'238'], dtype=object)>,\n",
      " 'timestamp': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([881514811, 881515011, 881515112, 881515193, 881515411])>,\n",
      " 'user_id': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'13', b'13', b'13', b'13', b'13'], dtype=object)>,\n",
      " 'user_occupation_text': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
      "array([b'educator', b'educator', b'educator', b'educator', b'educator'],\n",
      "      dtype=object)>,\n",
      " 'user_rating': <tf.Tensor: shape=(5,), dtype=float32, numpy=array([3., 4., 5., 3., 3.], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "for x in sorted_top_ten.batch(NUM_ACTIONS).take(3):\n",
    "    pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8959967-522e-41c8-9a1b-050ca8bc191f",
   "metadata": {},
   "source": [
    "### get vocab\n",
    "\n",
    "**TODO:** \n",
    "* streamline vocab calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b9169bc-d6dc-497e-9dff-6ebb175282ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATE_VOCABS: False\n"
     ]
    }
   ],
   "source": [
    "GENERATE_VOCABS = False\n",
    "print(f\"GENERATE_VOCABS: {GENERATE_VOCABS}\")\n",
    "\n",
    "VOCAB_SUBDIR   = \"vocabs\"\n",
    "VOCAB_FILENAME = \"vocab_dict.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3ccf137-7a72-42e7-aa89-3c81a99cf40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vocab...\n",
      "Downloaded vocab from: gs://mabv1-wortz-project-352116-bucket/vocabs/vocab_dict.pkl\n",
      "\n",
      "'movie_id'\n",
      "'user_id'\n",
      "'user_occupation_text'\n",
      "'movie_genres'\n",
      "'bucketized_user_age'\n",
      "'max_timestamp'\n",
      "'min_timestamp'\n",
      "'timestamp_buckets'\n"
     ]
    }
   ],
   "source": [
    "if not GENERATE_VOCABS:\n",
    "\n",
    "    EXISTING_VOCAB_FILE = f'gs://{BUCKET_NAME}/{VOCAB_SUBDIR}/{VOCAB_FILENAME}'\n",
    "    print(f\"Downloading vocab...\")\n",
    "    \n",
    "    os.system(f'gsutil -q cp {EXISTING_VOCAB_FILE} .')\n",
    "    print(f\"Downloaded vocab from: {EXISTING_VOCAB_FILE}\\n\")\n",
    "\n",
    "    filehandler = open(VOCAB_FILENAME, 'rb')\n",
    "    vocab_dict = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "    for key in vocab_dict.keys():\n",
    "        pprint(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b941063b-ad48-4817-aef0-9afa8a444632",
   "metadata": {},
   "source": [
    "# Per-Arm Bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28770b8d-836b-448d-8dd1-203d76fc6d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.environments import stationary_stochastic_per_arm_py_environment as p_a_env\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "nest = tf.nest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0138a295-2b0d-4359-8496-af8552ec8cff",
   "metadata": {},
   "source": [
    "## Preprocessing layers for global and arm features\n",
    "\n",
    "The preproccesing layers will ultimately feed the two functions described below, both of which will ultimately feed the `Environment`\n",
    "\n",
    "`global_context_sampling_fn`: \n",
    "* A function that outputs a random 1d array or list of ints or floats\n",
    "* This output is the global context. Its shape and type must be consistent across calls.\n",
    "\n",
    "`arm_context_sampling_fn`: \n",
    "* A function that outputs a random 1 array or list of ints or floats (same type as the output of `global_context_sampling_fn`). * This output is the per-arm context. Its shape must be consistent across calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8d77956-635c-438a-916a-185eec52f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OOV_BUCKETS        = 1\n",
    "GLOBAL_EMBEDDING_SIZE  = 4\n",
    "MV_EMBEDDING_SIZE      = 8 #32\n",
    "\n",
    "HPARAMS = {\n",
    "    \"batch_size\":8,\n",
    "    \"num_docs_to_rank\":3,\n",
    "    \"model_type\": \"neural_epsilon_greedy\",\n",
    "    \"network_type\": 'commontower',\n",
    "    \"global_layers\": [16,4],\n",
    "    \"per_arm_layers\": [16,4],\n",
    "    \"common_layers\": [4],\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"epsilon\":0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142e63e-0a20-4d51-997c-7a4733517f7e",
   "metadata": {},
   "source": [
    "### global context (user) features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195acd92-06b6-42e4-bef7-798fd09da856",
   "metadata": {},
   "source": [
    "#### user ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c28e887b-421a-4603-8899-87071056783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_input_layer = tf.keras.Input(\n",
    "    name=\"user_id\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.string\n",
    ")\n",
    "\n",
    "user_id_lookup = tf.keras.layers.StringLookup(\n",
    "    max_tokens=len(vocab_dict['user_id']) + NUM_OOV_BUCKETS,\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    mask_token=None,\n",
    "    vocabulary=vocab_dict['user_id'],\n",
    ")(user_id_input_layer)\n",
    "\n",
    "user_id_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['user_id']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_id_lookup)\n",
    "\n",
    "user_id_embedding = tf.reduce_sum(user_id_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_id_input_layer)\n",
    "# global_features.append(user_id_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d6a0fe7-26cb-4c62-a3ef-17f98e6ccddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'346'], shape=(1,), dtype=string)\n",
      "tf.Tensor([[-0.02258484 -0.00642679 -0.01963132 -0.04176483]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_user_id_model = tf.keras.Model(inputs=user_id_input_layer, outputs=user_id_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"user_id\"])\n",
    "    print(test_user_id_model(x[\"user_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352d2227-92ec-4386-926f-df2fdb9434ec",
   "metadata": {},
   "source": [
    "#### user AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70785bf0-5ece-4875-ab72-06d9c45ea9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_age_input_layer = tf.keras.Input(\n",
    "    name=\"bucketized_user_age\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "user_age_lookup = tf.keras.layers.IntegerLookup(\n",
    "    vocabulary=vocab_dict['bucketized_user_age'],\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    oov_value=0,\n",
    ")(user_age_input_layer)\n",
    "\n",
    "user_age_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['bucketized_user_age']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_age_lookup)\n",
    "\n",
    "user_age_embedding = tf.reduce_sum(user_age_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_age_input_layer)\n",
    "# global_features.append(user_age_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e01622a-9418-4ca7-8925-9b0ebef8940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([25.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([[-0.03874453  0.00775563 -0.04652616  0.04724056]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_user_age_model = tf.keras.Model(inputs=user_age_input_layer, outputs=user_age_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"bucketized_user_age\"])\n",
    "    print(test_user_age_model(x[\"bucketized_user_age\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997ffaa8-ca92-4851-b7e3-bb06fba8958b",
   "metadata": {},
   "source": [
    "#### user OCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03e7344d-71fb-423a-89dd-f1abeb270e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_occ_input_layer = tf.keras.Input(\n",
    "    name=\"user_occupation_text\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.string\n",
    ")\n",
    "\n",
    "user_occ_lookup = tf.keras.layers.StringLookup(\n",
    "    max_tokens=len(vocab_dict['user_occupation_text']) + NUM_OOV_BUCKETS,\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    mask_token=None,\n",
    "    vocabulary=vocab_dict['user_occupation_text'],\n",
    ")(user_occ_input_layer)\n",
    "\n",
    "user_occ_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['user_occupation_text']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_occ_lookup)\n",
    "\n",
    "user_occ_embedding = tf.reduce_sum(user_occ_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_occ_input_layer)\n",
    "# global_features.append(user_occ_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39cbbc31-ca43-4f8f-a804-a4b830e99d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'other'], shape=(1,), dtype=string)\n",
      "tf.Tensor([[ 0.00721186 -0.0224213  -0.01734282 -0.0030665 ]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_user_occ_model = tf.keras.Model(inputs=user_occ_input_layer, outputs=user_occ_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"user_occupation_text\"])\n",
    "    print(test_user_occ_model(x[\"user_occupation_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee0098-a48a-4de6-88bf-6219ce8c0533",
   "metadata": {},
   "source": [
    "#### user Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61a4e01a-e742-4c68-93a9-aa66eb9a5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ts_input_layer = tf.keras.Input(\n",
    "    name=\"timestamp\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.int64\n",
    ")\n",
    "\n",
    "user_ts_lookup = tf.keras.layers.Discretization(\n",
    "    vocab_dict['timestamp_buckets'].tolist()\n",
    ")(user_ts_input_layer)\n",
    "\n",
    "user_ts_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['timestamp_buckets'].tolist()) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_ts_lookup)\n",
    "\n",
    "user_ts_embedding = tf.reduce_sum(user_ts_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_ts_input_layer)\n",
    "# global_features.append(user_ts_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db99f90b-57f8-45e6-9f28-871658e17358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([874948475], shape=(1,), dtype=int64)\n",
      "tf.Tensor([[-0.0196561  -0.04081204 -0.0125438  -0.00011444]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_user_ts_model = tf.keras.Model(inputs=user_ts_input_layer, outputs=user_ts_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"timestamp\"])\n",
    "    print(test_user_ts_model(x[\"timestamp\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc734ea-cb5e-4c6b-8b94-2a8853220178",
   "metadata": {},
   "source": [
    "#### define sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff58c380-8b53-4dfa-b5b4-d36853638ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_context_sampling_fn():\n",
    "    \"\"\"\n",
    "    This function generates a single global observation vector.\n",
    "    \"\"\"\n",
    "    # for x in train_dataset.take(1).as_numpy_iterator():\n",
    "    for x in train_dataset.batch(1).take(1):\n",
    "        user_id_value = x['user_id']\n",
    "        user_age_value = x['bucketized_user_age']\n",
    "        user_occ_value = x['user_occupation_text']\n",
    "        user_ts_value = x['timestamp']\n",
    "        \n",
    "        _id = test_user_id_model(user_id_value)\n",
    "        _age = test_user_age_model(user_age_value)\n",
    "        _occ = test_user_occ_model(user_occ_value)\n",
    "        _ts = test_user_ts_model(user_ts_value)\n",
    "        \n",
    "        # # tmp - insepct numpy() values\n",
    "        # print(_id.numpy()) #[0])\n",
    "        # print(_age.numpy()) #[0])\n",
    "        # print(_occ.numpy()) #[0])\n",
    "        # print(_ts.numpy()) #[0])\n",
    "        \n",
    "        # to numpy array\n",
    "        _id = np.array(_id.numpy()[0])\n",
    "        _age = np.array(_age.numpy()[0])\n",
    "        _occ = np.array(_occ.numpy()[0])\n",
    "        _ts = np.array(_ts.numpy()[0])\n",
    "        \n",
    "        concat = np.concatenate(\n",
    "            [_id, _age, _occ, _ts], axis=-1\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bba133ab-bf12-4b3b-926d-6d1dba940837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02258484, -0.00642679, -0.01963132, -0.04176483, -0.03874453,\n",
       "        0.00775563, -0.04652616,  0.04724056,  0.00721186, -0.0224213 ,\n",
       "       -0.01734282, -0.0030665 , -0.0196561 , -0.04081204, -0.0125438 ,\n",
       "       -0.00011444], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_context_sampling_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249fa771-35d7-4d04-ab68-2b70911bac17",
   "metadata": {},
   "source": [
    "### arm preprocessing layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534b3bf1-a2ea-4bfb-8c77-efa057f4e391",
   "metadata": {},
   "source": [
    "#### movie ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa53cbe9-2616-4da4-90dc-dc5616258af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_id_input_layer = tf.keras.Input(\n",
    "    name=\"movie_id\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.string\n",
    ")\n",
    "\n",
    "mv_id_lookup = tf.keras.layers.StringLookup(\n",
    "    max_tokens=len(vocab_dict['movie_id']) + NUM_OOV_BUCKETS,\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    mask_token=None,\n",
    "    vocabulary=vocab_dict['movie_id'],\n",
    ")(mv_id_input_layer)\n",
    "\n",
    "mv_id_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['movie_id']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=MV_EMBEDDING_SIZE\n",
    ")(mv_id_lookup)\n",
    "\n",
    "mv_id_embedding = tf.reduce_sum(mv_id_embedding, axis=-2)\n",
    "\n",
    "# arm_inputs.append(mv_id_input_layer)\n",
    "# arm_features.append(mv_id_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bd19f09-a12e-4a21-a1a1-5ec5bc116559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'211'], shape=(1,), dtype=string)\n",
      "tf.Tensor(\n",
      "[[ 0.04284109 -0.04365351  0.02602072  0.011352   -0.00629252 -0.00146711\n",
      "  -0.00050433  0.04409957]], shape=(1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_mv_id_model = tf.keras.Model(inputs=mv_id_input_layer, outputs=mv_id_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"movie_id\"])\n",
    "    print(test_mv_id_model(x[\"movie_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a0e97-c477-4042-b9c0-fcb0f428de0d",
   "metadata": {},
   "source": [
    "#### movie genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f04a0091-d7b0-4f90-ba7c-3eb41dd0b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_genre_input_layer = tf.keras.Input(\n",
    "    name=\"movie_genres\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "mv_genre_lookup = tf.keras.layers.IntegerLookup(\n",
    "    vocabulary=vocab_dict['movie_genres'],\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    oov_value=0,\n",
    ")(mv_genre_input_layer)\n",
    "\n",
    "mv_genre_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['movie_genres']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=MV_EMBEDDING_SIZE\n",
    ")(mv_genre_lookup)\n",
    "\n",
    "mv_genre_embedding = tf.reduce_sum(mv_genre_embedding, axis=-2)\n",
    "\n",
    "# arm_inputs.append(mv_genre_input_layer)\n",
    "# arm_features.append(mv_genre_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51701f0a-9b3e-461c-a9d9-a0c146e310ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[-0.04975596 -0.01630256 -0.00955644  0.03126628  0.04718431  0.04732894\n",
      "   0.03216967 -0.02492697]], shape=(1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_mv_gen_model = tf.keras.Model(inputs=mv_genre_input_layer, outputs=mv_genre_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"movie_genres\"])\n",
    "    print(test_mv_gen_model(x[\"movie_genres\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b41cc9-63f5-4559-a943-1288be9c0892",
   "metadata": {},
   "source": [
    "#### define sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8727904e-e9b6-4005-8cf3-9da461ca88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_per_arm_features(x):\n",
    "    \"\"\"\n",
    "    This function generates a single per-arm observation vector\n",
    "    \"\"\"\n",
    "    mv_id_value = x['movie_id']\n",
    "    mv_gen_value = x['movie_genres'][0]\n",
    "\n",
    "    _mid = test_mv_id_model(mv_id_value)\n",
    "    _mgen = test_mv_gen_model(mv_gen_value)\n",
    "\n",
    "    # to numpy array\n",
    "    _mid = np.array(_mid.numpy()[0])\n",
    "    _mgen = np.array(_mgen.numpy()[0])\n",
    "\n",
    "    # print(_mid)\n",
    "    # print(_mgen)\n",
    "\n",
    "    concat = np.concatenate(\n",
    "        [_mid, _mgen], axis=-1 # -1\n",
    "    ).astype(np.float32)\n",
    "    # concat = tf.concat([_mid, _mgen], axis=-1).astype(np.float32)\n",
    "\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97b42fcb-c62b-49f2-9924-94c184ed1464",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'per_arm_context_sampling_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mper_arm_context_sampling_fn\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'per_arm_context_sampling_fn' is not defined"
     ]
    }
   ],
   "source": [
    "def per_arm_context_sampling_fn():\n",
    "    \"\"\"\n",
    "    This function generates a single per-arm observation vector\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a1344662-b1fc-4d58-9536-cb0b3c08092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'per_arm_context_sampling_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m GLOBAL_DIM \u001b[38;5;241m=\u001b[39m GLOBAL_DIM\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(GLOBAL_DIM)\n\u001b[0;32m----> 5\u001b[0m PER_ARM_DIM \u001b[38;5;241m=\u001b[39m \u001b[43mper_arm_context_sampling_fn\u001b[49m()\n\u001b[1;32m      6\u001b[0m PER_ARM_DIM \u001b[38;5;241m=\u001b[39m PER_ARM_DIM\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(PER_ARM_DIM)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'per_arm_context_sampling_fn' is not defined"
     ]
    }
   ],
   "source": [
    "GLOBAL_DIM = global_context_sampling_fn()\n",
    "GLOBAL_DIM = GLOBAL_DIM.shape[0]\n",
    "print(GLOBAL_DIM)\n",
    "\n",
    "PER_ARM_DIM = per_arm_context_sampling_fn()\n",
    "PER_ARM_DIM = PER_ARM_DIM.shape[0]\n",
    "print(PER_ARM_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc492b-45fd-4c0c-a4c4-452e7d087501",
   "metadata": {},
   "source": [
    "## define reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "201daba8-c751-4e63-b619-45e5a319e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_rewards(element):\n",
    "    \"\"\"Calculates reward for the actions.\"\"\"\n",
    "\n",
    "    def _calc_reward(x):\n",
    "        \"\"\"Calculates reward for a single action.\"\"\"\n",
    "        r0 = lambda: tf.constant(0.0)\n",
    "        r1 = lambda: tf.constant(-10.0)\n",
    "        r2 = lambda: tf.constant(2.0)\n",
    "        r3 = lambda: tf.constant(3.0)\n",
    "        r4 = lambda: tf.constant(4.0)\n",
    "        r5 = lambda: tf.constant(10.0)\n",
    "        c1 = tf.equal(x, 1.0)\n",
    "        c2 = tf.equal(x, 2.0)\n",
    "        c3 = tf.equal(x, 3.0)\n",
    "        c4 = tf.equal(x, 4.0)\n",
    "        c5 = tf.equal(x, 5.0)\n",
    "        return tf.case([(c1, r1), (c2, r2), (c3, r3),(c4, r4),(c5, r5)], default=r0, exclusive=True)\n",
    "\n",
    "    return tf.map_fn(\n",
    "        fn=_calc_reward, \n",
    "        elems=element['user_rating'], \n",
    "        dtype=tf.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ea0a29-3f33-4afe-becf-7718d3b3ff73",
   "metadata": {},
   "source": [
    "### helper functions\n",
    "\n",
    "**TODO:**\n",
    "* modularize in a train_utils or similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f62ecfbf-272c-47bd-9fa8-81d216783998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_global_feature_list(input_features):\n",
    "    \"\"\"Return list of global features.\"\"\"\n",
    "    global_feature_names = ['user_id', 'bucketized_user_age', 'user_occupation_text', 'timestamp']\n",
    "    global_features = []\n",
    "    for global_feature in global_feature_names:\n",
    "        if global_feature in input_features:\n",
    "            global_features.append(input_features[global_feature])\n",
    "        else:\n",
    "            logging.error('Missing global feature %s', global_feature)\n",
    "    return global_features\n",
    "\n",
    "def _get_per_arm_feature_dict(input_features):\n",
    "    \"\"\"Returns a dictionary mapping feature key to per arm features.\"\"\"\n",
    "    per_arm_feature_names = ['movie_id', 'movie_genres']\n",
    "    arm_features = {}\n",
    "    for per_arm_feature in per_arm_feature_names:\n",
    "        if per_arm_feature in inpbut_features:\n",
    "            arm_features[per_arm_feature] = input_features[per_arm_feature]\n",
    "        else:\n",
    "            logging.error('Missing per arm feature %s', per_arm_feature)\n",
    "    return arm_features\n",
    "\n",
    "def _add_outer_dimension(x):\n",
    "    \"\"\"Adds an extra outer dimension.\"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        for key, value in x.items():\n",
    "            x[key] = tf.expand_dims(value, 1)\n",
    "        return x\n",
    "    return tf.expand_dims(x, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed087f72-1e3b-478e-88ca-877741c7ec0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NEW JW \n",
    "\n",
    "#### Creating an environment that iterates through one movie at a time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a8ed57-a7c3-49ad-941f-d9b4690038c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Class implementation of the per-arm MovieLens Bandit environment.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import random\n",
    "from typing import Optional, Text\n",
    "import gin\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.bandits.environments import bandit_py_environment\n",
    "from tf_agents.bandits.environments import dataset_utilities\n",
    "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "\n",
    "GLOBAL_KEY = bandit_spec_utils.GLOBAL_FEATURE_KEY\n",
    "PER_ARM_KEY = bandit_spec_utils.PER_ARM_FEATURE_KEY\n",
    "\n",
    "\n",
    "# @gin.configurable\n",
    "class MovieLensPerArmPyEnvironment(bandit_py_environment.BanditPyEnvironment):\n",
    "    \"\"\"Implements the per-arm version of the MovieLens Bandit environment.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               batch_size,\n",
    "               num_actions,\n",
    "               dataset = train_dataset, \n",
    "               name: Optional[Text] = 'movielens_per_arm'):\n",
    "        \"\"\"Initializes the Per-arm MovieLens Bandit environment.\n",
    "\n",
    "        Args:\n",
    "          data_dir: (string) Directory where the data lies (in text form).\n",
    "          batch_size: (int) Number of observations generated per call.\n",
    "          num_actions: (int) How many movies to choose from per round.\n",
    "        \"\"\"\n",
    "        self._batch_size = batch_size\n",
    "        self._num_actions = num_actions\n",
    "\n",
    "\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(),\n",
    "            dtype=np.int32,\n",
    "            minimum=0,\n",
    "            maximum=num_actions - 1,\n",
    "            name='action')\n",
    "        observation_spec = {\n",
    "            GLOBAL_KEY:\n",
    "                array_spec.ArraySpec(shape=[16], dtype=np.float32), #creating +space for user age and occupation\n",
    "            PER_ARM_KEY:\n",
    "                array_spec.ArraySpec(\n",
    "                    shape=[8], dtype=np.float32), #creating +1 space for movie genre\n",
    "        }\n",
    "        self._time_step_spec = ts.time_step_spec(observation_spec)\n",
    "\n",
    "        self._current_user_indices = np.zeros(batch_size, dtype=np.int32)\n",
    "        self._previous_user_indices = np.zeros(batch_size, dtype=np.int32)\n",
    "\n",
    "        self._current_movie_indices = np.zeros([batch_size, num_actions],\n",
    "                                               dtype=np.int32)\n",
    "        self._previous_movie_indices = np.zeros([batch_size, num_actions],\n",
    "                                                dtype=np.int32)\n",
    "\n",
    "        self._observation = {\n",
    "            GLOBAL_KEY:\n",
    "                np.zeros([batch_size, rank_k+2], dtype=np.int32), #making space like above for dimensions\n",
    "            PER_ARM_KEY:\n",
    "                np.zeros([batch_size, num_actions, rank_k+1], dtype=np.int32),\n",
    "        }\n",
    "\n",
    "        super(MovieLensPerArmPyEnvironment, self).__init__(\n",
    "            observation_spec, self._action_spec, name=name)\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def batched(self):\n",
    "        return True\n",
    "    \n",
    "    def _observe(self):\n",
    "        for x in dataset.batch(1).take(1):\n",
    "            user_id_value = x['user_id']\n",
    "            user_age_value = x['bucketized_user_age']\n",
    "            user_occ_value = x['user_occupation_text']\n",
    "            user_ts_value = x['timestamp']\n",
    "\n",
    "            _id = test_user_id_model(user_id_value)\n",
    "            _age = test_user_age_model(user_age_value)\n",
    "            _occ = test_user_occ_model(user_occ_value)\n",
    "            _ts = test_user_ts_model(user_ts_value)\n",
    "\n",
    "            # # tmp - insepct numpy() values\n",
    "            # print(_id.numpy()) #[0])\n",
    "            # print(_age.numpy()) #[0])\n",
    "            # print(_occ.numpy()) #[0])\n",
    "            # print(_ts.numpy()) #[0])\n",
    "\n",
    "            # to numpy array\n",
    "            _id = np.array(_id.numpy()[0])\n",
    "            _age = np.array(_age.numpy()[0])\n",
    "            _occ = np.array(_occ.numpy()[0])\n",
    "            _ts = np.array(_ts.numpy()[0])\n",
    "\n",
    "            combined_user_features = np.concatenate(\n",
    "                [_id, _age, _occ, _ts], axis=-1\n",
    "            ).astype(np.float32)\n",
    "\n",
    "#         sampled_user_indices = np.random.randint(\n",
    "#             self._num_users, size=self._batch_size)\n",
    "#         self._previous_user_indices = self._current_user_indices\n",
    "#         self._current_user_indices = sampled_user_indices\n",
    "\n",
    "#         sampled_movie_indices = np.array([\n",
    "#             random.sample(range(self._num_movies), self._num_actions)\n",
    "#             for _ in range(self._batch_size)\n",
    "#         ])\n",
    "#         sampled_user_ages = self._user_age_int[sampled_user_indices]\n",
    "#         sampled_user_occ = self._user_occ_int[sampled_user_indices]\n",
    "#         combined_user_features = np.concatenate((self._u_hat[sampled_user_indices]\n",
    "#                                                  , sampled_user_ages.reshape(-1,1)\n",
    "#                                                  , sampled_user_occ.reshape(-1,1)), axis=1)\n",
    "        # current_users = combined_user_features.reshape([self._batch_size, self.rank_k+2])\n",
    "        \n",
    "        movie_index_vector = sampled_movie_indices.reshape(-1)\n",
    "        print(movie_index_vector.shape)\n",
    "        flat_genre_list = self._mov_gen_int[movie_index_vector] #shape of 1\n",
    "        flat_movie_list = self._v_hat[movie_index_vector] #shape of 2\n",
    "        combined_movie_features = np.concatenate((flat_movie_list,flat_genre_list.reshape(-1,1)), axis=1)\n",
    "        current_movies = combined_movie_features.reshape(\n",
    "            [self._batch_size, self._num_actions, self.rank_k+1])\n",
    "\n",
    "        self._previous_movie_indices = self._current_movie_indices\n",
    "        self._current_movie_indices = sampled_movie_indices\n",
    "\n",
    "        batched_observations = {\n",
    "            GLOBAL_KEY:\n",
    "                tf.convert_to_tensor(combined_user_features, dtype=tf.float32),\n",
    "            PER_ARM_KEY:\n",
    "                tf.convert_to_tensor(current_movies, dtype=tf.float32),\n",
    "        }\n",
    "        return batched_observations\n",
    "\n",
    "    def _apply_action(self, action):\n",
    "        chosen_arm_indices = self._current_movie_indices[range(self._batch_size),\n",
    "                                                         action]\n",
    "        return self._approx_ratings_matrix[self._current_user_indices,\n",
    "                                           chosen_arm_indices]\n",
    "\n",
    "    def _rewards_for_all_actions(self):\n",
    "        rewards_matrix = self._approx_ratings_matrix[\n",
    "            np.expand_dims(self._previous_user_indices, axis=-1),\n",
    "            self._previous_movie_indices]\n",
    "        return rewards_matrix\n",
    "\n",
    "    def compute_optimal_action(self):\n",
    "        return np.argmax(self._rewards_for_all_actions(), axis=-1)\n",
    "\n",
    "    def compute_optimal_reward(self):\n",
    "        return np.max(self._rewards_for_all_actions(), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ed427-f438-4158-a1f7-cecf7a968d20",
   "metadata": {},
   "source": [
    "# End New JW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a576f592-6458-45b4-a212-eebd05035a70",
   "metadata": {},
   "source": [
    "### test sampling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "034889d8-83db-4e7f-a361-a55525776236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in train_dataset.batch(1).take(1):\n",
    "#     test_globals = _get_global_context_features(x)\n",
    "\n",
    "# print(test_globals.shape)\n",
    "# test_globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78909d81-48af-4988-b3f6-d34152fe460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in train_dataset.batch(1).take(1):\n",
    "#     test_arms = _get_global_context_features(x)\n",
    "\n",
    "# print(test_arms.shape)\n",
    "# test_arms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb4902-5bb9-4587-8d29-c01d78b006be",
   "metadata": {},
   "source": [
    "## Observation Spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c4746-d01b-4ca4-aa53-ab68da54d37a",
   "metadata": {},
   "source": [
    "**XLA compatibility:**\n",
    "* Since dynamic tensor shapes are not compatible with XLA, the number of arm features (and consequently, number of arms for a step) cannot be dynamic. \n",
    "* One workaround is to fix the maximum number of arms for a problem, then pad the arm features in steps with fewer arms, and use action masking to indicate how many arms are actually active. \n",
    "\n",
    "In this case, the specs change as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c8cf002e-9266-42ff-9dc5-4ca9e7bb7455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PER_ARM_DIM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(num_actions)\n\u001b[1;32m      5\u001b[0m global_spec \u001b[38;5;241m=\u001b[39m tensor_spec\u001b[38;5;241m.\u001b[39mTensorSpec(shape\u001b[38;5;241m=\u001b[39m[GLOBAL_DIM], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m----> 6\u001b[0m per_arm_spec \u001b[38;5;241m=\u001b[39m tensor_spec\u001b[38;5;241m.\u001b[39mTensorSpec(shape\u001b[38;5;241m=\u001b[39m[num_actions, \u001b[43mPER_ARM_DIM\u001b[49m], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# add outer nested dim\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# global_spec = tensor_spec.add_outer_dims_nest(      # add_outer_dim\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#     specs=global_spec,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     outer_dims=[HPARAMS['batch_size']]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     19\u001b[0m observation_spec \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal\u001b[39m\u001b[38;5;124m'\u001b[39m: global_spec, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mper_arm\u001b[39m\u001b[38;5;124m'\u001b[39m: per_arm_spec}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PER_ARM_DIM' is not defined"
     ]
    }
   ],
   "source": [
    "num_actions = HPARAMS['num_docs_to_rank']\n",
    "# num_actions=tf.convert_to_tensor(num_actions, dtype=tf.int32)\n",
    "print(num_actions)\n",
    "\n",
    "global_spec = tensor_spec.TensorSpec(shape=[GLOBAL_DIM], dtype=tf.float32)\n",
    "per_arm_spec = tensor_spec.TensorSpec(shape=[num_actions, PER_ARM_DIM], dtype=tf.float32)\n",
    "\n",
    "# add outer nested dim\n",
    "# global_spec = tensor_spec.add_outer_dims_nest(      # add_outer_dim\n",
    "#     specs=global_spec,\n",
    "#     outer_dims=[HPARAMS['batch_size']]\n",
    "# )\n",
    "# per_arm_spec = tensor_spec.add_outer_dims_nest( # add_outer_dim\n",
    "#     specs=per_arm_spec,\n",
    "#     outer_dims=[HPARAMS['batch_size']]\n",
    "# )\n",
    "\n",
    "\n",
    "observation_spec = {'global': global_spec, 'per_arm': per_arm_spec}\n",
    "observation_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be171882-fd49-4096-85f8-0a851cce1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tf_agents\n",
    "\n",
    "# test_chosen_arm_feats = tf_agents.policies.utils.create_chosen_arm_features_info_spec(observation_spec=observation_spec)\n",
    "# test_chosen_arm_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2da92-7db2-4f42-94a7-b7bad1c8fc42",
   "metadata": {},
   "source": [
    "### Action Spec\n",
    "\n",
    "> The time_step_spec and action_spec are specifications for the input time step and the output action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "549a123c-349a-4103-b39a-4502f47d1e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(2, dtype=int32))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=(), \n",
    "    dtype=tf.int32,\n",
    "    minimum=tf.Variable(0), # 0 \n",
    "    maximum=num_actions-tf.Variable(1), # -1\n",
    "    name=\"action_spec\"\n",
    ")\n",
    "action_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a51efce-9f1b-42d1-bec4-7b788e3fd7e0",
   "metadata": {},
   "source": [
    "### TimeStep spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95f05860-0fbf-4a5a-8273-9c81761e0ea5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'observation_spec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m time_step_spec \u001b[38;5;241m=\u001b[39m ts\u001b[38;5;241m.\u001b[39mtime_step_spec(\n\u001b[0;32m----> 2\u001b[0m     observation_spec \u001b[38;5;241m=\u001b[39m \u001b[43mobservation_spec\u001b[49m, \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# reward_spec = _reward_spec\u001b[39;00m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# time_step_spec.discount\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'observation_spec' is not defined"
     ]
    }
   ],
   "source": [
    "time_step_spec = ts.time_step_spec(\n",
    "    observation_spec = observation_spec, \n",
    "    # reward_spec = _reward_spec\n",
    ")\n",
    "# time_step_spec.discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "014dae31-0308-47e5-9607-3ab5c1eb29ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time_step_spec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtime_step_spec\u001b[49m\u001b[38;5;66;03m#['discount']\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time_step_spec' is not defined"
     ]
    }
   ],
   "source": [
    "time_step_spec#['discount']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9c197-ae9b-461d-8956-f078b929ac12",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "**Note** that contextual bandits form a special case of RL, where the actions taken by the agent do not alter the state of the environment \n",
    "\n",
    "> Contextual refers to the fact that the agent chooses among a set of actions while having knowledge of the context (environment observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075e8a6-11a9-4346-8725-3653fba4bac4",
   "metadata": {},
   "source": [
    "**Possible Agent Types:**\n",
    "\n",
    "```\n",
    "AGENT_TYPE = ['LinUCB', 'LinTS', 'epsGreedy', 'NeuralLinUCB']\n",
    "```\n",
    "\n",
    "**LinearUCBAgent:** (`LinUCB`)\n",
    "* An agent implementing the Linear UCB bandit algorithm\n",
    "* (whitepaper) [A contextual bandit approach to personalized news recommendation](https://arxiv.org/abs/1003.0146)\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/lin_ucb_agent/LinearUCBAgent)\n",
    "\n",
    "**LinearThompsonSamplingAgent:** (`LinTS`)\n",
    "* Implements the Linear Thompson Sampling Agent from the paper: [Thompson Sampling for Contextual Bandits with Linear Payoffs](https://arxiv.org/abs/1209.3352)\n",
    "* the agent maintains two parameters `weight_covariances` and `parameter_estimators`, and updates them based on experience.\n",
    "* The inverse of the weight covariance parameters are updated with the outer product of the observations using the Woodbury inverse matrix update, while the parameter estimators are updated by the reward-weighted observation vectors for every action\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/linear_thompson_sampling_agent/LinearThompsonSamplingAgent)\n",
    "\n",
    "**NeuralEpsilonGreedyAgent:** (`epsGreedy`) \n",
    "* A neural network based epsilon greedy agent\n",
    "* This agent receives a neural network that it trains to predict rewards\n",
    "* The action is chosen greedily with respect to the prediction with probability `1 - epsilon`, and uniformly randomly with probability epsilon\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_epsilon_greedy_agent/NeuralEpsilonGreedyAgent)\n",
    "\n",
    "**NeuralLinUCBAgent:** (`NeuralLinUCB`)\n",
    "* An agent implementing the LinUCB algorithm on top of a neural network\n",
    "* `ENCODING_DIM` is the output dimension of the encoding network \n",
    "> * This output will be used by either a linear reward layer and epsilon greedy exploration, or by a LinUCB logic, depending on the number of training steps executed so far\n",
    "* `EPS_PHASE_STEPS` is the number training steps to run for training the encoding network before switching to `LinUCB`\n",
    "> * If negative, the encoding network is assumed to be already trained\n",
    "> * If the number of steps is less than or equal to `EPS_PHASE_STEPS`, `epsilon greedy` is used, otherwise `LinUCB`\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_linucb_agent/NeuralLinUCBAgent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8e88e-c8ea-4193-a911-0d974ef3b1a3",
   "metadata": {},
   "source": [
    "### network types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547087d-5fad-4521-a584-cb46ce52897d",
   "metadata": {},
   "source": [
    "Which network architecture to use for the `epsGreedy` or `NeuralLinUCB` agents\n",
    "\n",
    "```\n",
    "NETWORK_TYPE = ['commontower', 'dotproduct']\n",
    "```\n",
    "\n",
    "**GlobalAndArmCommonTowerNetwork:** (`commontower`)\n",
    "* This network takes the output of the global and per-arm networks, and leads them through a common network, that in turn outputs reward estimates\n",
    "> * `GLOBAL_LAYERS` - Iterable of ints. Specifies the layers of the global tower\n",
    "> * `ARM_LAYERS` - Iterable of ints. Specifies the layers of the arm tower\n",
    "> * `COMMON_LAYERS` - Iterable of ints. Specifies the layers of the common tower\n",
    "* The network produced by this function can be used either in `GreedyRewardPredictionPolicy`, or `NeuralLinUCBPolicy`\n",
    "> * In the former case, the network must have `output_dim=1`, it is going to be an instance of `QNetwork`, and used in the policy as a reward prediction network\n",
    "> * In the latter case, the network will be an encoding network with its output consumed by a reward layer or a `LinUCB` method. The specified `output_dim` will be the encoding dimension\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmCommonTowerNetwork)\n",
    "\n",
    "**GlobalAndArmDotProductNetwork:** (`dotproduct`)\n",
    "* This network calculates the **dot product** of the output of the global and per-arm networks and returns them as reward estimates\n",
    "> * `GLOBAL_LAYERS` - Iterable of ints. Specifies the layers of the global tower\n",
    "> * `ARM_LAYERS` - Iterable of ints. Specifies the layers of the arm tower\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmDotProductNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3dc270-deb5-4e96-8276-74759a06c318",
   "metadata": {},
   "source": [
    "### define agent and network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d817bf2-1fa6-4bae-af90-745fad996b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENT_TYPE      : epsGreedy\n",
      "NETWORK_TYPE    : dotproduct\n",
      "AGENT_ALPHA     : 0.1\n",
      "EPSILON         : 0.01\n",
      "LR              : 0.05\n",
      "ENCODING_DIM    : 5\n",
      "EPS_PHASE_STEPS : 1000\n",
      "GLOBAL_LAYERS   : [16, 4]\n",
      "ARM_LAYERS      : [16, 4]\n",
      "COMMON_LAYERS   : [4]\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Agents\n",
    "# ================================\n",
    "AGENT_TYPE      = 'epsGreedy' # 'LinUCB' | 'LinTS |, 'epsGreedy' | 'NeuralLinUCB'\n",
    "\n",
    "# Parameters for linear agents (LinUCB and LinTS).\n",
    "AGENT_ALPHA     = 0.1\n",
    "\n",
    "# Parameters for neural agents (NeuralEpsGreedy and NerualLinUCB).\n",
    "EPSILON         = 0.01\n",
    "LR              = 0.05\n",
    "\n",
    "# Parameters for NeuralLinUCB\n",
    "ENCODING_DIM    = 5\n",
    "EPS_PHASE_STEPS = 1000\n",
    "\n",
    "# ================================\n",
    "# Agent's Preprocess Network\n",
    "# ================================\n",
    "NETWORK_TYPE    = \"dotproduct\" # 'commontower' | 'dotproduct'\n",
    "\n",
    "if AGENT_TYPE == 'NeuralLinUCB':\n",
    "    NETWORK_TYPE = 'commontower'\n",
    "    \n",
    "\n",
    "GLOBAL_LAYERS   = [16, 4]\n",
    "ARM_LAYERS      = [16, 4]\n",
    "COMMON_LAYERS   = [4]\n",
    "\n",
    "observation_and_action_constraint_splitter = None\n",
    "\n",
    "print(f\"AGENT_TYPE      : {AGENT_TYPE}\")\n",
    "print(f\"NETWORK_TYPE    : {NETWORK_TYPE}\")\n",
    "print(f\"AGENT_ALPHA     : {AGENT_ALPHA}\")\n",
    "print(f\"EPSILON         : {EPSILON}\")\n",
    "print(f\"LR              : {LR}\")\n",
    "print(f\"ENCODING_DIM    : {ENCODING_DIM}\")\n",
    "print(f\"EPS_PHASE_STEPS : {EPS_PHASE_STEPS}\")\n",
    "print(f\"GLOBAL_LAYERS   : {GLOBAL_LAYERS}\")\n",
    "print(f\"ARM_LAYERS      : {ARM_LAYERS}\")\n",
    "print(f\"COMMON_LAYERS   : {COMMON_LAYERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db165067-6e9d-4b79-b675-bae69ec98c10",
   "metadata": {},
   "source": [
    "### Agent Factory\n",
    "\n",
    "**TODO:**\n",
    "* consolidate agent, network, and hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c350049d-0e84-4c2a-9658-b671e2b9fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf_agents.bandits.agents import greedy_reward_prediction_agent\n",
    "\n",
    "# network = None\n",
    "# observation_and_action_constraint_splitter = None\n",
    "\n",
    "# # global_step = tf.Variable(0)\n",
    "# global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "# if HPARAMS['network_type'] == 'commontower':\n",
    "#     network = global_and_arm_feature_network.create_feed_forward_common_tower_network(\n",
    "#         observation_spec = observation_spec, \n",
    "#         global_layers = HPARAMS['global_layers'], \n",
    "#         arm_layers = HPARAMS['per_arm_layers'], \n",
    "#         common_layers = HPARAMS['common_layers'],\n",
    "#         # output_dim = 1\n",
    "#     )\n",
    "# elif HPARAMS['network_type'] == 'dotproduct':\n",
    "#     network = global_and_arm_feature_network.create_feed_forward_dot_product_network(\n",
    "#         observation_spec = observation_spec, \n",
    "#         global_layers = HPARAMS['global_layers'], \n",
    "#         arm_layers = HPARAMS['per_arm_layers']\n",
    "#     )\n",
    "    \n",
    "# # agent = greedy_reward_prediction_agent.GreedyRewardPredictionAgent()\n",
    "    \n",
    "# agent = neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(\n",
    "#     time_step_spec=time_step_spec,\n",
    "#     action_spec=action_spec,\n",
    "#     reward_network=network,\n",
    "#     optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=HPARAMS['learning_rate']),\n",
    "#     epsilon=HPARAMS['epsilon'],\n",
    "#     observation_and_action_constraint_splitter=(\n",
    "#         observation_and_action_constraint_splitter\n",
    "#     ),\n",
    "#     accepts_per_arm_features=True,\n",
    "#     emit_policy_info=policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN,\n",
    "#     train_step_counter=global_step,\n",
    "#     info_fields_to_inherit_from_greedy=[\n",
    "#         policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN\n",
    "#     ],\n",
    "#     name='OffpolicyNeuralEpsGreedyAgent'\n",
    "# )\n",
    "# agent.initialize()\n",
    "\n",
    "# print(f\"Agent: {agent.name}\\n\")\n",
    "# if network:\n",
    "#     print(f\"Network: {network.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6cb60f0b-90b7-49ab-9c41-046ea00ab750",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'per_arm_tf_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 25\u001b[0m\n\u001b[1;32m     14\u001b[0m     agent \u001b[38;5;241m=\u001b[39m lin_ts_agent\u001b[38;5;241m.\u001b[39mLinearThompsonSamplingAgent(\n\u001b[1;32m     15\u001b[0m         time_step_spec\u001b[38;5;241m=\u001b[39mper_arm_tf_env\u001b[38;5;241m.\u001b[39mtime_step_spec(),\n\u001b[1;32m     16\u001b[0m         action_spec\u001b[38;5;241m=\u001b[39mper_arm_tf_env\u001b[38;5;241m.\u001b[39maction_spec(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m AGENT_TYPE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsGreedy\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 25\u001b[0m     obs_spec \u001b[38;5;241m=\u001b[39m \u001b[43mper_arm_tf_env\u001b[49m\u001b[38;5;241m.\u001b[39mobservation_spec()\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m NETWORK_TYPE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommontower\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     27\u001b[0m         network \u001b[38;5;241m=\u001b[39m global_and_arm_feature_network\u001b[38;5;241m.\u001b[39mcreate_feed_forward_common_tower_network(\n\u001b[1;32m     28\u001b[0m             observation_spec \u001b[38;5;241m=\u001b[39m obs_spec, \n\u001b[1;32m     29\u001b[0m             global_layers \u001b[38;5;241m=\u001b[39m GLOBAL_LAYERS, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;66;03m# output_dim = 1\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'per_arm_tf_env' is not defined"
     ]
    }
   ],
   "source": [
    "from tf_agents.bandits.policies import policy_utilities\n",
    "\n",
    "network = None\n",
    "\n",
    "if AGENT_TYPE == 'LinUCB':\n",
    "    agent = lin_ucb_agent.LinearUCBAgent(\n",
    "        time_step_spec=per_arm_tf_env.time_step_spec(),\n",
    "        action_spec=per_arm_tf_env.action_spec(),\n",
    "        alpha=AGENT_ALPHA,\n",
    "        accepts_per_arm_features=True,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "elif AGENT_TYPE == 'LinTS':\n",
    "    agent = lin_ts_agent.LinearThompsonSamplingAgent(\n",
    "        time_step_spec=per_arm_tf_env.time_step_spec(),\n",
    "        action_spec=per_arm_tf_env.action_spec(),\n",
    "        alpha=AGENT_ALPHA,\n",
    "        observation_and_action_constraint_splitter=(\n",
    "            observation_and_action_constraint_splitter\n",
    "        ),\n",
    "        accepts_per_arm_features=True,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "elif AGENT_TYPE == 'epsGreedy':\n",
    "    obs_spec = per_arm_tf_env.observation_spec()\n",
    "    if NETWORK_TYPE == 'commontower':\n",
    "        network = global_and_arm_feature_network.create_feed_forward_common_tower_network(\n",
    "            observation_spec = obs_spec, \n",
    "            global_layers = GLOBAL_LAYERS, \n",
    "            arm_layers = ARM_LAYERS, \n",
    "            common_layers = COMMON_LAYERS,\n",
    "            # output_dim = 1\n",
    "        )\n",
    "    elif NETWORK_TYPE == 'dotproduct':\n",
    "        network = global_and_arm_feature_network.create_feed_forward_dot_product_network(\n",
    "            observation_spec = obs_spec, \n",
    "            global_layers = GLOBAL_LAYERS, \n",
    "            arm_layers = ARM_LAYERS\n",
    "        )\n",
    "    agent = neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(\n",
    "        time_step_spec=per_arm_tf_env.time_step_spec(),\n",
    "        action_spec=per_arm_tf_env.action_spec(),\n",
    "        reward_network=network,\n",
    "        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n",
    "        epsilon=EPSILON,\n",
    "        observation_and_action_constraint_splitter=(\n",
    "            observation_and_action_constraint_splitter\n",
    "        ),\n",
    "        accepts_per_arm_features=True,\n",
    "        emit_policy_info=policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN,\n",
    "    )\n",
    "\n",
    "elif AGENT_TYPE == 'NeuralLinUCB':\n",
    "    obs_spec = per_arm_tf_env.observation_spec()\n",
    "    network = (\n",
    "        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n",
    "            observation_spec = obs_spec, \n",
    "            global_layers = GLOBAL_LAYERS, \n",
    "            arm_layers = ARM_LAYERS, \n",
    "            common_layers = COMMON_LAYERS,\n",
    "            output_dim = ENCODING_DIM\n",
    "        )\n",
    "    )\n",
    "    agent = neural_linucb_agent.NeuralLinUCBAgent(\n",
    "        time_step_spec=per_arm_tf_env.time_step_spec(),\n",
    "        action_spec=per_arm_tf_env.action_spec(),\n",
    "        encoding_network=network,\n",
    "        encoding_network_num_train_steps=EPS_PHASE_STEPS,\n",
    "        encoding_dim=ENCODING_DIM,\n",
    "        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n",
    "        alpha=1.0,\n",
    "        gamma=1.0,\n",
    "        epsilon_greedy=EPSILON,\n",
    "        accepts_per_arm_features=True,\n",
    "        debug_summaries=True,\n",
    "        summarize_grads_and_vars=True,\n",
    "        emit_policy_info=policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN,\n",
    "    )\n",
    "    \n",
    "print(f\"Agent: {agent.name}\\n\")\n",
    "\n",
    "if network:\n",
    "    print(f\"Network: {network.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ce1a6d02-f041-4a4a-a1b6-48a92a0b2eaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pprint(\u001b[43magent\u001b[49m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mtrajectory_spec)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "pprint(agent.policy.trajectory_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d161bf2-a8ce-4ebc-ae90-a311d2c2a1f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining data spec: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43magent\u001b[49m\u001b[38;5;241m.\u001b[39mtraining_data_spec)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "print('training data spec: ', agent.training_data_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb800f43-c743-4ff9-9605-dbcacfa79792",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation spec in training: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43magent\u001b[49m\u001b[38;5;241m.\u001b[39mtraining_data_spec\u001b[38;5;241m.\u001b[39mobservation)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "print('observation spec in training: ', agent.training_data_spec.observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d4a95eb-4a48-4a4b-a014-609ebc3feee2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchosen arm features: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43magent\u001b[49m\u001b[38;5;241m.\u001b[39mtraining_data_spec\u001b[38;5;241m.\u001b[39mpolicy_info\u001b[38;5;241m.\u001b[39mchosen_arm_features)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "print('chosen arm features: ', agent.training_data_spec.policy_info.chosen_arm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452a333f-00b1-4f06-a8f7-42af78f505f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TimeStep Spec (for each batch):\\n\", agent.time_step_spec, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47365427-31c7-4f7a-ae51-e1177fd5544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action Spec (for each batch):\\n\", agent.action_spec, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c312e0-d2fa-4b59-8aa9-0e8ef238e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(agent.policy.trajectory_spec) # TODO check observation between this and next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04682f4-32bb-4729-af41-6a0cd0a7ce56",
   "metadata": {},
   "source": [
    "### replay buffers (wip)\n",
    "\n",
    "* Note that when the replay buffer object is initialized, it requires the `data_spec` of the elements that it will store. * This spec corresponds to the TensorSpec of trajectory elements that will be added to the buffer\n",
    "* This spec is usually acquired by looking at an agent's `agent.collect_data_spec` which defines the shapes, types, and structures expected by the agent when training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c83fc93-ebef-4893-b3fc-ac459eaf41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reverb\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f456bbd-01d2-4978-9eab-6eebf9e995fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.collect_data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c95fcc-50b4-4563-b8e7-14b2ccd204d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_signature = tensor_spec.from_spec(\n",
    "      agent.collect_data_spec\n",
    ")\n",
    "replay_buffer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798082cb-3c6a-4347-bbb5-f72adf8d9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "      replay_buffer_signature\n",
    ")\n",
    "replay_buffer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ecf824-e9ce-447b-95e3-db63fac28182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_name = 'uniform_table'\n",
    "# replay_buffer_capacity = 2000 # @param {type:\"integer\"}\n",
    "\n",
    "# table = reverb.Table(\n",
    "#     table_name,\n",
    "#     max_size=replay_buffer_capacity,\n",
    "#     sampler=reverb.selectors.Uniform(),\n",
    "#     remover=reverb.selectors.Fifo(),\n",
    "#     rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "#     signature=replay_buffer_signature\n",
    "# )\n",
    "# reverb_server = reverb.Server([table])\n",
    "\n",
    "# replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "#     agent.collect_data_spec,\n",
    "#     table_name=table_name,\n",
    "#     sequence_length=None,\n",
    "#     local_server=reverb_server\n",
    "# )\n",
    "\n",
    "# rb_observer = reverb_utils.ReverbAddEpisodeObserver(\n",
    "#     replay_buffer.py_client,\n",
    "#     table_name,\n",
    "#     replay_buffer_capacity\n",
    "# )\n",
    "\n",
    "# replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d3bc9-489c-44b7-849a-6d57edbe3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 15                 # @param {type:\"integer\"}\n",
    "collect_episodes_per_iteration = 2  # @param {type:\"integer\"}\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "learning_rate = 1e-3   # @param {type:\"number\"}\n",
    "log_interval = 25      # @param {type:\"integer\"}\n",
    "num_eval_episodes = 10 # @param {type:\"integer\"}\n",
    "eval_interval = 50     # @param {type:\"integer\"}\n",
    "\n",
    "def collect_episode(\n",
    "    environment, policy, num_episodes\n",
    "):    \n",
    "    driver = py_driver.PyDriver(\n",
    "        environment,\n",
    "        py_tf_eager_policy.PyTFEagerPolicy(\n",
    "            policy, \n",
    "            use_tf_function=True\n",
    "        ),\n",
    "        [rb_observer],\n",
    "        max_episodes=num_episodes\n",
    "    )\n",
    "    initial_time_step = environment.reset()\n",
    "    driver.run(initial_time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59703a-9a00-4eba-a3b7-45b21b738dd4",
   "metadata": {},
   "source": [
    "## trajectory function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3605d9f-d239-4871-9a61-6e9b21f9c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "# replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "#       replay_buffer_signature\n",
    "# )\n",
    "\n",
    "\n",
    "def _trajectory_fn(element): # hparams\n",
    "    \"\"\"Converts a dataset element into a trajectory.\"\"\"\n",
    "    global_features = _get_global_context_features(element)\n",
    "    arm_features = _get_per_arm_features(element)\n",
    "    \n",
    "    # # tmp \n",
    "    # print(f\"global_features: {global_features}\")\n",
    "    # print(f\"arm_features: {arm_features}\")\n",
    "    \n",
    "    # Adds a time dimension.\n",
    "    # arm_features = _add_outer_dimension(arm_features)\n",
    "    # arm_features = tensor_spec.add_outer_dim(arm_features)\n",
    "    \n",
    "    # obs spec\n",
    "    observation = {\n",
    "        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n",
    "            global_features,\n",
    "            # _add_outer_dimension(global_features)\n",
    "        # bandit_spec_utils.PER_ARM_FEATURE_KEY:\n",
    "        #     _add_outer_dimension(arm_features),\n",
    "    }\n",
    "    # print(\"after adding extra dim...\")\n",
    "    # print(f\"observation: {observation}\")\n",
    "    # print(f\"arm_features: {arm_features}\")\n",
    "    \n",
    "    # reward = tensor_spec.add_outer_dim(_get_rewards(element))\n",
    "    reward = _get_rewards(element)\n",
    "    # print(f\"reward: {reward}\")\n",
    "    \n",
    "    # To emit the predicted rewards in policy_info, we need to create dummy\n",
    "    # rewards to match the definition in TensorSpec for the ones specified in\n",
    "    # emit_policy_info set.\n",
    "    # dummy_rewards = tf.zeros([HPARAMS['batch_size'], 1, HPARAMS['num_docs_to_rank']])\n",
    "    # dummy_rewards = tf.zeros([HPARAMS['batch_size'], HPARAMS['num_docs_to_rank']])\n",
    "    dummy_rewards = tf.zeros([HPARAMS['num_docs_to_rank']])\n",
    "    policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "        chosen_arm_features=arm_features,\n",
    "        # Pass dummy mean rewards here to match the model_spec for emitting\n",
    "        # mean rewards in policy info\n",
    "        predicted_rewards_mean=dummy_rewards\n",
    "    )\n",
    "    \n",
    "# tf_agents.policies.utils.create_chosen_arm_features_info_spec(\n",
    "#     observation_spec: tf_agents.typing.types.NestedTensorSpec\n",
    "# ) -> tf_agents.typing.types.NestedTensorSpec\n",
    "    \n",
    "    if HPARAMS['model_type'] == 'neural_ucb':\n",
    "        policy_info = policy_info._replace(\n",
    "            predicted_rewards_optimistic=dummy_rewards\n",
    "        )\n",
    "        \n",
    "    # print(f\"observation: {observation}\")\n",
    "    # print(f\"reward: {reward}\")\n",
    "    # print(f\"policy_info: {policy_info}\")\n",
    "    # print(f\"dummy_rewards: {dummy_rewards}\")\n",
    "    \n",
    "    return trajectory.single_step(\n",
    "        observation=observation,\n",
    "        action=tf.zeros_like(\n",
    "            reward, dtype=tf.int32\n",
    "        ),  # Arm features are copied from policy info, put dummy zeros here\n",
    "        policy_info=policy_info,\n",
    "        reward=reward,\n",
    "        discount=tf.zeros_like(reward)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ce30c23-0d5b-4de6-b701-e899bd383c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_trajectory_from_environment(element): # hparams\n",
    "#     \"\"\"Converts a dataset element into a trajectory.\"\"\"\n",
    "    \n",
    "#     orig_trajectory = agent.policy.trajectory_spec\n",
    "#     # print(f\"orig_trajectory.step_type: {orig_trajectory.step_type}\")\n",
    "#     # print(f\"orig_trajectory.next_step_type: {orig_trajectory.next_step_type}\")\n",
    "    \n",
    "#     global_features = _get_global_context_features(element)\n",
    "#     arm_features = _get_per_arm_features(element)\n",
    "    \n",
    "#     # Adds a time dimension.\n",
    "#     # arm_features = _add_outer_dimension(arm_features)\n",
    "#     observation = {\n",
    "#         bandit_spec_utils.GLOBAL_FEATURE_KEY:\n",
    "#             global_features,\n",
    "#             # _add_outer_dimension(global_features)\n",
    "#         # bandit_spec_utils.PER_ARM_FEATURE_KEY:\n",
    "#         #     _add_outer_dimension(arm_features),\n",
    "#     }\n",
    "    \n",
    "#     # reward = _add_outer_dimension(_get_rewards(element))\n",
    "#     reward = _get_rewards(element)\n",
    "#     # print(f\"reward:  {reward}\")\n",
    "#     # print(f\"reward shape:  {tf.shape(reward).numpy()}\")\n",
    "    \n",
    "#     reward_2 = tf.expand_dims(reward, 0)\n",
    "#     # print(f\"reward_2:  {reward_2}\")\n",
    "#     # print(f\"reward_2 shape:  {tf.shape(reward_2).numpy()}\")\n",
    "    \n",
    "    \n",
    "#     dummy_rewards = tf.zeros([HPARAMS['num_docs_to_rank']])\n",
    "#     policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "#         chosen_arm_features=arm_features,\n",
    "#         predicted_rewards_mean=dummy_rewards\n",
    "#     )\n",
    "#     if HPARAMS['model_type'] == 'neural_ucb':\n",
    "#         policy_info = policy_info._replace(\n",
    "#             predicted_rewards_optimistic=dummy_rewards\n",
    "#         )\n",
    "        \n",
    "#     # observation\n",
    "#     obs = observation['global']\n",
    "#     # print(f\"obs:  {obs}\")\n",
    "#     # print(f\"obs shape:  {tf.shape(obs).numpy()}\")\n",
    "    \n",
    "#     obs_2 = tf.expand_dims(obs, 0)\n",
    "#     # print(f\"obs_2:  {obs_2}\")\n",
    "#     # print(f\"obs_2 shape:  {tf.shape(obs_2).numpy()}\")\n",
    "    \n",
    "#     return trajectory.Trajectory(\n",
    "#         observation=obs_2,\n",
    "#         action=tf.zeros_like(\n",
    "#             reward_2, dtype=tf.int32\n",
    "#         ),  # Arm features are copied from policy info, put dummy zeros here\n",
    "#         policy_info=policy_info,\n",
    "#         reward=reward_2,\n",
    "#         discount=tf.zeros_like(reward_2),\n",
    "#         step_type=orig_trajectory.step_type,\n",
    "#         next_step_type=orig_trajectory.next_step_type\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ec78cf5-6e75-4935-91e6-8040d6faae67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trajectories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_dict_from_trajectory\u001b[39m(\n\u001b[1;32m      2\u001b[0m     step: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m      3\u001b[0m     next_step: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m----> 4\u001b[0m     trajectory: \u001b[43mtrajectories\u001b[49m\u001b[38;5;241m.\u001b[39mTrajectory) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Builds a dict from `trajectory` data.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    A dict holding the same data as `trajectory`.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     trajectory_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: [step]\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscount\u001b[39m\u001b[38;5;124m\"\u001b[39m: trajectory\u001b[38;5;241m.\u001b[39mdiscount\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m     23\u001b[0m     }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trajectories' is not defined"
     ]
    }
   ],
   "source": [
    "def build_dict_from_trajectory(\n",
    "    step: int,\n",
    "    next_step: int,\n",
    "    trajectory: trajectories.Trajectory) -> Dict[str, Any]:\n",
    "    \"\"\"Builds a dict from `trajectory` data.\n",
    "\n",
    "    Args:\n",
    "    trajectory: A `trajectories.Trajectory` object.\n",
    "\n",
    "    Returns:\n",
    "    A dict holding the same data as `trajectory`.\n",
    "    \"\"\"\n",
    "    trajectory_dict = {\n",
    "        \"step_type\": [step].numpy(),\n",
    "        \"observation\": [{\n",
    "            \"observation_batch\": batch\n",
    "        } for batch in trajectory.observation.numpy().tolist()],\n",
    "        \"action\": trajectory.action.numpy().tolist(),\n",
    "        \"policy_info\": trajectory.policy_info,\n",
    "        \"next_step_type\": [next_step],\n",
    "        \"reward\": trajectory.reward.numpy().tolist(),\n",
    "        \"discount\": trajectory.discount.numpy().tolist(),\n",
    "    }\n",
    "    return trajectory_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc024910-1632-4ac2-9b31-88344d7d8c16",
   "metadata": {},
   "source": [
    "### write trajectories to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "189a8503-5048-40f1-ba4d-f42d89f02e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'v1'\n",
    "\n",
    "DATASET_FILE = f'{VERSION}-off-policy-trajectories.json'\n",
    "!touch $DATASET_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c79e742-39e5-4823-b7a4-594b72785a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_size: 20000\n",
      "small_count: 200.0\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(list(train_dataset))\n",
    "print(f\"dataset_size: {dataset_size}\")\n",
    "\n",
    "small_count = dataset_size/100\n",
    "print(f\"small_count: {small_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "783d0cb9-d1b9-43b5-aafe-5e67f6462a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "def write_trajectories_to_file(\n",
    "    dataset_size: int,\n",
    "    data_file: str,\n",
    "    batch_size: int,\n",
    "):\n",
    "    batched_dataset = train_dataset.batch(batch_size)\n",
    "    print(f\"writting file...\")\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    step = 1\n",
    "    with open(data_file, \"w\") as f:\n",
    "        for x in batched_dataset.take(count=dataset_size):\n",
    "            # print(f\"step: {step}\")\n",
    "            nexx_step = step + 1\n",
    "            # print(f\"nexx_step: {nexx_step}\")\n",
    "\n",
    "            single_traj = get_trajectory_from_environment(x)\n",
    "            print(single_traj)\n",
    "            \n",
    "            _trajectory_dict = build_dict_from_trajectory(step=step, next_step=nexx_step, trajectory=single_traj)\n",
    "            # print(type(trajectory_dict))\n",
    "            decoded = _trajectory_dict.decode('utf-8')\n",
    "            print(f\"decoded: {decoded}\")\n",
    "            data_list.append(_trajectory_dict)\n",
    "\n",
    "            step+=1\n",
    "            \n",
    "            break\n",
    "            \n",
    "        for entry in data_list:\n",
    "            traj_dict_tmp = {}\n",
    "            traj_dict_tmp['step_type'] = entry['step_type']\n",
    "            traj_dict_tmp['observation'] = entry['observation']\n",
    "            traj_dict_tmp['action'] = entry['action']\n",
    "            traj_dict_tmp['policy_info'] = entry['policy_info']\n",
    "            traj_dict_tmp['next_step_type'] = entry['next_step_type']\n",
    "            traj_dict_tmp['reward'] = entry['reward']\n",
    "            traj_dict_tmp['discount'] = entry['discount']\n",
    "            \n",
    "            # f.write(json.dumps(traj_dict_tmp) + \"\\n\")\n",
    "            \n",
    "        print(f\"writting to file complete...\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime_mins = int((end_time - start_time) / 60)\n",
    "    print(f\"runtime_mins: {runtime_mins}\")\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f3d70e9d-b2b4-4f07-af9b-a76d4207d84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writting file...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_trajectory_from_environment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample_data_list \u001b[38;5;241m=\u001b[39m \u001b[43mwrite_trajectories_to_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msmall_count\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATASET_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# sample_data_list[0]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# sample_data_list[0]['observation']\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[57], line 22\u001b[0m, in \u001b[0;36mwrite_trajectories_to_file\u001b[0;34m(dataset_size, data_file, batch_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m nexx_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print(f\"nexx_step: {nexx_step}\")\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m single_traj \u001b[38;5;241m=\u001b[39m \u001b[43mget_trajectory_from_environment\u001b[49m(x)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(single_traj)\n\u001b[1;32m     25\u001b[0m _trajectory_dict \u001b[38;5;241m=\u001b[39m build_dict_from_trajectory(step\u001b[38;5;241m=\u001b[39mstep, next_step\u001b[38;5;241m=\u001b[39mnexx_step, trajectory\u001b[38;5;241m=\u001b[39msingle_traj)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_trajectory_from_environment' is not defined"
     ]
    }
   ],
   "source": [
    "sample_data_list = write_trajectories_to_file(\n",
    "    dataset_size=int(small_count),\n",
    "    data_file=DATASET_FILE,\n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "# sample_data_list[0]\n",
    "# sample_data_list[0]['observation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d7d797-0408-4d09-b90e-471f42b9336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_utils.upload_blob(\n",
    "#     bucket_name='',\n",
    "#     source_file_name=,\n",
    "#     destination_blob_name=f'{RUN_NAME}/candidates/xxxx.json'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbe97d9-3708-43d0-bffe-66442e022782",
   "metadata": {},
   "source": [
    "### validate shapes and dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a66037-1b38-444d-b81e-a3ec5cb55db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.utils import nest_utils\n",
    "\n",
    "nest_utils.is_batched_nested_tensors(\n",
    "    tensors=single_traj.policy_info.chosen_arm_features,\n",
    "    specs=agent.training_data_spec.policy_info.chosen_arm_features,\n",
    "    num_outer_dims=1,\n",
    "    allow_extra_fields=False,\n",
    "    check_dtypes=True\n",
    ")\n",
    "\n",
    "# nest_utils.is_batched_nested_tensors(\n",
    "#     tensors=single_traj.observation['global'],\n",
    "#     specs=agent.training_data_spec.observation['global'],\n",
    "#     num_outer_dims=0,\n",
    "#     allow_extra_fields=False,\n",
    "#     check_dtypes=True\n",
    "# )\n",
    "\n",
    "# nest_utils.is_batched_nested_tensors(\n",
    "#     tensors=single_traj.action,\n",
    "#     specs=agent.training_data_spec.action,\n",
    "#     num_outer_dims=1,\n",
    "#     allow_extra_fields=False,\n",
    "#     check_dtypes=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "01c19d89-60c6-49cb-b3bd-393e460028a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arm_observations = per_arm_context_sampling_fn()\n",
    "# print(arm_observations)\n",
    "\n",
    "# outer_rank = nest_utils.get_outer_rank(tensors = arm_observations, specs = observation_spec['per_arm'])\n",
    "# outer_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a25dda1-4504-4ce0-bce8-a1ac92e1446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per_arm_spec = tensor_spec.add_outer_dims_nest( # add_outer_dim\n",
    "#     specs=per_arm_spec,\n",
    "#     outer_dims=[HPARAMS['batch_size']]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b94fd-fb61-4ed5-9467-1b0be6f091e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(thing['global'].numpy().tolist())\n",
    "test = thing['global'].numpy().tolist()\n",
    "print(f\"test: {len(test)}\")\n",
    "print(tf.shape(test).numpy())\n",
    "\n",
    "thingy = tf.expand_dims(test, axis=0)\n",
    "print(f\"thingy: {thingy}\")\n",
    "print(tf.shape(thingy).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b3d3c-75f7-46f4-9a1b-6329e419b7f5",
   "metadata": {},
   "source": [
    "## train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb7ffc-00df-4d46-857b-86c87b78f597",
   "metadata": {},
   "source": [
    "`agent.train(experience=...)`\n",
    "\n",
    "where `experience` is a batch of trajectories data in the form of a Trajectory. \n",
    "* The structure of experience must match that of `self.training_data_spec`. \n",
    "* All tensors in experience must be shaped [batch, time, ...] where time must be equal to self.train_step_length if that property is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4680f3-693b-4530-8e4d-88bc43036c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from tf_agents.utils import common\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "# global_step = tf.compat.v1.train.get_global_step()\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "train_loss = collections.defaultdict(list)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    iterator = iter(train_dataset.batch(2))\n",
    "    data = next(iterator)\n",
    "    pprint(f\"print data: {data}\")\n",
    "    \n",
    "    trajectories = _trajectory_fn(data)\n",
    "    pprint(f\"print trajectories: {trajectories}\")\n",
    "    \n",
    "    # All tensors in experience must be shaped [batch, time, ...] \n",
    "    step = agent.train_step_counter.numpy()\n",
    "    loss = agent.train(experience=trajectories)\n",
    "    \n",
    "    # break\n",
    "    \n",
    "    \n",
    "#     for x in train_dataset.batch(1).take(1): #HPARAMS['batch_size']).take(1):\n",
    "#         # print(f\"print X: {len(x)}\")\n",
    "#         # break\n",
    "#         step = agent.train_step_counter.numpy()\n",
    "#         print(f\"step X: {step}\")\n",
    "#         trajectories = _trajectory_fn(x)\n",
    "#         # print(f\"print trajectories: {trajectories}\")\n",
    "#         # break\n",
    "    \n",
    "#         loss = agent.train(experience=trajectories)\n",
    "\n",
    "#         train_loss[f\"epoch:{epoch + 1}\"].append(loss.numpy())\n",
    "    \n",
    "# train_outputs = collections.namedtuple(\n",
    "#     \"TrainOutputs\",[\"policy\", \"train_loss\"]\n",
    "# )\n",
    "\n",
    "# train_outputs(agent.policy, train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cb14ae-955d-4ceb-b390-6e0d76ba105d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7f174b-81da-4e90-9c43-1917cb163068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3a67e-ca5c-4064-b20e-77916880e266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4828dd9a-61e5-43cf-8eaf-46a314ddebcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad98430-d836-40f1-8e4f-eb46f0c91fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108ed33c-d8ef-4524-97e9-ced9e01c154b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c98398-d545-4dcc-a71f-32249b5f0336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba65417f-54e0-436e-b0d4-93cffb981ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1d819-0152-437c-aefc-45ffd34d808a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "workbench-notebooks.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m109"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "local-conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
