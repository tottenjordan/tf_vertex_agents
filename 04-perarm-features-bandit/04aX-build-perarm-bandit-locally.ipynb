{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385307d6-2058-47ec-8da0-57f7ef5c43d6",
   "metadata": {},
   "source": [
    "# Train Bandits with per-arm features\n",
    "\n",
    "**Exploring linear and nonlinear** (e.g., those with neural network-based value functions) bandit methods for recommendations using TF-Agents\n",
    "\n",
    "> Neural linear bandits provide a nice way to leverage the representation power of deep learning and the bandit approach for uncertainty measure and efficient exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fd257-b98b-426a-a2cd-024429b014f1",
   "metadata": {},
   "source": [
    "## Load notebook config\n",
    "\n",
    "* use the prefix defined in `00-env-setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39c9d08-d118-4013-a47f-88450f49f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'mabv1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908f6b95-b539-4a9f-a836-840d26ea3b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"mabv1\"\n",
      "VERSION                  = \"v1\"\n",
      "\n",
      "BUCKET_NAME              = \"mabv1-hybrid-vertex-bucket\"\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://mabv1-hybrid-vertex-bucket/data\"\n",
      "BUCKET_URI               = \"gs://mabv1-hybrid-vertex-bucket\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BIGQUERY_DATASET_ID      = \"hybrid-vertex.movielens_dataset_mabv1\"\n",
      "BIGQUERY_TABLE_ID        = \"hybrid-vertex.movielens_dataset_mabv1.training_dataset\"\n",
      "\n",
      "REPO_DOCKER_PATH_PREFIX  = \"src\"\n",
      "RL_SUB_DIR               = \"per_arm_rl\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c690a9-e2bd-4759-ba41-4e2469098aee",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d0dfe4-695c-4dd4-9f24-67f7488ce1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c669f1a1-1af7-4efb-ab2d-6bf3b3847991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, List, Optional, TypeVar\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pprint import pprint\n",
    "import pickle as pkl\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# google cloud\n",
    "from google.cloud import aiplatform, storage\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "# from tf_agents.agents import TFAgent\n",
    "\n",
    "# from tf_agents.bandits.environments import stationary_stochastic_per_arm_py_environment as p_a_env\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "# from tf_agents.drivers import dynamic_step_driver\n",
    "# from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "# from tf_agents.bandits.agents import lin_ucb_agent\n",
    "# from tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\n",
    "from tf_agents.bandits.agents import neural_epsilon_greedy_agent\n",
    "from tf_agents.bandits.agents import neural_linucb_agent\n",
    "from tf_agents.bandits.networks import global_and_arm_feature_network\n",
    "from tf_agents.bandits.policies import policy_utilities\n",
    "\n",
    "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "# GPU\n",
    "from numba import cuda \n",
    "import gc\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# this repo\n",
    "from src.per_arm_rl import data_utils\n",
    "from src.per_arm_rl import data_config\n",
    "\n",
    "# tf exceptions and vars\n",
    "if tf.__version__[0] != \"2\":\n",
    "    raise Exception(\"The trainer only runs with TensorFlow version 2.\")\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e58dd7-ab2b-419f-9771-bf1e98db758b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "# gpus\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4969d3e-1fc0-45db-8a69-aa6b342019de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "274e7f4a-1802-4946-888e-876638f5c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a866b1-85b9-43e6-9546-edfbbf886bce",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5b953-14c0-42ed-a511-77147a1bc0ac",
   "metadata": {},
   "source": [
    "### Read TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d0157c8-a04c-4dbd-b6d9-a1ede97687a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c934c06-bf08-4c7f-b0cc-0de04ef3515c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://mabv1-hybrid-vertex-bucket/data/val/ml-ratings-100k-val.tfrecord']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT = \"val\" # \"train\" | \"val\"\n",
    "\n",
    "train_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'{DATA_GCS_PREFIX}/{SPLIT}'):\n",
    "    if '.tfrecord' in blob.name:\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "        \n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7511e4d-bf81-4800-bde7-8b16dec9aeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([25.], dtype=float32)>,\n",
      " 'movie_genres': <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[4]])>,\n",
      " 'movie_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'211'], dtype=object)>,\n",
      " 'timestamp': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([874948475])>,\n",
      " 'user_id': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'346'], dtype=object)>,\n",
      " 'user_occupation_text': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'other'], dtype=object)>,\n",
      " 'user_rating': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([4.], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "train_dataset = train_dataset.map(data_utils.parse_tfrecord)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8959967-522e-41c8-9a1b-050ca8bc191f",
   "metadata": {},
   "source": [
    "### get vocab\n",
    "\n",
    "**TODO:** \n",
    "* streamline vocab calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b9169bc-d6dc-497e-9dff-6ebb175282ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATE_VOCABS: False\n"
     ]
    }
   ],
   "source": [
    "GENERATE_VOCABS = False\n",
    "print(f\"GENERATE_VOCABS: {GENERATE_VOCABS}\")\n",
    "\n",
    "VOCAB_SUBDIR   = \"vocabs\"\n",
    "VOCAB_FILENAME = \"vocab_dict.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3ccf137-7a72-42e7-aa89-3c81a99cf40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vocab...\n",
      "Downloaded vocab from: gs://mabv1-hybrid-vertex-bucket/vocabs/vocab_dict.pkl\n",
      "\n",
      "'movie_id'\n",
      "'user_id'\n",
      "'user_occupation_text'\n",
      "'movie_genres'\n",
      "'bucketized_user_age'\n",
      "'max_timestamp'\n",
      "'min_timestamp'\n",
      "'timestamp_buckets'\n"
     ]
    }
   ],
   "source": [
    "if not GENERATE_VOCABS:\n",
    "\n",
    "    EXISTING_VOCAB_FILE = f'gs://{BUCKET_NAME}/{VOCAB_SUBDIR}/{VOCAB_FILENAME}'\n",
    "    print(f\"Downloading vocab...\")\n",
    "    \n",
    "    os.system(f'gsutil -q cp {EXISTING_VOCAB_FILE} .')\n",
    "    print(f\"Downloaded vocab from: {EXISTING_VOCAB_FILE}\\n\")\n",
    "\n",
    "    filehandler = open(VOCAB_FILENAME, 'rb')\n",
    "    vocab_dict = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "    for key in vocab_dict.keys():\n",
    "        pprint(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccaef62-882a-46ff-a1b1-3837e69fdf74",
   "metadata": {},
   "source": [
    "## helper functions\n",
    "\n",
    "**TODO:**\n",
    "* modularize in a train_utils or similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cba2bb14-bf94-466b-b60f-8c7d96c7aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _get_global_feature_list(input_features):\n",
    "#     \"\"\"Return list of global features.\"\"\"\n",
    "#     global_feature_names = ['user_id', 'bucketized_user_age', 'user_occupation_text', 'timestamp']\n",
    "#     global_features = []\n",
    "#     for global_feature in global_feature_names:\n",
    "#         if global_feature in input_features:\n",
    "#             global_features.append(input_features[global_feature])\n",
    "#         else:\n",
    "#             logging.error('Missing global feature %s', global_feature)\n",
    "#     return global_features\n",
    "\n",
    "# def _get_per_arm_feature_dict(input_features):\n",
    "#     \"\"\"Returns a dictionary mapping feature key to per arm features.\"\"\"\n",
    "#     per_arm_feature_names = ['movie_id', 'movie_genres']\n",
    "#     arm_features = {}\n",
    "#     for per_arm_feature in per_arm_feature_names:\n",
    "#         if per_arm_feature in input_features:\n",
    "#             arm_features[per_arm_feature] = input_features[per_arm_feature]\n",
    "#         else:\n",
    "#             logging.error('Missing per arm feature %s', per_arm_feature)\n",
    "#     return arm_features\n",
    "\n",
    "def _add_outer_dimension(x):\n",
    "    \"\"\"Adds an extra outer dimension.\"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        for key, value in x.items():\n",
    "            x[key] = tf.expand_dims(value, 1)\n",
    "        return x\n",
    "    return tf.expand_dims(x, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b941063b-ad48-4817-aef0-9afa8a444632",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits with Per-Arm Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28770b8d-836b-448d-8dd1-203d76fc6d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf_agents.bandits.agents import lin_ucb_agent\n",
    "# from tf_agents.bandits.environments import stationary_stochastic_per_arm_py_environment as p_a_env\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "# from tf_agents.drivers import dynamic_step_driver\n",
    "# from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "nest = tf.nest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0138a295-2b0d-4359-8496-af8552ec8cff",
   "metadata": {},
   "source": [
    "## Preprocessing layers for global and arm features\n",
    "\n",
    "The preproccesing layers will ultimately feed the two functions described below, both of which will ultimately feed the `Environment`\n",
    "\n",
    "`global_context_sampling_fn`: \n",
    "* A function that outputs a random 1d array or list of ints or floats\n",
    "* This output is the global context. Its shape and type must be consistent across calls.\n",
    "\n",
    "`arm_context_sampling_fn`: \n",
    "* A function that outputs a random 1 array or list of ints or floats (same type as the output of `global_context_sampling_fn`). * This output is the per-arm context. Its shape must be consistent across calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8d77956-635c-438a-916a-185eec52f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OOV_BUCKETS        = 1\n",
    "GLOBAL_EMBEDDING_SIZE  = 4\n",
    "MV_EMBEDDING_SIZE      = 8 #32\n",
    "\n",
    "# HPARAMS = { # TODO - explain these and their options\n",
    "#     \"batch_size\": 8,\n",
    "#     \"num_actions\": 3,\n",
    "#     \"model_type\": \"neural_epsilon_greedy\",\n",
    "#     \"network_type\": 'commontower',\n",
    "#     \"global_layers\": [16,4],\n",
    "#     \"per_arm_layers\": [16,4],\n",
    "#     \"common_layers\": [4],\n",
    "#     \"learning_rate\": 0.05,\n",
    "#     \"epsilon\":0.01,\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142e63e-0a20-4d51-997c-7a4733517f7e",
   "metadata": {},
   "source": [
    "### global context (user) features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195acd92-06b6-42e4-bef7-798fd09da856",
   "metadata": {},
   "source": [
    "#### user ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c28e887b-421a-4603-8899-87071056783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_input_layer = tf.keras.Input(\n",
    "    name=\"user_id\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.string\n",
    ")\n",
    "\n",
    "user_id_lookup = tf.keras.layers.StringLookup(\n",
    "    max_tokens=len(vocab_dict['user_id']) + NUM_OOV_BUCKETS,\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    mask_token=None,\n",
    "    vocabulary=vocab_dict['user_id'],\n",
    ")(user_id_input_layer)\n",
    "\n",
    "user_id_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['user_id']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_id_lookup)\n",
    "\n",
    "user_id_embedding = tf.reduce_sum(user_id_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_id_input_layer)\n",
    "# global_features.append(user_id_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d6a0fe7-26cb-4c62-a3ef-17f98e6ccddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'346'], shape=(1,), dtype=string)\n",
      "tf.Tensor([[-0.04257477 -0.0204685  -0.03652425 -0.0370932 ]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_user_id_model = tf.keras.Model(inputs=user_id_input_layer, outputs=user_id_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"user_id\"])\n",
    "    print(test_user_id_model(x[\"user_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352d2227-92ec-4386-926f-df2fdb9434ec",
   "metadata": {},
   "source": [
    "#### user AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70785bf0-5ece-4875-ab72-06d9c45ea9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_age_input_layer = tf.keras.Input(\n",
    "    name=\"bucketized_user_age\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "user_age_lookup = tf.keras.layers.IntegerLookup(\n",
    "    vocabulary=vocab_dict['bucketized_user_age'],\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    oov_value=0,\n",
    ")(user_age_input_layer)\n",
    "\n",
    "user_age_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['bucketized_user_age']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_age_lookup)\n",
    "\n",
    "user_age_embedding = tf.reduce_sum(user_age_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_age_input_layer)\n",
    "# global_features.append(user_age_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e01622a-9418-4ca7-8925-9b0ebef8940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([25.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([[-0.01470114  0.03408908 -0.01681744  0.04299467]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_user_age_model = tf.keras.Model(inputs=user_age_input_layer, outputs=user_age_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"bucketized_user_age\"])\n",
    "    print(test_user_age_model(x[\"bucketized_user_age\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997ffaa8-ca92-4851-b7e3-bb06fba8958b",
   "metadata": {},
   "source": [
    "#### user OCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03e7344d-71fb-423a-89dd-f1abeb270e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_occ_input_layer = tf.keras.Input(\n",
    "    name=\"user_occupation_text\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.string\n",
    ")\n",
    "\n",
    "user_occ_lookup = tf.keras.layers.StringLookup(\n",
    "    max_tokens=len(vocab_dict['user_occupation_text']) + NUM_OOV_BUCKETS,\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    mask_token=None,\n",
    "    vocabulary=vocab_dict['user_occupation_text'],\n",
    ")(user_occ_input_layer)\n",
    "\n",
    "user_occ_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['user_occupation_text']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_occ_lookup)\n",
    "\n",
    "user_occ_embedding = tf.reduce_sum(user_occ_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_occ_input_layer)\n",
    "# global_features.append(user_occ_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39cbbc31-ca43-4f8f-a804-a4b830e99d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'other'], shape=(1,), dtype=string)\n",
      "tf.Tensor([[-0.04057591 -0.01261982  0.04479137 -0.04452629]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_user_occ_model = tf.keras.Model(inputs=user_occ_input_layer, outputs=user_occ_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"user_occupation_text\"])\n",
    "    print(test_user_occ_model(x[\"user_occupation_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee0098-a48a-4de6-88bf-6219ce8c0533",
   "metadata": {},
   "source": [
    "#### user Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61a4e01a-e742-4c68-93a9-aa66eb9a5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ts_input_layer = tf.keras.Input(\n",
    "    name=\"timestamp\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.int64\n",
    ")\n",
    "\n",
    "user_ts_lookup = tf.keras.layers.Discretization(\n",
    "    vocab_dict['timestamp_buckets'].tolist()\n",
    ")(user_ts_input_layer)\n",
    "\n",
    "user_ts_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['timestamp_buckets'].tolist()) + NUM_OOV_BUCKETS,\n",
    "    output_dim=GLOBAL_EMBEDDING_SIZE\n",
    ")(user_ts_lookup)\n",
    "\n",
    "user_ts_embedding = tf.reduce_sum(user_ts_embedding, axis=-2)\n",
    "\n",
    "# global_inputs.append(user_ts_input_layer)\n",
    "# global_features.append(user_ts_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db99f90b-57f8-45e6-9f28-871658e17358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([874948475], shape=(1,), dtype=int64)\n",
      "tf.Tensor([[-0.04684504  0.03435652 -0.02922553  0.01416435]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_user_ts_model = tf.keras.Model(inputs=user_ts_input_layer, outputs=user_ts_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"timestamp\"])\n",
    "    print(test_user_ts_model(x[\"timestamp\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc734ea-cb5e-4c6b-8b94-2a8853220178",
   "metadata": {},
   "source": [
    "#### define global sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff58c380-8b53-4dfa-b5b4-d36853638ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_global_context_features(x):\n",
    "    \"\"\"\n",
    "    This function generates a single global observation vector.\n",
    "    \"\"\"\n",
    "    user_id_value = x['user_id']\n",
    "    user_age_value = x['bucketized_user_age']\n",
    "    user_occ_value = x['user_occupation_text']\n",
    "    user_ts_value = x['timestamp']\n",
    "\n",
    "    _id = test_user_id_model(user_id_value) # input_tensor=tf.Tensor(shape=(4,), dtype=float32)\n",
    "    _age = test_user_age_model(user_age_value)\n",
    "    _occ = test_user_occ_model(user_occ_value)\n",
    "    _ts = test_user_ts_model(user_ts_value)\n",
    "\n",
    "    # # tmp - insepct numpy() values\n",
    "    # print(_id.numpy()) #[0])\n",
    "    # print(_age.numpy()) #[0])\n",
    "    # print(_occ.numpy()) #[0])\n",
    "    # print(_ts.numpy()) #[0])\n",
    "\n",
    "    # to numpy array\n",
    "    _id = np.array(_id.numpy()[0])\n",
    "    _age = np.array(_age.numpy()[0])\n",
    "    _occ = np.array(_occ.numpy()[0])\n",
    "    _ts = np.array(_ts.numpy()[0])\n",
    "\n",
    "    concat = np.concatenate(\n",
    "        [_id, _age, _occ, _ts], axis=-1 # -1\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bba133ab-bf12-4b3b-926d-6d1dba940837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL_DIM: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.04257477, -0.0204685 , -0.03652425, -0.0370932 , -0.01470114,\n",
       "        0.03408908, -0.01681744,  0.04299467, -0.04057591, -0.01261982,\n",
       "        0.04479137, -0.04452629, -0.04684504,  0.03435652, -0.02922553,\n",
       "        0.01416435], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in train_dataset.batch(1).take(1):\n",
    "    test_globals = _get_global_context_features(x)\n",
    "\n",
    "GLOBAL_DIM = test_globals.shape[0]\n",
    "print(f\"GLOBAL_DIM: {GLOBAL_DIM}\")\n",
    "\n",
    "test_globals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249fa771-35d7-4d04-ab68-2b70911bac17",
   "metadata": {},
   "source": [
    "### arm preprocessing layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534b3bf1-a2ea-4bfb-8c77-efa057f4e391",
   "metadata": {},
   "source": [
    "#### movie ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa53cbe9-2616-4da4-90dc-dc5616258af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_id_input_layer = tf.keras.Input(\n",
    "    name=\"movie_id\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.string\n",
    ")\n",
    "\n",
    "mv_id_lookup = tf.keras.layers.StringLookup(\n",
    "    max_tokens=len(vocab_dict['movie_id']) + NUM_OOV_BUCKETS,\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    mask_token=None,\n",
    "    vocabulary=vocab_dict['movie_id'],\n",
    ")(mv_id_input_layer)\n",
    "\n",
    "mv_id_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['movie_id']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=MV_EMBEDDING_SIZE\n",
    ")(mv_id_lookup)\n",
    "\n",
    "mv_id_embedding = tf.reduce_sum(mv_id_embedding, axis=-2)\n",
    "\n",
    "# arm_inputs.append(mv_id_input_layer)\n",
    "# arm_features.append(mv_id_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bd19f09-a12e-4a21-a1a1-5ec5bc116559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'211'], shape=(1,), dtype=string)\n",
      "tf.Tensor(\n",
      "[[-0.04094781  0.03707052 -0.02573409 -0.02102193 -0.0301262  -0.00709243\n",
      "   0.03189244  0.02086892]], shape=(1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_mv_id_model = tf.keras.Model(inputs=mv_id_input_layer, outputs=mv_id_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"movie_id\"])\n",
    "    print(test_mv_id_model(x[\"movie_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a0e97-c477-4042-b9c0-fcb0f428de0d",
   "metadata": {},
   "source": [
    "#### movie genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f04a0091-d7b0-4f90-ba7c-3eb41dd0b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_genre_input_layer = tf.keras.Input(\n",
    "    name=\"movie_genres\",\n",
    "    shape=(1,),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "mv_genre_lookup = tf.keras.layers.IntegerLookup(\n",
    "    vocabulary=vocab_dict['movie_genres'],\n",
    "    num_oov_indices=NUM_OOV_BUCKETS,\n",
    "    oov_value=0,\n",
    ")(mv_genre_input_layer)\n",
    "\n",
    "mv_genre_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=len(vocab_dict['movie_genres']) + NUM_OOV_BUCKETS,\n",
    "    output_dim=MV_EMBEDDING_SIZE\n",
    ")(mv_genre_lookup)\n",
    "\n",
    "mv_genre_embedding = tf.reduce_sum(mv_genre_embedding, axis=-2)\n",
    "\n",
    "# arm_inputs.append(mv_genre_input_layer)\n",
    "# arm_features.append(mv_genre_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51701f0a-9b3e-461c-a9d9-a0c146e310ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[-0.00483013 -0.03928108  0.0106586   0.01059937  0.00203977  0.00540863\n",
      "  -0.04190093  0.03950352]], shape=(1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_mv_gen_model = tf.keras.Model(inputs=mv_genre_input_layer, outputs=mv_genre_embedding)\n",
    "\n",
    "for x in train_dataset.batch(1).take(1):\n",
    "    print(x[\"movie_genres\"])\n",
    "    print(test_mv_gen_model(x[\"movie_genres\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b41cc9-63f5-4559-a943-1288be9c0892",
   "metadata": {},
   "source": [
    "#### define sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8727904e-e9b6-4005-8cf3-9da461ca88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_per_arm_features(x):\n",
    "    \"\"\"\n",
    "    This function generates a single per-arm observation vector\n",
    "    \"\"\"\n",
    "    mv_id_value = x['movie_id']\n",
    "    mv_gen_value = x['movie_genres'][0]\n",
    "\n",
    "    _mid = test_mv_id_model(mv_id_value)\n",
    "    _mgen = test_mv_gen_model(mv_gen_value)\n",
    "\n",
    "    # to numpy array\n",
    "    _mid = np.array(_mid.numpy()[0])\n",
    "    _mgen = np.array(_mgen.numpy()[0])\n",
    "\n",
    "    # print(_mid)\n",
    "    # print(_mgen)\n",
    "\n",
    "    concat = np.concatenate(\n",
    "        [_mid, _mgen], axis=-1 # -1\n",
    "    ).astype(np.float32)\n",
    "    # concat = tf.concat([_mid, _mgen], axis=-1).astype(np.float32)\n",
    "\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97b42fcb-c62b-49f2-9924-94c184ed1464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER_ARM_DIM: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.04094781,  0.03707052, -0.02573409, -0.02102193, -0.0301262 ,\n",
       "       -0.00709243,  0.03189244,  0.02086892, -0.00483013, -0.03928108,\n",
       "        0.0106586 ,  0.01059937,  0.00203977,  0.00540863, -0.04190093,\n",
       "        0.03950352], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in train_dataset.batch(1).take(1):\n",
    "    test_arms = _get_per_arm_features(x)\n",
    "\n",
    "PER_ARM_DIM = test_arms.shape[0]\n",
    "print(f\"PER_ARM_DIM: {PER_ARM_DIM}\")\n",
    "\n",
    "test_arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1344662-b1fc-4d58-9536-cb0b3c08092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL_DIM = global_context_sampling_fn()\n",
    "# GLOBAL_DIM = GLOBAL_DIM.shape[0]\n",
    "# print(GLOBAL_DIM)\n",
    "\n",
    "# PER_ARM_DIM = per_arm_context_sampling_fn()\n",
    "# PER_ARM_DIM = PER_ARM_DIM.shape[0]\n",
    "# print(PER_ARM_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6836c-67b7-4fd4-917a-24ddad708edd",
   "metadata": {},
   "source": [
    "## TF-Agents implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877c79c-b6c8-4048-b1ce-05f011e8d69e",
   "metadata": {},
   "source": [
    "In TF-Agents, the *per-arm features* implementation differs from the *global-only* feature examples in the following aspects:\n",
    "* Reward is modeled not per-arm, but globally.\n",
    "* The arms are permutation invariant: it doesn’t matter which arm is arm 1 or arm 2, only their features.\n",
    "* One can have a different number of arms to choose from in every step (note that unspecified/dynamically changing number of arms will have a problem with XLA compatibility).\n",
    "\n",
    "When implementing per-arm features in TF-Bandits, the following details have to be discussed:\n",
    "* Observation spec and observations,\n",
    "* Action spec and actions,\n",
    "* Implementation of specific policies and agents.\n",
    "\n",
    "\n",
    "**TODO:**\n",
    "* outline the components and highlight their interactions, dependencies on eachother, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ff9baaf-987d-448d-a981-742a79b581e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE  : 8\n",
      "NUM_ACTIONS : 1\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE  = 8\n",
    "NUM_ACTIONS = 1 \n",
    "\n",
    "print(f\"BATCH_SIZE  : {BATCH_SIZE}\")\n",
    "print(f\"NUM_ACTIONS : {NUM_ACTIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb4902-5bb9-4587-8d29-c01d78b006be",
   "metadata": {},
   "source": [
    "## Tensor Specs\n",
    "\n",
    "**TODO:**\n",
    "* explain relationship between Tensor Specs and their Tensor counterparts\n",
    "* highlight the errors, lessons learned, and utility functions to address these"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68ebe8-116d-43b3-a6e1-4f5a5c7f4741",
   "metadata": {},
   "source": [
    "### Observation spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c4746-d01b-4ca4-aa53-ab68da54d37a",
   "metadata": {},
   "source": [
    "**This observation spec allows the user to have a global observation of fixed dimension**, and an unspecified number of *per-arm* features (also of fixed dimension)\n",
    "* The actions output by the policy are still integers as usual, and they indicate which row of the arm-features it has chosen \n",
    "* The action spec must be a single integer value without boundaries:\n",
    "\n",
    "```python\n",
    "global_spec = tensor_spec.TensorSpec([GLOBAL_DIM], tf.float32)\n",
    "per_arm_spec = tensor_spec.TensorSpec([None, PER_ARM_DIM], tf.float32)\n",
    "observation_spec = {'global': global_spec, 'per_arm': per_arm_spec}\n",
    "\n",
    "action_spec = tensor_spec.TensorSpec((), tf.int32)\n",
    "```\n",
    "> Here the only difference compared to the action spec with global features only is that the tensor spec is not bounded, as we don’t know how many arms there will be at any time step\n",
    "\n",
    "**XLA compatibility:**\n",
    "* Since dynamic tensor shapes are not compatible with XLA, the number of arm features (and consequently, number of arms for a step) cannot be dynamic. \n",
    "* One workaround is to fix the maximum number of arms for a problem, then pad the arm features in steps with fewer arms, and use action masking to indicate how many arms are actually active.\n",
    "\n",
    "```python\n",
    "per_arm_spec = tensor_spec.TensorSpec([NUM_ACTIONS, PER_ARM_DIM], tf.float32)\n",
    "\n",
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=(), dtype=tf.int32, minimum = 0, maximum = NUM_ACTIONS - 1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8cf002e-9266-42ff-9dc5-4ca9e7bb7455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_spec = tensor_spec.TensorSpec(shape=[GLOBAL_DIM], dtype=tf.float32)\n",
    "# per_arm_spec = tensor_spec.TensorSpec(shape=[NUM_ACTIONS, PER_ARM_DIM], dtype=tf.float32)\n",
    "\n",
    "# add outer nested dim\n",
    "# global_spec = tensor_spec.add_outer_dims_nest(  # add_outer_dim\n",
    "#     specs=global_spec,\n",
    "#     outer_dims=[HPARAMS['batch_size']]\n",
    "# )\n",
    "# per_arm_spec = tensor_spec.add_outer_dims_nest( # add_outer_dim\n",
    "#     specs=per_arm_spec,\n",
    "#     outer_dims=[HPARAMS['batch_size']]\n",
    "# )\n",
    "\n",
    "# observation_spec = {'global': global_spec, 'per_arm': per_arm_spec}\n",
    "# observation_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "36bd3b33-635a-4274-8b9e-7172696ebb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(16,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(1, 16), dtype=tf.float32, name=None),\n",
       " 'num_actions': BoundedTensorSpec(shape=(), dtype=tf.int32, name=None, minimum=array(1, dtype=int32), maximum=array(1, dtype=int32))}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-Agents has many helper and utility functions\n",
    "observation_spec = bandit_spec_utils.create_per_arm_observation_spec(\n",
    "    GLOBAL_DIM, PER_ARM_DIM, NUM_ACTIONS, \n",
    "    add_num_actions_feature=True\n",
    ") # 2,3,4\n",
    "\n",
    "observation_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2da92-7db2-4f42-94a7-b7bad1c8fc42",
   "metadata": {},
   "source": [
    "### Action spec\n",
    "\n",
    "> The time_step_spec and action_spec are specifications for the input time step and the output action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af42c7-18d7-480f-a7f3-d7da3f4840eb",
   "metadata": {},
   "source": [
    "```python\n",
    "    if (\n",
    "        not tensor_spec.is_bounded(action_spec)\n",
    "        or not tensor_spec.is_discrete(action_spec)\n",
    "        or action_spec.shape.rank > 1\n",
    "        or action_spec.shape.num_elements() != 1\n",
    "    ):\n",
    "      raise NotImplementedError(\n",
    "          'action_spec must be a BoundedTensorSpec of type int32 and shape (). '\n",
    "          'Found {}.'.format(action_spec)\n",
    "      )\n",
    "```\n",
    "\n",
    "* [src](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/policies/reward_prediction_base_policy.py#L97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "549a123c-349a-4103-b39a-4502f47d1e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(flat_action_spec): 1\n",
      "flat_action_spec     : [BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(0, dtype=int32))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(0, dtype=int32))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    shape=(), \n",
    "    dtype=tf.int32,\n",
    "    minimum=tf.constant(0),             # 0 \n",
    "    maximum=NUM_ACTIONS-tf.constant(1), # -1\n",
    "    name=\"action_spec\"\n",
    ")\n",
    "\n",
    "# # len() should not be > 1\n",
    "# flat_action_spec = tf.nest.flatten(action_spec)\n",
    "# print(f\"len(flat_action_spec): {len(flat_action_spec)}\")\n",
    "# print(f\"flat_action_spec     : {flat_action_spec}\")\n",
    "\n",
    "# action_spec = flat_action_spec[0]\n",
    "action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36ee2635-d4e3-4468-886c-ae9c62e3c80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected_num_actions: 1\n",
      "predicted_rewards_mean: TensorSpec(shape=(1,), dtype=tf.float32, name=None)\n"
     ]
    }
   ],
   "source": [
    "expected_num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "print(f\"expected_num_actions: {expected_num_actions}\")\n",
    "\n",
    "predicted_rewards_mean = tensor_spec.TensorSpec([expected_num_actions])\n",
    "print(f\"predicted_rewards_mean: {predicted_rewards_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a51efce-9f1b-42d1-bec4-7b788e3fd7e0",
   "metadata": {},
   "source": [
    "### TimeStep spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95f05860-0fbf-4a5a-8273-9c81761e0ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': {'global': TensorSpec(shape=(16,), dtype=tf.float32, name=None),\n",
       "                 'per_arm': TensorSpec(shape=(1, 16), dtype=tf.float32, name=None)},\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec = ts.time_step_spec(\n",
    "    observation_spec = observation_spec, \n",
    "    # reward_spec = _reward_spec\n",
    ")\n",
    "time_step_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9181303-6565-45f5-a293-08d50420a805",
   "metadata": {},
   "source": [
    "### Inspect chosen arm features spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6b526a5-997c-4621-b21c-82c6ca1d6a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': TensorSpec(shape=(16,), dtype=tf.float32, name=None),\n",
       " 'per_arm': TensorSpec(shape=(1, 16), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90d3e03e-619e-4fa4-b817-f02b4cefd6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(16,), dtype=tf.float32, name=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_arm_features_info = (\n",
    "  policy_utilities.create_chosen_arm_features_info_spec(\n",
    "      time_step_spec.observation,\n",
    "  )\n",
    ")\n",
    "chosen_arm_features_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a4a2a33-c7d4-4d81-8082-80dfc3596741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO - dont understand this\n",
    "bandit_policy_type = (\n",
    "    policy_utilities.create_bandit_policy_type_tensor_spec(shape=[1])\n",
    ")\n",
    "bandit_policy_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e7ac02b-18bd-4519-92fe-5541b1a8ddc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorSpec(shape=(1,), dtype=tf.float32, name=None), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=BoundedTensorSpec(shape=(1,), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32)), chosen_arm_features=TensorSpec(shape=(16,), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_spec = policy_utilities.PerArmPolicyInfo(\n",
    "  predicted_rewards_mean=predicted_rewards_mean,\n",
    "  bandit_policy_type=bandit_policy_type,\n",
    "  chosen_arm_features=chosen_arm_features_info,\n",
    ")\n",
    "info_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9c197-ae9b-461d-8956-f078b929ac12",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Agent\n",
    "\n",
    "**Note** that contextual bandits form a special case of RL, where the actions taken by the agent do not alter the state of the environment \n",
    "\n",
    "> “Contextual” refers to the fact that the agent chooses among a set of actions while having knowledge of the context (environment observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aadb01-eb5c-4870-ae14-9e66624ba594",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Agent types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075e8a6-11a9-4346-8725-3653fba4bac4",
   "metadata": {},
   "source": [
    "**Possible Agent Types:**\n",
    "\n",
    "```\n",
    "AGENT_TYPE = ['LinUCB', 'LinTS', 'epsGreedy', 'NeuralLinUCB']\n",
    "```\n",
    "\n",
    "**LinearUCBAgent:** (`LinUCB`)\n",
    "* An agent implementing the Linear UCB bandit algorithm\n",
    "* (whitepaper) [A contextual bandit approach to personalized news recommendation](https://arxiv.org/abs/1003.0146)\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/lin_ucb_agent/LinearUCBAgent)\n",
    "\n",
    "**LinearThompsonSamplingAgent:** (`LinTS`)\n",
    "* Implements the Linear Thompson Sampling Agent from the paper: [Thompson Sampling for Contextual Bandits with Linear Payoffs](https://arxiv.org/abs/1209.3352)\n",
    "* the agent maintains two parameters `weight_covariances` and `parameter_estimators`, and updates them based on experience.\n",
    "* The inverse of the weight covariance parameters are updated with the outer product of the observations using the Woodbury inverse matrix update, while the parameter estimators are updated by the reward-weighted observation vectors for every action\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/linear_thompson_sampling_agent/LinearThompsonSamplingAgent)\n",
    "\n",
    "**NeuralEpsilonGreedyAgent:** (`epsGreedy`) \n",
    "* A neural network based epsilon greedy agent\n",
    "* This agent receives a neural network that it trains to predict rewards\n",
    "* The action is chosen greedily with respect to the prediction with probability `1 - epsilon`, and uniformly randomly with probability epsilon\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_epsilon_greedy_agent/NeuralEpsilonGreedyAgent)\n",
    "\n",
    "**NeuralLinUCBAgent:** (`NeuralLinUCB`)\n",
    "* An agent implementing the LinUCB algorithm on top of a neural network\n",
    "* `ENCODING_DIM` is the output dimension of the encoding network \n",
    "> * This output will be used by either a linear reward layer and epsilon greedy exploration, or by a LinUCB logic, depending on the number of training steps executed so far\n",
    "* `EPS_PHASE_STEPS` is the number training steps to run for training the encoding network before switching to `LinUCB`\n",
    "> * If negative, the encoding network is assumed to be already trained\n",
    "> * If the number of steps is less than or equal to `EPS_PHASE_STEPS`, `epsilon greedy` is used, otherwise `LinUCB`\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_linucb_agent/NeuralLinUCBAgent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8e88e-c8ea-4193-a911-0d974ef3b1a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### network types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547087d-5fad-4521-a584-cb46ce52897d",
   "metadata": {},
   "source": [
    "Which network architecture to use for the `epsGreedy` or `NeuralLinUCB` agents\n",
    "\n",
    "```\n",
    "NETWORK_TYPE = ['commontower', 'dotproduct']\n",
    "```\n",
    "\n",
    "**GlobalAndArmCommonTowerNetwork:** (`commontower`)\n",
    "* This network takes the output of the global and per-arm networks, and leads them through a common network, that in turn outputs reward estimates\n",
    "> * `GLOBAL_LAYERS` - Iterable of ints. Specifies the layers of the global tower\n",
    "> * `ARM_LAYERS` - Iterable of ints. Specifies the layers of the arm tower\n",
    "> * `COMMON_LAYERS` - Iterable of ints. Specifies the layers of the common tower\n",
    "* The network produced by this function can be used either in `GreedyRewardPredictionPolicy`, or `NeuralLinUCBPolicy`\n",
    "> * In the former case, the network must have `output_dim=1`, it is going to be an instance of `QNetwork`, and used in the policy as a reward prediction network\n",
    "> * In the latter case, the network will be an encoding network with its output consumed by a reward layer or a `LinUCB` method. The specified `output_dim` will be the encoding dimension\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmCommonTowerNetwork)\n",
    "\n",
    "**GlobalAndArmDotProductNetwork:** (`dotproduct`)\n",
    "* This network calculates the **dot product** of the output of the global and per-arm networks and returns them as reward estimates\n",
    "> * `GLOBAL_LAYERS` - Iterable of ints. Specifies the layers of the global tower\n",
    "> * `ARM_LAYERS` - Iterable of ints. Specifies the layers of the arm tower\n",
    "* [docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/networks/global_and_arm_feature_network/GlobalAndArmDotProductNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3dc270-deb5-4e96-8276-74759a06c318",
   "metadata": {},
   "source": [
    "### define agent and network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d817bf2-1fa6-4bae-af90-745fad996b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 8,\n",
      " 'common_layers': [4],\n",
      " 'epsilon': 0.01,\n",
      " 'global_layers': [16, 4],\n",
      " 'learning_rate': 0.05,\n",
      " 'model_type': 'epsGreedy',\n",
      " 'network_type': 'dotproduct',\n",
      " 'num_actions': 1,\n",
      " 'per_arm_layers': [16, 4]}\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Agents\n",
    "# ================================\n",
    "AGENT_TYPE      = 'epsGreedy' # 'LinUCB' | 'LinTS |, 'epsGreedy' | 'NeuralLinUCB'\n",
    "\n",
    "# Parameters for linear agents (LinUCB and LinTS).\n",
    "AGENT_ALPHA     = 0.1\n",
    "\n",
    "# Parameters for neural agents (NeuralEpsGreedy and NerualLinUCB).\n",
    "EPSILON         = 0.01\n",
    "LR              = 0.05\n",
    "\n",
    "# Parameters for NeuralLinUCB\n",
    "ENCODING_DIM    = 1\n",
    "EPS_PHASE_STEPS = 1000\n",
    "\n",
    "# ================================\n",
    "# Agent's Preprocess Network\n",
    "# ================================\n",
    "NETWORK_TYPE    = \"dotproduct\" # 'commontower' | 'dotproduct'\n",
    "\n",
    "if AGENT_TYPE == 'NeuralLinUCB':\n",
    "    NETWORK_TYPE = 'commontower'\n",
    "    \n",
    "\n",
    "GLOBAL_LAYERS   = [16, 4]\n",
    "ARM_LAYERS      = [16, 4]\n",
    "COMMON_LAYERS   = [4]\n",
    "\n",
    "observation_and_action_constraint_splitter = None\n",
    "\n",
    "HPARAMS = {  # TODO - streamline and consolidate\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"num_actions\": NUM_ACTIONS,\n",
    "    \"model_type\": AGENT_TYPE,\n",
    "    \"network_type\": NETWORK_TYPE,\n",
    "    \"global_layers\": GLOBAL_LAYERS,\n",
    "    \"per_arm_layers\": ARM_LAYERS,\n",
    "    \"common_layers\": COMMON_LAYERS,\n",
    "    \"learning_rate\": LR,\n",
    "    \"epsilon\": EPSILON,\n",
    "}\n",
    "pprint(HPARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db165067-6e9d-4b79-b675-bae69ec98c10",
   "metadata": {},
   "source": [
    "### Agent Factory\n",
    "\n",
    "**TODO:**\n",
    "* consolidate agent, network, and hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350049d-0e84-4c2a-9658-b671e2b9fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf_agents.bandits.agents import greedy_reward_prediction_agent\n",
    "\n",
    "# network = None\n",
    "# observation_and_action_constraint_splitter = None\n",
    "\n",
    "# # global_step = tf.Variable(0)\n",
    "# global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "# if HPARAMS['network_type'] == 'commontower':\n",
    "#     network = global_and_arm_feature_network.create_feed_forward_common_tower_network(\n",
    "#         observation_spec = observation_spec, \n",
    "#         global_layers = HPARAMS['global_layers'], \n",
    "#         arm_layers = HPARAMS['per_arm_layers'], \n",
    "#         common_layers = HPARAMS['common_layers'],\n",
    "#         # output_dim = 1\n",
    "#     )\n",
    "# elif HPARAMS['network_type'] == 'dotproduct':\n",
    "#     network = global_and_arm_feature_network.create_feed_forward_dot_product_network(\n",
    "#         observation_spec = observation_spec, \n",
    "#         global_layers = HPARAMS['global_layers'], \n",
    "#         arm_layers = HPARAMS['per_arm_layers']\n",
    "#     )\n",
    "    \n",
    "# # agent = greedy_reward_prediction_agent.GreedyRewardPredictionAgent()\n",
    "    \n",
    "# agent = neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(\n",
    "#     time_step_spec=time_step_spec,\n",
    "#     action_spec=action_spec,\n",
    "#     reward_network=network,\n",
    "#     optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=HPARAMS['learning_rate']),\n",
    "#     epsilon=HPARAMS['epsilon'],\n",
    "#     observation_and_action_constraint_splitter=(\n",
    "#         observation_and_action_constraint_splitter\n",
    "#     ),\n",
    "#     accepts_per_arm_features=True,\n",
    "#     emit_policy_info=policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN,\n",
    "#     train_step_counter=global_step,\n",
    "#     info_fields_to_inherit_from_greedy=[\n",
    "#         policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN\n",
    "#     ],\n",
    "#     name='OffpolicyNeuralEpsGreedyAgent'\n",
    "# )\n",
    "# agent.initialize()\n",
    "\n",
    "# print(f\"Agent: {agent.name}\\n\")\n",
    "# if network:\n",
    "#     print(f\"Network: {network.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6cb60f0b-90b7-49ab-9c41-046ea00ab750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: OffpolicyNeuralEpsGreedyAgent\n",
      "\n",
      "Network: GlobalAndArmDotProductNetwork\n"
     ]
    }
   ],
   "source": [
    "# from tf_agents.bandits.policies import policy_utilities\n",
    "# from tf_agents.bandits.agents import greedy_reward_prediction_agent\n",
    "\n",
    "network = None\n",
    "observation_and_action_constraint_splitter = None\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "if AGENT_TYPE == 'LinUCB':\n",
    "    agent = lin_ucb_agent.LinearUCBAgent(\n",
    "        time_step_spec=time_step_spec,\n",
    "        action_spec=action_spec,\n",
    "        alpha=AGENT_ALPHA,\n",
    "        accepts_per_arm_features=True,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "elif AGENT_TYPE == 'LinTS':\n",
    "    agent = lin_ts_agent.LinearThompsonSamplingAgent(\n",
    "        time_step_spec=time_step_spec,\n",
    "        action_spec=action_spec,\n",
    "        alpha=AGENT_ALPHA,\n",
    "        observation_and_action_constraint_splitter=(\n",
    "            observation_and_action_constraint_splitter\n",
    "        ),\n",
    "        accepts_per_arm_features=True,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "elif AGENT_TYPE == 'epsGreedy':\n",
    "    # obs_spec = per_arm_tf_env.observation_spec()\n",
    "    if NETWORK_TYPE == 'commontower':\n",
    "        network = global_and_arm_feature_network.create_feed_forward_common_tower_network(\n",
    "            observation_spec = observation_spec, \n",
    "            global_layers = GLOBAL_LAYERS, \n",
    "            arm_layers = ARM_LAYERS, \n",
    "            common_layers = COMMON_LAYERS,\n",
    "            # output_dim = 1\n",
    "        )\n",
    "    elif NETWORK_TYPE == 'dotproduct':\n",
    "        network = global_and_arm_feature_network.create_feed_forward_dot_product_network(\n",
    "            observation_spec = observation_spec, \n",
    "            global_layers = GLOBAL_LAYERS, \n",
    "            arm_layers = ARM_LAYERS\n",
    "        )\n",
    "    agent = neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(\n",
    "        time_step_spec=time_step_spec,\n",
    "        action_spec=action_spec,\n",
    "        reward_network=network,\n",
    "        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=HPARAMS['learning_rate']),\n",
    "        epsilon=HPARAMS['epsilon'],\n",
    "        observation_and_action_constraint_splitter=(\n",
    "            observation_and_action_constraint_splitter\n",
    "        ),\n",
    "        accepts_per_arm_features=True,\n",
    "        emit_policy_info=(policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN),\n",
    "        train_step_counter=global_step,\n",
    "        # info_fields_to_inherit_from_greedy=[\n",
    "        #     policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN\n",
    "        # ],\n",
    "        name='OffpolicyNeuralEpsGreedyAgent'\n",
    "    )\n",
    "\n",
    "elif AGENT_TYPE == 'NeuralLinUCB':\n",
    "    # obs_spec = per_arm_tf_env.observation_spec()\n",
    "    network = (\n",
    "        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n",
    "            observation_spec = observation_spec, \n",
    "            global_layers = GLOBAL_LAYERS, \n",
    "            arm_layers = ARM_LAYERS, \n",
    "            common_layers = COMMON_LAYERS,\n",
    "            output_dim = ENCODING_DIM\n",
    "        )\n",
    "    )\n",
    "    agent = neural_linucb_agent.NeuralLinUCBAgent(\n",
    "        time_step_spec=per_arm_tf_env.time_step_spec(),\n",
    "        action_spec=per_arm_tf_env.action_spec(),\n",
    "        encoding_network=network,\n",
    "        encoding_network_num_train_steps=EPS_PHASE_STEPS,\n",
    "        encoding_dim=ENCODING_DIM,\n",
    "        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n",
    "        alpha=1.0,\n",
    "        gamma=1.0,\n",
    "        epsilon_greedy=EPSILON,\n",
    "        accepts_per_arm_features=True,\n",
    "        debug_summaries=True,\n",
    "        summarize_grads_and_vars=True,\n",
    "        emit_policy_info=policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN,\n",
    "    )\n",
    "    \n",
    "agent.initialize() # TODO - does this go here?\n",
    "    \n",
    "print(f\"Agent: {agent.name}\\n\")\n",
    "if network:\n",
    "    print(f\"Network: {network.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ce1a6d02-f041-4a4a-a1b6-48a92a0b2eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_TupleWrapper(Trajectory(\n",
      "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(0, dtype=int32)),\n",
      " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
      " 'observation': DictWrapper({'global': TensorSpec(shape=(16,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(1, 16), dtype=tf.float32, name=None)}),\n",
      " 'policy_info': PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorSpec(shape=(1,), dtype=tf.float32, name=None), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=(), chosen_arm_features=TensorSpec(shape=(16,), dtype=tf.float32, name=None)),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))\n"
     ]
    }
   ],
   "source": [
    "pprint(agent.policy.trajectory_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d161bf2-a8ce-4ebc-ae90-a311d2c2a1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data spec:  Trajectory(\n",
      "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(0, dtype=int32)),\n",
      " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
      " 'observation': DictWrapper({'global': TensorSpec(shape=(16,), dtype=tf.float32, name=None)}),\n",
      " 'policy_info': PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorSpec(shape=(1,), dtype=tf.float32, name=None), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=(), chosen_arm_features=TensorSpec(shape=(16,), dtype=tf.float32, name=None)),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "print('training data spec: ', agent.training_data_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fb800f43-c743-4ff9-9605-dbcacfa79792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation spec in training:  {'global': TensorSpec(shape=(16,), dtype=tf.float32, name=None)}\n"
     ]
    }
   ],
   "source": [
    "print('observation spec in training: ', agent.training_data_spec.observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d4a95eb-4a48-4a4b-a014-609ebc3feee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen arm features:  TensorSpec(shape=(16,), dtype=tf.float32, name=None)\n"
     ]
    }
   ],
   "source": [
    "print('chosen arm features: ', agent.training_data_spec.policy_info.chosen_arm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "452a333f-00b1-4f06-a8f7-42af78f505f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep Spec (for each batch):\n",
      " TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': DictWrapper({'global': TensorSpec(shape=(16,), dtype=tf.float32, name=None), 'per_arm': TensorSpec(shape=(1, 16), dtype=tf.float32, name=None)}),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TimeStep Spec (for each batch):\\n\", agent.time_step_spec, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47365427-31c7-4f7a-ae51-e1177fd5544e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Spec (for each batch):\n",
      " BoundedTensorSpec(shape=(), dtype=tf.int32, name='action_spec', minimum=array(0, dtype=int32), maximum=array(0, dtype=int32)) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Spec (for each batch):\\n\", agent.action_spec, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd6b0b12-53b4-4bb1-ab16-5cf6886db37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy at 0x7fb738520c70>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d404dae-3cc3-4f81-962b-4641455ca4f2",
   "metadata": {},
   "source": [
    "## Reward function\n",
    "\n",
    "**TODO:**\n",
    "* explain how to translate reward to this common recommendation objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df9aecd6-d20a-48a7-9ead-4da3bbcfdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_rewards(element):\n",
    "    \"\"\"Calculates reward for the actions.\"\"\"\n",
    "\n",
    "    def _calc_reward(x):\n",
    "        \"\"\"Calculates reward for a single action.\"\"\"\n",
    "        r0 = lambda: tf.constant(0.0)\n",
    "        r1 = lambda: tf.constant(-10.0)\n",
    "        r2 = lambda: tf.constant(2.0)\n",
    "        r3 = lambda: tf.constant(3.0)\n",
    "        r4 = lambda: tf.constant(4.0)\n",
    "        r5 = lambda: tf.constant(10.0)\n",
    "        c1 = tf.equal(x, 1.0)\n",
    "        c2 = tf.equal(x, 2.0)\n",
    "        c3 = tf.equal(x, 3.0)\n",
    "        c4 = tf.equal(x, 4.0)\n",
    "        c5 = tf.equal(x, 5.0)\n",
    "        return tf.case(\n",
    "            [(c1, r1), (c2, r2), (c3, r3),(c4, r4),(c5, r5)], \n",
    "            default=r0, exclusive=True\n",
    "        )\n",
    "\n",
    "    return tf.map_fn(\n",
    "        fn=_calc_reward, \n",
    "        elems=element['user_rating'], \n",
    "        dtype=tf.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59703a-9a00-4eba-a3b7-45b21b738dd4",
   "metadata": {},
   "source": [
    "## Trajectory function\n",
    "\n",
    "**parking lot**\n",
    "* does trajectory fn need concept of `dummy_chosen_arm_features`, similar to [this](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/policies/reward_prediction_base_policy.py#L297)\n",
    "\n",
    "```python\n",
    "      dummy_chosen_arm_features = tf.nest.map_structure(\n",
    "          lambda obs: tf.zeros_like(obs[:, 0, ...]),\n",
    "          time_step.observation[bandit_spec_utils.PER_ARM_FEATURE_KEY],\n",
    "      )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a1f1262-252f-44e6-992f-98c70a72f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # specs\n",
    "# obs_spec = bandit_spec_utils.create_per_arm_observation_spec(\n",
    "#     GLOBAL_DIM, PER_ARM_DIM, NUM_ACTIONS, add_num_actions_feature=False\n",
    "# )\n",
    "# time_step_spec = ts.time_step_spec(obs_spec)\n",
    "# action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 0, num_actions-tf.constant(1))\n",
    "\n",
    "# agent.policy.trajectory_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e9e28-e59e-41c7-b6ac-54f568c9ead9",
   "metadata": {},
   "source": [
    "```python\n",
    "def sample_spec_nest(\n",
    "    structure,\n",
    "    seed=None,\n",
    "    outer_dims=(),\n",
    "    minimum=None,\n",
    "    maximum=None\n",
    "):\n",
    "  \"\"\"Samples the given nest of specs.\n",
    "\n",
    "  Args:\n",
    "    structure: A nest of `TensorSpec`.\n",
    "    seed: A seed used for sampling ops\n",
    "    outer_dims: An optional `Tensor` specifying outer dimensions to add to the\n",
    "      spec shape before sampling.\n",
    "    minimum: An optional numeric value. If set, numeric specs within the nest\n",
    "      (both bounded and unbounded) will be restricted to this minimum.\n",
    "    maximum: Similar to the above but with maximums.\n",
    "\n",
    "  Returns:\n",
    "    A nest of sampled values following the ArraySpec definition.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "56d8ae7d-0d72-4aa2-a626-af574ff8e117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': <tf.Tensor: shape=(8, 16), dtype=float32, numpy=\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       dtype=float32)>,\n",
       " 'per_arm': <tf.Tensor: shape=(8, 1, 16), dtype=float32, numpy=\n",
       " array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       "       dtype=float32)>}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_sample = tensor_spec.sample_spec_nest(\n",
    "    observation_spec, outer_dims=[HPARAMS['batch_size']], minimum=0, maximum=HPARAMS['num_actions'] -1\n",
    ")\n",
    "observation_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3605d9f-d239-4871-9a61-6e9b21f9c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "def _trajectory_fn(element): # hparams\n",
    "    \"\"\"Converts a dataset element into a trajectory.\"\"\"\n",
    "    global_features = _get_global_context_features(element)\n",
    "    arm_features = _get_per_arm_features(element)\n",
    "    \n",
    "    # # tmp \n",
    "    # print(f\"global_features: {global_features}\")\n",
    "    # print(f\"arm_features: {arm_features}\")\n",
    "    \n",
    "    # Adds a time dimension.\n",
    "    # arm_features = _add_outer_dimension(arm_features)\n",
    "    # arm_features = tensor_spec.add_outer_dim(arm_features)\n",
    "    \n",
    "    # obs spec\n",
    "    observation = {\n",
    "        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n",
    "            global_features,\n",
    "            # _add_outer_dimension(global_features)\n",
    "        # bandit_spec_utils.PER_ARM_FEATURE_KEY:\n",
    "        #     _add_outer_dimension(arm_features),\n",
    "    }\n",
    "    # print(\"after adding extra dim...\")\n",
    "    # print(f\"observation: {observation}\")\n",
    "    # print(f\"arm_features: {arm_features}\")\n",
    "    \n",
    "    # reward = tensor_spec.add_outer_dim(_get_rewards(element))\n",
    "    reward = _get_rewards(element)\n",
    "    # print(f\"reward: {reward}\")\n",
    "    \n",
    "    # To emit the predicted rewards in policy_info, we need to create dummy\n",
    "    # rewards to match the definition in TensorSpec for the ones specified in\n",
    "    # emit_policy_info set.\n",
    "    dummy_rewards = tf.zeros([HPARAMS['batch_size'], 1, HPARAMS['num_actions']])\n",
    "    # dummy_rewards = tf.zeros([HPARAMS['batch_size'], HPARAMS['num_actions']])\n",
    "    # dummy_rewards = tf.zeros([HPARAMS['num_actions']])\n",
    "    policy_info = policy_utilities.PerArmPolicyInfo(\n",
    "        chosen_arm_features=arm_features,\n",
    "        # Pass dummy mean rewards here to match the model_spec for emitting\n",
    "        # mean rewards in policy info\n",
    "        predicted_rewards_mean=dummy_rewards\n",
    "    )\n",
    "    \n",
    "# tf_agents.policies.utils.create_chosen_arm_features_info_spec(\n",
    "#     observation_spec: tf_agents.typing.types.NestedTensorSpec\n",
    "# ) -> tf_agents.typing.types.NestedTensorSpec\n",
    "    \n",
    "    if HPARAMS['model_type'] == 'neural_ucb':\n",
    "        policy_info = policy_info._replace(\n",
    "            predicted_rewards_optimistic=dummy_rewards\n",
    "        )\n",
    "        \n",
    "    # print(f\"observation: {observation}\")\n",
    "    # print(f\"reward: {reward}\")\n",
    "    # print(f\"policy_info: {policy_info}\")\n",
    "    # print(f\"dummy_rewards: {dummy_rewards}\")\n",
    "    \n",
    "    return trajectory.single_step(\n",
    "        observation=observation,\n",
    "        action=tf.zeros_like(\n",
    "            reward, dtype=tf.int32\n",
    "        ),  # Arm features are copied from policy info, put dummy zeros here\n",
    "        policy_info=policy_info,\n",
    "        reward=reward,\n",
    "        discount=tf.zeros_like(reward)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6ec78cf5-6e75-4935-91e6-8040d6faae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# def build_dict_from_trajectory(\n",
    "#     step: int,\n",
    "#     next_step: int,\n",
    "#     trajectory: trajectories.Trajectory) -> Dict[str, Any]:\n",
    "#     \"\"\"Builds a dict from `trajectory` data.\n",
    "\n",
    "#     Args:\n",
    "#     trajectory: A `trajectories.Trajectory` object.\n",
    "\n",
    "#     Returns:\n",
    "#     A dict holding the same data as `trajectory`.\n",
    "#     \"\"\"\n",
    "#     trajectory_dict = {\n",
    "#         \"step_type\": [step].numpy(),\n",
    "#         \"observation\": [{\n",
    "#             \"observation_batch\": batch\n",
    "#         } for batch in trajectory.observation.numpy().tolist()],\n",
    "#         \"action\": trajectory.action.numpy().tolist(),\n",
    "#         \"policy_info\": trajectory.policy_info,\n",
    "#         \"next_step_type\": [next_step],\n",
    "#         \"reward\": trajectory.reward.numpy().tolist(),\n",
    "#         \"discount\": trajectory.discount.numpy().tolist(),\n",
    "#     }\n",
    "#     return trajectory_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc024910-1632-4ac2-9b31-88344d7d8c16",
   "metadata": {},
   "source": [
    "### write trajectories to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189a8503-5048-40f1-ba4d-f42d89f02e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION = 'v1'\n",
    "\n",
    "# DATASET_FILE = f'{VERSION}-off-policy-trajectories.json'\n",
    "# !touch $DATASET_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c79e742-39e5-4823-b7a4-594b72785a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_size = len(list(train_dataset))\n",
    "# print(f\"dataset_size: {dataset_size}\")\n",
    "\n",
    "# small_count = dataset_size/100\n",
    "# print(f\"small_count: {small_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d0cb9-d1b9-43b5-aafe-5e67f6462a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import json\n",
    "\n",
    "# def write_trajectories_to_file(\n",
    "#     dataset_size: int,\n",
    "#     data_file: str,\n",
    "#     batch_size: int,\n",
    "# ):\n",
    "#     batched_dataset = train_dataset.batch(batch_size)\n",
    "#     print(f\"writting file...\")\n",
    "    \n",
    "#     data_list = []\n",
    "    \n",
    "#     start_time = time.time()\n",
    "#     step = 1\n",
    "#     with open(data_file, \"w\") as f:\n",
    "#         for x in batched_dataset.take(count=dataset_size):\n",
    "#             # print(f\"step: {step}\")\n",
    "#             nexx_step = step + 1\n",
    "#             # print(f\"nexx_step: {nexx_step}\")\n",
    "\n",
    "#             single_traj = get_trajectory_from_environment(x)\n",
    "#             print(single_traj)\n",
    "            \n",
    "#             _trajectory_dict = build_dict_from_trajectory(step=step, next_step=nexx_step, trajectory=single_traj)\n",
    "#             # print(type(trajectory_dict))\n",
    "#             decoded = _trajectory_dict.decode('utf-8')\n",
    "#             print(f\"decoded: {decoded}\")\n",
    "#             data_list.append(_trajectory_dict)\n",
    "\n",
    "#             step+=1\n",
    "            \n",
    "#             break\n",
    "            \n",
    "#         for entry in data_list:\n",
    "#             traj_dict_tmp = {}\n",
    "#             traj_dict_tmp['step_type'] = entry['step_type']\n",
    "#             traj_dict_tmp['observation'] = entry['observation']\n",
    "#             traj_dict_tmp['action'] = entry['action']\n",
    "#             traj_dict_tmp['policy_info'] = entry['policy_info']\n",
    "#             traj_dict_tmp['next_step_type'] = entry['next_step_type']\n",
    "#             traj_dict_tmp['reward'] = entry['reward']\n",
    "#             traj_dict_tmp['discount'] = entry['discount']\n",
    "            \n",
    "#             # f.write(json.dumps(traj_dict_tmp) + \"\\n\")\n",
    "            \n",
    "#         print(f\"writting to file complete...\")\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     runtime_mins = int((end_time - start_time) / 60)\n",
    "#     print(f\"runtime_mins: {runtime_mins}\")\n",
    "\n",
    "#     return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d70e9d-b2b4-4f07-af9b-a76d4207d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_data_list = write_trajectories_to_file(\n",
    "#     dataset_size=int(small_count),\n",
    "#     data_file=DATASET_FILE,\n",
    "#     batch_size=2\n",
    "# )\n",
    "\n",
    "# sample_data_list[0]\n",
    "# sample_data_list[0]['observation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d7d797-0408-4d09-b90e-471f42b9336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_utils.upload_blob(\n",
    "#     bucket_name='',\n",
    "#     source_file_name=,\n",
    "#     destination_blob_name=f'{RUN_NAME}/candidates/xxxx.json'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbe97d9-3708-43d0-bffe-66442e022782",
   "metadata": {},
   "source": [
    "### validate shapes and dims\n",
    "\n",
    "**TODO:** add auto test for these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c3b82867-30bf-4c33-8f8b-1a83c0d2d88c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(\n",
       "{'action': <tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>,\n",
       " 'discount': <tf.Tensor: shape=(8,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " 'next_step_type': <tf.Tensor: shape=(8,), dtype=int32, numpy=array([2, 2, 2, 2, 2, 2, 2, 2], dtype=int32)>,\n",
       " 'observation': {'global': <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
       "array([-0.04257477, -0.0204685 , -0.03652425, -0.0370932 , -0.01470114,\n",
       "        0.03408908, -0.01681744,  0.04299467, -0.04057591, -0.01261982,\n",
       "        0.04479137, -0.04452629, -0.04684504,  0.03435652, -0.02922553,\n",
       "        0.01416435], dtype=float32)>},\n",
       " 'policy_info': PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=<tf.Tensor: shape=(8, 1, 1), dtype=float32, numpy=\n",
       "array([[[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]]], dtype=float32)>, multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=(), chosen_arm_features=<tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
       "array([-0.04094781,  0.03707052, -0.02573409, -0.02102193, -0.0301262 ,\n",
       "       -0.00709243,  0.03189244,  0.02086892, -0.00483013, -0.03928108,\n",
       "        0.0106586 ,  0.01059937,  0.00203977,  0.00540863, -0.04190093,\n",
       "        0.03950352], dtype=float32)>),\n",
       " 'reward': <tf.Tensor: shape=(8,), dtype=float32, numpy=array([  4.,   4., -10.,  10.,   3.,   2.,  10.,  10.], dtype=float32)>,\n",
       " 'step_type': <tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in train_dataset.batch(HPARAMS['batch_size']).take(1):\n",
    "    sample_trajectory = _trajectory_fn(x)\n",
    "    \n",
    "sample_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "76a66037-1b38-444d-b81e-a3ec5cb55db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.utils import nest_utils\n",
    "\n",
    "# nest_utils.is_batched_nested_tensors(\n",
    "#     tensors=sample_trajectory.policy_info.chosen_arm_features,\n",
    "#     specs=agent.training_data_spec.policy_info.chosen_arm_features,\n",
    "#     num_outer_dims=1,  # 2\n",
    "#     allow_extra_fields=False,\n",
    "#     check_dtypes=True\n",
    "# )\n",
    "\n",
    "# nest_utils.is_batched_nested_tensors(\n",
    "#     tensors=sample_trajectory.observation['global'],\n",
    "#     specs=agent.training_data_spec.observation['global'],\n",
    "#     num_outer_dims=0,\n",
    "#     allow_extra_fields=False,\n",
    "#     check_dtypes=True\n",
    "# )\n",
    "\n",
    "nest_utils.is_batched_nested_tensors(\n",
    "    tensors=sample_trajectory.action,\n",
    "    specs=agent.training_data_spec.action,\n",
    "    num_outer_dims=1,\n",
    "    allow_extra_fields=False,\n",
    "    check_dtypes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c19d89-60c6-49cb-b3bd-393e460028a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arm_observations = per_arm_context_sampling_fn()\n",
    "# print(arm_observations)\n",
    "\n",
    "# outer_rank = nest_utils.get_outer_rank(tensors = arm_observations, specs = observation_spec['per_arm'])\n",
    "# outer_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf40f07-7f71-45ad-8688-42b33f18bacd",
   "metadata": {},
   "source": [
    "### replay buffer\n",
    "\n",
    "> replay buffer and observers keep track of Trajectory data\n",
    "\n",
    "**TODO** - is this needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431530e-c4f9-4cf6-8639-62dd29eeb862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver_steps = 2                           # number of steps to run per batch\n",
    "# data_spec = agent.policy.trajectory_spec\n",
    "\n",
    "# replay_buffer = trainer.get_replay_buffer(\n",
    "#     data_spec, HPARAMS['batch_size'], driver_steps\n",
    "# )\n",
    "# observers = [replay_buffer.add_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389edcf-262e-476f-9a61-6dbd6270c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the replay buffer as a Dataset,\n",
    "# # read batches of 4 elements, each with 2 timesteps:\n",
    "# dataset = replay_buffer.as_dataset(\n",
    "#     sample_batch_size=4,\n",
    "#     num_steps=2\n",
    "# )\n",
    "\n",
    "# DRIVER_STEPS = 3\n",
    "\n",
    "# replay_buffer = generate_simulation_data(\n",
    "#     raw_data_path=DATA_PATH,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     rank_k=RANK_K,\n",
    "#     num_actions=NUM_ACTIONS,\n",
    "#     driver_steps=DRIVER_STEPS\n",
    "# )\n",
    "# replay_buffer\n",
    "\n",
    "# dataset = replay_buffer.as_dataset(\n",
    "#     sample_batch_size=BATCH_SIZE,\n",
    "#     num_steps=DRIVER_STEPS\n",
    "# )\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b3d3c-75f7-46f4-9a1b-6329e419b7f5",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb7ffc-00df-4d46-857b-86c87b78f597",
   "metadata": {},
   "source": [
    "`agent.train(experience=...)`\n",
    "\n",
    "where `experience` is a batch of trajectories data in the form of a Trajectory. \n",
    "* The structure of experience must match that of `self.training_data_spec`. \n",
    "* All tensors in experience must be shaped [batch, time, ...] where time must be equal to self.train_step_length if that property is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "da4680f3-693b-4530-8e4d-88bc43036c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print data: {'bucketized_user_age': <tf.Tensor: shape=(8,), dtype=float32, numpy=array([25., 45., 18., 25., 35., 45., 50., 25.], dtype=float32)>, 'movie_genres': <tf.Tensor: shape=(8, 1), dtype=int64, numpy=\n",
      "array([[4],\n",
      "       [7],\n",
      "       [7],\n",
      "       [1],\n",
      "       [0],\n",
      "       [4],\n",
      "       [7],\n",
      "       [0]])>, 'movie_id': <tf.Tensor: shape=(8,), dtype=string, numpy=\n",
      "array([b'211', b'678', b'135', b'97', b'568', b'150', b'483', b'121'],\n",
      "      dtype=object)>, 'timestamp': <tf.Tensor: shape=(8,), dtype=int64, numpy=\n",
      "array([874948475, 888638193, 887747108, 882475618, 875350485, 875946055,\n",
      "       879453933, 880149166])>, 'user_id': <tf.Tensor: shape=(8,), dtype=string, numpy=\n",
      "array([b'346', b'602', b'393', b'152', b'738', b'382', b'85', b'152'],\n",
      "      dtype=object)>, 'user_occupation_text': <tf.Tensor: shape=(8,), dtype=string, numpy=\n",
      "array([b'other', b'other', b'student', b'educator', b'technician',\n",
      "       b'engineer', b'educator', b'educator'], dtype=object)>, 'user_rating': <tf.Tensor: shape=(8,), dtype=float32, numpy=array([4., 4., 1., 5., 3., 2., 5., 5.], dtype=float32)>}\n",
      "print trajectories: Trajectory(\n",
      "{'action': <tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>,\n",
      " 'discount': <tf.Tensor: shape=(8,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(8,), dtype=int32, numpy=array([2, 2, 2, 2, 2, 2, 2, 2], dtype=int32)>,\n",
      " 'observation': {'global': <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([-0.04257477, -0.0204685 , -0.03652425, -0.0370932 , -0.01470114,\n",
      "        0.03408908, -0.01681744,  0.04299467, -0.04057591, -0.01261982,\n",
      "        0.04479137, -0.04452629, -0.04684504,  0.03435652, -0.02922553,\n",
      "        0.01416435], dtype=float32)>},\n",
      " 'policy_info': PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=<tf.Tensor: shape=(8, 1, 1), dtype=float32, numpy=\n",
      "array([[[0.]],\n",
      "\n",
      "       [[0.]],\n",
      "\n",
      "       [[0.]],\n",
      "\n",
      "       [[0.]],\n",
      "\n",
      "       [[0.]],\n",
      "\n",
      "       [[0.]],\n",
      "\n",
      "       [[0.]],\n",
      "\n",
      "       [[0.]]], dtype=float32)>, multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=(), chosen_arm_features=<tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([-0.04094781,  0.03707052, -0.02573409, -0.02102193, -0.0301262 ,\n",
      "       -0.00709243,  0.03189244,  0.02086892, -0.00483013, -0.03928108,\n",
      "        0.0106586 ,  0.01059937,  0.00203977,  0.00540863, -0.04190093,\n",
      "        0.03950352], dtype=float32)>),\n",
      " 'reward': <tf.Tensor: shape=(8,), dtype=float32, numpy=array([  4.,   4., -10.,  10.,   3.,   2.,  10.,  10.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>})\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Received a mix of batched and unbatched Tensors, or Tensors are not compatible with Specs.  num_outer_dims: 2.\nSaw tensor_shapes:\n   Trajectory(\n{'action': TensorShape([8]),\n 'discount': TensorShape([8]),\n 'next_step_type': TensorShape([8]),\n 'observation': DictWrapper({'global': TensorShape([16])}),\n 'policy_info': PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorShape([8, 1, 1]), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=(), chosen_arm_features=TensorShape([16])),\n 'reward': TensorShape([8]),\n 'step_type': TensorShape([8])})\nAnd spec_shapes:\n   Trajectory(\n{'action': TensorShape([]),\n 'discount': TensorShape([]),\n 'next_step_type': TensorShape([]),\n 'observation': DictWrapper({'global': TensorShape([16])}),\n 'policy_info': PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorShape([1]), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=(), chosen_arm_features=TensorShape([16])),\n 'reward': TensorShape([]),\n 'step_type': TensorShape([])})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# All tensors in experience must be shaped [batch, time, ...] \u001b[39;00m\n\u001b[1;32m     26\u001b[0m step \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrain_step_counter\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 27\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrajectories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# break\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tf_agents/agents/tf_agent.py:330\u001b[0m, in \u001b[0;36mTFAgent.train\u001b[0;34m(self, experience, weights, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    326\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find _train_fn.  Did \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.__init__ call super?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m       \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_functions:\n\u001b[0;32m--> 330\u001b[0m   loss_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexperience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m   loss_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(experience\u001b[38;5;241m=\u001b[39mexperience, weights\u001b[38;5;241m=\u001b[39mweights, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tf_agents/utils/common.py:188\u001b[0m, in \u001b[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m check_tf1_allowed()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_eager_been_enabled():\n\u001b[1;32m    186\u001b[0m   \u001b[38;5;66;03m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[39;00m\n\u001b[1;32m    187\u001b[0m   \u001b[38;5;66;03m# autodep-like behavior is already expected of fn.\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resource_variables_enabled():\n\u001b[1;32m    190\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tf_agents/bandits/agents/greedy_reward_prediction_agent.py:228\u001b[0m, in \u001b[0;36mGreedyRewardPredictionAgent._train\u001b[0;34m(self, experience, weights)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, experience, weights):\n\u001b[0;32m--> 228\u001b[0m   experience \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperience\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m    231\u001b[0m     loss_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(experience, weights\u001b[38;5;241m=\u001b[39mweights, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tf_agents/agents/data_converter.py:340\u001b[0m, in \u001b[0;36mAsTrajectory.__call__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    339\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput type not supported: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(value))\n\u001b[0;32m--> 340\u001b[0m \u001b[43m_validate_trajectory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrajectory_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_outer_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outer_dims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m value \u001b[38;5;241m=\u001b[39m nest_utils\u001b[38;5;241m.\u001b[39mprune_extra_keys(\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_context\u001b[38;5;241m.\u001b[39mtrajectory_spec, value)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tf_agents/agents/data_converter.py:174\u001b[0m, in \u001b[0;36m_validate_trajectory\u001b[0;34m(value, trajectory_spec, sequence_length, num_outer_dims)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_trajectory\u001b[39m(\n\u001b[1;32m    169\u001b[0m     value: trajectory\u001b[38;5;241m.\u001b[39mTrajectory,\n\u001b[1;32m    170\u001b[0m     trajectory_spec: trajectory\u001b[38;5;241m.\u001b[39mTrajectory,\n\u001b[1;32m    171\u001b[0m     sequence_length: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    172\u001b[0m     num_outer_dims: te\u001b[38;5;241m.\u001b[39mLiteral[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=bad-whitespace\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Validate a Trajectory given its spec and a sequence length.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnest_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_batched_nested_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrajectory_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outer_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outer_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m      \u001b[49m\u001b[43mallow_extra_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m:\n\u001b[1;32m    177\u001b[0m     debug_str_1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m tp: tp\u001b[38;5;241m.\u001b[39mshape, value)\n\u001b[1;32m    178\u001b[0m     debug_str_2 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m spec: spec\u001b[38;5;241m.\u001b[39mshape, trajectory_spec)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tf_agents/utils/nest_utils.py:546\u001b[0m, in \u001b[0;36mis_batched_nested_tensors\u001b[0;34m(tensors, specs, num_outer_dims, allow_extra_fields, check_dtypes)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    542\u001b[0m     discrepancy \u001b[38;5;241m==\u001b[39m tensor_ndims_discrepancy[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m discrepancy \u001b[38;5;129;01min\u001b[39;00m tensor_ndims_discrepancy) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(tensor_matches_spec):\n\u001b[1;32m    544\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived a mix of batched and unbatched Tensors, or Tensors\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are not compatible with Specs.  num_outer_dims: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaw tensor_shapes:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m   \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnd spec_shapes:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m   \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    551\u001b[0m     (num_outer_dims, tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(specs, tensor_shapes),\n\u001b[1;32m    552\u001b[0m      tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(specs, spec_shapes)))\n",
      "\u001b[0;31mValueError\u001b[0m: Received a mix of batched and unbatched Tensors, or Tensors are not compatible with Specs.  num_outer_dims: 2.\nSaw tensor_shapes:\n   Trajectory(\n{'action': TensorShape([8]),\n 'discount': TensorShape([8]),\n 'next_step_type': TensorShape([8]),\n 'observation': DictWrapper({'global': TensorShape([16])}),\n 'policy_info': PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorShape([8, 1, 1]), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=(), chosen_arm_features=TensorShape([16])),\n 'reward': TensorShape([8]),\n 'step_type': TensorShape([8])})\nAnd spec_shapes:\n   Trajectory(\n{'action': TensorShape([]),\n 'discount': TensorShape([]),\n 'next_step_type': TensorShape([]),\n 'observation': DictWrapper({'global': TensorShape([16])}),\n 'policy_info': PerArmPolicyInfo(log_probability=(), predicted_rewards_mean=TensorShape([1]), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=(), chosen_arm_features=TensorShape([16])),\n 'reward': TensorShape([]),\n 'step_type': TensorShape([])})"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from tf_agents.utils import common\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "# global_step = tf.compat.v1.train.get_global_step()\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "train_loss = collections.defaultdict(list)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    iterator = iter(train_dataset.batch(HPARAMS['batch_size']))\n",
    "    data = next(iterator)\n",
    "    print(f\"print data: {data}\")\n",
    "    \n",
    "    trajectories = _trajectory_fn(data)\n",
    "    print(f\"print trajectories: {trajectories}\")\n",
    "    \n",
    "    # All tensors in experience must be shaped [batch, time, ...] \n",
    "    step = agent.train_step_counter.numpy()\n",
    "    loss = agent.train(experience=trajectories)\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb6393-e016-48c9-a1eb-0e39e35ee9e6",
   "metadata": {},
   "source": [
    "## debugging notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d010b6-1e97-4cbb-a6fc-30695852843f",
   "metadata": {},
   "source": [
    "* say you have a global observation spec of [17]. And you have two batch dimensions [4, 5]. Then your observation has to have the shape [4, 5, 17]\n",
    "* and then if you have arm_obs_spec with shape [9, 13], then the arm obs shape has to be exactly [4, 5, 9, 13]\n",
    "* and this has to be true for every single tensor in your tensor nest\n",
    "* the first 2 dims are the outer dims that are the same for all tensors, the rest of the dimensions have to follow the spec for each tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3a67e-ca5c-4064-b20e-77916880e266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4828dd9a-61e5-43cf-8eaf-46a314ddebcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad98430-d836-40f1-8e4f-eb46f0c91fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108ed33c-d8ef-4524-97e9-ced9e01c154b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c98398-d545-4dcc-a71f-32249b5f0336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba65417f-54e0-436e-b0d4-93cffb981ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1d819-0152-437c-aefc-45ffd34d808a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
