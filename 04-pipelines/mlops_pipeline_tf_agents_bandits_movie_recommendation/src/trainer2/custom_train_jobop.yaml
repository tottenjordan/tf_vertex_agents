# PIPELINE DEFINITION
# Name: train-reinforcement-learning-policy
# Description: Implements off-policy training for a policy on dataset of TFRecord files.
# Inputs:
#    agent_alpha: float
#    num_actions: int
#    num_epochs: int
#    rank_k: int
#    tfrecord_file: str
#    tikhonov_weight: float
#    training_artifacts_dir: str
# Outputs:
#    training_artifacts_dir: str
components:
  comp-train-reinforcement-learning-policy:
    executorLabel: exec-train-reinforcement-learning-policy
    inputDefinitions:
      parameters:
        agent_alpha:
          parameterType: NUMBER_DOUBLE
        num_actions:
          parameterType: NUMBER_INTEGER
        num_epochs:
          parameterType: NUMBER_INTEGER
        rank_k:
          parameterType: NUMBER_INTEGER
        tfrecord_file:
          parameterType: STRING
        tikhonov_weight:
          parameterType: NUMBER_DOUBLE
        training_artifacts_dir:
          parameterType: STRING
    outputDefinitions:
      parameters:
        training_artifacts_dir:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-train-reinforcement-learning-policy:
      container:
        args:
        - --training-artifacts-dir
        - '{{$.inputs.parameters[''training_artifacts_dir'']}}'
        - --tfrecord-file
        - '{{$.inputs.parameters[''tfrecord_file'']}}'
        - --num-epochs
        - '{{$.inputs.parameters[''num_epochs'']}}'
        - --rank-k
        - '{{$.inputs.parameters[''rank_k'']}}'
        - --num-actions
        - '{{$.inputs.parameters[''num_actions'']}}'
        - --tikhonov-weight
        - '{{$.inputs.parameters[''tikhonov_weight'']}}'
        - --agent-alpha
        - '{{$.inputs.parameters[''agent_alpha'']}}'
        - '----output-paths'
        - '{{$.outputs.parameters[''training_artifacts_dir''].output_file}}'
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'tensorflow==2.13.0' 'tf-agents==0.17.0' 'Pillow' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location 'tensorflow==2.13.0'
          'tf-agents==0.17.0' 'Pillow' --user) && "$0" "$@"
        - sh
        - -ec
        - 'program_path=$(mktemp)

          printf "%s" "$0" > "$program_path"

          python3 -u "$program_path" "$@"

          '
        - "def train_reinforcement_learning_policy(\n    training_artifacts_dir,\n\
          \    tfrecord_file,\n    num_epochs,\n    rank_k,\n    num_actions,\n  \
          \  tikhonov_weight,\n    agent_alpha\n):\n  \"\"\"Implements off-policy\
          \ training for a policy on dataset of TFRecord files.\n\n  The Trainer's\
          \ task is to submit a remote training job to Vertex AI, with the\n  training\
          \ logic of a specified custom training container. The task will be\n  handled\
          \ by: `kfp.v2.google.experimental.run_as_aiplatform_custom_job` (which\n\
          \  takes in the component made from this placeholder function)\n\n  This\
          \ function is to be built into a Kubeflow Pipelines (KFP) component. As\
          \ a\n  result, this function must be entirely self-contained. This means\
          \ that the\n  import statements and helper functions must reside within\
          \ itself.\n\n  Args:\n    training_artifacts_dir: Path to store the Trainer\
          \ artifacts (trained\n      policy).\n    tfrecord_file: Path to file to\
          \ write the ingestion result TFRecords.\n    num_epochs: Number of training\
          \ epochs.\n    rank_k: Rank for matrix factorization in the MovieLens environment;\
          \ also\n      the observation dimension.\n    num_actions: Number of actions\
          \ (movie items) to choose from.\n    tikhonov_weight: LinUCB Tikhonov regularization\
          \ weight of the Trainer.\n    agent_alpha: LinUCB exploration parameter\
          \ that multiplies the confidence\n      intervals of the Trainer.\n\n  Returns:\n\
          \    A NamedTuple of (`training_artifacts_dir`).\n  \"\"\"\n  # pylint:\
          \ disable=g-import-not-at-top\n  import collections\n  from typing import\
          \ Dict, List, NamedTuple  # pylint: disable=redefined-outer-name,reimported\n\
          \n  import tensorflow as tf\n\n  from tf_agents import agents\n  from tf_agents\
          \ import policies\n  from tf_agents import trajectories\n  from tf_agents.bandits.agents\
          \ import lin_ucb_agent\n  from tf_agents.policies import policy_saver\n\
          \  from tf_agents.specs import tensor_spec\n\n  import logging\n\n  per_arm\
          \ = False  # Using the non-per-arm version of the MovieLens environment.\n\
          \n  # Mapping from feature name to serialized value\n  feature_description\
          \ = {\n      \"step_type\": tf.io.FixedLenFeature((), tf.string),\n    \
          \  \"observation\": tf.io.FixedLenFeature((), tf.string),\n      \"action\"\
          : tf.io.FixedLenFeature((), tf.string),\n      \"policy_info\": tf.io.FixedLenFeature((),\
          \ tf.string),\n      \"next_step_type\": tf.io.FixedLenFeature((), tf.string),\n\
          \      \"reward\": tf.io.FixedLenFeature((), tf.string),\n      \"discount\"\
          : tf.io.FixedLenFeature((), tf.string),\n  }\n\n  def _parse_record(raw_record):\n\
          \    \"\"\"Parses a serialized `tf.train.Example` proto.\n\n    Args:\n\
          \      raw_record: A serialized data record of a `tf.train.Example` proto.\n\
          \n    Returns:\n      A dict mapping feature names to values as `tf.Tensor`\
          \ objects of type\n      string containing serialized protos, following\
          \ `feature_description`.\n    \"\"\"\n    return tf.io.parse_single_example(raw_record,\
          \ feature_description)\n\n  def build_trajectory(\n      parsed_record,\n\
          \      policy_info):\n    \"\"\"Builds a `trajectories.Trajectory` object\
          \ from `parsed_record`.\n\n    Args:\n      parsed_record: A dict mapping\
          \ feature names to values as `tf.Tensor`\n        objects of type string\
          \ containing serialized protos.\n      policy_info: Policy information specification.\n\
          \n    Returns:\n      A `trajectories.Trajectory` object that contains values\
          \ as de-serialized\n      `tf.Tensor` objects from `parsed_record`.\n  \
          \  \"\"\"\n    return trajectories.Trajectory(\n        step_type=tf.expand_dims(\n\
          \            tf.io.parse_tensor(parsed_record[\"step_type\"], out_type=tf.int32),\n\
          \            axis=1),\n        observation=tf.expand_dims(\n           \
          \ tf.io.parse_tensor(\n                parsed_record[\"observation\"], out_type=tf.float32),\n\
          \            axis=1),\n        action=tf.expand_dims(\n            tf.io.parse_tensor(parsed_record[\"\
          action\"], out_type=tf.int32),\n            axis=1),\n        policy_info=policy_info,\n\
          \        next_step_type=tf.expand_dims(\n            tf.io.parse_tensor(\n\
          \                parsed_record[\"next_step_type\"], out_type=tf.int32),\n\
          \            axis=1),\n        reward=tf.expand_dims(\n            tf.io.parse_tensor(parsed_record[\"\
          reward\"], out_type=tf.float32),\n            axis=1),\n        discount=tf.expand_dims(\n\
          \            tf.io.parse_tensor(parsed_record[\"discount\"], out_type=tf.float32),\n\
          \            axis=1))\n\n  def train_policy_on_trajectory(\n      agent,\n\
          \      tfrecord_file,\n      num_epochs\n  ):\n    \"\"\"Trains the policy\
          \ in `agent` on the dataset of `tfrecord_file`.\n\n    Parses `tfrecord_file`\
          \ as `tf.train.Example` objects, packages them into\n    `trajectories.Trajectory`\
          \ objects, and trains the agent's policy on these\n    trajectory objects.\n\
          \n    Args:\n      agent: A TF-Agents agent that carries the policy to train.\n\
          \      tfrecord_file: Path to the TFRecord file containing the training\
          \ dataset.\n      num_epochs: Number of epochs to train the policy.\n\n\
          \    Returns:\n      A NamedTuple of (a trained TF-Agents policy, a dict\
          \ mapping from\n      \"epoch<i>\" to lists of loss values produced at each\
          \ training step).\n    \"\"\"\n    raw_dataset = tf.data.TFRecordDataset([tfrecord_file])\n\
          \    parsed_dataset = raw_dataset.map(_parse_record)\n\n    train_loss =\
          \ collections.defaultdict(list)\n    for epoch in range(num_epochs):\n \
          \     for parsed_record in parsed_dataset:\n        trajectory = build_trajectory(parsed_record,\
          \ agent.policy.info_spec)\n        loss, _ = agent.train(trajectory)\n \
          \       train_loss[f\"epoch{epoch + 1}\"].append(loss.numpy())\n\n    train_outputs\
          \ = collections.namedtuple(\n        \"TrainOutputs\",\n        [\"policy\"\
          , \"train_loss\"])\n    return train_outputs(agent.policy, train_loss)\n\
          \n  def execute_training_and_save_policy(\n      training_artifacts_dir,\n\
          \      tfrecord_file,\n      num_epochs,\n      rank_k,\n      num_actions,\n\
          \      tikhonov_weight,\n      agent_alpha):\n    \"\"\"Executes training\
          \ for the policy and saves the policy.\n\n    Args:\n      training_artifacts_dir:\
          \ Path to store the Trainer artifacts (trained\n        policy).\n     \
          \ tfrecord_file: Path to file to write the ingestion result TFRecords.\n\
          \      num_epochs: Number of training epochs.\n      rank_k: Rank for matrix\
          \ factorization in the MovieLens environment; also\n        the observation\
          \ dimension.\n      num_actions: Number of actions (movie items) to choose\
          \ from.\n      tikhonov_weight: LinUCB Tikhonov regularization weight of\
          \ the Trainer.\n      agent_alpha: LinUCB exploration parameter that multiplies\
          \ the confidence\n        intervals of the Trainer.\n    \"\"\"\n    # Define\
          \ time step and action specs for one batch.\n    time_step_spec = trajectories.TimeStep(\n\
          \        step_type=tensor_spec.TensorSpec(\n            shape=(), dtype=tf.int32,\
          \ name=\"step_type\"),\n        reward=tensor_spec.TensorSpec(\n       \
          \     shape=(), dtype=tf.float32, name=\"reward\"),\n        discount=tensor_spec.BoundedTensorSpec(\n\
          \            shape=(), dtype=tf.float32, name=\"discount\", minimum=0.,\n\
          \            maximum=1.),\n        observation=tensor_spec.TensorSpec(\n\
          \            shape=(rank_k,), dtype=tf.float32,\n            name=\"observation\"\
          ))\n\n    action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(),\n\
          \        dtype=tf.int32,\n        name=\"action\",\n        minimum=0,\n\
          \        maximum=num_actions - 1)\n\n    # Define RL agent/algorithm.\n\
          \    agent = lin_ucb_agent.LinearUCBAgent(\n        time_step_spec=time_step_spec,\n\
          \        action_spec=action_spec,\n        tikhonov_weight=tikhonov_weight,\n\
          \        alpha=agent_alpha,\n        dtype=tf.float32,\n        accepts_per_arm_features=per_arm)\n\
          \    agent.initialize()\n    logging.info(\"TimeStep Spec (for each batch):\\\
          n%s\\n\", agent.time_step_spec)\n    logging.info(\"Action Spec (for each\
          \ batch):\\n%s\\n\", agent.action_spec)\n\n    # Perform off-policy training.\n\
          \    policy, _ = train_policy_on_trajectory(\n        agent=agent,\n   \
          \     tfrecord_file=tfrecord_file,\n        num_epochs=num_epochs)\n\n \
          \   # Save trained policy.\n    saver = policy_saver.PolicySaver(policy)\n\
          \    saver.save(training_artifacts_dir)\n\n  execute_training_and_save_policy(\n\
          \      training_artifacts_dir=training_artifacts_dir,\n      tfrecord_file=tfrecord_file,\n\
          \      num_epochs=num_epochs,\n      rank_k=rank_k,\n      num_actions=num_actions,\n\
          \      tikhonov_weight=tikhonov_weight,\n      agent_alpha=agent_alpha)\n\
          \n  outputs = collections.namedtuple(\n      \"Outputs\",\n      [\"training_artifacts_dir\"\
          ])\n\n  return outputs(training_artifacts_dir)\n\ndef _serialize_str(str_value:\
          \ str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value\
          \ \"{}\" has type \"{}\" instead of str.'.format(\n            str(str_value),\
          \ str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser\
          \ = argparse.ArgumentParser(prog='Train reinforcement learning policy',\
          \ description='Implements off-policy training for a policy on dataset of\
          \ TFRecord files.')\n_parser.add_argument(\"--training-artifacts-dir\",\
          \ dest=\"training_artifacts_dir\", type=str, required=True, default=argparse.SUPPRESS)\n\
          _parser.add_argument(\"--tfrecord-file\", dest=\"tfrecord_file\", type=str,\
          \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--num-epochs\"\
          , dest=\"num_epochs\", type=int, required=True, default=argparse.SUPPRESS)\n\
          _parser.add_argument(\"--rank-k\", dest=\"rank_k\", type=int, required=True,\
          \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--num-actions\", dest=\"\
          num_actions\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
          --tikhonov-weight\", dest=\"tikhonov_weight\", type=float, required=True,\
          \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--agent-alpha\", dest=\"\
          agent_alpha\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
          ----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args\
          \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
          , [])\n\n_outputs = train_reinforcement_learning_policy(**_parsed_args)\n\
          \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
          \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
          \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
          \        f.write(_output_serializers[idx](_outputs[idx]))\n"
        image: python:3.10
pipelineInfo:
  name: train-reinforcement-learning-policy
root:
  dag:
    outputs:
      parameters:
        training_artifacts_dir:
          valueFromParameter:
            outputParameterKey: training_artifacts_dir
            producerSubtask: train-reinforcement-learning-policy
    tasks:
      train-reinforcement-learning-policy:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-reinforcement-learning-policy
        inputs:
          parameters:
            agent_alpha:
              componentInputParameter: agent_alpha
            num_actions:
              componentInputParameter: num_actions
            num_epochs:
              componentInputParameter: num_epochs
            rank_k:
              componentInputParameter: rank_k
            tfrecord_file:
              componentInputParameter: tfrecord_file
            tikhonov_weight:
              componentInputParameter: tikhonov_weight
            training_artifacts_dir:
              componentInputParameter: training_artifacts_dir
        taskInfo:
          name: train-reinforcement-learning-policy
  inputDefinitions:
    parameters:
      agent_alpha:
        parameterType: NUMBER_DOUBLE
      num_actions:
        parameterType: NUMBER_INTEGER
      num_epochs:
        parameterType: NUMBER_INTEGER
      rank_k:
        parameterType: NUMBER_INTEGER
      tfrecord_file:
        parameterType: STRING
      tikhonov_weight:
        parameterType: NUMBER_DOUBLE
      training_artifacts_dir:
        parameterType: STRING
  outputDefinitions:
    parameters:
      training_artifacts_dir:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.3.0
