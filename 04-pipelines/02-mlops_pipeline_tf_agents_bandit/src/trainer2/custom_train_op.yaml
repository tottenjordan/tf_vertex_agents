# PIPELINE DEFINITION
# Name: train-reinforcement-learning-policy
# Inputs:
#    agent_alpha: float
#    num_actions: int
#    num_epochs: int
#    rank_k: int
#    tfrecord_file: str
#    tikhonov_weight: float
#    training_artifacts_dir: str
# Outputs:
#    training_artifacts_dir: str
components:
  comp-train-reinforcement-learning-policy:
    executorLabel: exec-train-reinforcement-learning-policy
    inputDefinitions:
      parameters:
        agent_alpha:
          description: 'LinUCB exploration parameter that multiplies the confidence

            intervals of the Trainer.'
          parameterType: NUMBER_DOUBLE
        num_actions:
          description: Number of actions (movie items) to choose from.
          parameterType: NUMBER_INTEGER
        num_epochs:
          description: Number of training epochs.
          parameterType: NUMBER_INTEGER
        rank_k:
          description: 'Rank for matrix factorization in the MovieLens environment;
            also

            the observation dimension.'
          parameterType: NUMBER_INTEGER
        tfrecord_file:
          description: Path to file to write the ingestion result TFRecords.
          parameterType: STRING
        tikhonov_weight:
          description: LinUCB Tikhonov regularization weight of the Trainer.
          parameterType: NUMBER_DOUBLE
        training_artifacts_dir:
          description: 'Path to store the Trainer artifacts (trained

            policy).'
          parameterType: STRING
    outputDefinitions:
      parameters:
        training_artifacts_dir:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-train-reinforcement-learning-policy:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_reinforcement_learning_policy
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'tensorflow==2.13.0'\
          \ 'tf-agents==0.17.0' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_reinforcement_learning_policy(\n    training_artifacts_dir:\
          \ str,\n    tfrecord_file: str,\n    num_epochs: int,\n    rank_k: int,\n\
          \    num_actions: int,\n    tikhonov_weight: float,\n    agent_alpha: float\n\
          ) -> NamedTuple(\"Outputs\", [\n    (\"training_artifacts_dir\", str),\n\
          ]):\n  \"\"\"Implements off-policy training for a policy on dataset of TFRecord\
          \ files.\n\n  The Trainer's task is to submit a remote training job to Vertex\
          \ AI, with the\n  training logic of a specified custom training container.\
          \ The task will be\n  handled by: `kfp.v2.google.experimental.run_as_aiplatform_custom_job`\
          \ (which\n  takes in the component made from this placeholder function)\n\
          \n  This function is to be built into a Kubeflow Pipelines (KFP) component.\
          \ As a\n  result, this function must be entirely self-contained. This means\
          \ that the\n  import statements and helper functions must reside within\
          \ itself.\n\n  Args:\n    training_artifacts_dir: Path to store the Trainer\
          \ artifacts (trained\n      policy).\n    tfrecord_file: Path to file to\
          \ write the ingestion result TFRecords.\n    num_epochs: Number of training\
          \ epochs.\n    rank_k: Rank for matrix factorization in the MovieLens environment;\
          \ also\n      the observation dimension.\n    num_actions: Number of actions\
          \ (movie items) to choose from.\n    tikhonov_weight: LinUCB Tikhonov regularization\
          \ weight of the Trainer.\n    agent_alpha: LinUCB exploration parameter\
          \ that multiplies the confidence\n      intervals of the Trainer.\n\n  Returns:\n\
          \    A NamedTuple of (`training_artifacts_dir`).\n  \"\"\"\n  # pylint:\
          \ disable=g-import-not-at-top\n  import collections\n  from typing import\
          \ Dict, List, NamedTuple  # pylint: disable=redefined-outer-name,reimported\n\
          \n  import tensorflow as tf\n\n  from tf_agents import agents\n  from tf_agents\
          \ import policies\n  from tf_agents import trajectories\n  from tf_agents.bandits.agents\
          \ import lin_ucb_agent\n  from tf_agents.policies import policy_saver\n\
          \  from tf_agents.specs import tensor_spec\n\n  import logging\n\n  per_arm\
          \ = False  # Using the non-per-arm version of the MovieLens environment.\n\
          \n  # Mapping from feature name to serialized value\n  feature_description\
          \ = {\n      \"step_type\": tf.io.FixedLenFeature((), tf.string),\n    \
          \  \"observation\": tf.io.FixedLenFeature((), tf.string),\n      \"action\"\
          : tf.io.FixedLenFeature((), tf.string),\n      \"policy_info\": tf.io.FixedLenFeature((),\
          \ tf.string),\n      \"next_step_type\": tf.io.FixedLenFeature((), tf.string),\n\
          \      \"reward\": tf.io.FixedLenFeature((), tf.string),\n      \"discount\"\
          : tf.io.FixedLenFeature((), tf.string),\n  }\n\n  def _parse_record(raw_record:\
          \ tf.Tensor) -> Dict[str, tf.Tensor]:\n    \"\"\"Parses a serialized `tf.train.Example`\
          \ proto.\n\n    Args:\n      raw_record: A serialized data record of a `tf.train.Example`\
          \ proto.\n\n    Returns:\n      A dict mapping feature names to values as\
          \ `tf.Tensor` objects of type\n      string containing serialized protos,\
          \ following `feature_description`.\n    \"\"\"\n    return tf.io.parse_single_example(raw_record,\
          \ feature_description)\n\n  def build_trajectory(\n      parsed_record:\
          \ Dict[str, tf.Tensor],\n      policy_info: policies.utils.PolicyInfo) ->\
          \ trajectories.Trajectory:\n    \"\"\"Builds a `trajectories.Trajectory`\
          \ object from `parsed_record`.\n\n    Args:\n      parsed_record: A dict\
          \ mapping feature names to values as `tf.Tensor`\n        objects of type\
          \ string containing serialized protos.\n      policy_info: Policy information\
          \ specification.\n\n    Returns:\n      A `trajectories.Trajectory` object\
          \ that contains values as de-serialized\n      `tf.Tensor` objects from\
          \ `parsed_record`.\n    \"\"\"\n    return trajectories.Trajectory(\n  \
          \      step_type=tf.expand_dims(\n            tf.io.parse_tensor(parsed_record[\"\
          step_type\"], out_type=tf.int32),\n            axis=1),\n        observation=tf.expand_dims(\n\
          \            tf.io.parse_tensor(\n                parsed_record[\"observation\"\
          ], out_type=tf.float32),\n            axis=1),\n        action=tf.expand_dims(\n\
          \            tf.io.parse_tensor(parsed_record[\"action\"], out_type=tf.int32),\n\
          \            axis=1),\n        policy_info=policy_info,\n        next_step_type=tf.expand_dims(\n\
          \            tf.io.parse_tensor(\n                parsed_record[\"next_step_type\"\
          ], out_type=tf.int32),\n            axis=1),\n        reward=tf.expand_dims(\n\
          \            tf.io.parse_tensor(parsed_record[\"reward\"], out_type=tf.float32),\n\
          \            axis=1),\n        discount=tf.expand_dims(\n            tf.io.parse_tensor(parsed_record[\"\
          discount\"], out_type=tf.float32),\n            axis=1))\n\n  def train_policy_on_trajectory(\n\
          \      agent: agents.TFAgent,\n      tfrecord_file: str,\n      num_epochs:\
          \ int\n  ) -> NamedTuple(\"TrainOutputs\", [\n      (\"policy\", policies.TFPolicy),\n\
          \      (\"train_loss\", Dict[str, List[float]]),\n  ]):\n    \"\"\"Trains\
          \ the policy in `agent` on the dataset of `tfrecord_file`.\n\n    Parses\
          \ `tfrecord_file` as `tf.train.Example` objects, packages them into\n  \
          \  `trajectories.Trajectory` objects, and trains the agent's policy on these\n\
          \    trajectory objects.\n\n    Args:\n      agent: A TF-Agents agent that\
          \ carries the policy to train.\n      tfrecord_file: Path to the TFRecord\
          \ file containing the training dataset.\n      num_epochs: Number of epochs\
          \ to train the policy.\n\n    Returns:\n      A NamedTuple of (a trained\
          \ TF-Agents policy, a dict mapping from\n      \"epoch<i>\" to lists of\
          \ loss values produced at each training step).\n    \"\"\"\n    raw_dataset\
          \ = tf.data.TFRecordDataset([tfrecord_file])\n    parsed_dataset = raw_dataset.map(_parse_record)\n\
          \n    train_loss = collections.defaultdict(list)\n    for epoch in range(num_epochs):\n\
          \      for parsed_record in parsed_dataset:\n        trajectory = build_trajectory(parsed_record,\
          \ agent.policy.info_spec)\n        loss, _ = agent.train(trajectory)\n \
          \       train_loss[f\"epoch{epoch + 1}\"].append(loss.numpy())\n\n    train_outputs\
          \ = collections.namedtuple(\n        \"TrainOutputs\",\n        [\"policy\"\
          , \"train_loss\"])\n    return train_outputs(agent.policy, train_loss)\n\
          \n  def execute_training_and_save_policy(\n      training_artifacts_dir:\
          \ str,\n      tfrecord_file: str,\n      num_epochs: int,\n      rank_k:\
          \ int,\n      num_actions: int,\n      tikhonov_weight: float,\n      agent_alpha:\
          \ float) -> None:\n    \"\"\"Executes training for the policy and saves\
          \ the policy.\n\n    Args:\n      training_artifacts_dir: Path to store\
          \ the Trainer artifacts (trained\n        policy).\n      tfrecord_file:\
          \ Path to file to write the ingestion result TFRecords.\n      num_epochs:\
          \ Number of training epochs.\n      rank_k: Rank for matrix factorization\
          \ in the MovieLens environment; also\n        the observation dimension.\n\
          \      num_actions: Number of actions (movie items) to choose from.\n  \
          \    tikhonov_weight: LinUCB Tikhonov regularization weight of the Trainer.\n\
          \      agent_alpha: LinUCB exploration parameter that multiplies the confidence\n\
          \        intervals of the Trainer.\n    \"\"\"\n    # Define time step and\
          \ action specs for one batch.\n    time_step_spec = trajectories.TimeStep(\n\
          \        step_type=tensor_spec.TensorSpec(\n            shape=(), dtype=tf.int32,\
          \ name=\"step_type\"),\n        reward=tensor_spec.TensorSpec(\n       \
          \     shape=(), dtype=tf.float32, name=\"reward\"),\n        discount=tensor_spec.BoundedTensorSpec(\n\
          \            shape=(), dtype=tf.float32, name=\"discount\", minimum=0.,\n\
          \            maximum=1.),\n        observation=tensor_spec.TensorSpec(\n\
          \            shape=(rank_k,), dtype=tf.float32,\n            name=\"observation\"\
          ))\n\n    action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(),\n\
          \        dtype=tf.int32,\n        name=\"action\",\n        minimum=0,\n\
          \        maximum=num_actions - 1)\n\n    # Define RL agent/algorithm.\n\
          \    agent = lin_ucb_agent.LinearUCBAgent(\n        time_step_spec=time_step_spec,\n\
          \        action_spec=action_spec,\n        tikhonov_weight=tikhonov_weight,\n\
          \        alpha=agent_alpha,\n        dtype=tf.float32,\n        accepts_per_arm_features=per_arm)\n\
          \    agent.initialize()\n    logging.info(\"TimeStep Spec (for each batch):\\\
          n%s\\n\", agent.time_step_spec)\n    logging.info(\"Action Spec (for each\
          \ batch):\\n%s\\n\", agent.action_spec)\n\n    # Perform off-policy training.\n\
          \    policy, _ = train_policy_on_trajectory(\n        agent=agent,\n   \
          \     tfrecord_file=tfrecord_file,\n        num_epochs=num_epochs)\n\n \
          \   # Save trained policy.\n    saver = policy_saver.PolicySaver(policy)\n\
          \    saver.save(training_artifacts_dir)\n\n  execute_training_and_save_policy(\n\
          \      training_artifacts_dir=training_artifacts_dir,\n      tfrecord_file=tfrecord_file,\n\
          \      num_epochs=num_epochs,\n      rank_k=rank_k,\n      num_actions=num_actions,\n\
          \      tikhonov_weight=tikhonov_weight,\n      agent_alpha=agent_alpha)\n\
          \n  outputs = collections.namedtuple(\n      \"Outputs\",\n      [\"training_artifacts_dir\"\
          ])\n\n  return outputs(training_artifacts_dir)\n\n"
        image: tensorflow/tensorflow:2.13.0
pipelineInfo:
  name: train-reinforcement-learning-policy
root:
  dag:
    outputs:
      parameters:
        training_artifacts_dir:
          valueFromParameter:
            outputParameterKey: training_artifacts_dir
            producerSubtask: train-reinforcement-learning-policy
    tasks:
      train-reinforcement-learning-policy:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-reinforcement-learning-policy
        inputs:
          parameters:
            agent_alpha:
              componentInputParameter: agent_alpha
            num_actions:
              componentInputParameter: num_actions
            num_epochs:
              componentInputParameter: num_epochs
            rank_k:
              componentInputParameter: rank_k
            tfrecord_file:
              componentInputParameter: tfrecord_file
            tikhonov_weight:
              componentInputParameter: tikhonov_weight
            training_artifacts_dir:
              componentInputParameter: training_artifacts_dir
        taskInfo:
          name: train-reinforcement-learning-policy
  inputDefinitions:
    parameters:
      agent_alpha:
        description: 'LinUCB exploration parameter that multiplies the confidence

          intervals of the Trainer.'
        parameterType: NUMBER_DOUBLE
      num_actions:
        description: Number of actions (movie items) to choose from.
        parameterType: NUMBER_INTEGER
      num_epochs:
        description: Number of training epochs.
        parameterType: NUMBER_INTEGER
      rank_k:
        description: 'Rank for matrix factorization in the MovieLens environment;
          also

          the observation dimension.'
        parameterType: NUMBER_INTEGER
      tfrecord_file:
        description: Path to file to write the ingestion result TFRecords.
        parameterType: STRING
      tikhonov_weight:
        description: LinUCB Tikhonov regularization weight of the Trainer.
        parameterType: NUMBER_DOUBLE
      training_artifacts_dir:
        description: 'Path to store the Trainer artifacts (trained

          policy).'
        parameterType: STRING
  outputDefinitions:
    parameters:
      training_artifacts_dir:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.3.0
