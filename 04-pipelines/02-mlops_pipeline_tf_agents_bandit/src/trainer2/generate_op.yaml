# PIPELINE DEFINITION
# Name: generate-movielens-dataset-for-bigquery
# Inputs:
#    batch_size: int
#    bigquery_dataset_name: str
#    bigquery_location: str
#    bigquery_table_name: str
#    bigquery_tmp_file: str
#    driver_steps: int
#    num_actions: int
#    project_id: str
#    rank_k: int
#    raw_data_path: str
# Outputs:
#    bigquery_dataset_name: str
#    bigquery_location: str
#    bigquery_table_name: str
components:
  comp-generate-movielens-dataset-for-bigquery:
    executorLabel: exec-generate-movielens-dataset-for-bigquery
    inputDefinitions:
      parameters:
        batch_size:
          description: Batch size of environment generated quantities eg. rewards.
          parameterType: NUMBER_INTEGER
        bigquery_dataset_name:
          description: 'A string of the BigQuery dataset ID in the format of

            "project.dataset".'
          parameterType: STRING
        bigquery_location:
          description: A string of the BigQuery dataset location.
          parameterType: STRING
        bigquery_table_name:
          description: 'A string of the BigQuery table ID in the format of

            "table_name".'
          parameterType: STRING
        bigquery_tmp_file:
          description: Path to a JSON file containing the training dataset.
          parameterType: STRING
        driver_steps:
          description: Number of steps to run per batch.
          parameterType: NUMBER_INTEGER
        num_actions:
          description: Number of actions (movie items) to choose from.
          parameterType: NUMBER_INTEGER
        project_id:
          description: 'GCP project ID. This is required because otherwise the BigQuery

            client will use the ID of the tenant GCP project created as a result of

            KFP, which doesn''t have proper access to BigQuery.'
          parameterType: STRING
        rank_k:
          description: 'Rank for matrix factorization in the MovieLens environment;
            also

            the observation dimension.'
          parameterType: NUMBER_INTEGER
        raw_data_path:
          description: Path to MovieLens 100K's "u.data" file.
          parameterType: STRING
    outputDefinitions:
      parameters:
        bigquery_dataset_name:
          parameterType: STRING
        bigquery_location:
          parameterType: STRING
        bigquery_table_name:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-generate-movielens-dataset-for-bigquery:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - generate_movielens_dataset_for_bigquery
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-bigquery'\
          \ 'tensorflow==2.13.0' 'tf-agents==0.17.0' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef generate_movielens_dataset_for_bigquery(\n    project_id: str,\n\
          \    raw_data_path: str,\n    batch_size: int,\n    rank_k: int,\n    num_actions:\
          \ int,\n    driver_steps: int,\n    bigquery_tmp_file: str,\n    bigquery_dataset_name:\
          \ str,\n    bigquery_location: str,\n    bigquery_table_name: str\n) ->\
          \ NamedTuple(\"Outputs\", [\n    (\"bigquery_dataset_name\", str),\n   \
          \ (\"bigquery_location\", str),\n    (\"bigquery_table_name\", str),\n]):\n\
          \  \"\"\"Generates BigQuery training data using a MovieLens simulation environment.\n\
          \n  Serves as the Generator pipeline component:\n  1. Generates `trajectories.Trajectory`\
          \ data by applying a random policy on\n    MovieLens simulation environment.\n\
          \  2. Converts `trajectories.Trajectory` data to JSON format.\n  3. Loads\
          \ JSON-formatted data into BigQuery.\n\n  This function is to be built into\
          \ a Kubeflow Pipelines (KFP) component. As a\n  result, this function must\
          \ be entirely self-contained. This means that the\n  import statements and\
          \ helper functions must reside within itself.\n\n  Args:\n    project_id:\
          \ GCP project ID. This is required because otherwise the BigQuery\n    \
          \  client will use the ID of the tenant GCP project created as a result\
          \ of\n      KFP, which doesn't have proper access to BigQuery.\n    raw_data_path:\
          \ Path to MovieLens 100K's \"u.data\" file.\n    batch_size: Batch size\
          \ of environment generated quantities eg. rewards.\n    rank_k: Rank for\
          \ matrix factorization in the MovieLens environment; also\n      the observation\
          \ dimension.\n    num_actions: Number of actions (movie items) to choose\
          \ from.\n    driver_steps: Number of steps to run per batch.\n    bigquery_tmp_file:\
          \ Path to a JSON file containing the training dataset.\n    bigquery_dataset_name:\
          \ A string of the BigQuery dataset ID in the format of\n      \"project.dataset\"\
          .\n    bigquery_location: A string of the BigQuery dataset location.\n \
          \   bigquery_table_name: A string of the BigQuery table ID in the format\
          \ of\n      \"table_name\".\n\n  Returns:\n    A NamedTuple of (`bigquery_dataset_name`,\
          \ `bigquery_location`,\n    `bigquery_table_name`).\n  \"\"\"\n  # pylint:\
          \ disable=g-import-not-at-top\n  import collections\n  import json\n  from\
          \ typing import Any, Dict\n\n  from google.cloud import bigquery\n\n  from\
          \ tf_agents import replay_buffers\n  from tf_agents import trajectories\n\
          \  from tf_agents.bandits.agents.examples.v2 import trainer\n  from tf_agents.bandits.environments\
          \ import movielens_py_environment\n  from tf_agents.drivers import dynamic_step_driver\n\
          \  from tf_agents.environments import tf_py_environment\n  from tf_agents.policies\
          \ import random_tf_policy\n\n  def generate_simulation_data(\n      raw_data_path:\
          \ str,\n      batch_size: int,\n      rank_k: int,\n      num_actions: int,\n\
          \      driver_steps: int) -> replay_buffers.TFUniformReplayBuffer:\n   \
          \ \"\"\"Generates `trajectories.Trajectory` data from the simulation environment.\n\
          \n    Constructs a MovieLens simulation environment, and generates a set\
          \ of\n    `trajectories.Trajectory` data using a random policy.\n\n    Args:\n\
          \      raw_data_path: Path to MovieLens 100K's \"u.data\" file.\n      batch_size:\
          \ Batch size of environment generated quantities eg. rewards.\n      rank_k:\
          \ Rank for matrix factorization in the MovieLens environment; also\n   \
          \     the observation dimension.\n      num_actions: Number of actions (movie\
          \ items) to choose from.\n      driver_steps: Number of steps to run per\
          \ batch.\n\n    Returns:\n      A replay buffer holding randomly generated`trajectories.Trajectory`\
          \ data.\n    \"\"\"\n    # Create MovieLens simulation environment.\n  \
          \  env = movielens_py_environment.MovieLensPyEnvironment(\n        raw_data_path,\n\
          \        rank_k,\n        batch_size,\n        num_movies=num_actions,\n\
          \        csv_delimiter=\"\\t\")\n    environment = tf_py_environment.TFPyEnvironment(env)\n\
          \n    # Define random policy for collecting data.\n    random_policy = random_tf_policy.RandomTFPolicy(\n\
          \        action_spec=environment.action_spec(),\n        time_step_spec=environment.time_step_spec())\n\
          \n    # Use replay buffer and observers to keep track of Trajectory data.\n\
          \    data_spec = random_policy.trajectory_spec\n    replay_buffer = trainer._get_replay_buffer(\n\
          \        data_spec\n        , environment.batch_size\n        , driver_steps\n\
          \        , 1\n    )\n    observers = [replay_buffer.add_batch]\n\n    #\
          \ Run driver to apply the random policy in the simulation environment.\n\
          \    driver = dynamic_step_driver.DynamicStepDriver(\n        env=environment,\n\
          \        policy=random_policy,\n        num_steps=driver_steps * environment.batch_size,\n\
          \        observers=observers)\n    driver.run()\n\n    return replay_buffer\n\
          \n  def build_dict_from_trajectory(\n      trajectory: trajectories.Trajectory)\
          \ -> Dict[str, Any]:\n    \"\"\"Builds a dict from `trajectory` data.\n\n\
          \    Args:\n      trajectory: A `trajectories.Trajectory` object.\n\n  \
          \  Returns:\n      A dict holding the same data as `trajectory`.\n    \"\
          \"\"\n    trajectory_dict = {\n        \"step_type\": trajectory.step_type.numpy().tolist(),\n\
          \        \"observation\": [{\n            \"observation_batch\": batch\n\
          \        } for batch in trajectory.observation.numpy().tolist()],\n    \
          \    \"action\": trajectory.action.numpy().tolist(),\n        \"policy_info\"\
          : trajectory.policy_info,\n        \"next_step_type\": trajectory.next_step_type.numpy().tolist(),\n\
          \        \"reward\": trajectory.reward.numpy().tolist(),\n        \"discount\"\
          : trajectory.discount.numpy().tolist(),\n    }\n    return trajectory_dict\n\
          \n  def write_replay_buffer_to_file(\n      replay_buffer: replay_buffers.TFUniformReplayBuffer,\n\
          \      batch_size: int,\n      dataset_file: str) -> None:\n    \"\"\"Writes\
          \ replay buffer data to a file, each JSON in one line.\n\n    Each `trajectories.Trajectory`\
          \ object in `replay_buffer` will be written as\n    one line to the `dataset_file`\
          \ in JSON format. I.e., the `dataset_file`\n    would be a newline-delimited\
          \ JSON file.\n\n    Args:\n      replay_buffer: A `replay_buffers.TFUniformReplayBuffer`\
          \ holding\n        `trajectories.Trajectory` objects.\n      batch_size:\
          \ Batch size of environment generated quantities eg. rewards.\n      dataset_file:\
          \ File path. Will be overwritten if already exists.\n    \"\"\"\n    dataset\
          \ = replay_buffer.as_dataset(sample_batch_size=batch_size)\n    dataset_size\
          \ = replay_buffer.num_frames().numpy()\n\n    with open(dataset_file, \"\
          w\") as f:\n      for example in dataset.take(count=dataset_size):\n   \
          \     traj_dict = build_dict_from_trajectory(example[0])\n        f.write(json.dumps(traj_dict)\
          \ + \"\\n\")\n\n  def load_dataset_into_bigquery(\n      project_id: str,\n\
          \      dataset_file: str,\n      bigquery_dataset_name: str,\n      bigquery_location:\
          \ str,\n      bigquery_table_name: str) -> None:\n    \"\"\"Loads training\
          \ dataset into BigQuery table.\n\n    Loads training dataset of `trajectories.Trajectory`\
          \ in newline delimited\n    JSON into a BigQuery dataset and table, using\
          \ a BigQuery client.\n\n    Args:\n      project_id: GCP project ID. This\
          \ is required because otherwise the\n        BigQuery client will use the\
          \ ID of the tenant GCP project created as a\n        result of KFP, which\
          \ doesn't have proper access to BigQuery.\n      dataset_file: Path to a\
          \ JSON file containing the training dataset.\n      bigquery_dataset_name:\
          \ A string of the BigQuery dataset ID in the format of\n        \"dataset_name\"\
          .\n      bigquery_location: A string of the BigQuery dataset location.\n\
          \      bigquery_table_name: A string of the BigQuery table ID in the format\
          \ of\n        \"project.dataset.table\".\n    \"\"\"\n\n    _bq_dataset_ref\
          \ = f\"{project_id}.{bigquery_dataset_name}\"\n\n    # Construct a BigQuery\
          \ client object.\n    client = bigquery.Client(project=project_id)\n\n \
          \   # Construct a full Dataset object to send to the API.\n    dataset =\
          \ bigquery.Dataset(_bq_dataset_ref)\n\n    # Specify the geographic location\
          \ where the dataset should reside.\n    dataset.location = bigquery_location\n\
          \n    # Create the dataset, or get the dataset if it exists.\n    dataset\
          \ = client.create_dataset(dataset = dataset, exists_ok=True, timeout=30)\n\
          \n    job_config = bigquery.LoadJobConfig(\n        schema=[\n         \
          \   bigquery.SchemaField(\"step_type\", \"INT64\", mode=\"REPEATED\"),\n\
          \            bigquery.SchemaField(\n                \"observation\",\n \
          \               \"RECORD\",\n                mode=\"REPEATED\",\n      \
          \          fields=[\n                    bigquery.SchemaField(\"observation_batch\"\
          , \"FLOAT64\",\n                                         \"REPEATED\")\n\
          \                ]),\n            bigquery.SchemaField(\"action\", \"INT64\"\
          , mode=\"REPEATED\"),\n            bigquery.SchemaField(\"policy_info\"\
          , \"FLOAT64\", mode=\"REPEATED\"),\n            bigquery.SchemaField(\"\
          next_step_type\", \"INT64\", mode=\"REPEATED\"),\n            bigquery.SchemaField(\"\
          reward\", \"FLOAT64\", mode=\"REPEATED\"),\n            bigquery.SchemaField(\"\
          discount\", \"FLOAT64\", mode=\"REPEATED\"),\n        ],\n        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n\
          \    )\n\n    _bq_table_ref = f\"{project_id}.{bigquery_dataset_name}.{bigquery_table_name}\"\
          \n\n    with open(dataset_file, \"rb\") as source_file:\n      load_job\
          \ = client.load_table_from_file(\n          source_file, _bq_table_ref,\
          \ job_config=job_config)\n\n    load_job.result()  # Wait for the job to\
          \ complete.\n\n  replay_buffer = generate_simulation_data(\n      raw_data_path=raw_data_path,\n\
          \      batch_size=batch_size,\n      rank_k=rank_k,\n      num_actions=num_actions,\n\
          \      driver_steps=driver_steps\n  )\n\n  write_replay_buffer_to_file(\n\
          \      replay_buffer=replay_buffer,\n      batch_size=batch_size,\n    \
          \  dataset_file=bigquery_tmp_file\n  )\n\n  load_dataset_into_bigquery(project_id,\
          \ bigquery_tmp_file, bigquery_dataset_name,\n                          \
          \   bigquery_location, bigquery_table_name)\n\n  outputs = collections.namedtuple(\n\
          \      \"Outputs\",\n      [\"bigquery_dataset_name\", \"bigquery_location\"\
          , \"bigquery_table_name\"])\n\n  return outputs(bigquery_dataset_name, bigquery_location,\
          \ bigquery_table_name)\n\n"
        image: tensorflow/tensorflow:2.13.0
pipelineInfo:
  name: generate-movielens-dataset-for-bigquery
root:
  dag:
    outputs:
      parameters:
        bigquery_dataset_name:
          valueFromParameter:
            outputParameterKey: bigquery_dataset_name
            producerSubtask: generate-movielens-dataset-for-bigquery
        bigquery_location:
          valueFromParameter:
            outputParameterKey: bigquery_location
            producerSubtask: generate-movielens-dataset-for-bigquery
        bigquery_table_name:
          valueFromParameter:
            outputParameterKey: bigquery_table_name
            producerSubtask: generate-movielens-dataset-for-bigquery
    tasks:
      generate-movielens-dataset-for-bigquery:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-generate-movielens-dataset-for-bigquery
        inputs:
          parameters:
            batch_size:
              componentInputParameter: batch_size
            bigquery_dataset_name:
              componentInputParameter: bigquery_dataset_name
            bigquery_location:
              componentInputParameter: bigquery_location
            bigquery_table_name:
              componentInputParameter: bigquery_table_name
            bigquery_tmp_file:
              componentInputParameter: bigquery_tmp_file
            driver_steps:
              componentInputParameter: driver_steps
            num_actions:
              componentInputParameter: num_actions
            project_id:
              componentInputParameter: project_id
            rank_k:
              componentInputParameter: rank_k
            raw_data_path:
              componentInputParameter: raw_data_path
        taskInfo:
          name: generate-movielens-dataset-for-bigquery
  inputDefinitions:
    parameters:
      batch_size:
        description: Batch size of environment generated quantities eg. rewards.
        parameterType: NUMBER_INTEGER
      bigquery_dataset_name:
        description: 'A string of the BigQuery dataset ID in the format of

          "project.dataset".'
        parameterType: STRING
      bigquery_location:
        description: A string of the BigQuery dataset location.
        parameterType: STRING
      bigquery_table_name:
        description: 'A string of the BigQuery table ID in the format of

          "table_name".'
        parameterType: STRING
      bigquery_tmp_file:
        description: Path to a JSON file containing the training dataset.
        parameterType: STRING
      driver_steps:
        description: Number of steps to run per batch.
        parameterType: NUMBER_INTEGER
      num_actions:
        description: Number of actions (movie items) to choose from.
        parameterType: NUMBER_INTEGER
      project_id:
        description: 'GCP project ID. This is required because otherwise the BigQuery

          client will use the ID of the tenant GCP project created as a result of

          KFP, which doesn''t have proper access to BigQuery.'
        parameterType: STRING
      rank_k:
        description: 'Rank for matrix factorization in the MovieLens environment;
          also

          the observation dimension.'
        parameterType: NUMBER_INTEGER
      raw_data_path:
        description: Path to MovieLens 100K's "u.data" file.
        parameterType: STRING
  outputDefinitions:
    parameters:
      bigquery_dataset_name:
        parameterType: STRING
      bigquery_location:
        parameterType: STRING
      bigquery_table_name:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.3.0
